/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[1, 1, 1], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=35, base_lr=0.001, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
9 iterations per epoch. 1800 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 183459.687500, loss_kl: 316.397705, loss_recon: 181703.921875, loss_pred: 17.526020
iteration 2: loss: 181059.593750, loss_kl: 673.666260, loss_recon: 179550.718750, loss_pred: 15.021460
iteration 3: loss: 175767.359375, loss_kl: 556.863281, loss_recon: 174342.125000, loss_pred: 14.196750
iteration 4: loss: 177955.515625, loss_kl: 432.909973, loss_recon: 176880.343750, loss_pred: 10.708430
iteration 5: loss: 176248.125000, loss_kl: 308.361603, loss_recon: 175057.406250, loss_pred: 11.876387
iteration 6: loss: 173362.375000, loss_kl: 335.411499, loss_recon: 171994.453125, loss_pred: 13.645619
iteration 7: loss: 172021.687500, loss_kl: 401.523804, loss_recon: 170798.421875, loss_pred: 12.192499
iteration 8: loss: 175669.390625, loss_kl: 390.274323, loss_recon: 174729.890625, loss_pred: 9.355891
iteration 9: loss: 170101.015625, loss_kl: 351.010315, loss_recon: 168947.203125, loss_pred: 11.503024
  0%|▏                              | 1/200 [01:32<5:06:38, 92.46s/it]iteration 10: loss: 175538.250000, loss_kl: 350.735321, loss_recon: 174812.296875, loss_pred: 7.224462
iteration 11: loss: 169541.187500, loss_kl: 361.079773, loss_recon: 168723.812500, loss_pred: 8.137630
iteration 12: loss: 174878.343750, loss_kl: 383.769073, loss_recon: 174156.140625, loss_pred: 7.183615
iteration 13: loss: 171740.484375, loss_kl: 354.081238, loss_recon: 171132.421875, loss_pred: 6.045169
iteration 14: loss: 167885.687500, loss_kl: 331.303101, loss_recon: 167293.687500, loss_pred: 5.886902
iteration 15: loss: 170214.593750, loss_kl: 335.380859, loss_recon: 169727.171875, loss_pred: 4.840571
iteration 16: loss: 168877.484375, loss_kl: 355.334076, loss_recon: 168432.609375, loss_pred: 4.413319
iteration 17: loss: 166982.859375, loss_kl: 333.823547, loss_recon: 166536.546875, loss_pred: 4.429650
iteration 18: loss: 165425.906250, loss_kl: 333.257111, loss_recon: 165064.500000, loss_pred: 3.580726
  1%|▎                              | 2/200 [02:26<3:50:22, 69.81s/it]iteration 19: loss: 165615.171875, loss_kl: 345.600891, loss_recon: 165292.625000, loss_pred: 3.190876
iteration 20: loss: 163013.921875, loss_kl: 351.180969, loss_recon: 162800.562500, loss_pred: 2.098459
iteration 21: loss: 161556.296875, loss_kl: 362.239136, loss_recon: 161334.984375, loss_pred: 2.176804
iteration 22: loss: 165799.031250, loss_kl: 373.948151, loss_recon: 165467.406250, loss_pred: 3.278898
iteration 23: loss: 165544.343750, loss_kl: 370.954407, loss_recon: 165260.406250, loss_pred: 2.802365
iteration 24: loss: 165599.906250, loss_kl: 356.194519, loss_recon: 165350.281250, loss_pred: 2.460570
iteration 25: loss: 163459.359375, loss_kl: 354.314606, loss_recon: 163223.203125, loss_pred: 2.326109
iteration 26: loss: 157548.046875, loss_kl: 350.695679, loss_recon: 157254.031250, loss_pred: 2.905171
iteration 27: loss: 158705.796875, loss_kl: 365.829224, loss_recon: 158419.546875, loss_pred: 2.825888
  2%|▍                              | 3/200 [03:19<3:24:16, 62.22s/it]iteration 28: loss: 161353.875000, loss_kl: 371.228088, loss_recon: 160763.921875, loss_pred: 5.862375
iteration 29: loss: 162458.765625, loss_kl: 348.063232, loss_recon: 162138.000000, loss_pred: 3.172837
iteration 30: loss: 157050.312500, loss_kl: 325.837555, loss_recon: 156785.765625, loss_pred: 2.612825
iteration 31: loss: 161381.250000, loss_kl: 316.292755, loss_recon: 161132.203125, loss_pred: 2.458839
iteration 32: loss: 160177.093750, loss_kl: 311.247467, loss_recon: 159886.312500, loss_pred: 2.876775
iteration 33: loss: 158056.718750, loss_kl: 318.486542, loss_recon: 157796.875000, loss_pred: 2.566576
iteration 34: loss: 161287.015625, loss_kl: 322.111115, loss_recon: 161046.093750, loss_pred: 2.377072
iteration 35: loss: 161366.281250, loss_kl: 327.821442, loss_recon: 161178.750000, loss_pred: 1.842516
iteration 36: loss: 158237.765625, loss_kl: 329.168732, loss_recon: 158015.203125, loss_pred: 2.192626
  2%|▌                              | 4/200 [04:13<3:12:17, 58.87s/it]iteration 37: loss: 159029.156250, loss_kl: 333.755402, loss_recon: 158857.546875, loss_pred: 1.682597
iteration 38: loss: 162669.953125, loss_kl: 344.491455, loss_recon: 162398.515625, loss_pred: 2.679952
iteration 39: loss: 160638.625000, loss_kl: 339.659912, loss_recon: 160476.203125, loss_pred: 1.590245
iteration 40: loss: 156847.468750, loss_kl: 341.434631, loss_recon: 156648.578125, loss_pred: 1.954654
iteration 41: loss: 161123.171875, loss_kl: 333.239227, loss_recon: 160934.406250, loss_pred: 1.854432
iteration 42: loss: 160328.984375, loss_kl: 335.240143, loss_recon: 160188.593750, loss_pred: 1.370306
iteration 43: loss: 158858.531250, loss_kl: 335.267426, loss_recon: 158694.406250, loss_pred: 1.607717
iteration 44: loss: 157811.921875, loss_kl: 332.848297, loss_recon: 157615.265625, loss_pred: 1.933341
iteration 45: loss: 160087.187500, loss_kl: 329.365234, loss_recon: 159938.953125, loss_pred: 1.449401
  2%|▊                              | 5/200 [05:06<3:05:12, 56.99s/it]iteration 46: loss: 159061.359375, loss_kl: 332.167542, loss_recon: 158890.046875, loss_pred: 1.679868
iteration 47: loss: 161813.218750, loss_kl: 326.842682, loss_recon: 161663.625000, loss_pred: 1.463209
iteration 48: loss: 159504.593750, loss_kl: 332.027313, loss_recon: 159359.203125, loss_pred: 1.420793
iteration 49: loss: 159757.343750, loss_kl: 335.846893, loss_recon: 159627.921875, loss_pred: 1.260588
iteration 50: loss: 161524.218750, loss_kl: 332.717804, loss_recon: 161424.093750, loss_pred: 0.968009
iteration 51: loss: 156983.640625, loss_kl: 332.734100, loss_recon: 156799.265625, loss_pred: 1.810496
iteration 52: loss: 156643.140625, loss_kl: 335.734558, loss_recon: 156504.500000, loss_pred: 1.352816
iteration 53: loss: 159902.625000, loss_kl: 335.031769, loss_recon: 159695.546875, loss_pred: 2.037275
iteration 54: loss: 160209.828125, loss_kl: 342.906769, loss_recon: 160032.109375, loss_pred: 1.742944
  3%|▉                              | 6/200 [06:00<3:00:36, 55.86s/it]iteration 55: loss: 159508.843750, loss_kl: 336.956238, loss_recon: 159374.703125, loss_pred: 1.307636
iteration 56: loss: 160675.625000, loss_kl: 331.807373, loss_recon: 160500.234375, loss_pred: 1.720705
iteration 57: loss: 158670.796875, loss_kl: 329.761261, loss_recon: 158523.640625, loss_pred: 1.438657
iteration 58: loss: 159919.093750, loss_kl: 331.422333, loss_recon: 159772.375000, loss_pred: 1.434099
iteration 59: loss: 156664.750000, loss_kl: 332.602448, loss_recon: 156472.890625, loss_pred: 1.885316
iteration 60: loss: 160838.140625, loss_kl: 339.620056, loss_recon: 160675.546875, loss_pred: 1.592006
iteration 61: loss: 159245.859375, loss_kl: 341.207916, loss_recon: 159121.515625, loss_pred: 1.209298
iteration 62: loss: 158963.468750, loss_kl: 333.988098, loss_recon: 158816.781250, loss_pred: 1.433400
iteration 63: loss: 160421.593750, loss_kl: 331.195648, loss_recon: 160210.234375, loss_pred: 2.080523
  4%|█                              | 7/200 [06:54<2:57:25, 55.16s/it]iteration 64: loss: 154641.031250, loss_kl: 339.363556, loss_recon: 154439.093750, loss_pred: 1.985503
iteration 65: loss: 160690.734375, loss_kl: 333.202057, loss_recon: 160559.921875, loss_pred: 1.274808
iteration 66: loss: 161883.781250, loss_kl: 341.988556, loss_recon: 161758.687500, loss_pred: 1.216669
iteration 67: loss: 159934.296875, loss_kl: 336.154510, loss_recon: 159781.828125, loss_pred: 1.491056
iteration 68: loss: 161148.015625, loss_kl: 329.973175, loss_recon: 161047.171875, loss_pred: 0.975403
iteration 69: loss: 158593.156250, loss_kl: 324.376495, loss_recon: 158388.859375, loss_pred: 2.010415
iteration 70: loss: 159241.921875, loss_kl: 323.147308, loss_recon: 159097.328125, loss_pred: 1.413654
iteration 71: loss: 159835.218750, loss_kl: 333.228882, loss_recon: 159686.078125, loss_pred: 1.458062
iteration 72: loss: 157904.343750, loss_kl: 336.991821, loss_recon: 157655.609375, loss_pred: 2.453604
  4%|█▏                             | 8/200 [07:47<2:54:26, 54.51s/it]iteration 73: loss: 155817.343750, loss_kl: 335.118591, loss_recon: 155672.093750, loss_pred: 1.419129
iteration 74: loss: 158770.734375, loss_kl: 339.937714, loss_recon: 158597.000000, loss_pred: 1.703272
iteration 75: loss: 158518.250000, loss_kl: 338.842865, loss_recon: 158376.875000, loss_pred: 1.379793
iteration 76: loss: 161811.390625, loss_kl: 339.901428, loss_recon: 161654.968750, loss_pred: 1.530149
iteration 77: loss: 162245.812500, loss_kl: 334.815674, loss_recon: 162094.421875, loss_pred: 1.480395
iteration 78: loss: 156431.453125, loss_kl: 331.935211, loss_recon: 156106.093750, loss_pred: 3.220513
iteration 79: loss: 158222.921875, loss_kl: 329.795197, loss_recon: 158049.687500, loss_pred: 1.699425
iteration 80: loss: 160317.359375, loss_kl: 337.633636, loss_recon: 160143.093750, loss_pred: 1.708863
iteration 81: loss: 161855.312500, loss_kl: 345.231903, loss_recon: 161615.718750, loss_pred: 2.361435
  4%|█▍                             | 9/200 [08:43<2:54:33, 54.84s/it]iteration 82: loss: 159159.703125, loss_kl: 331.285034, loss_recon: 159005.625000, loss_pred: 1.507646
iteration 83: loss: 158668.921875, loss_kl: 330.685822, loss_recon: 158534.718750, loss_pred: 1.308867
iteration 84: loss: 161911.109375, loss_kl: 328.323425, loss_recon: 161735.109375, loss_pred: 1.727130
iteration 85: loss: 161443.437500, loss_kl: 335.455780, loss_recon: 161275.421875, loss_pred: 1.646526
iteration 86: loss: 153761.593750, loss_kl: 338.989349, loss_recon: 153551.687500, loss_pred: 2.065119
iteration 87: loss: 160705.875000, loss_kl: 357.155090, loss_recon: 160458.656250, loss_pred: 2.436349
iteration 88: loss: 160740.015625, loss_kl: 356.794769, loss_recon: 160467.031250, loss_pred: 2.694213
iteration 89: loss: 157312.578125, loss_kl: 350.802521, loss_recon: 157097.625000, loss_pred: 2.114440
iteration 90: loss: 160185.703125, loss_kl: 338.656830, loss_recon: 160050.484375, loss_pred: 1.318239
  5%|█▌                            | 10/200 [09:37<2:53:42, 54.85s/it]iteration 91: loss: 161704.984375, loss_kl: 329.807739, loss_recon: 161593.406250, loss_pred: 1.082747
iteration 92: loss: 159257.046875, loss_kl: 327.591492, loss_recon: 158948.953125, loss_pred: 3.048170
iteration 93: loss: 160172.906250, loss_kl: 327.838837, loss_recon: 159991.234375, loss_pred: 1.783882
iteration 94: loss: 156059.828125, loss_kl: 331.698059, loss_recon: 155859.453125, loss_pred: 1.970678
iteration 95: loss: 159656.875000, loss_kl: 349.551178, loss_recon: 159528.828125, loss_pred: 1.245508
iteration 96: loss: 158592.140625, loss_kl: 358.697723, loss_recon: 158400.093750, loss_pred: 1.884595
iteration 97: loss: 157643.750000, loss_kl: 363.649567, loss_recon: 157462.156250, loss_pred: 1.779609
iteration 98: loss: 159363.000000, loss_kl: 359.897003, loss_recon: 159098.390625, loss_pred: 2.610088
iteration 99: loss: 161272.140625, loss_kl: 357.357147, loss_recon: 161110.265625, loss_pred: 1.583001
  6%|█▋                            | 11/200 [10:32<2:52:58, 54.91s/it]iteration 100: loss: 161064.015625, loss_kl: 343.322449, loss_recon: 160777.656250, loss_pred: 2.829227
iteration 101: loss: 161543.687500, loss_kl: 336.018738, loss_recon: 161372.828125, loss_pred: 1.674973
iteration 102: loss: 158790.140625, loss_kl: 327.662292, loss_recon: 158472.984375, loss_pred: 3.138817
iteration 103: loss: 157888.921875, loss_kl: 332.417358, loss_recon: 157576.890625, loss_pred: 3.087013
iteration 104: loss: 162375.390625, loss_kl: 336.272217, loss_recon: 162250.140625, loss_pred: 1.218875
iteration 105: loss: 156463.359375, loss_kl: 352.273834, loss_recon: 156317.265625, loss_pred: 1.425732
iteration 106: loss: 155969.890625, loss_kl: 358.053864, loss_recon: 155793.203125, loss_pred: 1.731061
iteration 107: loss: 160424.328125, loss_kl: 367.818878, loss_recon: 160188.234375, loss_pred: 2.324202
iteration 108: loss: 159244.203125, loss_kl: 363.853851, loss_recon: 159058.250000, loss_pred: 1.823053
  6%|█▊                            | 12/200 [11:26<2:50:53, 54.54s/it]iteration 109: loss: 160929.781250, loss_kl: 354.850281, loss_recon: 160769.062500, loss_pred: 1.571726
iteration 110: loss: 162482.046875, loss_kl: 347.420166, loss_recon: 162360.453125, loss_pred: 1.181209
iteration 111: loss: 156919.406250, loss_kl: 344.938171, loss_recon: 156766.500000, loss_pred: 1.494454
iteration 112: loss: 156975.875000, loss_kl: 342.487885, loss_recon: 156715.468750, loss_pred: 2.569845
iteration 113: loss: 159313.875000, loss_kl: 340.704285, loss_recon: 159136.343750, loss_pred: 1.741224
iteration 114: loss: 157947.390625, loss_kl: 345.673950, loss_recon: 157777.109375, loss_pred: 1.668280
iteration 115: loss: 161928.671875, loss_kl: 349.760559, loss_recon: 161738.734375, loss_pred: 1.864375
iteration 116: loss: 157769.265625, loss_kl: 355.753571, loss_recon: 157617.281250, loss_pred: 1.484211
iteration 117: loss: 158943.875000, loss_kl: 363.686890, loss_recon: 158747.578125, loss_pred: 1.926527
  6%|█▉                            | 13/200 [12:20<2:49:04, 54.25s/it]iteration 118: loss: 160666.640625, loss_kl: 358.880585, loss_recon: 160534.796875, loss_pred: 1.140412
iteration 119: loss: 156983.609375, loss_kl: 358.423004, loss_recon: 156867.921875, loss_pred: 0.979107
iteration 120: loss: 159442.953125, loss_kl: 356.821136, loss_recon: 159266.109375, loss_pred: 1.591330
iteration 121: loss: 160024.390625, loss_kl: 358.773315, loss_recon: 159856.515625, loss_pred: 1.500836
iteration 122: loss: 154952.843750, loss_kl: 354.460205, loss_recon: 154792.578125, loss_pred: 1.426849
iteration 123: loss: 162777.515625, loss_kl: 354.373047, loss_recon: 162564.078125, loss_pred: 1.958584
iteration 124: loss: 160005.531250, loss_kl: 357.678192, loss_recon: 159885.781250, loss_pred: 1.020233
iteration 125: loss: 156767.640625, loss_kl: 357.744934, loss_recon: 156514.203125, loss_pred: 2.356802
iteration 126: loss: 161629.718750, loss_kl: 361.535156, loss_recon: 161499.593750, loss_pred: 1.121929
  7%|██                            | 14/200 [13:13<2:47:25, 54.01s/it]iteration 127: loss: 159706.031250, loss_kl: 364.625214, loss_recon: 159559.968750, loss_pred: 1.135356
iteration 128: loss: 158725.765625, loss_kl: 358.409393, loss_recon: 158550.343750, loss_pred: 1.434456
iteration 129: loss: 157577.765625, loss_kl: 357.314972, loss_recon: 157381.343750, loss_pred: 1.645537
iteration 130: loss: 161029.640625, loss_kl: 359.183380, loss_recon: 160880.312500, loss_pred: 1.172869
iteration 131: loss: 159377.593750, loss_kl: 367.781769, loss_recon: 159214.156250, loss_pred: 1.306190
iteration 132: loss: 161155.375000, loss_kl: 358.597656, loss_recon: 161017.265625, loss_pred: 1.061272
iteration 133: loss: 157981.812500, loss_kl: 359.692230, loss_recon: 157838.984375, loss_pred: 1.107436
iteration 134: loss: 159723.296875, loss_kl: 358.705902, loss_recon: 159592.796875, loss_pred: 0.984925
iteration 135: loss: 157550.718750, loss_kl: 353.465851, loss_recon: 157407.890625, loss_pred: 1.112927
  8%|██▎                           | 15/200 [14:07<2:46:12, 53.91s/it]iteration 136: loss: 156747.328125, loss_kl: 357.854736, loss_recon: 156584.296875, loss_pred: 1.169391
iteration 137: loss: 157088.218750, loss_kl: 362.571075, loss_recon: 156941.046875, loss_pred: 1.004682
iteration 138: loss: 159402.625000, loss_kl: 363.313904, loss_recon: 159257.468750, loss_pred: 0.983578
iteration 139: loss: 161586.281250, loss_kl: 362.571930, loss_recon: 161328.828125, loss_pred: 2.107565
iteration 140: loss: 158272.015625, loss_kl: 353.823700, loss_recon: 158084.765625, loss_pred: 1.416766
iteration 141: loss: 158490.109375, loss_kl: 350.690582, loss_recon: 158322.234375, loss_pred: 1.227015
iteration 142: loss: 162071.531250, loss_kl: 339.302002, loss_recon: 161926.093750, loss_pred: 1.017282
iteration 143: loss: 161423.312500, loss_kl: 340.115753, loss_recon: 161197.343750, loss_pred: 1.821571
iteration 144: loss: 157847.906250, loss_kl: 341.746216, loss_recon: 157662.312500, loss_pred: 1.415840
  8%|██▍                           | 16/200 [15:02<2:46:28, 54.28s/it]iteration 145: loss: 164013.281250, loss_kl: 335.068359, loss_recon: 163823.203125, loss_pred: 1.336634
iteration 146: loss: 153950.671875, loss_kl: 342.592010, loss_recon: 153684.750000, loss_pred: 2.082411
iteration 147: loss: 159450.125000, loss_kl: 349.919464, loss_recon: 159287.234375, loss_pred: 1.039621
iteration 148: loss: 160724.843750, loss_kl: 351.277191, loss_recon: 160552.796875, loss_pred: 1.128932
iteration 149: loss: 156623.359375, loss_kl: 355.945496, loss_recon: 156475.343750, loss_pred: 0.880776
iteration 150: loss: 160038.718750, loss_kl: 355.859589, loss_recon: 159796.859375, loss_pred: 1.819321
iteration 151: loss: 160785.078125, loss_kl: 346.366028, loss_recon: 160587.515625, loss_pred: 1.392332
iteration 152: loss: 159535.718750, loss_kl: 340.021698, loss_recon: 159357.093750, loss_pred: 1.213634
iteration 153: loss: 157978.421875, loss_kl: 331.873413, loss_recon: 157726.062500, loss_pred: 1.964704
  8%|██▌                           | 17/200 [15:56<2:45:12, 54.17s/it]iteration 154: loss: 161503.312500, loss_kl: 337.076355, loss_recon: 161338.625000, loss_pred: 0.945824
iteration 155: loss: 159380.781250, loss_kl: 331.756866, loss_recon: 159166.531250, loss_pred: 1.452525
iteration 156: loss: 160112.046875, loss_kl: 329.440643, loss_recon: 159953.062500, loss_pred: 0.904550
iteration 157: loss: 159723.578125, loss_kl: 339.821320, loss_recon: 159544.796875, loss_pred: 1.080890
iteration 158: loss: 158184.375000, loss_kl: 338.367432, loss_recon: 157977.656250, loss_pred: 1.363402
iteration 159: loss: 157951.734375, loss_kl: 345.509949, loss_recon: 157788.281250, loss_pred: 0.915968
iteration 160: loss: 162012.359375, loss_kl: 342.144257, loss_recon: 161816.406250, loss_pred: 1.247815
iteration 161: loss: 158835.234375, loss_kl: 345.788391, loss_recon: 158639.171875, loss_pred: 1.241461
iteration 162: loss: 155251.484375, loss_kl: 343.701935, loss_recon: 154969.640625, loss_pred: 2.103545
  9%|██▋                           | 18/200 [16:49<2:43:31, 53.91s/it]iteration 163: loss: 159574.921875, loss_kl: 333.803864, loss_recon: 159409.515625, loss_pred: 0.827531
iteration 164: loss: 159122.390625, loss_kl: 334.274750, loss_recon: 158869.937500, loss_pred: 1.696819
iteration 165: loss: 162856.375000, loss_kl: 325.890472, loss_recon: 162673.093750, loss_pred: 1.025953
iteration 166: loss: 157788.453125, loss_kl: 333.650604, loss_recon: 157574.656250, loss_pred: 1.311834
iteration 167: loss: 158240.875000, loss_kl: 333.172150, loss_recon: 158035.796875, loss_pred: 1.225783
iteration 168: loss: 159722.046875, loss_kl: 324.921539, loss_recon: 159504.593750, loss_pred: 1.370030
iteration 169: loss: 157910.296875, loss_kl: 329.831024, loss_recon: 157711.343750, loss_pred: 1.172804
iteration 170: loss: 156161.562500, loss_kl: 337.949463, loss_recon: 155950.515625, loss_pred: 1.273727
iteration 171: loss: 161414.687500, loss_kl: 334.347351, loss_recon: 161210.750000, loss_pred: 1.211628
 10%|██▊                           | 19/200 [17:42<2:41:46, 53.63s/it]iteration 172: loss: 162150.171875, loss_kl: 331.649780, loss_recon: 161930.250000, loss_pred: 1.246775
iteration 173: loss: 159461.390625, loss_kl: 324.969910, loss_recon: 159242.156250, loss_pred: 1.259083
iteration 174: loss: 161200.750000, loss_kl: 319.861725, loss_recon: 161000.656250, loss_pred: 1.082327
iteration 175: loss: 158562.859375, loss_kl: 328.195160, loss_recon: 158327.953125, loss_pred: 1.406518
iteration 176: loss: 159915.281250, loss_kl: 325.349457, loss_recon: 159719.437500, loss_pred: 1.024074
iteration 177: loss: 156136.250000, loss_kl: 322.554413, loss_recon: 155858.046875, loss_pred: 1.855654
iteration 178: loss: 158189.093750, loss_kl: 332.962860, loss_recon: 158018.093750, loss_pred: 0.753672
iteration 179: loss: 158316.156250, loss_kl: 337.201019, loss_recon: 158104.593750, loss_pred: 1.147243
iteration 180: loss: 159021.406250, loss_kl: 335.953796, loss_recon: 158828.796875, loss_pred: 0.961242
 10%|███                           | 20/200 [18:36<2:41:17, 53.76s/it]iteration 181: loss: 156565.437500, loss_kl: 335.567902, loss_recon: 156334.906250, loss_pred: 1.208684
iteration 182: loss: 158106.406250, loss_kl: 330.568512, loss_recon: 157890.078125, loss_pred: 1.082968
iteration 183: loss: 156315.593750, loss_kl: 328.892548, loss_recon: 156069.406250, loss_pred: 1.386976
iteration 184: loss: 159496.906250, loss_kl: 328.648163, loss_recon: 159296.859375, loss_pred: 0.926389
iteration 185: loss: 159187.578125, loss_kl: 330.078064, loss_recon: 158996.765625, loss_pred: 0.829436
iteration 186: loss: 158103.046875, loss_kl: 334.773315, loss_recon: 157845.031250, loss_pred: 1.486126
iteration 187: loss: 163928.171875, loss_kl: 318.772430, loss_recon: 163640.859375, loss_pred: 1.831460
iteration 188: loss: 159182.781250, loss_kl: 311.249207, loss_recon: 158944.593750, loss_pred: 1.364624
iteration 189: loss: 162309.531250, loss_kl: 312.257355, loss_recon: 162116.171875, loss_pred: 0.913097
 10%|███▏                          | 21/200 [19:30<2:40:22, 53.76s/it]iteration 190: loss: 158260.656250, loss_kl: 312.273712, loss_recon: 157991.218750, loss_pred: 1.550189
iteration 191: loss: 159004.031250, loss_kl: 311.810669, loss_recon: 158740.921875, loss_pred: 1.488639
iteration 192: loss: 158951.171875, loss_kl: 318.193573, loss_recon: 158712.937500, loss_pred: 1.216365
iteration 193: loss: 157631.968750, loss_kl: 320.613892, loss_recon: 157400.984375, loss_pred: 1.135144
iteration 194: loss: 161219.453125, loss_kl: 329.138184, loss_recon: 160936.796875, loss_pred: 1.620701
iteration 195: loss: 161951.562500, loss_kl: 321.149261, loss_recon: 161581.625000, loss_pred: 2.522663
iteration 196: loss: 160898.968750, loss_kl: 319.919098, loss_recon: 160627.375000, loss_pred: 1.543713
iteration 197: loss: 155906.843750, loss_kl: 312.783264, loss_recon: 155635.796875, loss_pred: 1.564426
iteration 198: loss: 159698.406250, loss_kl: 313.229889, loss_recon: 159501.453125, loss_pred: 0.821847
 11%|███▎                          | 22/200 [20:24<2:39:16, 53.69s/it]iteration 199: loss: 157406.546875, loss_kl: 309.551727, loss_recon: 157195.500000, loss_pred: 0.853687
iteration 200: loss: 161078.625000, loss_kl: 308.477814, loss_recon: 160880.562500, loss_pred: 0.728215
iteration 201: loss: 157195.703125, loss_kl: 311.691925, loss_recon: 156968.687500, loss_pred: 1.004743
iteration 202: loss: 163276.015625, loss_kl: 306.067474, loss_recon: 163094.046875, loss_pred: 0.576970
iteration 203: loss: 156707.125000, loss_kl: 315.927979, loss_recon: 156439.656250, loss_pred: 1.392063
iteration 204: loss: 158942.734375, loss_kl: 311.235840, loss_recon: 158691.578125, loss_pred: 1.247916
iteration 205: loss: 158918.203125, loss_kl: 314.457916, loss_recon: 158701.953125, loss_pred: 0.885733
iteration 206: loss: 160260.921875, loss_kl: 314.982391, loss_recon: 160036.484375, loss_pred: 0.965450
iteration 207: loss: 159363.359375, loss_kl: 319.154816, loss_recon: 159115.484375, loss_pred: 1.183040
 12%|███▍                          | 23/200 [21:17<2:38:24, 53.70s/it]iteration 208: loss: 157814.765625, loss_kl: 318.555420, loss_recon: 157565.078125, loss_pred: 1.077284
iteration 209: loss: 157775.125000, loss_kl: 317.395721, loss_recon: 157506.453125, loss_pred: 1.272354
iteration 210: loss: 164050.656250, loss_kl: 311.836212, loss_recon: 163756.250000, loss_pred: 1.554608
iteration 211: loss: 157411.015625, loss_kl: 308.398438, loss_recon: 157141.625000, loss_pred: 1.319711
iteration 212: loss: 159479.890625, loss_kl: 302.623779, loss_recon: 159241.703125, loss_pred: 1.033512
iteration 213: loss: 162078.265625, loss_kl: 298.965393, loss_recon: 161801.484375, loss_pred: 1.435649
iteration 214: loss: 158034.296875, loss_kl: 300.724182, loss_recon: 157808.281250, loss_pred: 0.920178
iteration 215: loss: 157110.218750, loss_kl: 303.049561, loss_recon: 156806.062500, loss_pred: 1.691293
iteration 216: loss: 159314.406250, loss_kl: 306.350342, loss_recon: 159065.906250, loss_pred: 1.119811
 12%|███▌                          | 24/200 [22:12<2:37:59, 53.86s/it]iteration 217: loss: 160358.109375, loss_kl: 305.843842, loss_recon: 160091.750000, loss_pred: 1.179684
iteration 218: loss: 160257.859375, loss_kl: 307.149506, loss_recon: 159949.921875, loss_pred: 1.589114
iteration 219: loss: 157336.796875, loss_kl: 302.745544, loss_recon: 157094.281250, loss_pred: 0.956322
iteration 220: loss: 160321.375000, loss_kl: 303.992645, loss_recon: 160111.453125, loss_pred: 0.624259
iteration 221: loss: 161211.296875, loss_kl: 305.783325, loss_recon: 160953.312500, loss_pred: 1.096312
iteration 222: loss: 158854.812500, loss_kl: 302.676147, loss_recon: 158582.796875, loss_pred: 1.251566
iteration 223: loss: 157489.375000, loss_kl: 293.865417, loss_recon: 157220.265625, loss_pred: 1.265315
iteration 224: loss: 156224.515625, loss_kl: 305.940399, loss_recon: 155974.828125, loss_pred: 1.012576
iteration 225: loss: 160930.234375, loss_kl: 305.317108, loss_recon: 160541.671875, loss_pred: 2.404159
 12%|███▊                          | 25/200 [23:05<2:36:53, 53.79s/it]iteration 226: loss: 162537.421875, loss_kl: 300.293304, loss_recon: 162275.718750, loss_pred: 1.041109
iteration 227: loss: 160409.265625, loss_kl: 297.460205, loss_recon: 160169.062500, loss_pred: 0.840944
iteration 228: loss: 161221.125000, loss_kl: 292.170135, loss_recon: 160889.234375, loss_pred: 1.785691
iteration 229: loss: 160080.390625, loss_kl: 290.813507, loss_recon: 159771.546875, loss_pred: 1.562138
iteration 230: loss: 158971.593750, loss_kl: 294.424225, loss_recon: 158660.578125, loss_pred: 1.564992
iteration 231: loss: 157819.109375, loss_kl: 298.360077, loss_recon: 157498.109375, loss_pred: 1.644252
iteration 232: loss: 154932.750000, loss_kl: 300.263367, loss_recon: 154607.265625, loss_pred: 1.679032
iteration 233: loss: 160293.859375, loss_kl: 305.460785, loss_recon: 159994.312500, loss_pred: 1.392307
iteration 234: loss: 157317.359375, loss_kl: 302.504028, loss_recon: 157085.312500, loss_pred: 0.732943
 13%|███▉                          | 26/200 [23:59<2:36:19, 53.91s/it]iteration 235: loss: 157974.328125, loss_kl: 297.875122, loss_recon: 157697.421875, loss_pred: 1.087806
iteration 236: loss: 158910.250000, loss_kl: 296.776642, loss_recon: 158642.000000, loss_pred: 1.007533
iteration 237: loss: 162450.484375, loss_kl: 298.688446, loss_recon: 162198.421875, loss_pred: 0.834822
iteration 238: loss: 160713.656250, loss_kl: 292.697113, loss_recon: 160443.375000, loss_pred: 1.050760
iteration 239: loss: 158388.500000, loss_kl: 297.885498, loss_recon: 158131.375000, loss_pred: 0.890039
iteration 240: loss: 157140.281250, loss_kl: 296.970764, loss_recon: 156818.062500, loss_pred: 1.546024
iteration 241: loss: 158619.171875, loss_kl: 296.648102, loss_recon: 158276.343750, loss_pred: 1.754022
iteration 242: loss: 159876.578125, loss_kl: 294.458618, loss_recon: 159636.109375, loss_pred: 0.742868
iteration 243: loss: 159280.140625, loss_kl: 289.995758, loss_recon: 159039.765625, loss_pred: 0.767056
 14%|████                          | 27/200 [24:53<2:35:09, 53.81s/it]iteration 244: loss: 160757.281250, loss_kl: 287.395844, loss_recon: 160478.968750, loss_pred: 1.047109
iteration 245: loss: 160672.359375, loss_kl: 288.893524, loss_recon: 160384.359375, loss_pred: 1.135216
iteration 246: loss: 158910.171875, loss_kl: 286.841919, loss_recon: 158632.453125, loss_pred: 1.044669
iteration 247: loss: 160647.781250, loss_kl: 289.521027, loss_recon: 160384.250000, loss_pred: 0.886584
iteration 248: loss: 160523.734375, loss_kl: 292.164551, loss_recon: 160265.921875, loss_pred: 0.813486
iteration 249: loss: 159845.375000, loss_kl: 294.659088, loss_recon: 159536.234375, loss_pred: 1.311718
iteration 250: loss: 159643.562500, loss_kl: 289.886261, loss_recon: 159344.921875, loss_pred: 1.235501
iteration 251: loss: 157459.000000, loss_kl: 291.748322, loss_recon: 157171.078125, loss_pred: 1.117027
iteration 252: loss: 155148.875000, loss_kl: 287.941803, loss_recon: 154814.515625, loss_pred: 1.604422
 14%|████▏                         | 28/200 [25:47<2:34:08, 53.77s/it]iteration 253: loss: 159534.234375, loss_kl: 283.882599, loss_recon: 159235.375000, loss_pred: 1.161616
iteration 254: loss: 159724.609375, loss_kl: 289.436401, loss_recon: 159463.921875, loss_pred: 0.744034
iteration 255: loss: 159772.109375, loss_kl: 288.493134, loss_recon: 159531.796875, loss_pred: 0.546474
iteration 256: loss: 157107.234375, loss_kl: 289.239075, loss_recon: 156846.031250, loss_pred: 0.750438
iteration 257: loss: 159950.625000, loss_kl: 288.865295, loss_recon: 159689.765625, loss_pred: 0.749564
iteration 258: loss: 159543.000000, loss_kl: 282.400543, loss_recon: 159277.234375, loss_pred: 0.840177
iteration 259: loss: 158767.828125, loss_kl: 282.389008, loss_recon: 158500.906250, loss_pred: 0.851793
iteration 260: loss: 160252.828125, loss_kl: 280.751465, loss_recon: 160000.937500, loss_pred: 0.711963
iteration 261: loss: 158251.328125, loss_kl: 283.509064, loss_recon: 157984.718750, loss_pred: 0.841417
 14%|████▎                         | 29/200 [26:41<2:33:52, 53.99s/it]iteration 262: loss: 160321.359375, loss_kl: 285.560486, loss_recon: 160034.593750, loss_pred: 0.916693
iteration 263: loss: 156962.500000, loss_kl: 283.858643, loss_recon: 156669.359375, loss_pred: 0.992027
iteration 264: loss: 161634.296875, loss_kl: 282.043488, loss_recon: 161377.890625, loss_pred: 0.637201
iteration 265: loss: 160018.140625, loss_kl: 287.759979, loss_recon: 159699.859375, loss_pred: 1.216880
iteration 266: loss: 156238.625000, loss_kl: 282.483307, loss_recon: 155954.218750, loss_pred: 0.914120
iteration 267: loss: 157814.218750, loss_kl: 282.132080, loss_recon: 157536.687500, loss_pred: 0.847752
iteration 268: loss: 158977.171875, loss_kl: 276.596069, loss_recon: 158705.515625, loss_pred: 0.826865
iteration 269: loss: 158484.750000, loss_kl: 279.359375, loss_recon: 158198.109375, loss_pred: 0.957813
iteration 270: loss: 162716.953125, loss_kl: 278.914154, loss_recon: 162443.609375, loss_pred: 0.828033
 15%|████▌                         | 30/200 [27:35<2:32:56, 53.98s/it]iteration 271: loss: 159290.171875, loss_kl: 278.466919, loss_recon: 159028.578125, loss_pred: 0.603074
iteration 272: loss: 158694.296875, loss_kl: 278.602875, loss_recon: 158380.781250, loss_pred: 1.121419
iteration 273: loss: 162620.203125, loss_kl: 274.022095, loss_recon: 162343.890625, loss_pred: 0.782551
iteration 274: loss: 160476.500000, loss_kl: 275.009308, loss_recon: 160228.046875, loss_pred: 0.496686
iteration 275: loss: 156898.125000, loss_kl: 274.456757, loss_recon: 156621.281250, loss_pred: 0.784656
iteration 276: loss: 158247.171875, loss_kl: 277.016785, loss_recon: 157990.328125, loss_pred: 0.566170
iteration 277: loss: 158675.062500, loss_kl: 276.611877, loss_recon: 158409.500000, loss_pred: 0.656298
iteration 278: loss: 158493.343750, loss_kl: 272.940247, loss_recon: 158230.062500, loss_pred: 0.660035
iteration 279: loss: 159199.000000, loss_kl: 273.518494, loss_recon: 158933.359375, loss_pred: 0.679442
 16%|████▋                         | 31/200 [28:29<2:31:57, 53.95s/it]iteration 280: loss: 162355.375000, loss_kl: 270.083710, loss_recon: 162087.062500, loss_pred: 0.624027
iteration 281: loss: 158095.531250, loss_kl: 273.172333, loss_recon: 157812.109375, loss_pred: 0.751495
iteration 282: loss: 157864.015625, loss_kl: 275.078735, loss_recon: 157581.250000, loss_pred: 0.730493
iteration 283: loss: 160338.078125, loss_kl: 274.476685, loss_recon: 160068.187500, loss_pred: 0.606239
iteration 284: loss: 157892.218750, loss_kl: 272.229095, loss_recon: 157624.265625, loss_pred: 0.604010
iteration 285: loss: 161004.359375, loss_kl: 268.644897, loss_recon: 160722.421875, loss_pred: 0.771270
iteration 286: loss: 157691.343750, loss_kl: 269.331146, loss_recon: 157336.859375, loss_pred: 1.491426
iteration 287: loss: 159418.734375, loss_kl: 272.394714, loss_recon: 159122.515625, loss_pred: 0.885468
iteration 288: loss: 158522.796875, loss_kl: 269.975159, loss_recon: 158209.328125, loss_pred: 1.076434
 16%|████▊                         | 32/200 [29:23<2:30:55, 53.90s/it]iteration 289: loss: 159703.312500, loss_kl: 275.968628, loss_recon: 159394.859375, loss_pred: 0.871259
iteration 290: loss: 160384.703125, loss_kl: 271.298187, loss_recon: 160081.515625, loss_pred: 0.856068
iteration 291: loss: 161345.203125, loss_kl: 267.156067, loss_recon: 161066.796875, loss_pred: 0.641450
iteration 292: loss: 160225.250000, loss_kl: 265.030640, loss_recon: 159935.578125, loss_pred: 0.771252
iteration 293: loss: 161343.953125, loss_kl: 260.554779, loss_recon: 161077.593750, loss_pred: 0.573837
iteration 294: loss: 155283.593750, loss_kl: 264.934692, loss_recon: 154934.562500, loss_pred: 1.365423
iteration 295: loss: 161392.468750, loss_kl: 262.183777, loss_recon: 161083.687500, loss_pred: 0.985118
iteration 296: loss: 156191.656250, loss_kl: 264.918365, loss_recon: 155886.687500, loss_pred: 0.924955
iteration 297: loss: 157240.328125, loss_kl: 267.080536, loss_recon: 156935.265625, loss_pred: 0.908633
 16%|████▉                         | 33/200 [30:16<2:29:49, 53.83s/it]iteration 298: loss: 158366.484375, loss_kl: 266.179291, loss_recon: 158051.171875, loss_pred: 0.912928
iteration 299: loss: 157755.328125, loss_kl: 268.433655, loss_recon: 157463.203125, loss_pred: 0.662142
iteration 300: loss: 154512.937500, loss_kl: 268.063202, loss_recon: 154216.062500, loss_pred: 0.712676
iteration 301: loss: 160378.453125, loss_kl: 266.409363, loss_recon: 160082.000000, loss_pred: 0.722432
iteration 302: loss: 161205.953125, loss_kl: 262.135376, loss_recon: 160909.468750, loss_pred: 0.758676
iteration 303: loss: 158538.546875, loss_kl: 255.432449, loss_recon: 158182.453125, loss_pred: 1.411304
iteration 304: loss: 163191.781250, loss_kl: 257.825775, loss_recon: 162904.765625, loss_pred: 0.700361
iteration 305: loss: 161272.890625, loss_kl: 258.114044, loss_recon: 160977.000000, loss_pred: 0.786497
iteration 306: loss: 158320.375000, loss_kl: 258.930939, loss_recon: 158015.234375, loss_pred: 0.872198
 17%|█████                         | 34/200 [31:10<2:28:53, 53.82s/it]iteration 307: loss: 161904.484375, loss_kl: 257.929810, loss_recon: 161592.515625, loss_pred: 0.846926
iteration 308: loss: 156191.187500, loss_kl: 257.179199, loss_recon: 155855.421875, loss_pred: 1.091373
iteration 309: loss: 160690.296875, loss_kl: 258.933746, loss_recon: 160369.343750, loss_pred: 0.927793
iteration 310: loss: 158300.593750, loss_kl: 261.772308, loss_recon: 157995.203125, loss_pred: 0.747168
iteration 311: loss: 158576.515625, loss_kl: 258.409790, loss_recon: 158273.312500, loss_pred: 0.754957
iteration 312: loss: 159459.546875, loss_kl: 258.102203, loss_recon: 159144.640625, loss_pred: 0.874739
iteration 313: loss: 158383.468750, loss_kl: 255.154800, loss_recon: 158100.062500, loss_pred: 0.585645
iteration 314: loss: 158030.812500, loss_kl: 257.649933, loss_recon: 157741.953125, loss_pred: 0.618110
iteration 315: loss: 161796.765625, loss_kl: 255.970764, loss_recon: 161511.828125, loss_pred: 0.593779
 18%|█████▎                        | 35/200 [32:04<2:28:02, 53.83s/it]iteration 316: loss: 158629.671875, loss_kl: 253.349167, loss_recon: 158346.203125, loss_pred: 0.501867
iteration 317: loss: 159544.000000, loss_kl: 256.727570, loss_recon: 159231.343750, loss_pred: 0.762720
iteration 318: loss: 158213.078125, loss_kl: 254.362228, loss_recon: 157898.546875, loss_pred: 0.803199
iteration 319: loss: 161061.656250, loss_kl: 253.612274, loss_recon: 160767.796875, loss_pred: 0.603339
iteration 320: loss: 157928.468750, loss_kl: 250.373474, loss_recon: 157622.921875, loss_pred: 0.749992
iteration 321: loss: 158568.734375, loss_kl: 250.298019, loss_recon: 158255.375000, loss_pred: 0.828981
iteration 322: loss: 160028.843750, loss_kl: 253.880615, loss_recon: 159728.765625, loss_pred: 0.663049
iteration 323: loss: 161114.562500, loss_kl: 250.032791, loss_recon: 160821.250000, loss_pred: 0.630746
iteration 324: loss: 157992.468750, loss_kl: 251.330811, loss_recon: 157691.109375, loss_pred: 0.699353
 18%|█████▍                        | 36/200 [32:57<2:26:40, 53.66s/it]iteration 325: loss: 159995.671875, loss_kl: 248.675110, loss_recon: 159707.250000, loss_pred: 0.495951
iteration 326: loss: 158539.312500, loss_kl: 252.378769, loss_recon: 158207.203125, loss_pred: 0.897258
iteration 327: loss: 157056.671875, loss_kl: 249.529099, loss_recon: 156750.578125, loss_pred: 0.664495
iteration 328: loss: 161086.078125, loss_kl: 243.669815, loss_recon: 160792.140625, loss_pred: 0.599249
iteration 329: loss: 159432.812500, loss_kl: 246.896988, loss_recon: 159121.953125, loss_pred: 0.737365
iteration 330: loss: 158001.453125, loss_kl: 246.518646, loss_recon: 157688.515625, loss_pred: 0.761941
iteration 331: loss: 161276.546875, loss_kl: 244.892853, loss_recon: 160978.312500, loss_pred: 0.630505
iteration 332: loss: 157235.046875, loss_kl: 246.999359, loss_recon: 156928.968750, loss_pred: 0.688542
iteration 333: loss: 160600.125000, loss_kl: 243.180756, loss_recon: 160323.609375, loss_pred: 0.429654
 18%|█████▌                        | 37/200 [33:50<2:25:12, 53.45s/it]iteration 334: loss: 160228.765625, loss_kl: 246.704971, loss_recon: 159933.500000, loss_pred: 0.485629
iteration 335: loss: 160753.796875, loss_kl: 239.423523, loss_recon: 160453.671875, loss_pred: 0.607093
iteration 336: loss: 156652.296875, loss_kl: 244.706329, loss_recon: 156344.546875, loss_pred: 0.630488
iteration 337: loss: 160691.453125, loss_kl: 240.870422, loss_recon: 160381.187500, loss_pred: 0.693889
iteration 338: loss: 159913.375000, loss_kl: 239.341965, loss_recon: 159626.328125, loss_pred: 0.476978
iteration 339: loss: 157077.281250, loss_kl: 239.248322, loss_recon: 156764.609375, loss_pred: 0.734182
iteration 340: loss: 158894.187500, loss_kl: 244.069107, loss_recon: 158576.765625, loss_pred: 0.733537
iteration 341: loss: 161608.453125, loss_kl: 243.195816, loss_recon: 161284.093750, loss_pred: 0.811488
iteration 342: loss: 157069.687500, loss_kl: 237.994781, loss_recon: 156741.765625, loss_pred: 0.899286
 19%|█████▋                        | 38/200 [34:44<2:24:29, 53.51s/it]iteration 343: loss: 158283.812500, loss_kl: 241.090881, loss_recon: 157993.546875, loss_pred: 0.491765
iteration 344: loss: 158803.609375, loss_kl: 238.718643, loss_recon: 158502.593750, loss_pred: 0.622969
iteration 345: loss: 156558.265625, loss_kl: 242.865463, loss_recon: 156247.656250, loss_pred: 0.677453
iteration 346: loss: 161719.328125, loss_kl: 233.204910, loss_recon: 161421.171875, loss_pred: 0.649504
iteration 347: loss: 158199.593750, loss_kl: 237.523544, loss_recon: 157916.000000, loss_pred: 0.460635
iteration 348: loss: 158725.984375, loss_kl: 238.088287, loss_recon: 158424.609375, loss_pred: 0.632743
iteration 349: loss: 160559.078125, loss_kl: 238.087341, loss_recon: 160256.406250, loss_pred: 0.645751
iteration 350: loss: 159320.421875, loss_kl: 237.704193, loss_recon: 159006.015625, loss_pred: 0.767024
iteration 351: loss: 160663.281250, loss_kl: 230.605560, loss_recon: 160379.828125, loss_pred: 0.528374
 20%|█████▊                        | 39/200 [35:39<2:24:41, 53.92s/it]iteration 352: loss: 162215.328125, loss_kl: 230.293442, loss_recon: 161917.406250, loss_pred: 0.676250
iteration 353: loss: 159150.203125, loss_kl: 233.568573, loss_recon: 158843.984375, loss_pred: 0.726622
iteration 354: loss: 158755.734375, loss_kl: 232.233170, loss_recon: 158457.796875, loss_pred: 0.656983
iteration 355: loss: 160761.140625, loss_kl: 235.318359, loss_recon: 160471.937500, loss_pred: 0.538944
iteration 356: loss: 155715.703125, loss_kl: 232.135193, loss_recon: 155396.375000, loss_pred: 0.871873
iteration 357: loss: 158090.062500, loss_kl: 234.871063, loss_recon: 157804.328125, loss_pred: 0.508636
iteration 358: loss: 158529.359375, loss_kl: 234.893005, loss_recon: 158237.078125, loss_pred: 0.573881
iteration 359: loss: 161535.062500, loss_kl: 234.577896, loss_recon: 161239.203125, loss_pred: 0.612830
iteration 360: loss: 158796.281250, loss_kl: 230.614899, loss_recon: 158474.140625, loss_pred: 0.915317
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_39.pth
 20%|██████                        | 40/200 [36:32<2:23:23, 53.77s/it]iteration 361: loss: 159322.406250, loss_kl: 230.007553, loss_recon: 159024.421875, loss_pred: 0.679880
iteration 362: loss: 159856.812500, loss_kl: 231.459854, loss_recon: 159556.765625, loss_pred: 0.685930
iteration 363: loss: 155495.343750, loss_kl: 232.286285, loss_recon: 155184.453125, loss_pred: 0.786152
iteration 364: loss: 161098.265625, loss_kl: 234.023438, loss_recon: 160782.171875, loss_pred: 0.820774
iteration 365: loss: 160556.546875, loss_kl: 223.648102, loss_recon: 160266.093750, loss_pred: 0.668067
iteration 366: loss: 158995.437500, loss_kl: 229.862640, loss_recon: 158699.265625, loss_pred: 0.663099
iteration 367: loss: 158630.734375, loss_kl: 232.819717, loss_recon: 158346.687500, loss_pred: 0.512364
iteration 368: loss: 158216.359375, loss_kl: 232.748840, loss_recon: 157879.265625, loss_pred: 1.043412
iteration 369: loss: 161302.375000, loss_kl: 225.239761, loss_recon: 161021.218750, loss_pred: 0.559245
 20%|██████▏                       | 41/200 [37:26<2:22:32, 53.79s/it]iteration 370: loss: 158264.500000, loss_kl: 227.623154, loss_recon: 157953.734375, loss_pred: 0.831363
iteration 371: loss: 159218.203125, loss_kl: 221.895813, loss_recon: 158864.890625, loss_pred: 1.314253
iteration 372: loss: 159232.156250, loss_kl: 223.227310, loss_recon: 158920.140625, loss_pred: 0.887748
iteration 373: loss: 159422.421875, loss_kl: 227.782394, loss_recon: 159087.078125, loss_pred: 1.075615
iteration 374: loss: 158141.218750, loss_kl: 224.959839, loss_recon: 157839.078125, loss_pred: 0.771823
iteration 375: loss: 159286.312500, loss_kl: 225.771881, loss_recon: 159013.203125, loss_pred: 0.473417
iteration 376: loss: 160793.015625, loss_kl: 220.784302, loss_recon: 160499.171875, loss_pred: 0.730686
iteration 377: loss: 160013.593750, loss_kl: 227.272690, loss_recon: 159706.718750, loss_pred: 0.796150
iteration 378: loss: 158780.984375, loss_kl: 219.018234, loss_recon: 158479.484375, loss_pred: 0.824876
 21%|██████▎                       | 42/200 [38:20<2:22:00, 53.93s/it]iteration 379: loss: 156796.296875, loss_kl: 225.779663, loss_recon: 156492.656250, loss_pred: 0.778618
iteration 380: loss: 154488.500000, loss_kl: 225.410416, loss_recon: 154212.140625, loss_pred: 0.509546
iteration 381: loss: 159371.968750, loss_kl: 224.343140, loss_recon: 159095.750000, loss_pred: 0.518684
iteration 382: loss: 161533.500000, loss_kl: 220.329529, loss_recon: 161223.093750, loss_pred: 0.900745
iteration 383: loss: 161223.187500, loss_kl: 221.355896, loss_recon: 160943.093750, loss_pred: 0.587416
iteration 384: loss: 161658.171875, loss_kl: 217.041855, loss_recon: 161361.515625, loss_pred: 0.796125
iteration 385: loss: 158479.093750, loss_kl: 218.846603, loss_recon: 158143.265625, loss_pred: 1.169837
iteration 386: loss: 161281.546875, loss_kl: 220.412888, loss_recon: 160999.781250, loss_pred: 0.613643
iteration 387: loss: 158274.484375, loss_kl: 219.778793, loss_recon: 157967.546875, loss_pred: 0.871618
 22%|██████▍                       | 43/200 [39:14<2:20:33, 53.71s/it]iteration 388: loss: 159445.984375, loss_kl: 219.234589, loss_recon: 159142.421875, loss_pred: 0.843276
iteration 389: loss: 162146.203125, loss_kl: 222.574554, loss_recon: 161870.281250, loss_pred: 0.533476
iteration 390: loss: 160558.750000, loss_kl: 215.376572, loss_recon: 160283.187500, loss_pred: 0.601833
iteration 391: loss: 158712.593750, loss_kl: 214.122696, loss_recon: 158420.937500, loss_pred: 0.775268
iteration 392: loss: 161617.843750, loss_kl: 216.184158, loss_recon: 161350.968750, loss_pred: 0.506893
iteration 393: loss: 157568.234375, loss_kl: 216.671570, loss_recon: 157286.109375, loss_pred: 0.654543
iteration 394: loss: 158992.421875, loss_kl: 217.931931, loss_recon: 158708.000000, loss_pred: 0.664870
iteration 395: loss: 159806.359375, loss_kl: 213.354385, loss_recon: 159537.531250, loss_pred: 0.554614
iteration 396: loss: 153751.328125, loss_kl: 214.955643, loss_recon: 153439.515625, loss_pred: 0.968611
 22%|██████▌                       | 44/200 [40:07<2:19:37, 53.70s/it]iteration 397: loss: 157503.953125, loss_kl: 216.352417, loss_recon: 157235.375000, loss_pred: 0.522134
iteration 398: loss: 161107.843750, loss_kl: 211.896271, loss_recon: 160826.171875, loss_pred: 0.697769
iteration 399: loss: 156714.984375, loss_kl: 212.960800, loss_recon: 156449.937500, loss_pred: 0.520997
iteration 400: loss: 160468.140625, loss_kl: 212.658249, loss_recon: 160200.265625, loss_pred: 0.552209
iteration 401: loss: 161179.312500, loss_kl: 209.460770, loss_recon: 160914.234375, loss_pred: 0.556187
iteration 402: loss: 161014.890625, loss_kl: 212.583939, loss_recon: 160731.062500, loss_pred: 0.712496
iteration 403: loss: 161985.421875, loss_kl: 209.932098, loss_recon: 161682.640625, loss_pred: 0.928460
iteration 404: loss: 155737.109375, loss_kl: 212.704224, loss_recon: 155418.921875, loss_pred: 1.054809
iteration 405: loss: 157049.093750, loss_kl: 209.641434, loss_recon: 156791.187500, loss_pred: 0.482602
 22%|██████▊                       | 45/200 [41:00<2:18:23, 53.57s/it]iteration 406: loss: 160161.218750, loss_kl: 209.266525, loss_recon: 159889.953125, loss_pred: 0.620071
iteration 407: loss: 160355.937500, loss_kl: 207.862396, loss_recon: 160075.812500, loss_pred: 0.722634
iteration 408: loss: 163056.203125, loss_kl: 205.596329, loss_recon: 162796.656250, loss_pred: 0.539489
iteration 409: loss: 157172.328125, loss_kl: 206.966934, loss_recon: 156901.343750, loss_pred: 0.640194
iteration 410: loss: 157767.078125, loss_kl: 210.694794, loss_recon: 157494.281250, loss_pred: 0.621117
iteration 411: loss: 162460.671875, loss_kl: 211.136856, loss_recon: 162124.375000, loss_pred: 1.251501
iteration 412: loss: 155860.375000, loss_kl: 205.674088, loss_recon: 155584.593750, loss_pred: 0.701038
iteration 413: loss: 155037.953125, loss_kl: 204.779297, loss_recon: 154719.750000, loss_pred: 1.134276
iteration 414: loss: 160780.734375, loss_kl: 207.051086, loss_recon: 160533.031250, loss_pred: 0.406638
 23%|██████▉                       | 46/200 [41:54<2:17:18, 53.50s/it]iteration 415: loss: 158610.750000, loss_kl: 204.865891, loss_recon: 158355.375000, loss_pred: 0.505104
iteration 416: loss: 157916.609375, loss_kl: 208.630051, loss_recon: 157660.812500, loss_pred: 0.471715
iteration 417: loss: 162508.328125, loss_kl: 203.565414, loss_recon: 162257.546875, loss_pred: 0.472136
iteration 418: loss: 160464.812500, loss_kl: 203.032394, loss_recon: 160205.625000, loss_pred: 0.561559
iteration 419: loss: 159240.781250, loss_kl: 207.941132, loss_recon: 158980.265625, loss_pred: 0.525722
iteration 420: loss: 161426.906250, loss_kl: 205.084213, loss_recon: 161164.625000, loss_pred: 0.572102
iteration 421: loss: 157625.906250, loss_kl: 204.397812, loss_recon: 157367.609375, loss_pred: 0.539110
iteration 422: loss: 156869.171875, loss_kl: 201.002029, loss_recon: 156587.906250, loss_pred: 0.802629
iteration 423: loss: 157540.875000, loss_kl: 204.321869, loss_recon: 157283.765625, loss_pred: 0.527790
 24%|███████                       | 47/200 [42:48<2:16:38, 53.59s/it]iteration 424: loss: 156429.234375, loss_kl: 202.964844, loss_recon: 156147.203125, loss_pred: 0.790662
iteration 425: loss: 159407.500000, loss_kl: 199.694565, loss_recon: 159152.515625, loss_pred: 0.552926
iteration 426: loss: 161341.578125, loss_kl: 199.425659, loss_recon: 161085.421875, loss_pred: 0.567305
iteration 427: loss: 158212.406250, loss_kl: 200.008240, loss_recon: 157934.515625, loss_pred: 0.778759
iteration 428: loss: 158938.390625, loss_kl: 202.063782, loss_recon: 158667.453125, loss_pred: 0.688820
iteration 429: loss: 161476.453125, loss_kl: 201.325729, loss_recon: 161205.312500, loss_pred: 0.698080
iteration 430: loss: 157048.093750, loss_kl: 203.224121, loss_recon: 156799.468750, loss_pred: 0.453995
iteration 431: loss: 159994.187500, loss_kl: 198.507538, loss_recon: 159729.703125, loss_pred: 0.659766
iteration 432: loss: 159352.593750, loss_kl: 197.873520, loss_recon: 159108.125000, loss_pred: 0.465925
 24%|███████▏                      | 48/200 [43:41<2:15:24, 53.45s/it]iteration 433: loss: 157768.218750, loss_kl: 199.833786, loss_recon: 157512.953125, loss_pred: 0.554424
iteration 434: loss: 163297.890625, loss_kl: 197.135742, loss_recon: 163039.687500, loss_pred: 0.610646
iteration 435: loss: 159444.000000, loss_kl: 197.604630, loss_recon: 159191.500000, loss_pred: 0.548913
iteration 436: loss: 158983.390625, loss_kl: 196.718811, loss_recon: 158715.531250, loss_pred: 0.711484
iteration 437: loss: 157313.578125, loss_kl: 198.534348, loss_recon: 157007.453125, loss_pred: 1.075907
iteration 438: loss: 159628.859375, loss_kl: 200.056137, loss_recon: 159360.687500, loss_pred: 0.681029
iteration 439: loss: 157677.312500, loss_kl: 200.177826, loss_recon: 157390.375000, loss_pred: 0.867589
iteration 440: loss: 156657.718750, loss_kl: 198.634064, loss_recon: 156405.015625, loss_pred: 0.540700
iteration 441: loss: 161448.515625, loss_kl: 198.194595, loss_recon: 161195.625000, loss_pred: 0.546993
 24%|███████▎                      | 49/200 [44:34<2:14:36, 53.49s/it]iteration 442: loss: 160675.296875, loss_kl: 196.299545, loss_recon: 160417.765625, loss_pred: 0.612362
iteration 443: loss: 162565.578125, loss_kl: 191.252914, loss_recon: 162284.328125, loss_pred: 0.900004
iteration 444: loss: 159353.453125, loss_kl: 195.398804, loss_recon: 159058.593750, loss_pred: 0.994484
iteration 445: loss: 157714.296875, loss_kl: 197.251129, loss_recon: 157435.703125, loss_pred: 0.813361
iteration 446: loss: 158488.187500, loss_kl: 196.901794, loss_recon: 158166.406250, loss_pred: 1.248753
iteration 447: loss: 159296.781250, loss_kl: 193.930222, loss_recon: 159033.515625, loss_pred: 0.693247
iteration 448: loss: 159220.296875, loss_kl: 196.274979, loss_recon: 158923.718750, loss_pred: 1.002893
iteration 449: loss: 156341.546875, loss_kl: 193.619339, loss_recon: 156082.796875, loss_pred: 0.651213
iteration 450: loss: 158642.968750, loss_kl: 194.059708, loss_recon: 158397.406250, loss_pred: 0.514981
 25%|███████▌                      | 50/200 [45:27<2:13:29, 53.39s/it]iteration 451: loss: 153148.531250, loss_kl: 191.656815, loss_recon: 153079.203125, loss_pred: 0.674076
iteration 452: loss: 158266.031250, loss_kl: 195.691437, loss_recon: 158200.546875, loss_pred: 0.635271
iteration 453: loss: 159265.500000, loss_kl: 192.324997, loss_recon: 159197.171875, loss_pred: 0.664125
iteration 454: loss: 160068.953125, loss_kl: 191.793488, loss_recon: 160011.328125, loss_pred: 0.557000
iteration 455: loss: 159178.828125, loss_kl: 189.821411, loss_recon: 159134.578125, loss_pred: 0.423659
iteration 456: loss: 163313.687500, loss_kl: 188.032730, loss_recon: 163262.531250, loss_pred: 0.492876
iteration 457: loss: 157329.125000, loss_kl: 190.162537, loss_recon: 157266.937500, loss_pred: 0.602884
iteration 458: loss: 161288.984375, loss_kl: 188.652039, loss_recon: 161236.718750, loss_pred: 0.503813
iteration 459: loss: 158720.343750, loss_kl: 189.333221, loss_recon: 158683.281250, loss_pred: 0.351679
 26%|███████▋                      | 51/200 [46:21<2:12:39, 53.42s/it]iteration 460: loss: 157789.218750, loss_kl: 191.943283, loss_recon: 157750.437500, loss_pred: 0.368591
iteration 461: loss: 161407.203125, loss_kl: 190.214966, loss_recon: 161369.921875, loss_pred: 0.353696
iteration 462: loss: 158123.171875, loss_kl: 192.202011, loss_recon: 158081.250000, loss_pred: 0.400050
iteration 463: loss: 156911.906250, loss_kl: 190.687424, loss_recon: 156841.593750, loss_pred: 0.683999
iteration 464: loss: 157664.281250, loss_kl: 192.143143, loss_recon: 157622.109375, loss_pred: 0.402480
iteration 465: loss: 158029.437500, loss_kl: 192.246429, loss_recon: 157969.000000, loss_pred: 0.585209
iteration 466: loss: 161170.937500, loss_kl: 189.217834, loss_recon: 161130.109375, loss_pred: 0.389375
iteration 467: loss: 160890.906250, loss_kl: 188.188278, loss_recon: 160829.968750, loss_pred: 0.590675
iteration 468: loss: 158275.125000, loss_kl: 187.832291, loss_recon: 158186.093750, loss_pred: 0.871552
 26%|███████▊                      | 52/200 [47:15<2:12:01, 53.53s/it]iteration 469: loss: 160783.859375, loss_kl: 188.722916, loss_recon: 160732.687500, loss_pred: 0.492884
iteration 470: loss: 160154.078125, loss_kl: 188.079575, loss_recon: 160092.859375, loss_pred: 0.593375
iteration 471: loss: 156265.218750, loss_kl: 191.770920, loss_recon: 156215.000000, loss_pred: 0.482947
iteration 472: loss: 158147.828125, loss_kl: 189.700638, loss_recon: 158082.796875, loss_pred: 0.631431
iteration 473: loss: 159587.343750, loss_kl: 192.656113, loss_recon: 159518.765625, loss_pred: 0.666593
iteration 474: loss: 161948.468750, loss_kl: 191.402618, loss_recon: 161894.921875, loss_pred: 0.516401
iteration 475: loss: 160491.562500, loss_kl: 194.435364, loss_recon: 160451.453125, loss_pred: 0.381728
iteration 476: loss: 156327.140625, loss_kl: 190.082520, loss_recon: 156225.171875, loss_pred: 1.000610
iteration 477: loss: 156578.765625, loss_kl: 191.861252, loss_recon: 156540.687500, loss_pred: 0.361595
 26%|███████▉                      | 53/200 [48:09<2:11:22, 53.62s/it]iteration 478: loss: 161277.000000, loss_kl: 190.583801, loss_recon: 161215.625000, loss_pred: 0.594715
iteration 479: loss: 160038.250000, loss_kl: 191.389954, loss_recon: 159976.093750, loss_pred: 0.602530
iteration 480: loss: 156358.671875, loss_kl: 193.407028, loss_recon: 156311.265625, loss_pred: 0.454710
iteration 481: loss: 158415.593750, loss_kl: 192.545654, loss_recon: 158344.375000, loss_pred: 0.692892
iteration 482: loss: 163254.281250, loss_kl: 192.230881, loss_recon: 163216.921875, loss_pred: 0.354333
iteration 483: loss: 158477.765625, loss_kl: 190.163528, loss_recon: 158377.343750, loss_pred: 0.985167
iteration 484: loss: 160005.140625, loss_kl: 190.929688, loss_recon: 159969.593750, loss_pred: 0.336390
iteration 485: loss: 157828.546875, loss_kl: 195.162750, loss_recon: 157755.125000, loss_pred: 0.714624
iteration 486: loss: 154742.250000, loss_kl: 192.288834, loss_recon: 154671.343750, loss_pred: 0.689805
 27%|████████                      | 54/200 [49:02<2:10:09, 53.49s/it]iteration 487: loss: 158837.984375, loss_kl: 192.921265, loss_recon: 158796.468750, loss_pred: 0.395863
iteration 488: loss: 157362.250000, loss_kl: 192.469208, loss_recon: 157284.609375, loss_pred: 0.757225
iteration 489: loss: 156995.156250, loss_kl: 193.513687, loss_recon: 156949.109375, loss_pred: 0.441124
iteration 490: loss: 159497.359375, loss_kl: 192.825851, loss_recon: 159446.921875, loss_pred: 0.485159
iteration 491: loss: 161629.312500, loss_kl: 193.070068, loss_recon: 161582.312500, loss_pred: 0.450671
iteration 492: loss: 157328.171875, loss_kl: 193.072281, loss_recon: 157246.906250, loss_pred: 0.793221
iteration 493: loss: 159552.703125, loss_kl: 190.926926, loss_recon: 159506.421875, loss_pred: 0.443798
iteration 494: loss: 157912.828125, loss_kl: 194.120148, loss_recon: 157849.078125, loss_pred: 0.618142
iteration 495: loss: 160727.031250, loss_kl: 191.628204, loss_recon: 160683.093750, loss_pred: 0.420178
 28%|████████▎                     | 55/200 [49:55<2:09:20, 53.52s/it]iteration 496: loss: 157988.281250, loss_kl: 191.037720, loss_recon: 157954.421875, loss_pred: 0.319499
iteration 497: loss: 159201.375000, loss_kl: 188.156540, loss_recon: 159132.234375, loss_pred: 0.672631
iteration 498: loss: 161854.734375, loss_kl: 190.187271, loss_recon: 161826.140625, loss_pred: 0.266927
iteration 499: loss: 157418.421875, loss_kl: 193.058334, loss_recon: 157377.109375, loss_pred: 0.393692
iteration 500: loss: 157886.343750, loss_kl: 193.150192, loss_recon: 157828.718750, loss_pred: 0.556914
iteration 501: loss: 158144.718750, loss_kl: 195.702988, loss_recon: 158085.921875, loss_pred: 0.568485
iteration 502: loss: 157284.906250, loss_kl: 191.948776, loss_recon: 157231.546875, loss_pred: 0.514345
iteration 503: loss: 157797.765625, loss_kl: 191.721878, loss_recon: 157756.593750, loss_pred: 0.392431
iteration 504: loss: 162410.515625, loss_kl: 191.784332, loss_recon: 162367.968750, loss_pred: 0.406184
 28%|████████▍                     | 56/200 [50:49<2:08:41, 53.62s/it]iteration 505: loss: 159975.062500, loss_kl: 188.625778, loss_recon: 159912.625000, loss_pred: 0.605508
iteration 506: loss: 160488.984375, loss_kl: 187.914261, loss_recon: 160430.031250, loss_pred: 0.570851
iteration 507: loss: 159492.953125, loss_kl: 189.122757, loss_recon: 159451.093750, loss_pred: 0.399736
iteration 508: loss: 156994.390625, loss_kl: 195.226013, loss_recon: 156945.890625, loss_pred: 0.465505
iteration 509: loss: 158120.343750, loss_kl: 193.885254, loss_recon: 158072.406250, loss_pred: 0.459980
iteration 510: loss: 158919.156250, loss_kl: 191.287308, loss_recon: 158878.406250, loss_pred: 0.388404
iteration 511: loss: 161164.437500, loss_kl: 192.085373, loss_recon: 161125.625000, loss_pred: 0.368856
iteration 512: loss: 157622.281250, loss_kl: 192.519119, loss_recon: 157566.109375, loss_pred: 0.542496
iteration 513: loss: 157565.593750, loss_kl: 192.739670, loss_recon: 157526.656250, loss_pred: 0.370158
 28%|████████▌                     | 57/200 [51:43<2:08:05, 53.74s/it]iteration 514: loss: 160566.640625, loss_kl: 192.511810, loss_recon: 160501.921875, loss_pred: 0.628010
iteration 515: loss: 159151.437500, loss_kl: 192.232056, loss_recon: 159104.578125, loss_pred: 0.449330
iteration 516: loss: 159184.500000, loss_kl: 192.293030, loss_recon: 159127.421875, loss_pred: 0.551527
iteration 517: loss: 157502.375000, loss_kl: 191.835861, loss_recon: 157427.562500, loss_pred: 0.728964
iteration 518: loss: 159021.921875, loss_kl: 193.019623, loss_recon: 158982.875000, loss_pred: 0.371046
iteration 519: loss: 159313.593750, loss_kl: 197.049057, loss_recon: 159248.750000, loss_pred: 0.628743
iteration 520: loss: 158494.375000, loss_kl: 194.863495, loss_recon: 158442.031250, loss_pred: 0.503918
iteration 521: loss: 157356.343750, loss_kl: 194.729080, loss_recon: 157313.062500, loss_pred: 0.413230
iteration 522: loss: 159556.953125, loss_kl: 193.113464, loss_recon: 159517.921875, loss_pred: 0.370984
 29%|████████▋                     | 58/200 [52:37<2:07:14, 53.76s/it]iteration 523: loss: 157032.140625, loss_kl: 194.116302, loss_recon: 156983.078125, loss_pred: 0.471310
iteration 524: loss: 156873.562500, loss_kl: 193.949066, loss_recon: 156826.578125, loss_pred: 0.450404
iteration 525: loss: 157820.968750, loss_kl: 194.728409, loss_recon: 157782.234375, loss_pred: 0.367850
iteration 526: loss: 158675.906250, loss_kl: 196.393494, loss_recon: 158626.109375, loss_pred: 0.478263
iteration 527: loss: 161876.796875, loss_kl: 194.383896, loss_recon: 161847.593750, loss_pred: 0.272581
iteration 528: loss: 163842.843750, loss_kl: 189.720047, loss_recon: 163792.640625, loss_pred: 0.483104
iteration 529: loss: 158468.765625, loss_kl: 192.704941, loss_recon: 158437.265625, loss_pred: 0.295793
iteration 530: loss: 158045.000000, loss_kl: 191.341080, loss_recon: 158013.656250, loss_pred: 0.294323
iteration 531: loss: 157419.187500, loss_kl: 193.969177, loss_recon: 157383.078125, loss_pred: 0.341711
 30%|████████▊                     | 59/200 [53:30<2:06:03, 53.64s/it]iteration 532: loss: 157931.312500, loss_kl: 198.878128, loss_recon: 157883.734375, loss_pred: 0.455979
iteration 533: loss: 162032.546875, loss_kl: 193.646072, loss_recon: 161973.937500, loss_pred: 0.566728
iteration 534: loss: 162863.359375, loss_kl: 193.074478, loss_recon: 162802.078125, loss_pred: 0.593487
iteration 535: loss: 162842.671875, loss_kl: 193.128265, loss_recon: 162796.312500, loss_pred: 0.444142
iteration 536: loss: 155965.046875, loss_kl: 192.562759, loss_recon: 155900.750000, loss_pred: 0.623689
iteration 537: loss: 155863.046875, loss_kl: 194.221039, loss_recon: 155826.921875, loss_pred: 0.341928
iteration 538: loss: 158350.609375, loss_kl: 195.576965, loss_recon: 158296.609375, loss_pred: 0.520464
iteration 539: loss: 156617.921875, loss_kl: 196.830215, loss_recon: 156579.281250, loss_pred: 0.366697
iteration 540: loss: 157676.859375, loss_kl: 192.096146, loss_recon: 157617.687500, loss_pred: 0.572576
 30%|█████████                     | 60/200 [54:23<2:04:48, 53.49s/it]iteration 541: loss: 156041.593750, loss_kl: 195.683044, loss_recon: 156000.859375, loss_pred: 0.387777
iteration 542: loss: 159157.390625, loss_kl: 198.443420, loss_recon: 159103.671875, loss_pred: 0.517303
iteration 543: loss: 159786.703125, loss_kl: 195.791260, loss_recon: 159740.890625, loss_pred: 0.438660
iteration 544: loss: 160165.015625, loss_kl: 196.008453, loss_recon: 160116.343750, loss_pred: 0.467237
iteration 545: loss: 158701.515625, loss_kl: 193.815460, loss_recon: 158621.000000, loss_pred: 0.785711
iteration 546: loss: 158855.421875, loss_kl: 193.203568, loss_recon: 158807.859375, loss_pred: 0.456306
iteration 547: loss: 158762.859375, loss_kl: 193.655121, loss_recon: 158714.484375, loss_pred: 0.464328
iteration 548: loss: 158736.328125, loss_kl: 195.562775, loss_recon: 158691.968750, loss_pred: 0.424024
iteration 549: loss: 159767.015625, loss_kl: 195.322601, loss_recon: 159713.968750, loss_pred: 0.510896
 30%|█████████▏                    | 61/200 [55:17<2:03:38, 53.37s/it]iteration 550: loss: 160881.984375, loss_kl: 192.581543, loss_recon: 160840.750000, loss_pred: 0.393113
iteration 551: loss: 156027.171875, loss_kl: 196.640930, loss_recon: 155948.671875, loss_pred: 0.765313
iteration 552: loss: 162556.578125, loss_kl: 194.144318, loss_recon: 162520.765625, loss_pred: 0.338764
iteration 553: loss: 160780.937500, loss_kl: 194.807007, loss_recon: 160737.734375, loss_pred: 0.412491
iteration 554: loss: 157658.718750, loss_kl: 193.348969, loss_recon: 157616.796875, loss_pred: 0.399806
iteration 555: loss: 158000.515625, loss_kl: 194.977798, loss_recon: 157952.421875, loss_pred: 0.461373
iteration 556: loss: 153731.328125, loss_kl: 199.242859, loss_recon: 153673.750000, loss_pred: 0.555830
iteration 557: loss: 159527.109375, loss_kl: 196.324310, loss_recon: 159470.796875, loss_pred: 0.543467
iteration 558: loss: 160665.343750, loss_kl: 199.385574, loss_recon: 160585.593750, loss_pred: 0.777567
 31%|█████████▎                    | 62/200 [56:10<2:02:59, 53.47s/it]iteration 559: loss: 158704.140625, loss_kl: 197.530167, loss_recon: 158658.546875, loss_pred: 0.436305
iteration 560: loss: 156344.937500, loss_kl: 198.194977, loss_recon: 156263.937500, loss_pred: 0.790123
iteration 561: loss: 158688.906250, loss_kl: 197.225388, loss_recon: 158647.312500, loss_pred: 0.396220
iteration 562: loss: 159568.765625, loss_kl: 199.636581, loss_recon: 159501.546875, loss_pred: 0.652120
iteration 563: loss: 159988.578125, loss_kl: 193.139145, loss_recon: 159945.453125, loss_pred: 0.411858
iteration 564: loss: 159059.921875, loss_kl: 193.683701, loss_recon: 159016.687500, loss_pred: 0.412902
iteration 565: loss: 159238.484375, loss_kl: 193.256241, loss_recon: 159174.828125, loss_pred: 0.617156
iteration 566: loss: 156725.218750, loss_kl: 195.226303, loss_recon: 156675.000000, loss_pred: 0.482616
iteration 567: loss: 161601.265625, loss_kl: 194.382782, loss_recon: 161545.921875, loss_pred: 0.533989
 32%|█████████▍                    | 63/200 [57:04<2:02:19, 53.57s/it]iteration 568: loss: 159398.765625, loss_kl: 198.298965, loss_recon: 159322.656250, loss_pred: 0.662781
iteration 569: loss: 154725.765625, loss_kl: 197.114944, loss_recon: 154670.031250, loss_pred: 0.459551
iteration 570: loss: 157579.812500, loss_kl: 198.475861, loss_recon: 157537.203125, loss_pred: 0.327630
iteration 571: loss: 159258.953125, loss_kl: 196.793579, loss_recon: 159211.593750, loss_pred: 0.375991
iteration 572: loss: 162958.093750, loss_kl: 193.962143, loss_recon: 162921.890625, loss_pred: 0.265781
iteration 573: loss: 156392.109375, loss_kl: 195.202560, loss_recon: 156322.796875, loss_pred: 0.596204
iteration 574: loss: 160300.562500, loss_kl: 196.628922, loss_recon: 160250.406250, loss_pred: 0.404082
iteration 575: loss: 161632.109375, loss_kl: 195.280197, loss_recon: 161562.265625, loss_pred: 0.601536
iteration 576: loss: 157769.250000, loss_kl: 198.618134, loss_recon: 157714.203125, loss_pred: 0.452071
 32%|█████████▌                    | 64/200 [57:58<2:01:33, 53.63s/it]iteration 577: loss: 158618.812500, loss_kl: 197.327866, loss_recon: 158557.406250, loss_pred: 0.437991
iteration 578: loss: 156920.984375, loss_kl: 200.238419, loss_recon: 156836.328125, loss_pred: 0.668043
iteration 579: loss: 162867.593750, loss_kl: 197.705475, loss_recon: 162810.234375, loss_pred: 0.397235
iteration 580: loss: 157479.515625, loss_kl: 196.726151, loss_recon: 157429.593750, loss_pred: 0.323789
iteration 581: loss: 159239.218750, loss_kl: 198.248779, loss_recon: 159184.453125, loss_pred: 0.370804
iteration 582: loss: 159672.812500, loss_kl: 195.174484, loss_recon: 159622.890625, loss_pred: 0.325124
iteration 583: loss: 156865.609375, loss_kl: 197.002289, loss_recon: 156813.609375, loss_pred: 0.344257
iteration 584: loss: 159070.296875, loss_kl: 196.159271, loss_recon: 159020.718750, loss_pred: 0.320711
iteration 585: loss: 159016.484375, loss_kl: 196.442856, loss_recon: 158970.031250, loss_pred: 0.289330
 32%|█████████▊                    | 65/200 [58:52<2:00:45, 53.67s/it]iteration 586: loss: 158183.250000, loss_kl: 195.915878, loss_recon: 158113.796875, loss_pred: 0.442138
iteration 587: loss: 160699.156250, loss_kl: 198.057007, loss_recon: 160642.625000, loss_pred: 0.310226
iteration 588: loss: 156788.156250, loss_kl: 198.738190, loss_recon: 156731.093750, loss_pred: 0.314699
iteration 589: loss: 159826.921875, loss_kl: 198.257935, loss_recon: 159777.140625, loss_pred: 0.242422
iteration 590: loss: 156643.906250, loss_kl: 198.125168, loss_recon: 156595.656250, loss_pred: 0.227365
iteration 591: loss: 158559.109375, loss_kl: 195.239197, loss_recon: 158493.640625, loss_pred: 0.403289
iteration 592: loss: 162431.046875, loss_kl: 194.346344, loss_recon: 162364.468750, loss_pred: 0.415444
iteration 593: loss: 158522.906250, loss_kl: 196.860550, loss_recon: 158452.171875, loss_pred: 0.453797
iteration 594: loss: 158303.828125, loss_kl: 197.942795, loss_recon: 158224.109375, loss_pred: 0.542196
 33%|█████████▉                    | 66/200 [59:45<1:59:51, 53.67s/it]iteration 595: loss: 158229.921875, loss_kl: 198.165390, loss_recon: 158165.421875, loss_pred: 0.311194
iteration 596: loss: 156997.500000, loss_kl: 197.834854, loss_recon: 156923.828125, loss_pred: 0.403599
iteration 597: loss: 160522.765625, loss_kl: 197.199982, loss_recon: 160442.375000, loss_pred: 0.471855
iteration 598: loss: 162486.546875, loss_kl: 193.334564, loss_recon: 162421.406250, loss_pred: 0.325714
iteration 599: loss: 157892.265625, loss_kl: 199.553406, loss_recon: 157826.312500, loss_pred: 0.323394
iteration 600: loss: 156979.265625, loss_kl: 199.773544, loss_recon: 156899.234375, loss_pred: 0.463922
iteration 601: loss: 158720.031250, loss_kl: 197.650833, loss_recon: 158654.781250, loss_pred: 0.319623
iteration 602: loss: 160389.765625, loss_kl: 195.378937, loss_recon: 160319.093750, loss_pred: 0.377718
iteration 603: loss: 157716.203125, loss_kl: 193.599243, loss_recon: 157652.515625, loss_pred: 0.310709
 34%|█████████▍                  | 67/200 [1:00:39<1:59:16, 53.81s/it]iteration 604: loss: 160460.312500, loss_kl: 197.225815, loss_recon: 160391.578125, loss_pred: 0.277242
iteration 605: loss: 156708.312500, loss_kl: 195.337952, loss_recon: 156637.687500, loss_pred: 0.300042
iteration 606: loss: 161490.984375, loss_kl: 195.514999, loss_recon: 161419.234375, loss_pred: 0.310806
iteration 607: loss: 160020.890625, loss_kl: 196.238174, loss_recon: 159940.015625, loss_pred: 0.400621
iteration 608: loss: 159197.140625, loss_kl: 199.265594, loss_recon: 159132.000000, loss_pred: 0.236863
iteration 609: loss: 159604.703125, loss_kl: 198.292862, loss_recon: 159539.562500, loss_pred: 0.238968
iteration 610: loss: 159876.296875, loss_kl: 198.017563, loss_recon: 159809.937500, loss_pred: 0.251667
iteration 611: loss: 155097.890625, loss_kl: 197.744797, loss_recon: 155023.734375, loss_pred: 0.330321
iteration 612: loss: 157404.671875, loss_kl: 199.646439, loss_recon: 157326.046875, loss_pred: 0.371003
 34%|█████████▌                  | 68/200 [1:01:34<1:58:37, 53.92s/it]iteration 613: loss: 159019.500000, loss_kl: 197.996140, loss_recon: 158937.625000, loss_pred: 0.328435
iteration 614: loss: 159798.734375, loss_kl: 196.083984, loss_recon: 159721.156250, loss_pred: 0.290369
iteration 615: loss: 158728.609375, loss_kl: 194.110123, loss_recon: 158627.796875, loss_pred: 0.527483
iteration 616: loss: 155636.531250, loss_kl: 197.643021, loss_recon: 155560.281250, loss_pred: 0.273176
iteration 617: loss: 160487.203125, loss_kl: 199.357071, loss_recon: 160363.406250, loss_pred: 0.744299
iteration 618: loss: 157180.859375, loss_kl: 197.613098, loss_recon: 157091.671875, loss_pred: 0.402676
iteration 619: loss: 161874.265625, loss_kl: 193.373123, loss_recon: 161784.687500, loss_pred: 0.417031
iteration 620: loss: 156630.171875, loss_kl: 194.796661, loss_recon: 156512.000000, loss_pred: 0.699303
iteration 621: loss: 160989.843750, loss_kl: 193.216888, loss_recon: 160904.062500, loss_pred: 0.379336
 34%|█████████▋                  | 69/200 [1:02:28<1:57:59, 54.05s/it]iteration 622: loss: 159042.531250, loss_kl: 195.158264, loss_recon: 158947.546875, loss_pred: 0.389420
iteration 623: loss: 162158.984375, loss_kl: 194.981918, loss_recon: 162062.453125, loss_pred: 0.405359
iteration 624: loss: 159031.921875, loss_kl: 194.768616, loss_recon: 158942.140625, loss_pred: 0.338483
iteration 625: loss: 157468.359375, loss_kl: 198.671066, loss_recon: 157372.343750, loss_pred: 0.389519
iteration 626: loss: 157975.203125, loss_kl: 198.420227, loss_recon: 157883.812500, loss_pred: 0.344082
iteration 627: loss: 157633.062500, loss_kl: 200.370865, loss_recon: 157541.203125, loss_pred: 0.343192
iteration 628: loss: 158834.328125, loss_kl: 196.530426, loss_recon: 158748.937500, loss_pred: 0.289575
iteration 629: loss: 156691.234375, loss_kl: 196.629791, loss_recon: 156602.921875, loss_pred: 0.318395
iteration 630: loss: 161384.015625, loss_kl: 194.814102, loss_recon: 161298.546875, loss_pred: 0.295191
 35%|█████████▊                  | 70/200 [1:03:22<1:56:56, 53.97s/it]iteration 631: loss: 158474.750000, loss_kl: 196.970566, loss_recon: 158379.546875, loss_pred: 0.308287
iteration 632: loss: 159078.609375, loss_kl: 195.079544, loss_recon: 158974.890625, loss_pred: 0.399669
iteration 633: loss: 160258.390625, loss_kl: 187.648560, loss_recon: 160159.671875, loss_pred: 0.373870
iteration 634: loss: 159406.640625, loss_kl: 194.521194, loss_recon: 159295.812500, loss_pred: 0.472726
iteration 635: loss: 160715.828125, loss_kl: 191.334824, loss_recon: 160614.281250, loss_pred: 0.390215
iteration 636: loss: 160834.390625, loss_kl: 193.684738, loss_recon: 160745.265625, loss_pred: 0.258311
iteration 637: loss: 157095.843750, loss_kl: 194.696289, loss_recon: 156976.281250, loss_pred: 0.559350
iteration 638: loss: 156166.812500, loss_kl: 196.849976, loss_recon: 156069.843750, loss_pred: 0.326450
iteration 639: loss: 158649.500000, loss_kl: 198.969055, loss_recon: 158483.984375, loss_pred: 1.004932
 36%|█████████▉                  | 71/200 [1:04:15<1:55:43, 53.82s/it]iteration 640: loss: 159479.500000, loss_kl: 196.933762, loss_recon: 159316.093750, loss_pred: 0.912531
iteration 641: loss: 160315.953125, loss_kl: 191.875900, loss_recon: 160202.578125, loss_pred: 0.430734
iteration 642: loss: 155717.390625, loss_kl: 191.996384, loss_recon: 155543.296875, loss_pred: 1.037526
iteration 643: loss: 163051.046875, loss_kl: 187.318390, loss_recon: 162926.156250, loss_pred: 0.562506
iteration 644: loss: 159553.968750, loss_kl: 190.595566, loss_recon: 159435.187500, loss_pred: 0.489564
iteration 645: loss: 158456.296875, loss_kl: 197.157562, loss_recon: 158341.093750, loss_pred: 0.429664
iteration 646: loss: 159606.640625, loss_kl: 193.400955, loss_recon: 159487.437500, loss_pred: 0.483364
iteration 647: loss: 156870.093750, loss_kl: 194.965378, loss_recon: 156747.062500, loss_pred: 0.515950
iteration 648: loss: 157551.031250, loss_kl: 193.916107, loss_recon: 157440.406250, loss_pred: 0.395731
 36%|██████████                  | 72/200 [1:05:09<1:54:54, 53.86s/it]iteration 649: loss: 158637.531250, loss_kl: 194.157837, loss_recon: 158507.968750, loss_pred: 0.507345
iteration 650: loss: 157828.781250, loss_kl: 191.773773, loss_recon: 157702.109375, loss_pred: 0.488125
iteration 651: loss: 156496.171875, loss_kl: 193.283127, loss_recon: 156366.937500, loss_pred: 0.507606
iteration 652: loss: 160198.187500, loss_kl: 192.324997, loss_recon: 160082.171875, loss_pred: 0.379403
iteration 653: loss: 160089.437500, loss_kl: 192.945572, loss_recon: 159977.593750, loss_pred: 0.335177
iteration 654: loss: 160242.812500, loss_kl: 190.442398, loss_recon: 160139.531250, loss_pred: 0.259709
iteration 655: loss: 159110.250000, loss_kl: 191.285645, loss_recon: 158997.265625, loss_pred: 0.353223
iteration 656: loss: 159155.859375, loss_kl: 192.875900, loss_recon: 159044.062500, loss_pred: 0.334810
iteration 657: loss: 158629.453125, loss_kl: 190.925949, loss_recon: 158527.937500, loss_pred: 0.240035
 36%|██████████▏                 | 73/200 [1:06:03<1:53:47, 53.76s/it]iteration 658: loss: 158613.234375, loss_kl: 194.126556, loss_recon: 158487.640625, loss_pred: 0.390941
iteration 659: loss: 157605.453125, loss_kl: 190.410690, loss_recon: 157483.921875, loss_pred: 0.366893
iteration 660: loss: 162173.109375, loss_kl: 188.565735, loss_recon: 162053.421875, loss_pred: 0.356569
iteration 661: loss: 160629.484375, loss_kl: 191.550629, loss_recon: 160512.031250, loss_pred: 0.320997
iteration 662: loss: 157132.781250, loss_kl: 188.307114, loss_recon: 156994.125000, loss_pred: 0.547523
iteration 663: loss: 156774.734375, loss_kl: 188.980545, loss_recon: 156646.765625, loss_pred: 0.437620
iteration 664: loss: 160180.265625, loss_kl: 192.720871, loss_recon: 160064.406250, loss_pred: 0.299770
iteration 665: loss: 158394.046875, loss_kl: 192.739395, loss_recon: 158256.578125, loss_pred: 0.515819
iteration 666: loss: 158991.125000, loss_kl: 188.533157, loss_recon: 158874.421875, loss_pred: 0.326853
 37%|██████████▎                 | 74/200 [1:06:56<1:52:46, 53.71s/it]iteration 667: loss: 156909.000000, loss_kl: 187.856445, loss_recon: 156783.703125, loss_pred: 0.341517
iteration 668: loss: 159176.765625, loss_kl: 189.812943, loss_recon: 159049.203125, loss_pred: 0.354640
iteration 669: loss: 160058.375000, loss_kl: 187.578827, loss_recon: 159929.921875, loss_pred: 0.374321
iteration 670: loss: 159543.953125, loss_kl: 190.521866, loss_recon: 159423.687500, loss_pred: 0.278337
iteration 671: loss: 161831.156250, loss_kl: 191.338333, loss_recon: 161710.093750, loss_pred: 0.282163
iteration 672: loss: 156526.640625, loss_kl: 188.452179, loss_recon: 156406.265625, loss_pred: 0.289411
iteration 673: loss: 159714.921875, loss_kl: 191.406815, loss_recon: 159595.203125, loss_pred: 0.268508
iteration 674: loss: 159356.453125, loss_kl: 188.760330, loss_recon: 159233.937500, loss_pred: 0.309267
iteration 675: loss: 157235.000000, loss_kl: 188.964966, loss_recon: 157107.203125, loss_pred: 0.361145
 38%|██████████▌                 | 75/200 [1:07:50<1:51:44, 53.64s/it]iteration 676: loss: 158452.437500, loss_kl: 186.550613, loss_recon: 158300.296875, loss_pred: 0.542297
iteration 677: loss: 156894.906250, loss_kl: 188.046967, loss_recon: 156750.968750, loss_pred: 0.452550
iteration 678: loss: 160838.046875, loss_kl: 188.500748, loss_recon: 160660.125000, loss_pred: 0.789927
iteration 679: loss: 162730.609375, loss_kl: 185.298050, loss_recon: 162579.578125, loss_pred: 0.537766
iteration 680: loss: 159160.796875, loss_kl: 187.456894, loss_recon: 158995.375000, loss_pred: 0.670514
iteration 681: loss: 158330.984375, loss_kl: 185.520584, loss_recon: 158117.343750, loss_pred: 1.162773
iteration 682: loss: 160520.937500, loss_kl: 183.311447, loss_recon: 160367.875000, loss_pred: 0.568565
iteration 683: loss: 155305.921875, loss_kl: 190.342850, loss_recon: 155149.062500, loss_pred: 0.569640
iteration 684: loss: 158706.687500, loss_kl: 187.892715, loss_recon: 158558.625000, loss_pred: 0.494465
 38%|██████████▋                 | 76/200 [1:08:43<1:50:44, 53.59s/it]iteration 685: loss: 162488.406250, loss_kl: 186.954193, loss_recon: 162330.125000, loss_pred: 0.527725
iteration 686: loss: 159457.921875, loss_kl: 185.773834, loss_recon: 159321.031250, loss_pred: 0.320409
iteration 687: loss: 160630.250000, loss_kl: 184.997696, loss_recon: 160484.156250, loss_pred: 0.416833
iteration 688: loss: 158696.750000, loss_kl: 181.652435, loss_recon: 158545.703125, loss_pred: 0.485085
iteration 689: loss: 157803.046875, loss_kl: 181.719177, loss_recon: 157632.625000, loss_pred: 0.678590
iteration 690: loss: 156313.625000, loss_kl: 183.791489, loss_recon: 156168.312500, loss_pred: 0.415821
iteration 691: loss: 157478.109375, loss_kl: 187.720795, loss_recon: 157336.265625, loss_pred: 0.358922
iteration 692: loss: 156975.468750, loss_kl: 186.675293, loss_recon: 156825.078125, loss_pred: 0.450291
iteration 693: loss: 160707.765625, loss_kl: 186.860458, loss_recon: 160556.062500, loss_pred: 0.462362
 38%|██████████▊                 | 77/200 [1:09:37<1:49:48, 53.57s/it]iteration 694: loss: 159723.312500, loss_kl: 184.491531, loss_recon: 159566.312500, loss_pred: 0.455628
iteration 695: loss: 155020.078125, loss_kl: 185.479507, loss_recon: 154857.453125, loss_pred: 0.505956
iteration 696: loss: 157122.000000, loss_kl: 184.697433, loss_recon: 156969.687500, loss_pred: 0.407492
iteration 697: loss: 162233.609375, loss_kl: 187.273087, loss_recon: 162082.968750, loss_pred: 0.375312
iteration 698: loss: 158159.734375, loss_kl: 180.267548, loss_recon: 158024.750000, loss_pred: 0.261145
iteration 699: loss: 154811.734375, loss_kl: 184.520630, loss_recon: 154665.906250, loss_pred: 0.343738
iteration 700: loss: 160735.140625, loss_kl: 184.380676, loss_recon: 160589.421875, loss_pred: 0.343660
iteration 701: loss: 163272.812500, loss_kl: 180.783203, loss_recon: 163103.421875, loss_pred: 0.602044
iteration 702: loss: 159610.421875, loss_kl: 181.880417, loss_recon: 159458.671875, loss_pred: 0.418850
 39%|██████████▉                 | 78/200 [1:10:31<1:49:14, 53.73s/it]iteration 703: loss: 156426.062500, loss_kl: 180.753357, loss_recon: 156259.203125, loss_pred: 0.505378
iteration 704: loss: 157776.265625, loss_kl: 181.930435, loss_recon: 157609.328125, loss_pred: 0.498411
iteration 705: loss: 159929.468750, loss_kl: 181.617310, loss_recon: 159779.875000, loss_pred: 0.327008
iteration 706: loss: 162835.046875, loss_kl: 183.906799, loss_recon: 162639.781250, loss_pred: 0.769128
iteration 707: loss: 161368.781250, loss_kl: 182.371185, loss_recon: 161208.312500, loss_pred: 0.430974
iteration 708: loss: 159793.328125, loss_kl: 180.835754, loss_recon: 159624.781250, loss_pred: 0.521493
iteration 709: loss: 158525.031250, loss_kl: 178.706558, loss_recon: 158359.406250, loss_pred: 0.506111
iteration 710: loss: 159593.703125, loss_kl: 180.752319, loss_recon: 159439.937500, loss_pred: 0.374399
iteration 711: loss: 154737.953125, loss_kl: 180.353409, loss_recon: 154563.328125, loss_pred: 0.585428
 40%|███████████                 | 79/200 [1:11:25<1:48:30, 53.80s/it]iteration 712: loss: 157304.937500, loss_kl: 180.327377, loss_recon: 157144.968750, loss_pred: 0.367620
iteration 713: loss: 156797.828125, loss_kl: 181.004883, loss_recon: 156644.281250, loss_pred: 0.298831
iteration 714: loss: 160427.406250, loss_kl: 180.644608, loss_recon: 160252.140625, loss_pred: 0.518390
iteration 715: loss: 157892.015625, loss_kl: 177.342377, loss_recon: 157735.500000, loss_pred: 0.353523
iteration 716: loss: 161024.718750, loss_kl: 177.961777, loss_recon: 160853.796875, loss_pred: 0.493365
iteration 717: loss: 158839.312500, loss_kl: 180.236740, loss_recon: 158663.187500, loss_pred: 0.529798
iteration 718: loss: 159552.546875, loss_kl: 177.738983, loss_recon: 159385.765625, loss_pred: 0.453504
iteration 719: loss: 161473.640625, loss_kl: 176.869614, loss_recon: 161318.953125, loss_pred: 0.338480
iteration 720: loss: 157401.062500, loss_kl: 177.110474, loss_recon: 157204.921875, loss_pred: 0.751466
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_79.pth
 40%|███████████▏                | 80/200 [1:12:19<1:47:34, 53.79s/it]iteration 721: loss: 159117.734375, loss_kl: 178.403961, loss_recon: 158932.031250, loss_pred: 0.567552
iteration 722: loss: 155791.359375, loss_kl: 177.231110, loss_recon: 155623.125000, loss_pred: 0.401262
iteration 723: loss: 160333.421875, loss_kl: 173.109161, loss_recon: 160135.375000, loss_pred: 0.729145
iteration 724: loss: 156633.218750, loss_kl: 178.069443, loss_recon: 156466.000000, loss_pred: 0.385219
iteration 725: loss: 160545.218750, loss_kl: 178.207047, loss_recon: 160335.015625, loss_pred: 0.813905
iteration 726: loss: 162432.031250, loss_kl: 176.961212, loss_recon: 162216.312500, loss_pred: 0.878177
iteration 727: loss: 160100.015625, loss_kl: 175.777618, loss_recon: 159931.062500, loss_pred: 0.419117
iteration 728: loss: 157617.609375, loss_kl: 175.585129, loss_recon: 157418.593750, loss_pred: 0.721087
iteration 729: loss: 158398.437500, loss_kl: 173.144058, loss_recon: 158189.468750, loss_pred: 0.838104
 40%|███████████▎                | 81/200 [1:13:12<1:46:16, 53.58s/it]iteration 730: loss: 160127.734375, loss_kl: 170.981979, loss_recon: 159953.796875, loss_pred: 0.435820
iteration 731: loss: 157585.281250, loss_kl: 173.957123, loss_recon: 157394.859375, loss_pred: 0.578001
iteration 732: loss: 157629.281250, loss_kl: 175.764938, loss_recon: 157461.093750, loss_pred: 0.341892
iteration 733: loss: 156903.406250, loss_kl: 175.407990, loss_recon: 156723.375000, loss_pred: 0.462907
iteration 734: loss: 159888.718750, loss_kl: 174.873734, loss_recon: 159679.218750, loss_pred: 0.761723
iteration 735: loss: 160556.078125, loss_kl: 173.872040, loss_recon: 160392.015625, loss_pred: 0.315036
iteration 736: loss: 160930.203125, loss_kl: 173.284653, loss_recon: 160761.593750, loss_pred: 0.365069
iteration 737: loss: 156740.468750, loss_kl: 171.692886, loss_recon: 156546.250000, loss_pred: 0.633189
iteration 738: loss: 160403.156250, loss_kl: 169.219910, loss_recon: 160231.625000, loss_pred: 0.425190
 41%|███████████▍                | 82/200 [1:14:06<1:45:31, 53.66s/it]iteration 739: loss: 162688.250000, loss_kl: 171.997101, loss_recon: 162523.718750, loss_pred: 0.265981
iteration 740: loss: 157907.390625, loss_kl: 171.705948, loss_recon: 157738.093750, loss_pred: 0.315877
iteration 741: loss: 157265.796875, loss_kl: 171.358124, loss_recon: 157090.562500, loss_pred: 0.378143
iteration 742: loss: 152645.343750, loss_kl: 172.392105, loss_recon: 152467.546875, loss_pred: 0.395357
iteration 743: loss: 161957.281250, loss_kl: 170.919952, loss_recon: 161752.546875, loss_pred: 0.676601
iteration 744: loss: 158388.046875, loss_kl: 172.451447, loss_recon: 158194.140625, loss_pred: 0.555993
iteration 745: loss: 161402.734375, loss_kl: 172.350174, loss_recon: 161227.375000, loss_pred: 0.371473
iteration 746: loss: 159901.609375, loss_kl: 169.555023, loss_recon: 159717.281250, loss_pred: 0.483476
iteration 747: loss: 158593.171875, loss_kl: 168.602097, loss_recon: 158398.593750, loss_pred: 0.593640
 42%|███████████▌                | 83/200 [1:14:59<1:44:35, 53.64s/it]iteration 748: loss: 161363.812500, loss_kl: 169.515320, loss_recon: 161192.890625, loss_pred: 0.282545
iteration 749: loss: 158557.406250, loss_kl: 166.977737, loss_recon: 158381.062500, loss_pred: 0.358132
iteration 750: loss: 152659.281250, loss_kl: 170.689728, loss_recon: 152475.718750, loss_pred: 0.399048
iteration 751: loss: 160189.734375, loss_kl: 167.035690, loss_recon: 160010.843750, loss_pred: 0.383149
iteration 752: loss: 159017.906250, loss_kl: 165.896317, loss_recon: 158849.515625, loss_pred: 0.287632
iteration 753: loss: 162041.406250, loss_kl: 168.222214, loss_recon: 161869.953125, loss_pred: 0.298741
iteration 754: loss: 161312.343750, loss_kl: 169.412109, loss_recon: 161147.937500, loss_pred: 0.218207
iteration 755: loss: 157734.109375, loss_kl: 168.364990, loss_recon: 157563.109375, loss_pred: 0.292957
iteration 756: loss: 157774.140625, loss_kl: 166.637924, loss_recon: 157601.203125, loss_pred: 0.326861
 42%|███████████▊                | 84/200 [1:15:53<1:43:40, 53.63s/it]iteration 757: loss: 157885.265625, loss_kl: 167.740189, loss_recon: 157714.031250, loss_pred: 0.234157
iteration 758: loss: 159659.015625, loss_kl: 166.191010, loss_recon: 159487.578125, loss_pred: 0.249814
iteration 759: loss: 159941.671875, loss_kl: 166.067856, loss_recon: 159764.375000, loss_pred: 0.309550
iteration 760: loss: 161919.656250, loss_kl: 165.445267, loss_recon: 161747.968750, loss_pred: 0.258969
iteration 761: loss: 160757.578125, loss_kl: 164.566986, loss_recon: 160585.343750, loss_pred: 0.272259
iteration 762: loss: 155890.671875, loss_kl: 165.488647, loss_recon: 155717.531250, loss_pred: 0.273145
iteration 763: loss: 159692.703125, loss_kl: 166.217941, loss_recon: 159508.765625, loss_pred: 0.374696
iteration 764: loss: 155441.343750, loss_kl: 166.799789, loss_recon: 155259.406250, loss_pred: 0.349467
iteration 765: loss: 159347.109375, loss_kl: 164.460007, loss_recon: 159166.921875, loss_pred: 0.352705
 42%|███████████▉                | 85/200 [1:16:46<1:42:43, 53.60s/it]iteration 766: loss: 159174.468750, loss_kl: 164.358398, loss_recon: 158992.593750, loss_pred: 0.305265
iteration 767: loss: 161210.921875, loss_kl: 165.428940, loss_recon: 161032.265625, loss_pred: 0.263249
iteration 768: loss: 157308.765625, loss_kl: 163.831924, loss_recon: 157121.921875, loss_pred: 0.359823
iteration 769: loss: 162378.265625, loss_kl: 159.718628, loss_recon: 162200.437500, loss_pred: 0.307722
iteration 770: loss: 160801.968750, loss_kl: 160.599442, loss_recon: 160600.531250, loss_pred: 0.535570
iteration 771: loss: 156744.671875, loss_kl: 162.516785, loss_recon: 156557.593750, loss_pred: 0.374380
iteration 772: loss: 156018.781250, loss_kl: 165.358795, loss_recon: 155837.093750, loss_pred: 0.294219
iteration 773: loss: 157888.890625, loss_kl: 161.581711, loss_recon: 157662.265625, loss_pred: 0.778435
iteration 774: loss: 159272.640625, loss_kl: 162.222183, loss_recon: 159089.625000, loss_pred: 0.336480
 43%|████████████                | 86/200 [1:17:40<1:41:40, 53.51s/it]iteration 775: loss: 159778.031250, loss_kl: 162.244843, loss_recon: 159582.187500, loss_pred: 0.400390
iteration 776: loss: 155864.750000, loss_kl: 160.558121, loss_recon: 155641.281250, loss_pred: 0.692687
iteration 777: loss: 157428.234375, loss_kl: 162.436050, loss_recon: 157238.843750, loss_pred: 0.333931
iteration 778: loss: 160859.687500, loss_kl: 159.757751, loss_recon: 160669.671875, loss_pred: 0.365779
iteration 779: loss: 158541.156250, loss_kl: 160.725327, loss_recon: 158352.109375, loss_pred: 0.346931
iteration 780: loss: 159497.234375, loss_kl: 159.759216, loss_recon: 159312.984375, loss_pred: 0.308146
iteration 781: loss: 158865.312500, loss_kl: 156.101028, loss_recon: 158677.453125, loss_pred: 0.379397
iteration 782: loss: 158509.421875, loss_kl: 159.908432, loss_recon: 158314.687500, loss_pred: 0.411539
iteration 783: loss: 161395.812500, loss_kl: 159.004715, loss_recon: 161212.937500, loss_pred: 0.301736
 44%|████████████▏               | 87/200 [1:18:35<1:42:07, 54.22s/it]iteration 784: loss: 159095.015625, loss_kl: 159.093048, loss_recon: 158908.406250, loss_pred: 0.275128
iteration 785: loss: 158145.203125, loss_kl: 155.539322, loss_recon: 157963.750000, loss_pred: 0.258997
iteration 786: loss: 158717.765625, loss_kl: 156.274857, loss_recon: 158532.015625, loss_pred: 0.294762
iteration 787: loss: 161641.500000, loss_kl: 155.412155, loss_recon: 161458.171875, loss_pred: 0.279164
iteration 788: loss: 158899.187500, loss_kl: 156.666733, loss_recon: 158707.968750, loss_pred: 0.345473
iteration 789: loss: 160409.218750, loss_kl: 155.063843, loss_recon: 160212.765625, loss_pred: 0.413969
iteration 790: loss: 157300.984375, loss_kl: 158.496536, loss_recon: 157100.265625, loss_pred: 0.422251
iteration 791: loss: 156645.875000, loss_kl: 154.347198, loss_recon: 156454.578125, loss_pred: 0.369567
iteration 792: loss: 160298.265625, loss_kl: 153.927765, loss_recon: 160101.781250, loss_pred: 0.425602
 44%|████████████▎               | 88/200 [1:19:29<1:40:58, 54.10s/it]iteration 793: loss: 159364.609375, loss_kl: 154.718948, loss_recon: 159174.656250, loss_pred: 0.352279
iteration 794: loss: 159159.390625, loss_kl: 154.107361, loss_recon: 158961.546875, loss_pred: 0.437361
iteration 795: loss: 160030.593750, loss_kl: 153.704575, loss_recon: 159830.984375, loss_pred: 0.459013
iteration 796: loss: 157582.718750, loss_kl: 153.877991, loss_recon: 157384.265625, loss_pred: 0.445706
iteration 797: loss: 161115.281250, loss_kl: 151.969223, loss_recon: 160937.375000, loss_pred: 0.259338
iteration 798: loss: 159768.796875, loss_kl: 151.015884, loss_recon: 159583.375000, loss_pred: 0.344015
iteration 799: loss: 159127.250000, loss_kl: 154.475052, loss_recon: 158928.203125, loss_pred: 0.445785
iteration 800: loss: 155629.562500, loss_kl: 153.081589, loss_recon: 155445.234375, loss_pred: 0.312535
iteration 801: loss: 158970.671875, loss_kl: 149.270126, loss_recon: 158785.765625, loss_pred: 0.356469
 44%|████████████▍               | 89/200 [1:20:23<1:40:09, 54.14s/it]iteration 802: loss: 162003.843750, loss_kl: 150.373611, loss_recon: 161822.671875, loss_pred: 0.307975
iteration 803: loss: 159360.109375, loss_kl: 150.273102, loss_recon: 159175.890625, loss_pred: 0.339563
iteration 804: loss: 159507.890625, loss_kl: 149.683563, loss_recon: 159312.484375, loss_pred: 0.457128
iteration 805: loss: 158311.140625, loss_kl: 149.478668, loss_recon: 158117.375000, loss_pred: 0.442755
iteration 806: loss: 157735.953125, loss_kl: 151.203293, loss_recon: 157559.937500, loss_pred: 0.248086
iteration 807: loss: 161127.218750, loss_kl: 147.996338, loss_recon: 160924.843750, loss_pred: 0.543707
iteration 808: loss: 155337.656250, loss_kl: 150.661072, loss_recon: 155155.984375, loss_pred: 0.310196
iteration 809: loss: 159365.875000, loss_kl: 147.130630, loss_recon: 159183.171875, loss_pred: 0.355859
iteration 810: loss: 158050.750000, loss_kl: 147.921600, loss_recon: 157875.968750, loss_pred: 0.268613
 45%|████████████▌               | 90/200 [1:21:17<1:38:40, 53.82s/it]iteration 811: loss: 158293.562500, loss_kl: 148.894379, loss_recon: 158106.421875, loss_pred: 0.382518
iteration 812: loss: 161731.625000, loss_kl: 148.745834, loss_recon: 161548.171875, loss_pred: 0.347005
iteration 813: loss: 158074.203125, loss_kl: 147.827972, loss_recon: 157888.687500, loss_pred: 0.376874
iteration 814: loss: 159339.343750, loss_kl: 146.082123, loss_recon: 159141.765625, loss_pred: 0.514991
iteration 815: loss: 161288.375000, loss_kl: 146.149246, loss_recon: 161114.796875, loss_pred: 0.274206
iteration 816: loss: 157005.515625, loss_kl: 148.759064, loss_recon: 156820.921875, loss_pred: 0.358210
iteration 817: loss: 158776.906250, loss_kl: 147.551483, loss_recon: 158567.828125, loss_pred: 0.615352
iteration 818: loss: 156811.593750, loss_kl: 146.114975, loss_recon: 156633.921875, loss_pred: 0.315611
iteration 819: loss: 159815.078125, loss_kl: 145.884781, loss_recon: 159630.546875, loss_pred: 0.386400
 46%|████████████▋               | 91/200 [1:22:10<1:37:37, 53.74s/it]iteration 820: loss: 155340.625000, loss_kl: 146.323975, loss_recon: 155119.406250, loss_pred: 0.748919
iteration 821: loss: 160363.531250, loss_kl: 146.067261, loss_recon: 160188.765625, loss_pred: 0.287091
iteration 822: loss: 157857.843750, loss_kl: 146.285233, loss_recon: 157642.546875, loss_pred: 0.690157
iteration 823: loss: 158350.890625, loss_kl: 143.197296, loss_recon: 158148.921875, loss_pred: 0.587714
iteration 824: loss: 160442.343750, loss_kl: 144.079361, loss_recon: 160255.125000, loss_pred: 0.431373
iteration 825: loss: 161344.671875, loss_kl: 140.927872, loss_recon: 161160.406250, loss_pred: 0.433468
iteration 826: loss: 159889.390625, loss_kl: 142.348221, loss_recon: 159681.625000, loss_pred: 0.654167
iteration 827: loss: 156912.531250, loss_kl: 142.182281, loss_recon: 156718.781250, loss_pred: 0.515607
iteration 828: loss: 160634.125000, loss_kl: 143.133377, loss_recon: 160465.859375, loss_pred: 0.251225
 46%|████████████▉               | 92/200 [1:23:04<1:36:50, 53.80s/it]iteration 829: loss: 160576.109375, loss_kl: 141.693970, loss_recon: 160388.859375, loss_pred: 0.455671
iteration 830: loss: 156788.468750, loss_kl: 144.209824, loss_recon: 156606.750000, loss_pred: 0.375127
iteration 831: loss: 159647.171875, loss_kl: 141.919937, loss_recon: 159478.359375, loss_pred: 0.268923
iteration 832: loss: 156346.203125, loss_kl: 142.781357, loss_recon: 156143.453125, loss_pred: 0.599740
iteration 833: loss: 160938.796875, loss_kl: 140.495819, loss_recon: 160753.406250, loss_pred: 0.448902
iteration 834: loss: 156440.890625, loss_kl: 140.705978, loss_recon: 156268.390625, loss_pred: 0.317916
iteration 835: loss: 157815.421875, loss_kl: 139.960892, loss_recon: 157640.515625, loss_pred: 0.349498
iteration 836: loss: 159741.546875, loss_kl: 138.203751, loss_recon: 159545.921875, loss_pred: 0.574274
iteration 837: loss: 162223.015625, loss_kl: 139.390884, loss_recon: 162056.890625, loss_pred: 0.267300
 46%|█████████████               | 93/200 [1:23:58<1:35:45, 53.69s/it]iteration 838: loss: 158529.734375, loss_kl: 138.091721, loss_recon: 158354.015625, loss_pred: 0.376327
iteration 839: loss: 159310.265625, loss_kl: 138.192734, loss_recon: 159115.500000, loss_pred: 0.565727
iteration 840: loss: 157918.093750, loss_kl: 137.958542, loss_recon: 157732.843750, loss_pred: 0.472910
iteration 841: loss: 160050.312500, loss_kl: 139.806030, loss_recon: 159865.000000, loss_pred: 0.454931
iteration 842: loss: 159320.343750, loss_kl: 136.563980, loss_recon: 159129.250000, loss_pred: 0.545344
iteration 843: loss: 159534.562500, loss_kl: 138.591187, loss_recon: 159370.843750, loss_pred: 0.251249
iteration 844: loss: 158013.578125, loss_kl: 138.251984, loss_recon: 157845.140625, loss_pred: 0.301852
iteration 845: loss: 158617.875000, loss_kl: 138.530106, loss_recon: 158432.093750, loss_pred: 0.472436
iteration 846: loss: 159298.593750, loss_kl: 136.493866, loss_recon: 159133.750000, loss_pred: 0.283433
 47%|█████████████▏              | 94/200 [1:24:51<1:34:58, 53.76s/it]iteration 847: loss: 158015.296875, loss_kl: 136.204865, loss_recon: 157844.750000, loss_pred: 0.343434
iteration 848: loss: 157999.640625, loss_kl: 135.691742, loss_recon: 157831.953125, loss_pred: 0.319960
iteration 849: loss: 160374.984375, loss_kl: 135.389755, loss_recon: 160205.375000, loss_pred: 0.342160
iteration 850: loss: 160354.953125, loss_kl: 138.405396, loss_recon: 160181.687500, loss_pred: 0.348665
iteration 851: loss: 158150.546875, loss_kl: 135.071213, loss_recon: 157970.265625, loss_pred: 0.451991
iteration 852: loss: 159949.156250, loss_kl: 135.093842, loss_recon: 159785.937500, loss_pred: 0.281174
iteration 853: loss: 160420.093750, loss_kl: 133.500183, loss_recon: 160234.875000, loss_pred: 0.517185
iteration 854: loss: 157334.765625, loss_kl: 134.454468, loss_recon: 157176.859375, loss_pred: 0.234547
iteration 855: loss: 158081.703125, loss_kl: 132.746597, loss_recon: 157915.171875, loss_pred: 0.337780
 48%|█████████████▎              | 95/200 [1:25:45<1:34:06, 53.78s/it]iteration 856: loss: 163227.031250, loss_kl: 131.513916, loss_recon: 163070.171875, loss_pred: 0.253439
iteration 857: loss: 158421.625000, loss_kl: 130.792755, loss_recon: 158248.843750, loss_pred: 0.419921
iteration 858: loss: 159813.062500, loss_kl: 131.851227, loss_recon: 159656.000000, loss_pred: 0.252241
iteration 859: loss: 158585.593750, loss_kl: 131.758118, loss_recon: 158424.921875, loss_pred: 0.289020
iteration 860: loss: 157796.281250, loss_kl: 132.348373, loss_recon: 157638.984375, loss_pred: 0.249506
iteration 861: loss: 156025.578125, loss_kl: 132.613266, loss_recon: 155863.468750, loss_pred: 0.294943
iteration 862: loss: 161231.343750, loss_kl: 131.301926, loss_recon: 161069.062500, loss_pred: 0.309777
iteration 863: loss: 157420.328125, loss_kl: 131.422226, loss_recon: 157254.781250, loss_pred: 0.341193
iteration 864: loss: 157951.671875, loss_kl: 132.010880, loss_recon: 157786.171875, loss_pred: 0.334766
 48%|█████████████▍              | 96/200 [1:26:38<1:32:50, 53.57s/it]iteration 865: loss: 161866.750000, loss_kl: 128.038010, loss_recon: 161712.265625, loss_pred: 0.264526
iteration 866: loss: 156579.156250, loss_kl: 128.912262, loss_recon: 156412.515625, loss_pred: 0.377313
iteration 867: loss: 159782.140625, loss_kl: 126.787888, loss_recon: 159612.140625, loss_pred: 0.432215
iteration 868: loss: 158757.500000, loss_kl: 130.350861, loss_recon: 158604.062500, loss_pred: 0.230951
iteration 869: loss: 160765.500000, loss_kl: 131.374054, loss_recon: 160599.859375, loss_pred: 0.342722
iteration 870: loss: 155673.890625, loss_kl: 129.146149, loss_recon: 155516.968750, loss_pred: 0.277847
iteration 871: loss: 159569.171875, loss_kl: 128.681366, loss_recon: 159402.937500, loss_pred: 0.375393
iteration 872: loss: 157319.375000, loss_kl: 128.245163, loss_recon: 157162.171875, loss_pred: 0.289471
iteration 873: loss: 159661.375000, loss_kl: 125.898018, loss_recon: 159508.171875, loss_pred: 0.273170
 48%|█████████████▌              | 97/200 [1:27:32<1:31:51, 53.51s/it]iteration 874: loss: 156884.484375, loss_kl: 127.680496, loss_recon: 156718.078125, loss_pred: 0.387118
iteration 875: loss: 158687.453125, loss_kl: 125.890182, loss_recon: 158539.031250, loss_pred: 0.225312
iteration 876: loss: 159073.265625, loss_kl: 125.467049, loss_recon: 158925.187500, loss_pred: 0.226024
iteration 877: loss: 157604.281250, loss_kl: 126.323914, loss_recon: 157447.125000, loss_pred: 0.308234
iteration 878: loss: 161281.687500, loss_kl: 125.060883, loss_recon: 161134.546875, loss_pred: 0.220784
iteration 879: loss: 158449.734375, loss_kl: 124.772125, loss_recon: 158298.171875, loss_pred: 0.267894
iteration 880: loss: 161455.609375, loss_kl: 127.851761, loss_recon: 161300.718750, loss_pred: 0.270280
iteration 881: loss: 159011.671875, loss_kl: 126.365524, loss_recon: 158818.656250, loss_pred: 0.666601
iteration 882: loss: 157956.046875, loss_kl: 126.901077, loss_recon: 157804.406250, loss_pred: 0.247393
 49%|█████████████▋              | 98/200 [1:28:25<1:31:00, 53.54s/it]iteration 883: loss: 158831.250000, loss_kl: 125.978600, loss_recon: 158668.953125, loss_pred: 0.363111
iteration 884: loss: 162364.750000, loss_kl: 124.681610, loss_recon: 162197.171875, loss_pred: 0.428976
iteration 885: loss: 158384.562500, loss_kl: 123.178070, loss_recon: 158206.796875, loss_pred: 0.545927
iteration 886: loss: 154907.109375, loss_kl: 124.623924, loss_recon: 154760.140625, loss_pred: 0.223481
iteration 887: loss: 160629.296875, loss_kl: 124.915443, loss_recon: 160450.109375, loss_pred: 0.542639
iteration 888: loss: 158161.328125, loss_kl: 124.434937, loss_recon: 157997.062500, loss_pred: 0.398353
iteration 889: loss: 161305.812500, loss_kl: 124.951645, loss_recon: 161147.531250, loss_pred: 0.333321
iteration 890: loss: 161858.796875, loss_kl: 124.266815, loss_recon: 161697.265625, loss_pred: 0.372687
iteration 891: loss: 153790.625000, loss_kl: 122.789635, loss_recon: 153611.625000, loss_pred: 0.561996
 50%|█████████████▊              | 99/200 [1:29:18<1:29:56, 53.43s/it]iteration 892: loss: 157630.656250, loss_kl: 124.267548, loss_recon: 157485.406250, loss_pred: 0.209893
iteration 893: loss: 159634.875000, loss_kl: 122.716850, loss_recon: 159455.281250, loss_pred: 0.568715
iteration 894: loss: 157972.671875, loss_kl: 123.745354, loss_recon: 157824.531250, loss_pred: 0.243895
iteration 895: loss: 159528.500000, loss_kl: 120.430389, loss_recon: 159376.562500, loss_pred: 0.315021
iteration 896: loss: 154688.171875, loss_kl: 123.731094, loss_recon: 154522.515625, loss_pred: 0.419199
iteration 897: loss: 161682.859375, loss_kl: 124.258514, loss_recon: 161529.718750, loss_pred: 0.288692
iteration 898: loss: 161143.015625, loss_kl: 119.239944, loss_recon: 160995.265625, loss_pred: 0.285193
iteration 899: loss: 157444.671875, loss_kl: 120.437462, loss_recon: 157298.328125, loss_pred: 0.259007
iteration 900: loss: 160964.171875, loss_kl: 117.807465, loss_recon: 160815.437500, loss_pred: 0.309241
 50%|█████████████▌             | 100/200 [1:30:13<1:29:22, 53.63s/it]iteration 901: loss: 157199.062500, loss_kl: 119.932663, loss_recon: 157175.093750, loss_pred: 0.227593
iteration 902: loss: 160037.046875, loss_kl: 120.204811, loss_recon: 159998.968750, loss_pred: 0.368795
iteration 903: loss: 157798.484375, loss_kl: 121.675781, loss_recon: 157763.093750, loss_pred: 0.341725
iteration 904: loss: 160627.046875, loss_kl: 119.378990, loss_recon: 160595.687500, loss_pred: 0.301688
iteration 905: loss: 158327.234375, loss_kl: 121.183235, loss_recon: 158268.406250, loss_pred: 0.576065
iteration 906: loss: 161604.140625, loss_kl: 118.809319, loss_recon: 161556.921875, loss_pred: 0.460280
iteration 907: loss: 157400.484375, loss_kl: 119.379280, loss_recon: 157372.343750, loss_pred: 0.269519
iteration 908: loss: 156253.796875, loss_kl: 121.861610, loss_recon: 156211.953125, loss_pred: 0.406269
iteration 909: loss: 159814.109375, loss_kl: 118.255524, loss_recon: 159777.140625, loss_pred: 0.357823
 50%|█████████████▋             | 101/200 [1:31:06<1:28:35, 53.69s/it]iteration 910: loss: 156942.625000, loss_kl: 119.734543, loss_recon: 156903.937500, loss_pred: 0.374788
iteration 911: loss: 161134.750000, loss_kl: 116.545799, loss_recon: 161112.921875, loss_pred: 0.206595
iteration 912: loss: 157916.437500, loss_kl: 118.705299, loss_recon: 157887.218750, loss_pred: 0.280268
iteration 913: loss: 158923.484375, loss_kl: 119.628265, loss_recon: 158897.296875, loss_pred: 0.249885
iteration 914: loss: 158172.765625, loss_kl: 117.919060, loss_recon: 158144.031250, loss_pred: 0.275674
iteration 915: loss: 156859.703125, loss_kl: 120.031502, loss_recon: 156826.421875, loss_pred: 0.320759
iteration 916: loss: 158279.125000, loss_kl: 116.702370, loss_recon: 158255.062500, loss_pred: 0.228893
iteration 917: loss: 159758.437500, loss_kl: 118.413788, loss_recon: 159734.359375, loss_pred: 0.228892
iteration 918: loss: 160915.078125, loss_kl: 120.067886, loss_recon: 160894.312500, loss_pred: 0.195658
 51%|█████████████▊             | 102/200 [1:32:00<1:27:38, 53.65s/it]iteration 919: loss: 157525.437500, loss_kl: 120.104828, loss_recon: 157500.921875, loss_pred: 0.233157
iteration 920: loss: 158925.843750, loss_kl: 121.742607, loss_recon: 158893.703125, loss_pred: 0.309179
iteration 921: loss: 159591.765625, loss_kl: 119.402901, loss_recon: 159558.859375, loss_pred: 0.317154
iteration 922: loss: 156707.937500, loss_kl: 118.348480, loss_recon: 156670.796875, loss_pred: 0.359531
iteration 923: loss: 158266.187500, loss_kl: 118.720398, loss_recon: 158230.546875, loss_pred: 0.344604
iteration 924: loss: 157557.687500, loss_kl: 120.916016, loss_recon: 157515.593750, loss_pred: 0.408911
iteration 925: loss: 158658.625000, loss_kl: 119.740921, loss_recon: 158624.890625, loss_pred: 0.325245
iteration 926: loss: 161102.984375, loss_kl: 121.454758, loss_recon: 161073.234375, loss_pred: 0.285298
iteration 927: loss: 160857.453125, loss_kl: 119.231178, loss_recon: 160810.171875, loss_pred: 0.460875
 52%|█████████████▉             | 103/200 [1:32:53<1:26:31, 53.52s/it]iteration 928: loss: 158863.390625, loss_kl: 119.836578, loss_recon: 158809.015625, loss_pred: 0.531737
iteration 929: loss: 159154.390625, loss_kl: 122.220589, loss_recon: 159128.578125, loss_pred: 0.245990
iteration 930: loss: 160164.703125, loss_kl: 119.110046, loss_recon: 160141.765625, loss_pred: 0.217462
iteration 931: loss: 160225.062500, loss_kl: 117.607521, loss_recon: 160189.062500, loss_pred: 0.348343
iteration 932: loss: 159382.265625, loss_kl: 118.324524, loss_recon: 159355.828125, loss_pred: 0.252431
iteration 933: loss: 159419.750000, loss_kl: 120.823326, loss_recon: 159385.968750, loss_pred: 0.325707
iteration 934: loss: 158397.921875, loss_kl: 119.245201, loss_recon: 158364.343750, loss_pred: 0.323887
iteration 935: loss: 155903.640625, loss_kl: 120.045563, loss_recon: 155879.359375, loss_pred: 0.230769
iteration 936: loss: 157698.156250, loss_kl: 121.682884, loss_recon: 157664.265625, loss_pred: 0.326679
 52%|██████████████             | 104/200 [1:33:47<1:25:38, 53.53s/it]iteration 937: loss: 159713.765625, loss_kl: 122.331001, loss_recon: 159685.593750, loss_pred: 0.269545
iteration 938: loss: 155526.109375, loss_kl: 122.069000, loss_recon: 155475.656250, loss_pred: 0.492301
iteration 939: loss: 157562.046875, loss_kl: 122.839088, loss_recon: 157536.500000, loss_pred: 0.243049
iteration 940: loss: 157649.781250, loss_kl: 122.351578, loss_recon: 157620.296875, loss_pred: 0.282696
iteration 941: loss: 158297.531250, loss_kl: 121.186943, loss_recon: 158270.062500, loss_pred: 0.262517
iteration 942: loss: 160064.109375, loss_kl: 122.019432, loss_recon: 160039.765625, loss_pred: 0.231292
iteration 943: loss: 160887.343750, loss_kl: 121.155930, loss_recon: 160864.859375, loss_pred: 0.212693
iteration 944: loss: 160347.671875, loss_kl: 122.030624, loss_recon: 160324.750000, loss_pred: 0.217024
iteration 945: loss: 158893.000000, loss_kl: 123.385857, loss_recon: 158869.093750, loss_pred: 0.226768
 52%|██████████████▏            | 105/200 [1:34:40<1:24:29, 53.37s/it]iteration 946: loss: 155090.890625, loss_kl: 122.370171, loss_recon: 155067.828125, loss_pred: 0.218393
iteration 947: loss: 162541.796875, loss_kl: 121.095985, loss_recon: 162512.421875, loss_pred: 0.281514
iteration 948: loss: 157720.593750, loss_kl: 123.904297, loss_recon: 157693.093750, loss_pred: 0.262587
iteration 949: loss: 157911.937500, loss_kl: 123.388420, loss_recon: 157884.015625, loss_pred: 0.266831
iteration 950: loss: 159654.265625, loss_kl: 122.447517, loss_recon: 159615.343750, loss_pred: 0.377058
iteration 951: loss: 156452.703125, loss_kl: 122.856796, loss_recon: 156425.093750, loss_pred: 0.263702
iteration 952: loss: 158580.906250, loss_kl: 122.760071, loss_recon: 158553.578125, loss_pred: 0.260949
iteration 953: loss: 161937.765625, loss_kl: 123.868752, loss_recon: 161914.687500, loss_pred: 0.218390
iteration 954: loss: 159182.609375, loss_kl: 122.293442, loss_recon: 159156.546875, loss_pred: 0.248457
 53%|██████████████▎            | 106/200 [1:35:34<1:23:52, 53.53s/it]iteration 955: loss: 158008.140625, loss_kl: 123.290764, loss_recon: 157983.578125, loss_pred: 0.233216
iteration 956: loss: 159865.421875, loss_kl: 121.671974, loss_recon: 159838.515625, loss_pred: 0.256868
iteration 957: loss: 155240.250000, loss_kl: 123.193665, loss_recon: 155201.890625, loss_pred: 0.371281
iteration 958: loss: 159737.937500, loss_kl: 123.347450, loss_recon: 159717.703125, loss_pred: 0.189996
iteration 959: loss: 160210.906250, loss_kl: 123.332802, loss_recon: 160148.359375, loss_pred: 0.613054
iteration 960: loss: 158736.687500, loss_kl: 124.370399, loss_recon: 158712.062500, loss_pred: 0.233774
iteration 961: loss: 157358.781250, loss_kl: 124.174667, loss_recon: 157313.578125, loss_pred: 0.439659
iteration 962: loss: 159762.765625, loss_kl: 122.221817, loss_recon: 159725.421875, loss_pred: 0.361299
iteration 963: loss: 159937.875000, loss_kl: 122.780762, loss_recon: 159914.218750, loss_pred: 0.224145
 54%|██████████████▍            | 107/200 [1:36:28<1:23:09, 53.65s/it]iteration 964: loss: 161464.796875, loss_kl: 122.719353, loss_recon: 161438.171875, loss_pred: 0.253899
iteration 965: loss: 159243.437500, loss_kl: 124.677261, loss_recon: 159219.921875, loss_pred: 0.222718
iteration 966: loss: 158705.515625, loss_kl: 124.331512, loss_recon: 158679.562500, loss_pred: 0.247042
iteration 967: loss: 158178.375000, loss_kl: 124.442024, loss_recon: 158151.390625, loss_pred: 0.257307
iteration 968: loss: 159871.828125, loss_kl: 124.684181, loss_recon: 159850.046875, loss_pred: 0.205275
iteration 969: loss: 158098.125000, loss_kl: 123.204636, loss_recon: 158062.015625, loss_pred: 0.348733
iteration 970: loss: 159045.734375, loss_kl: 123.654968, loss_recon: 159015.375000, loss_pred: 0.291214
iteration 971: loss: 158404.515625, loss_kl: 124.669113, loss_recon: 158369.343750, loss_pred: 0.339166
iteration 972: loss: 155752.406250, loss_kl: 124.242844, loss_recon: 155725.796875, loss_pred: 0.253583
 54%|██████████████▌            | 108/200 [1:37:21<1:22:09, 53.58s/it]iteration 973: loss: 163600.078125, loss_kl: 122.673660, loss_recon: 163572.625000, loss_pred: 0.262243
iteration 974: loss: 159237.109375, loss_kl: 126.402542, loss_recon: 159211.031250, loss_pred: 0.248106
iteration 975: loss: 158853.109375, loss_kl: 126.629128, loss_recon: 158825.265625, loss_pred: 0.265772
iteration 976: loss: 156656.640625, loss_kl: 127.214638, loss_recon: 156624.937500, loss_pred: 0.304413
iteration 977: loss: 159942.968750, loss_kl: 123.791016, loss_recon: 159915.734375, loss_pred: 0.260027
iteration 978: loss: 158949.234375, loss_kl: 123.514954, loss_recon: 158915.750000, loss_pred: 0.322505
iteration 979: loss: 157403.609375, loss_kl: 125.010910, loss_recon: 157358.750000, loss_pred: 0.436133
iteration 980: loss: 158105.218750, loss_kl: 124.813896, loss_recon: 158083.203125, loss_pred: 0.207639
iteration 981: loss: 155945.718750, loss_kl: 126.111748, loss_recon: 155913.093750, loss_pred: 0.313562
 55%|██████████████▋            | 109/200 [1:38:15<1:21:28, 53.72s/it]iteration 982: loss: 157223.171875, loss_kl: 126.730133, loss_recon: 157193.140625, loss_pred: 0.287729
iteration 983: loss: 161145.046875, loss_kl: 124.224190, loss_recon: 161121.750000, loss_pred: 0.220471
iteration 984: loss: 154788.328125, loss_kl: 126.447044, loss_recon: 154737.140625, loss_pred: 0.499255
iteration 985: loss: 158291.171875, loss_kl: 125.047646, loss_recon: 158263.734375, loss_pred: 0.261865
iteration 986: loss: 159788.640625, loss_kl: 125.255455, loss_recon: 159756.593750, loss_pred: 0.307932
iteration 987: loss: 160192.687500, loss_kl: 126.650757, loss_recon: 160168.484375, loss_pred: 0.229395
iteration 988: loss: 160468.562500, loss_kl: 125.020370, loss_recon: 160441.453125, loss_pred: 0.258543
iteration 989: loss: 156400.968750, loss_kl: 128.178299, loss_recon: 156366.953125, loss_pred: 0.327367
iteration 990: loss: 160461.671875, loss_kl: 123.242912, loss_recon: 160434.625000, loss_pred: 0.258161
 55%|██████████████▊            | 110/200 [1:39:09<1:20:45, 53.83s/it]iteration 991: loss: 156341.984375, loss_kl: 127.820511, loss_recon: 156300.140625, loss_pred: 0.405609
iteration 992: loss: 159773.218750, loss_kl: 125.014008, loss_recon: 159736.531250, loss_pred: 0.354321
iteration 993: loss: 158132.171875, loss_kl: 127.335579, loss_recon: 158098.593750, loss_pred: 0.323089
iteration 994: loss: 156265.703125, loss_kl: 130.035248, loss_recon: 156236.765625, loss_pred: 0.276342
iteration 995: loss: 158840.750000, loss_kl: 127.201202, loss_recon: 158794.984375, loss_pred: 0.444975
iteration 996: loss: 159609.968750, loss_kl: 128.577408, loss_recon: 159573.750000, loss_pred: 0.349419
iteration 997: loss: 157348.109375, loss_kl: 127.027260, loss_recon: 157317.984375, loss_pred: 0.288565
iteration 998: loss: 160824.906250, loss_kl: 125.439941, loss_recon: 160789.203125, loss_pred: 0.344591
iteration 999: loss: 161482.968750, loss_kl: 127.158020, loss_recon: 161457.359375, loss_pred: 0.243411
 56%|██████████████▉            | 111/200 [1:40:02<1:19:31, 53.61s/it]iteration 1000: loss: 159742.359375, loss_kl: 126.592468, loss_recon: 159716.281250, loss_pred: 0.248150
iteration 1001: loss: 156193.640625, loss_kl: 129.423218, loss_recon: 156165.140625, loss_pred: 0.272096
iteration 1002: loss: 157287.953125, loss_kl: 127.444794, loss_recon: 157259.656250, loss_pred: 0.270131
iteration 1003: loss: 155279.625000, loss_kl: 130.689896, loss_recon: 155245.625000, loss_pred: 0.326943
iteration 1004: loss: 157873.078125, loss_kl: 130.346741, loss_recon: 157835.718750, loss_pred: 0.360581
iteration 1005: loss: 159393.109375, loss_kl: 130.110489, loss_recon: 159352.156250, loss_pred: 0.396568
iteration 1006: loss: 159420.359375, loss_kl: 128.745697, loss_recon: 159391.203125, loss_pred: 0.278786
iteration 1007: loss: 161720.953125, loss_kl: 127.196510, loss_recon: 161682.515625, loss_pred: 0.371775
iteration 1008: loss: 162097.078125, loss_kl: 125.518860, loss_recon: 162043.765625, loss_pred: 0.520671
 56%|███████████████            | 112/200 [1:40:56<1:18:48, 53.73s/it]iteration 1009: loss: 159038.265625, loss_kl: 126.231613, loss_recon: 159007.203125, loss_pred: 0.297898
iteration 1010: loss: 159460.125000, loss_kl: 126.909988, loss_recon: 159407.875000, loss_pred: 0.509782
iteration 1011: loss: 156824.546875, loss_kl: 132.007584, loss_recon: 156775.968750, loss_pred: 0.472669
iteration 1012: loss: 157611.984375, loss_kl: 130.079163, loss_recon: 157589.890625, loss_pred: 0.207994
iteration 1013: loss: 159790.359375, loss_kl: 129.229736, loss_recon: 159754.734375, loss_pred: 0.343308
iteration 1014: loss: 157714.328125, loss_kl: 130.730072, loss_recon: 157676.437500, loss_pred: 0.365817
iteration 1015: loss: 159526.125000, loss_kl: 128.746429, loss_recon: 159499.515625, loss_pred: 0.253285
iteration 1016: loss: 159878.875000, loss_kl: 127.136108, loss_recon: 159840.515625, loss_pred: 0.370898
iteration 1017: loss: 159609.531250, loss_kl: 130.068314, loss_recon: 159564.828125, loss_pred: 0.434034
 56%|███████████████▎           | 113/200 [1:41:49<1:17:33, 53.48s/it]iteration 1018: loss: 158713.968750, loss_kl: 130.770844, loss_recon: 158667.765625, loss_pred: 0.397197
iteration 1019: loss: 158712.000000, loss_kl: 130.938797, loss_recon: 158679.281250, loss_pred: 0.262218
iteration 1020: loss: 157365.781250, loss_kl: 129.253769, loss_recon: 157298.203125, loss_pred: 0.611741
iteration 1021: loss: 158911.984375, loss_kl: 131.393204, loss_recon: 158859.734375, loss_pred: 0.457290
iteration 1022: loss: 161871.515625, loss_kl: 129.107819, loss_recon: 161833.171875, loss_pred: 0.319380
iteration 1023: loss: 159128.390625, loss_kl: 128.166077, loss_recon: 159078.531250, loss_pred: 0.434965
iteration 1024: loss: 161911.562500, loss_kl: 129.221176, loss_recon: 161858.406250, loss_pred: 0.467501
iteration 1025: loss: 155449.640625, loss_kl: 128.240814, loss_recon: 155412.171875, loss_pred: 0.311138
iteration 1026: loss: 157291.781250, loss_kl: 132.583038, loss_recon: 157253.578125, loss_pred: 0.316269
 57%|███████████████▍           | 114/200 [1:42:42<1:16:29, 53.36s/it]iteration 1027: loss: 160312.343750, loss_kl: 130.924103, loss_recon: 160255.515625, loss_pred: 0.451528
iteration 1028: loss: 158487.906250, loss_kl: 129.854507, loss_recon: 158446.234375, loss_pred: 0.300872
iteration 1029: loss: 156660.000000, loss_kl: 132.296646, loss_recon: 156610.187500, loss_pred: 0.380090
iteration 1030: loss: 159782.781250, loss_kl: 130.944092, loss_recon: 159750.562500, loss_pred: 0.205271
iteration 1031: loss: 158437.609375, loss_kl: 128.713882, loss_recon: 158393.171875, loss_pred: 0.329459
iteration 1032: loss: 160442.765625, loss_kl: 129.665405, loss_recon: 160381.750000, loss_pred: 0.494479
iteration 1033: loss: 160457.718750, loss_kl: 129.743500, loss_recon: 160407.093750, loss_pred: 0.390527
iteration 1034: loss: 154919.437500, loss_kl: 132.193161, loss_recon: 154872.859375, loss_pred: 0.347881
iteration 1035: loss: 160007.906250, loss_kl: 131.377731, loss_recon: 159934.453125, loss_pred: 0.617347
 57%|███████████████▌           | 115/200 [1:43:35<1:15:28, 53.27s/it]iteration 1036: loss: 161257.468750, loss_kl: 132.799301, loss_recon: 161211.093750, loss_pred: 0.292614
iteration 1037: loss: 160663.593750, loss_kl: 131.283432, loss_recon: 160620.390625, loss_pred: 0.262939
iteration 1038: loss: 160314.218750, loss_kl: 133.039001, loss_recon: 160243.968750, loss_pred: 0.531068
iteration 1039: loss: 157233.906250, loss_kl: 133.210022, loss_recon: 157169.609375, loss_pred: 0.471435
iteration 1040: loss: 159619.484375, loss_kl: 132.976166, loss_recon: 159570.703125, loss_pred: 0.316614
iteration 1041: loss: 155844.109375, loss_kl: 134.068344, loss_recon: 155791.828125, loss_pred: 0.350210
iteration 1042: loss: 154993.015625, loss_kl: 132.088531, loss_recon: 154936.750000, loss_pred: 0.392564
iteration 1043: loss: 160330.187500, loss_kl: 133.704269, loss_recon: 160278.921875, loss_pred: 0.340453
iteration 1044: loss: 159062.406250, loss_kl: 133.681366, loss_recon: 159014.468750, loss_pred: 0.307156
 58%|███████████████▋           | 116/200 [1:44:29<1:14:44, 53.39s/it]iteration 1045: loss: 158342.796875, loss_kl: 131.513092, loss_recon: 158286.281250, loss_pred: 0.343760
iteration 1046: loss: 159047.406250, loss_kl: 132.672516, loss_recon: 158995.796875, loss_pred: 0.292591
iteration 1047: loss: 158257.859375, loss_kl: 133.232010, loss_recon: 158199.593750, loss_pred: 0.358275
iteration 1048: loss: 156994.031250, loss_kl: 132.554520, loss_recon: 156943.140625, loss_pred: 0.285679
iteration 1049: loss: 159307.140625, loss_kl: 133.941650, loss_recon: 159254.312500, loss_pred: 0.302582
iteration 1050: loss: 156893.531250, loss_kl: 132.178024, loss_recon: 156829.171875, loss_pred: 0.420962
iteration 1051: loss: 159587.203125, loss_kl: 134.090851, loss_recon: 159539.937500, loss_pred: 0.246908
iteration 1052: loss: 159870.328125, loss_kl: 134.075882, loss_recon: 159824.281250, loss_pred: 0.234747
iteration 1053: loss: 160803.187500, loss_kl: 131.954849, loss_recon: 160751.781250, loss_pred: 0.291947
 58%|███████████████▊           | 117/200 [1:45:22<1:13:52, 53.40s/it]iteration 1054: loss: 158903.437500, loss_kl: 132.001175, loss_recon: 158844.718750, loss_pred: 0.312709
iteration 1055: loss: 158149.343750, loss_kl: 133.297623, loss_recon: 158087.843750, loss_pred: 0.337860
iteration 1056: loss: 154377.781250, loss_kl: 134.405014, loss_recon: 154314.093750, loss_pred: 0.357322
iteration 1057: loss: 156096.562500, loss_kl: 135.437271, loss_recon: 156038.656250, loss_pred: 0.297350
iteration 1058: loss: 160439.671875, loss_kl: 133.186630, loss_recon: 160375.453125, loss_pred: 0.365166
iteration 1059: loss: 160913.875000, loss_kl: 133.924393, loss_recon: 160865.093750, loss_pred: 0.209223
iteration 1060: loss: 155925.796875, loss_kl: 132.604919, loss_recon: 155843.921875, loss_pred: 0.543015
iteration 1061: loss: 163569.453125, loss_kl: 133.820175, loss_recon: 163508.890625, loss_pred: 0.327346
iteration 1062: loss: 160935.906250, loss_kl: 131.761688, loss_recon: 160869.140625, loss_pred: 0.393644
 59%|███████████████▉           | 118/200 [1:46:16<1:12:59, 53.41s/it]iteration 1063: loss: 157401.218750, loss_kl: 133.932617, loss_recon: 157348.234375, loss_pred: 0.198267
iteration 1064: loss: 156207.218750, loss_kl: 135.571564, loss_recon: 156138.625000, loss_pred: 0.350319
iteration 1065: loss: 159724.890625, loss_kl: 132.850647, loss_recon: 159628.234375, loss_pred: 0.637620
iteration 1066: loss: 160543.234375, loss_kl: 132.862778, loss_recon: 160459.796875, loss_pred: 0.505482
iteration 1067: loss: 161847.500000, loss_kl: 128.865341, loss_recon: 161787.921875, loss_pred: 0.276703
iteration 1068: loss: 159384.093750, loss_kl: 131.827774, loss_recon: 159303.109375, loss_pred: 0.483438
iteration 1069: loss: 157217.453125, loss_kl: 131.999863, loss_recon: 157122.109375, loss_pred: 0.626604
iteration 1070: loss: 158859.671875, loss_kl: 131.353622, loss_recon: 158783.812500, loss_pred: 0.433387
iteration 1071: loss: 157700.656250, loss_kl: 135.211777, loss_recon: 157639.093750, loss_pred: 0.280714
 60%|████████████████           | 119/200 [1:47:09<1:12:09, 53.45s/it]iteration 1072: loss: 160195.109375, loss_kl: 136.605164, loss_recon: 160127.218750, loss_pred: 0.286568
iteration 1073: loss: 157704.328125, loss_kl: 134.237106, loss_recon: 157643.812500, loss_pred: 0.219675
iteration 1074: loss: 158915.453125, loss_kl: 133.536179, loss_recon: 158856.890625, loss_pred: 0.202077
iteration 1075: loss: 159608.593750, loss_kl: 132.324921, loss_recon: 159542.343750, loss_pred: 0.282466
iteration 1076: loss: 161514.546875, loss_kl: 131.285583, loss_recon: 161439.546875, loss_pred: 0.372948
iteration 1077: loss: 153050.078125, loss_kl: 132.808365, loss_recon: 152981.281250, loss_pred: 0.306625
iteration 1078: loss: 160639.281250, loss_kl: 132.069183, loss_recon: 160545.453125, loss_pred: 0.558889
iteration 1079: loss: 158332.546875, loss_kl: 133.611771, loss_recon: 158264.671875, loss_pred: 0.295029
iteration 1080: loss: 158952.562500, loss_kl: 133.461334, loss_recon: 158886.406250, loss_pred: 0.278276
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_119.pth
 60%|████████████████▏          | 120/200 [1:48:03<1:11:15, 53.44s/it]iteration 1081: loss: 158999.843750, loss_kl: 134.042862, loss_recon: 158918.140625, loss_pred: 0.378888
iteration 1082: loss: 159285.343750, loss_kl: 136.180603, loss_recon: 159213.156250, loss_pred: 0.276894
iteration 1083: loss: 159898.609375, loss_kl: 132.210434, loss_recon: 159829.343750, loss_pred: 0.260690
iteration 1084: loss: 159739.531250, loss_kl: 134.664597, loss_recon: 159670.234375, loss_pred: 0.252802
iteration 1085: loss: 156452.343750, loss_kl: 131.979935, loss_recon: 156383.656250, loss_pred: 0.255648
iteration 1086: loss: 160343.890625, loss_kl: 133.290131, loss_recon: 160280.281250, loss_pred: 0.200423
iteration 1087: loss: 159055.750000, loss_kl: 133.337814, loss_recon: 158973.343750, loss_pred: 0.388206
iteration 1088: loss: 157272.734375, loss_kl: 136.004822, loss_recon: 157203.515625, loss_pred: 0.247686
iteration 1089: loss: 157966.859375, loss_kl: 132.670868, loss_recon: 157902.921875, loss_pred: 0.205825
 60%|████████████████▎          | 121/200 [1:48:56<1:10:23, 53.46s/it]iteration 1090: loss: 159483.703125, loss_kl: 133.066849, loss_recon: 159396.593750, loss_pred: 0.383523
iteration 1091: loss: 158973.312500, loss_kl: 134.449814, loss_recon: 158878.609375, loss_pred: 0.454403
iteration 1092: loss: 156774.343750, loss_kl: 135.303207, loss_recon: 156699.500000, loss_pred: 0.252654
iteration 1093: loss: 154672.421875, loss_kl: 135.420731, loss_recon: 154574.937500, loss_pred: 0.478661
iteration 1094: loss: 159652.875000, loss_kl: 132.457886, loss_recon: 159548.078125, loss_pred: 0.562609
iteration 1095: loss: 159640.500000, loss_kl: 132.172623, loss_recon: 159567.406250, loss_pred: 0.246663
iteration 1096: loss: 158634.375000, loss_kl: 132.612823, loss_recon: 158530.421875, loss_pred: 0.553591
iteration 1097: loss: 160093.375000, loss_kl: 132.501724, loss_recon: 159993.515625, loss_pred: 0.513047
iteration 1098: loss: 161327.140625, loss_kl: 132.776123, loss_recon: 161246.093750, loss_pred: 0.323961
 61%|████████████████▍          | 122/200 [1:49:50<1:09:28, 53.44s/it]iteration 1099: loss: 158082.015625, loss_kl: 133.612808, loss_recon: 158000.078125, loss_pred: 0.276924
iteration 1100: loss: 156312.406250, loss_kl: 133.744781, loss_recon: 156230.484375, loss_pred: 0.276304
iteration 1101: loss: 159940.062500, loss_kl: 132.318024, loss_recon: 159849.234375, loss_pred: 0.371122
iteration 1102: loss: 159585.296875, loss_kl: 135.875275, loss_recon: 159504.062500, loss_pred: 0.260590
iteration 1103: loss: 156938.234375, loss_kl: 132.237610, loss_recon: 156849.453125, loss_pred: 0.350979
iteration 1104: loss: 160994.328125, loss_kl: 133.618637, loss_recon: 160909.515625, loss_pred: 0.305690
iteration 1105: loss: 159460.218750, loss_kl: 133.627792, loss_recon: 159379.093750, loss_pred: 0.268782
iteration 1106: loss: 157286.265625, loss_kl: 133.249756, loss_recon: 157205.859375, loss_pred: 0.263116
iteration 1107: loss: 160454.296875, loss_kl: 130.879181, loss_recon: 160382.140625, loss_pred: 0.190209
 62%|████████████████▌          | 123/200 [1:50:43<1:08:37, 53.48s/it]iteration 1108: loss: 160975.328125, loss_kl: 130.868851, loss_recon: 160886.953125, loss_pred: 0.300607
iteration 1109: loss: 156920.218750, loss_kl: 133.213928, loss_recon: 156826.406250, loss_pred: 0.344525
iteration 1110: loss: 156038.765625, loss_kl: 131.147217, loss_recon: 155939.640625, loss_pred: 0.406941
iteration 1111: loss: 156915.500000, loss_kl: 134.281418, loss_recon: 156828.375000, loss_pred: 0.272925
iteration 1112: loss: 157967.718750, loss_kl: 135.492920, loss_recon: 157847.578125, loss_pred: 0.597661
iteration 1113: loss: 157700.328125, loss_kl: 134.910080, loss_recon: 157599.796875, loss_pred: 0.404269
iteration 1114: loss: 164010.859375, loss_kl: 132.577942, loss_recon: 163920.000000, loss_pred: 0.317882
iteration 1115: loss: 159582.734375, loss_kl: 132.272720, loss_recon: 159484.593750, loss_pred: 0.392106
iteration 1116: loss: 159309.656250, loss_kl: 132.030533, loss_recon: 159213.718750, loss_pred: 0.371074
 62%|████████████████▋          | 124/200 [1:51:36<1:07:33, 53.34s/it]iteration 1117: loss: 156378.828125, loss_kl: 132.846985, loss_recon: 156286.718750, loss_pred: 0.276486
iteration 1118: loss: 158534.218750, loss_kl: 132.309631, loss_recon: 158447.593750, loss_pred: 0.224225
iteration 1119: loss: 158608.156250, loss_kl: 130.482758, loss_recon: 158511.000000, loss_pred: 0.338365
iteration 1120: loss: 158952.625000, loss_kl: 131.607529, loss_recon: 158864.859375, loss_pred: 0.239042
iteration 1121: loss: 156908.296875, loss_kl: 131.353210, loss_recon: 156815.515625, loss_pred: 0.290522
iteration 1122: loss: 159735.265625, loss_kl: 130.183090, loss_recon: 159645.421875, loss_pred: 0.266781
iteration 1123: loss: 158838.109375, loss_kl: 134.301010, loss_recon: 158744.062500, loss_pred: 0.288980
iteration 1124: loss: 160142.625000, loss_kl: 131.019135, loss_recon: 160034.656250, loss_pred: 0.443983
iteration 1125: loss: 161430.656250, loss_kl: 130.452744, loss_recon: 161335.546875, loss_pred: 0.318153
 62%|████████████████▉          | 125/200 [1:52:30<1:06:41, 53.35s/it]iteration 1126: loss: 158975.281250, loss_kl: 131.185181, loss_recon: 158862.765625, loss_pred: 0.436751
iteration 1127: loss: 159005.281250, loss_kl: 130.333450, loss_recon: 158896.203125, loss_pred: 0.406717
iteration 1128: loss: 158393.343750, loss_kl: 131.290970, loss_recon: 158295.937500, loss_pred: 0.284936
iteration 1129: loss: 157889.953125, loss_kl: 131.201782, loss_recon: 157793.406250, loss_pred: 0.276917
iteration 1130: loss: 158258.234375, loss_kl: 130.277603, loss_recon: 158156.250000, loss_pred: 0.336021
iteration 1131: loss: 159592.937500, loss_kl: 129.717331, loss_recon: 159494.109375, loss_pred: 0.307452
iteration 1132: loss: 158886.656250, loss_kl: 130.446518, loss_recon: 158796.031250, loss_pred: 0.221712
iteration 1133: loss: 156155.312500, loss_kl: 130.003540, loss_recon: 156060.390625, loss_pred: 0.267087
iteration 1134: loss: 161992.296875, loss_kl: 129.499435, loss_recon: 161897.843750, loss_pred: 0.264864
 63%|█████████████████          | 126/200 [1:53:24<1:06:06, 53.60s/it]iteration 1135: loss: 155741.453125, loss_kl: 129.786224, loss_recon: 155634.203125, loss_pred: 0.339995
iteration 1136: loss: 158272.703125, loss_kl: 129.831467, loss_recon: 158175.312500, loss_pred: 0.241071
iteration 1137: loss: 158753.031250, loss_kl: 129.566040, loss_recon: 158654.703125, loss_pred: 0.251977
iteration 1138: loss: 160943.265625, loss_kl: 128.720367, loss_recon: 160836.906250, loss_pred: 0.337103
iteration 1139: loss: 159079.359375, loss_kl: 129.276001, loss_recon: 158960.718750, loss_pred: 0.456677
iteration 1140: loss: 158264.796875, loss_kl: 130.513199, loss_recon: 158161.265625, loss_pred: 0.298827
iteration 1141: loss: 160782.312500, loss_kl: 130.145462, loss_recon: 160667.687500, loss_pred: 0.411706
iteration 1142: loss: 159881.562500, loss_kl: 131.099533, loss_recon: 159752.343750, loss_pred: 0.552140
iteration 1143: loss: 157577.046875, loss_kl: 129.637329, loss_recon: 157459.515625, loss_pred: 0.443651
 64%|█████████████████▏         | 127/200 [1:54:17<1:05:10, 53.57s/it]iteration 1144: loss: 160040.250000, loss_kl: 127.772743, loss_recon: 159934.156250, loss_pred: 0.289160
iteration 1145: loss: 156965.156250, loss_kl: 128.562531, loss_recon: 156822.968750, loss_pred: 0.645349
iteration 1146: loss: 160776.109375, loss_kl: 127.911499, loss_recon: 160662.859375, loss_pred: 0.359853
iteration 1147: loss: 160008.859375, loss_kl: 128.621017, loss_recon: 159908.234375, loss_pred: 0.229342
iteration 1148: loss: 156506.046875, loss_kl: 127.609627, loss_recon: 156393.937500, loss_pred: 0.350382
iteration 1149: loss: 156308.203125, loss_kl: 129.284454, loss_recon: 156210.484375, loss_pred: 0.196264
iteration 1150: loss: 158013.406250, loss_kl: 127.807579, loss_recon: 157910.046875, loss_pred: 0.261534
iteration 1151: loss: 161382.000000, loss_kl: 127.385101, loss_recon: 161274.234375, loss_pred: 0.308284
iteration 1152: loss: 159416.875000, loss_kl: 131.483597, loss_recon: 159307.406250, loss_pred: 0.300530
 64%|█████████████████▎         | 128/200 [1:55:11<1:04:18, 53.58s/it]iteration 1153: loss: 161228.750000, loss_kl: 126.565399, loss_recon: 161116.921875, loss_pred: 0.303683
iteration 1154: loss: 159375.609375, loss_kl: 128.719833, loss_recon: 159268.500000, loss_pred: 0.242666
iteration 1155: loss: 156051.093750, loss_kl: 128.121933, loss_recon: 155928.343750, loss_pred: 0.402970
iteration 1156: loss: 161561.609375, loss_kl: 127.001266, loss_recon: 161452.796875, loss_pred: 0.270825
iteration 1157: loss: 159654.921875, loss_kl: 129.180588, loss_recon: 159545.671875, loss_pred: 0.261020
iteration 1158: loss: 160200.078125, loss_kl: 126.860214, loss_recon: 160089.265625, loss_pred: 0.291654
iteration 1159: loss: 156532.406250, loss_kl: 128.168884, loss_recon: 156413.000000, loss_pred: 0.369175
iteration 1160: loss: 158635.718750, loss_kl: 127.543373, loss_recon: 158529.656250, loss_pred: 0.239753
iteration 1161: loss: 156170.234375, loss_kl: 124.752403, loss_recon: 156031.921875, loss_pred: 0.580125
 64%|█████████████████▍         | 129/200 [1:56:07<1:04:12, 54.26s/it]iteration 1162: loss: 161374.031250, loss_kl: 124.775352, loss_recon: 161261.093750, loss_pred: 0.276901
iteration 1163: loss: 157356.843750, loss_kl: 128.287277, loss_recon: 157243.687500, loss_pred: 0.255112
iteration 1164: loss: 158071.484375, loss_kl: 128.322662, loss_recon: 157950.171875, loss_pred: 0.336340
iteration 1165: loss: 160291.359375, loss_kl: 127.016769, loss_recon: 160173.203125, loss_pred: 0.313685
iteration 1166: loss: 158029.796875, loss_kl: 128.548889, loss_recon: 157905.234375, loss_pred: 0.367378
iteration 1167: loss: 160315.125000, loss_kl: 125.360840, loss_recon: 160188.687500, loss_pred: 0.408020
iteration 1168: loss: 160575.296875, loss_kl: 127.528336, loss_recon: 160455.687500, loss_pred: 0.324919
iteration 1169: loss: 154045.156250, loss_kl: 127.911736, loss_recon: 153930.296875, loss_pred: 0.274714
iteration 1170: loss: 159129.156250, loss_kl: 127.899834, loss_recon: 159006.953125, loss_pred: 0.348243
 65%|█████████████████▌         | 130/200 [1:57:00<1:02:51, 53.88s/it]iteration 1171: loss: 158202.453125, loss_kl: 126.364388, loss_recon: 158063.750000, loss_pred: 0.473537
iteration 1172: loss: 157981.000000, loss_kl: 125.499153, loss_recon: 157851.218750, loss_pred: 0.390800
iteration 1173: loss: 161005.875000, loss_kl: 126.102203, loss_recon: 160887.734375, loss_pred: 0.269980
iteration 1174: loss: 159439.906250, loss_kl: 127.193947, loss_recon: 159311.375000, loss_pred: 0.365981
iteration 1175: loss: 159234.328125, loss_kl: 125.066322, loss_recon: 159084.515625, loss_pred: 0.594256
iteration 1176: loss: 155094.015625, loss_kl: 124.713646, loss_recon: 154965.937500, loss_pred: 0.379338
iteration 1177: loss: 161258.140625, loss_kl: 124.507286, loss_recon: 161143.765625, loss_pred: 0.243800
iteration 1178: loss: 160838.781250, loss_kl: 125.401146, loss_recon: 160705.343750, loss_pred: 0.427957
iteration 1179: loss: 156087.203125, loss_kl: 124.464172, loss_recon: 155967.656250, loss_pred: 0.295758
 66%|█████████████████▋         | 131/200 [1:57:53<1:01:52, 53.80s/it]iteration 1180: loss: 158465.500000, loss_kl: 123.098747, loss_recon: 158344.750000, loss_pred: 0.269003
iteration 1181: loss: 158296.937500, loss_kl: 124.431976, loss_recon: 158170.750000, loss_pred: 0.313339
iteration 1182: loss: 159412.140625, loss_kl: 123.888336, loss_recon: 159291.562500, loss_pred: 0.261224
iteration 1183: loss: 158547.375000, loss_kl: 125.333458, loss_recon: 158423.203125, loss_pred: 0.286292
iteration 1184: loss: 157279.984375, loss_kl: 124.470245, loss_recon: 157152.921875, loss_pred: 0.321666
iteration 1185: loss: 162271.406250, loss_kl: 122.935432, loss_recon: 162155.000000, loss_pred: 0.226889
iteration 1186: loss: 157659.781250, loss_kl: 122.090057, loss_recon: 157544.031250, loss_pred: 0.226788
iteration 1187: loss: 158145.515625, loss_kl: 122.941353, loss_recon: 158026.265625, loss_pred: 0.255227
iteration 1188: loss: 159096.953125, loss_kl: 123.804741, loss_recon: 158974.937500, loss_pred: 0.276226
 66%|█████████████████▊         | 132/200 [1:58:47<1:00:53, 53.73s/it]iteration 1189: loss: 156997.984375, loss_kl: 121.003365, loss_recon: 156867.562500, loss_pred: 0.333737
iteration 1190: loss: 160512.203125, loss_kl: 123.470398, loss_recon: 160378.921875, loss_pred: 0.342664
iteration 1191: loss: 159303.390625, loss_kl: 121.933067, loss_recon: 159185.968750, loss_pred: 0.196225
iteration 1192: loss: 160797.609375, loss_kl: 121.069519, loss_recon: 160661.578125, loss_pred: 0.389321
iteration 1193: loss: 157125.093750, loss_kl: 123.408539, loss_recon: 156991.140625, loss_pred: 0.349814
iteration 1194: loss: 157337.937500, loss_kl: 120.086388, loss_recon: 157214.406250, loss_pred: 0.272212
iteration 1195: loss: 156746.843750, loss_kl: 122.011887, loss_recon: 156613.078125, loss_pred: 0.359117
iteration 1196: loss: 158585.609375, loss_kl: 121.087639, loss_recon: 158451.968750, loss_pred: 0.365242
iteration 1197: loss: 161594.687500, loss_kl: 122.403603, loss_recon: 161475.593750, loss_pred: 0.209186
 66%|███████████████████▎         | 133/200 [1:59:41<59:59, 53.73s/it]iteration 1198: loss: 158789.375000, loss_kl: 121.658844, loss_recon: 158659.140625, loss_pred: 0.278470
iteration 1199: loss: 156083.875000, loss_kl: 121.474998, loss_recon: 155954.890625, loss_pred: 0.267576
iteration 1200: loss: 159298.656250, loss_kl: 122.500069, loss_recon: 159171.250000, loss_pred: 0.243178
iteration 1201: loss: 160510.203125, loss_kl: 120.075081, loss_recon: 160385.312500, loss_pred: 0.238243
iteration 1202: loss: 158579.312500, loss_kl: 120.777626, loss_recon: 158456.625000, loss_pred: 0.210424
iteration 1203: loss: 158469.703125, loss_kl: 121.824669, loss_recon: 158342.546875, loss_pred: 0.246206
iteration 1204: loss: 159315.500000, loss_kl: 120.214661, loss_recon: 159191.531250, loss_pred: 0.228030
iteration 1205: loss: 157948.078125, loss_kl: 120.531250, loss_recon: 157817.093750, loss_pred: 0.295491
iteration 1206: loss: 160004.406250, loss_kl: 119.181404, loss_recon: 159868.750000, loss_pred: 0.353597
 67%|███████████████████▍         | 134/200 [2:00:34<58:51, 53.51s/it]iteration 1207: loss: 157573.750000, loss_kl: 118.945511, loss_recon: 157436.406250, loss_pred: 0.325377
iteration 1208: loss: 155587.843750, loss_kl: 119.280777, loss_recon: 155454.718750, loss_pred: 0.280217
iteration 1209: loss: 159890.156250, loss_kl: 119.672882, loss_recon: 159757.796875, loss_pred: 0.269041
iteration 1210: loss: 161299.921875, loss_kl: 119.464577, loss_recon: 161165.109375, loss_pred: 0.295481
iteration 1211: loss: 157549.093750, loss_kl: 119.046875, loss_recon: 157417.890625, loss_pred: 0.263013
iteration 1212: loss: 161087.093750, loss_kl: 119.338043, loss_recon: 160960.421875, loss_pred: 0.215165
iteration 1213: loss: 159764.890625, loss_kl: 118.767258, loss_recon: 159637.250000, loss_pred: 0.229835
iteration 1214: loss: 157596.093750, loss_kl: 118.673630, loss_recon: 157460.859375, loss_pred: 0.306504
iteration 1215: loss: 158596.156250, loss_kl: 118.295929, loss_recon: 158469.046875, loss_pred: 0.228612
 68%|███████████████████▌         | 135/200 [2:01:27<57:58, 53.52s/it]iteration 1216: loss: 156985.484375, loss_kl: 118.691185, loss_recon: 156849.687500, loss_pred: 0.264995
iteration 1217: loss: 157647.984375, loss_kl: 116.881538, loss_recon: 157518.593750, loss_pred: 0.217707
iteration 1218: loss: 158828.656250, loss_kl: 119.188675, loss_recon: 158699.421875, loss_pred: 0.194895
iteration 1219: loss: 157941.468750, loss_kl: 115.736618, loss_recon: 157809.296875, loss_pred: 0.256114
iteration 1220: loss: 161445.750000, loss_kl: 116.760437, loss_recon: 161317.796875, loss_pred: 0.204354
iteration 1221: loss: 158667.562500, loss_kl: 116.026253, loss_recon: 158540.765625, loss_pred: 0.199547
iteration 1222: loss: 158281.625000, loss_kl: 115.344589, loss_recon: 158147.609375, loss_pred: 0.278192
iteration 1223: loss: 158709.078125, loss_kl: 116.958435, loss_recon: 158574.437500, loss_pred: 0.269314
iteration 1224: loss: 160431.125000, loss_kl: 115.405388, loss_recon: 160300.250000, loss_pred: 0.246033
 68%|███████████████████▋         | 136/200 [2:02:21<57:03, 53.49s/it]iteration 1225: loss: 161112.437500, loss_kl: 114.509674, loss_recon: 160971.218750, loss_pred: 0.312537
iteration 1226: loss: 157063.656250, loss_kl: 118.072517, loss_recon: 156927.593750, loss_pred: 0.226786
iteration 1227: loss: 161128.781250, loss_kl: 114.977959, loss_recon: 160994.375000, loss_pred: 0.239766
iteration 1228: loss: 158501.406250, loss_kl: 115.836227, loss_recon: 158365.656250, loss_pred: 0.245053
iteration 1229: loss: 158781.312500, loss_kl: 115.345314, loss_recon: 158641.078125, loss_pred: 0.294585
iteration 1230: loss: 160165.921875, loss_kl: 115.080399, loss_recon: 160034.921875, loss_pred: 0.204822
iteration 1231: loss: 157895.031250, loss_kl: 113.506584, loss_recon: 157761.718750, loss_pred: 0.242941
iteration 1232: loss: 154742.437500, loss_kl: 114.515099, loss_recon: 154600.281250, loss_pred: 0.321736
iteration 1233: loss: 159596.781250, loss_kl: 111.289276, loss_recon: 159455.296875, loss_pred: 0.346128
 68%|███████████████████▊         | 137/200 [2:03:14<56:02, 53.37s/it]iteration 1234: loss: 159344.453125, loss_kl: 113.105904, loss_recon: 159203.156250, loss_pred: 0.281852
iteration 1235: loss: 162622.859375, loss_kl: 111.505013, loss_recon: 162490.765625, loss_pred: 0.206014
iteration 1236: loss: 159658.625000, loss_kl: 114.843430, loss_recon: 159516.406250, loss_pred: 0.273820
iteration 1237: loss: 157353.218750, loss_kl: 113.931541, loss_recon: 157213.343750, loss_pred: 0.259407
iteration 1238: loss: 155496.437500, loss_kl: 115.462608, loss_recon: 155354.109375, loss_pred: 0.268589
iteration 1239: loss: 160354.843750, loss_kl: 113.291534, loss_recon: 160210.343750, loss_pred: 0.312022
iteration 1240: loss: 157046.531250, loss_kl: 112.829300, loss_recon: 156896.703125, loss_pred: 0.369966
iteration 1241: loss: 157160.828125, loss_kl: 111.602783, loss_recon: 156996.234375, loss_pred: 0.529875
iteration 1242: loss: 160104.750000, loss_kl: 110.530861, loss_recon: 159961.953125, loss_pred: 0.322704
 69%|████████████████████         | 138/200 [2:04:07<55:03, 53.28s/it]iteration 1243: loss: 160552.265625, loss_kl: 110.998405, loss_recon: 160405.562500, loss_pred: 0.357043
iteration 1244: loss: 157421.640625, loss_kl: 112.012268, loss_recon: 157263.750000, loss_pred: 0.458819
iteration 1245: loss: 157676.593750, loss_kl: 110.097626, loss_recon: 157539.312500, loss_pred: 0.271948
iteration 1246: loss: 159536.031250, loss_kl: 109.953697, loss_recon: 159399.609375, loss_pred: 0.264633
iteration 1247: loss: 158921.578125, loss_kl: 112.662079, loss_recon: 158787.875000, loss_pred: 0.210542
iteration 1248: loss: 158463.843750, loss_kl: 112.588158, loss_recon: 158326.937500, loss_pred: 0.243097
iteration 1249: loss: 160109.875000, loss_kl: 112.550308, loss_recon: 159969.578125, loss_pred: 0.277521
iteration 1250: loss: 156568.593750, loss_kl: 113.067978, loss_recon: 156429.765625, loss_pred: 0.257703
iteration 1251: loss: 160011.671875, loss_kl: 109.842094, loss_recon: 159873.171875, loss_pred: 0.286512
 70%|████████████████████▏        | 139/200 [2:05:00<54:14, 53.34s/it]iteration 1252: loss: 160879.921875, loss_kl: 110.068390, loss_recon: 160734.140625, loss_pred: 0.357218
iteration 1253: loss: 159389.187500, loss_kl: 109.266632, loss_recon: 159246.421875, loss_pred: 0.335017
iteration 1254: loss: 158871.109375, loss_kl: 109.623177, loss_recon: 158729.546875, loss_pred: 0.319341
iteration 1255: loss: 157420.343750, loss_kl: 108.359985, loss_recon: 157287.453125, loss_pred: 0.245244
iteration 1256: loss: 156077.218750, loss_kl: 107.953430, loss_recon: 155927.921875, loss_pred: 0.413423
iteration 1257: loss: 159278.796875, loss_kl: 109.769569, loss_recon: 159148.453125, loss_pred: 0.205803
iteration 1258: loss: 160823.093750, loss_kl: 110.667374, loss_recon: 160676.031250, loss_pred: 0.363950
iteration 1259: loss: 157797.203125, loss_kl: 108.177483, loss_recon: 157649.406250, loss_pred: 0.396219
iteration 1260: loss: 158791.859375, loss_kl: 106.052979, loss_recon: 158652.921875, loss_pred: 0.328939
 70%|████████████████████▎        | 140/200 [2:05:54<53:27, 53.47s/it]iteration 1261: loss: 156299.250000, loss_kl: 106.250374, loss_recon: 156140.312500, loss_pred: 0.526884
iteration 1262: loss: 162830.718750, loss_kl: 107.104332, loss_recon: 162696.343750, loss_pred: 0.272685
iteration 1263: loss: 161257.875000, loss_kl: 108.799194, loss_recon: 161118.671875, loss_pred: 0.304076
iteration 1264: loss: 158441.281250, loss_kl: 108.954353, loss_recon: 158278.234375, loss_pred: 0.540963
iteration 1265: loss: 157806.390625, loss_kl: 108.530113, loss_recon: 157660.234375, loss_pred: 0.376270
iteration 1266: loss: 159964.750000, loss_kl: 106.471611, loss_recon: 159827.453125, loss_pred: 0.308307
iteration 1267: loss: 162760.484375, loss_kl: 106.336708, loss_recon: 162608.468750, loss_pred: 0.456686
iteration 1268: loss: 153767.468750, loss_kl: 106.691673, loss_recon: 153551.062500, loss_pred: 1.097204
iteration 1269: loss: 156124.765625, loss_kl: 107.103615, loss_recon: 155985.109375, loss_pred: 0.325396
 70%|████████████████████▍        | 141/200 [2:06:48<52:35, 53.48s/it]iteration 1270: loss: 157641.984375, loss_kl: 107.438477, loss_recon: 157473.562500, loss_pred: 0.609812
iteration 1271: loss: 159908.187500, loss_kl: 108.076538, loss_recon: 159727.718750, loss_pred: 0.723875
iteration 1272: loss: 159599.968750, loss_kl: 106.399063, loss_recon: 159464.250000, loss_pred: 0.293071
iteration 1273: loss: 159119.031250, loss_kl: 106.162010, loss_recon: 158968.578125, loss_pred: 0.442973
iteration 1274: loss: 160033.703125, loss_kl: 105.703590, loss_recon: 159865.750000, loss_pred: 0.622505
iteration 1275: loss: 158701.234375, loss_kl: 103.553009, loss_recon: 158562.062500, loss_pred: 0.356183
iteration 1276: loss: 155480.500000, loss_kl: 104.359970, loss_recon: 155348.937500, loss_pred: 0.271962
iteration 1277: loss: 159076.843750, loss_kl: 104.493645, loss_recon: 158938.765625, loss_pred: 0.335803
iteration 1278: loss: 159651.968750, loss_kl: 105.287659, loss_recon: 159515.281250, loss_pred: 0.314105
 71%|████████████████████▌        | 142/200 [2:07:41<51:43, 53.50s/it]iteration 1279: loss: 162128.828125, loss_kl: 103.561218, loss_recon: 162001.687500, loss_pred: 0.235767
iteration 1280: loss: 161539.328125, loss_kl: 103.560799, loss_recon: 161402.453125, loss_pred: 0.333162
iteration 1281: loss: 155921.171875, loss_kl: 103.112030, loss_recon: 155784.828125, loss_pred: 0.332337
iteration 1282: loss: 161142.875000, loss_kl: 102.483788, loss_recon: 161009.375000, loss_pred: 0.310163
iteration 1283: loss: 159059.421875, loss_kl: 104.654449, loss_recon: 158927.828125, loss_pred: 0.269331
iteration 1284: loss: 154997.406250, loss_kl: 104.912949, loss_recon: 154850.765625, loss_pred: 0.417273
iteration 1285: loss: 158244.484375, loss_kl: 103.080650, loss_recon: 158116.406250, loss_pred: 0.249959
iteration 1286: loss: 158615.000000, loss_kl: 103.675789, loss_recon: 158470.234375, loss_pred: 0.410944
iteration 1287: loss: 157620.687500, loss_kl: 104.384140, loss_recon: 157461.093750, loss_pred: 0.551994
 72%|████████████████████▋        | 143/200 [2:08:35<50:53, 53.56s/it]iteration 1288: loss: 157114.281250, loss_kl: 102.313110, loss_recon: 156989.750000, loss_pred: 0.222161
iteration 1289: loss: 157758.234375, loss_kl: 102.947922, loss_recon: 157623.343750, loss_pred: 0.319310
iteration 1290: loss: 156901.656250, loss_kl: 100.809021, loss_recon: 156751.593750, loss_pred: 0.492534
iteration 1291: loss: 160189.296875, loss_kl: 102.979286, loss_recon: 160058.421875, loss_pred: 0.278966
iteration 1292: loss: 159435.609375, loss_kl: 102.172600, loss_recon: 159296.734375, loss_pred: 0.367095
iteration 1293: loss: 156393.578125, loss_kl: 103.199585, loss_recon: 156262.359375, loss_pred: 0.280092
iteration 1294: loss: 158741.000000, loss_kl: 100.663773, loss_recon: 158616.906250, loss_pred: 0.234335
iteration 1295: loss: 161061.562500, loss_kl: 102.136971, loss_recon: 160927.281250, loss_pred: 0.321428
iteration 1296: loss: 161764.453125, loss_kl: 102.399574, loss_recon: 161636.875000, loss_pred: 0.251705
 72%|████████████████████▉        | 144/200 [2:09:28<49:49, 53.38s/it]iteration 1297: loss: 158264.406250, loss_kl: 100.722473, loss_recon: 158132.765625, loss_pred: 0.309233
iteration 1298: loss: 156371.046875, loss_kl: 103.729866, loss_recon: 156230.828125, loss_pred: 0.364801
iteration 1299: loss: 160330.687500, loss_kl: 100.305542, loss_recon: 160195.859375, loss_pred: 0.345135
iteration 1300: loss: 158898.609375, loss_kl: 99.512627, loss_recon: 158756.671875, loss_pred: 0.424201
iteration 1301: loss: 155757.484375, loss_kl: 98.447220, loss_recon: 155611.500000, loss_pred: 0.475350
iteration 1302: loss: 158104.625000, loss_kl: 99.780342, loss_recon: 157963.796875, loss_pred: 0.410394
iteration 1303: loss: 160524.531250, loss_kl: 99.040749, loss_recon: 160395.406250, loss_pred: 0.300772
iteration 1304: loss: 159366.843750, loss_kl: 98.950012, loss_recon: 159238.093750, loss_pred: 0.297933
iteration 1305: loss: 161557.078125, loss_kl: 99.052864, loss_recon: 161429.265625, loss_pred: 0.287606
 72%|█████████████████████        | 145/200 [2:10:21<49:00, 53.46s/it]iteration 1306: loss: 160528.468750, loss_kl: 99.135086, loss_recon: 160395.765625, loss_pred: 0.335597
iteration 1307: loss: 156184.875000, loss_kl: 99.863281, loss_recon: 156055.984375, loss_pred: 0.290234
iteration 1308: loss: 157085.031250, loss_kl: 98.904869, loss_recon: 156949.093750, loss_pred: 0.370382
iteration 1309: loss: 160493.421875, loss_kl: 99.860397, loss_recon: 160358.515625, loss_pred: 0.350536
iteration 1310: loss: 158334.828125, loss_kl: 98.547081, loss_recon: 158210.828125, loss_pred: 0.254595
iteration 1311: loss: 157440.265625, loss_kl: 98.527649, loss_recon: 157298.562500, loss_pred: 0.431683
iteration 1312: loss: 159452.984375, loss_kl: 97.606468, loss_recon: 159316.656250, loss_pred: 0.387236
iteration 1313: loss: 162888.062500, loss_kl: 97.698753, loss_recon: 162766.984375, loss_pred: 0.233775
iteration 1314: loss: 157202.000000, loss_kl: 97.488632, loss_recon: 157075.140625, loss_pred: 0.293759
 73%|█████████████████████▏       | 146/200 [2:11:15<48:08, 53.49s/it]iteration 1315: loss: 158952.687500, loss_kl: 99.329132, loss_recon: 158816.406250, loss_pred: 0.369543
iteration 1316: loss: 156035.796875, loss_kl: 99.281494, loss_recon: 155894.984375, loss_pred: 0.415237
iteration 1317: loss: 156531.171875, loss_kl: 98.370628, loss_recon: 156408.515625, loss_pred: 0.242770
iteration 1318: loss: 160204.046875, loss_kl: 97.412239, loss_recon: 160076.421875, loss_pred: 0.302230
iteration 1319: loss: 159503.515625, loss_kl: 95.723549, loss_recon: 159370.687500, loss_pred: 0.371162
iteration 1320: loss: 157033.781250, loss_kl: 95.463074, loss_recon: 156901.796875, loss_pred: 0.365223
iteration 1321: loss: 161933.875000, loss_kl: 97.465477, loss_recon: 161791.281250, loss_pred: 0.451274
iteration 1322: loss: 157605.765625, loss_kl: 98.597420, loss_recon: 157479.828125, loss_pred: 0.273389
iteration 1323: loss: 161404.140625, loss_kl: 97.832298, loss_recon: 161284.578125, loss_pred: 0.217340
 74%|█████████████████████▎       | 147/200 [2:12:08<47:14, 53.47s/it]iteration 1324: loss: 159308.750000, loss_kl: 97.105751, loss_recon: 159176.000000, loss_pred: 0.356357
iteration 1325: loss: 158672.812500, loss_kl: 97.483620, loss_recon: 158537.765625, loss_pred: 0.375628
iteration 1326: loss: 162512.140625, loss_kl: 97.678612, loss_recon: 162383.171875, loss_pred: 0.313023
iteration 1327: loss: 159017.812500, loss_kl: 97.716423, loss_recon: 158883.937500, loss_pred: 0.361579
iteration 1328: loss: 155129.921875, loss_kl: 96.611649, loss_recon: 155007.156250, loss_pred: 0.261592
iteration 1329: loss: 159268.984375, loss_kl: 94.466560, loss_recon: 159144.421875, loss_pred: 0.300983
iteration 1330: loss: 158501.156250, loss_kl: 95.006149, loss_recon: 158380.375000, loss_pred: 0.257885
iteration 1331: loss: 160545.781250, loss_kl: 93.142273, loss_recon: 160422.062500, loss_pred: 0.305818
iteration 1332: loss: 156167.671875, loss_kl: 94.181389, loss_recon: 156038.546875, loss_pred: 0.349380
 74%|█████████████████████▍       | 148/200 [2:13:01<46:13, 53.33s/it]iteration 1333: loss: 156721.140625, loss_kl: 95.435196, loss_recon: 156593.718750, loss_pred: 0.319823
iteration 1334: loss: 156648.171875, loss_kl: 94.182503, loss_recon: 156521.375000, loss_pred: 0.326023
iteration 1335: loss: 161915.781250, loss_kl: 96.784889, loss_recon: 161783.125000, loss_pred: 0.358723
iteration 1336: loss: 158799.437500, loss_kl: 95.873146, loss_recon: 158659.015625, loss_pred: 0.445467
iteration 1337: loss: 158479.921875, loss_kl: 94.217804, loss_recon: 158362.281250, loss_pred: 0.234217
iteration 1338: loss: 160273.687500, loss_kl: 92.211815, loss_recon: 160153.078125, loss_pred: 0.283893
iteration 1339: loss: 160489.859375, loss_kl: 93.910263, loss_recon: 160370.171875, loss_pred: 0.257820
iteration 1340: loss: 155163.468750, loss_kl: 93.768227, loss_recon: 155040.515625, loss_pred: 0.291938
iteration 1341: loss: 160550.671875, loss_kl: 92.420586, loss_recon: 160422.968750, loss_pred: 0.352820
 74%|█████████████████████▌       | 149/200 [2:13:54<45:15, 53.25s/it]iteration 1342: loss: 159099.328125, loss_kl: 92.937630, loss_recon: 158957.859375, loss_pred: 0.485350
iteration 1343: loss: 160535.906250, loss_kl: 94.327705, loss_recon: 160420.109375, loss_pred: 0.214618
iteration 1344: loss: 158239.875000, loss_kl: 95.741005, loss_recon: 158104.546875, loss_pred: 0.395981
iteration 1345: loss: 160707.218750, loss_kl: 93.755646, loss_recon: 160544.875000, loss_pred: 0.685977
iteration 1346: loss: 157567.062500, loss_kl: 94.504669, loss_recon: 157444.703125, loss_pred: 0.278670
iteration 1347: loss: 160670.687500, loss_kl: 93.662651, loss_recon: 160536.109375, loss_pred: 0.409227
iteration 1348: loss: 157821.218750, loss_kl: 93.573875, loss_recon: 157672.312500, loss_pred: 0.553289
iteration 1349: loss: 156131.046875, loss_kl: 92.831787, loss_recon: 155994.687500, loss_pred: 0.435274
iteration 1350: loss: 158351.906250, loss_kl: 93.539841, loss_recon: 158224.828125, loss_pred: 0.335343
 75%|█████████████████████▊       | 150/200 [2:14:48<44:32, 53.45s/it]iteration 1351: loss: 157574.484375, loss_kl: 93.908791, loss_recon: 157528.765625, loss_pred: 0.447872
iteration 1352: loss: 157406.703125, loss_kl: 92.700882, loss_recon: 157377.375000, loss_pred: 0.284060
iteration 1353: loss: 158281.953125, loss_kl: 92.383026, loss_recon: 158244.500000, loss_pred: 0.365336
iteration 1354: loss: 157969.312500, loss_kl: 92.917953, loss_recon: 157932.765625, loss_pred: 0.356217
iteration 1355: loss: 161976.875000, loss_kl: 92.195038, loss_recon: 161951.453125, loss_pred: 0.245004
iteration 1356: loss: 158205.843750, loss_kl: 93.003876, loss_recon: 158168.375000, loss_pred: 0.365311
iteration 1357: loss: 155853.546875, loss_kl: 93.188835, loss_recon: 155823.953125, loss_pred: 0.286589
iteration 1358: loss: 158691.640625, loss_kl: 93.618584, loss_recon: 158661.062500, loss_pred: 0.296442
iteration 1359: loss: 161927.296875, loss_kl: 93.456711, loss_recon: 161886.656250, loss_pred: 0.396992
 76%|█████████████████████▉       | 151/200 [2:15:42<43:42, 53.53s/it]iteration 1360: loss: 157473.484375, loss_kl: 93.181877, loss_recon: 157445.203125, loss_pred: 0.273500
iteration 1361: loss: 158083.218750, loss_kl: 92.811905, loss_recon: 158049.062500, loss_pred: 0.332312
iteration 1362: loss: 157450.296875, loss_kl: 94.819992, loss_recon: 157419.593750, loss_pred: 0.297576
iteration 1363: loss: 162048.203125, loss_kl: 94.544693, loss_recon: 162022.890625, loss_pred: 0.243524
iteration 1364: loss: 160320.421875, loss_kl: 94.057213, loss_recon: 160294.484375, loss_pred: 0.249963
iteration 1365: loss: 156307.062500, loss_kl: 94.166481, loss_recon: 156284.625000, loss_pred: 0.215017
iteration 1366: loss: 156503.312500, loss_kl: 95.435402, loss_recon: 156483.687500, loss_pred: 0.186665
iteration 1367: loss: 159978.750000, loss_kl: 94.278770, loss_recon: 159953.828125, loss_pred: 0.239817
iteration 1368: loss: 159340.750000, loss_kl: 96.180122, loss_recon: 159311.093750, loss_pred: 0.286918
 76%|██████████████████████       | 152/200 [2:16:37<43:09, 53.96s/it]iteration 1369: loss: 160671.296875, loss_kl: 97.070587, loss_recon: 160636.437500, loss_pred: 0.338940
iteration 1370: loss: 158503.531250, loss_kl: 97.124901, loss_recon: 158483.796875, loss_pred: 0.187581
iteration 1371: loss: 159316.750000, loss_kl: 96.178673, loss_recon: 159292.328125, loss_pred: 0.234498
iteration 1372: loss: 157163.359375, loss_kl: 93.983948, loss_recon: 157140.687500, loss_pred: 0.217294
iteration 1373: loss: 157085.562500, loss_kl: 94.151711, loss_recon: 157059.921875, loss_pred: 0.247083
iteration 1374: loss: 156656.750000, loss_kl: 97.502174, loss_recon: 156633.656250, loss_pred: 0.221227
iteration 1375: loss: 162066.750000, loss_kl: 98.762810, loss_recon: 162041.093750, loss_pred: 0.246699
iteration 1376: loss: 159151.687500, loss_kl: 98.663574, loss_recon: 159127.609375, loss_pred: 0.230923
iteration 1377: loss: 156989.843750, loss_kl: 98.806068, loss_recon: 156957.890625, loss_pred: 0.309669
 76%|██████████████████████▏      | 153/200 [2:17:31<42:10, 53.84s/it]iteration 1378: loss: 158562.390625, loss_kl: 98.076302, loss_recon: 158528.703125, loss_pred: 0.327024
iteration 1379: loss: 158471.718750, loss_kl: 100.382462, loss_recon: 158447.656250, loss_pred: 0.230605
iteration 1380: loss: 156324.218750, loss_kl: 100.181488, loss_recon: 156294.593750, loss_pred: 0.286190
iteration 1381: loss: 157171.687500, loss_kl: 98.222641, loss_recon: 157137.750000, loss_pred: 0.329524
iteration 1382: loss: 158158.796875, loss_kl: 99.798538, loss_recon: 158126.062500, loss_pred: 0.317401
iteration 1383: loss: 160309.140625, loss_kl: 98.771278, loss_recon: 160285.078125, loss_pred: 0.230754
iteration 1384: loss: 162383.250000, loss_kl: 97.868629, loss_recon: 162342.875000, loss_pred: 0.393929
iteration 1385: loss: 161347.062500, loss_kl: 99.584541, loss_recon: 161322.625000, loss_pred: 0.234407
iteration 1386: loss: 155211.265625, loss_kl: 99.680069, loss_recon: 155183.859375, loss_pred: 0.264065
 77%|██████████████████████▎      | 154/200 [2:18:26<41:34, 54.23s/it]iteration 1387: loss: 155322.625000, loss_kl: 99.665253, loss_recon: 155291.687500, loss_pred: 0.299335
iteration 1388: loss: 160964.890625, loss_kl: 101.135628, loss_recon: 160932.937500, loss_pred: 0.309302
iteration 1389: loss: 159751.656250, loss_kl: 102.764465, loss_recon: 159714.343750, loss_pred: 0.362835
iteration 1390: loss: 161305.531250, loss_kl: 102.110970, loss_recon: 161282.171875, loss_pred: 0.223391
iteration 1391: loss: 156106.796875, loss_kl: 100.878731, loss_recon: 156074.093750, loss_pred: 0.316863
iteration 1392: loss: 159357.140625, loss_kl: 101.300858, loss_recon: 159324.265625, loss_pred: 0.318590
iteration 1393: loss: 157012.046875, loss_kl: 102.394951, loss_recon: 156979.593750, loss_pred: 0.314249
iteration 1394: loss: 159172.203125, loss_kl: 103.432465, loss_recon: 159143.484375, loss_pred: 0.276912
iteration 1395: loss: 158653.078125, loss_kl: 104.948074, loss_recon: 158623.625000, loss_pred: 0.284062
 78%|██████████████████████▍      | 155/200 [2:19:19<40:24, 53.87s/it]iteration 1396: loss: 155812.015625, loss_kl: 103.261093, loss_recon: 155779.968750, loss_pred: 0.310162
iteration 1397: loss: 158601.203125, loss_kl: 103.622757, loss_recon: 158566.968750, loss_pred: 0.332096
iteration 1398: loss: 159718.296875, loss_kl: 102.893791, loss_recon: 159692.000000, loss_pred: 0.252701
iteration 1399: loss: 157926.140625, loss_kl: 101.971893, loss_recon: 157904.484375, loss_pred: 0.206343
iteration 1400: loss: 156852.328125, loss_kl: 102.194946, loss_recon: 156823.750000, loss_pred: 0.275641
iteration 1401: loss: 158153.484375, loss_kl: 102.496887, loss_recon: 158118.953125, loss_pred: 0.334936
iteration 1402: loss: 161967.656250, loss_kl: 103.098007, loss_recon: 161936.593750, loss_pred: 0.300389
iteration 1403: loss: 160641.312500, loss_kl: 103.451965, loss_recon: 160606.875000, loss_pred: 0.333989
iteration 1404: loss: 158013.937500, loss_kl: 104.374138, loss_recon: 157980.000000, loss_pred: 0.328904
 78%|██████████████████████▌      | 156/200 [2:20:12<39:18, 53.61s/it]iteration 1405: loss: 161214.187500, loss_kl: 104.580345, loss_recon: 161191.281250, loss_pred: 0.218665
iteration 1406: loss: 157084.046875, loss_kl: 103.497604, loss_recon: 157011.421875, loss_pred: 0.715905
iteration 1407: loss: 158790.453125, loss_kl: 104.410164, loss_recon: 158754.906250, loss_pred: 0.344983
iteration 1408: loss: 158617.437500, loss_kl: 106.131889, loss_recon: 158579.578125, loss_pred: 0.367902
iteration 1409: loss: 157851.343750, loss_kl: 105.692955, loss_recon: 157791.781250, loss_pred: 0.584960
iteration 1410: loss: 160533.718750, loss_kl: 105.880806, loss_recon: 160508.750000, loss_pred: 0.239051
iteration 1411: loss: 158146.843750, loss_kl: 108.052689, loss_recon: 158111.546875, loss_pred: 0.342236
iteration 1412: loss: 158427.125000, loss_kl: 107.153534, loss_recon: 158375.046875, loss_pred: 0.509939
iteration 1413: loss: 156997.593750, loss_kl: 106.326965, loss_recon: 156948.062500, loss_pred: 0.484692
 78%|██████████████████████▊      | 157/200 [2:21:05<38:24, 53.59s/it]iteration 1414: loss: 159347.015625, loss_kl: 107.859482, loss_recon: 159314.078125, loss_pred: 0.318600
iteration 1415: loss: 158909.312500, loss_kl: 105.939217, loss_recon: 158874.828125, loss_pred: 0.334185
iteration 1416: loss: 156217.421875, loss_kl: 108.665123, loss_recon: 156180.546875, loss_pred: 0.357792
iteration 1417: loss: 158354.796875, loss_kl: 108.967331, loss_recon: 158330.781250, loss_pred: 0.229276
iteration 1418: loss: 158430.390625, loss_kl: 109.962364, loss_recon: 158404.421875, loss_pred: 0.248763
iteration 1419: loss: 159210.484375, loss_kl: 109.426804, loss_recon: 159186.328125, loss_pred: 0.230671
iteration 1420: loss: 160644.656250, loss_kl: 110.577278, loss_recon: 160622.265625, loss_pred: 0.212888
iteration 1421: loss: 157868.140625, loss_kl: 111.214851, loss_recon: 157844.156250, loss_pred: 0.228813
iteration 1422: loss: 158054.843750, loss_kl: 109.236893, loss_recon: 158032.765625, loss_pred: 0.209768
 79%|██████████████████████▉      | 158/200 [2:21:59<37:26, 53.50s/it]iteration 1423: loss: 157902.718750, loss_kl: 110.978889, loss_recon: 157876.328125, loss_pred: 0.252766
iteration 1424: loss: 160016.000000, loss_kl: 112.210411, loss_recon: 159990.265625, loss_pred: 0.246097
iteration 1425: loss: 157048.875000, loss_kl: 111.800385, loss_recon: 157025.031250, loss_pred: 0.227256
iteration 1426: loss: 156275.437500, loss_kl: 111.482399, loss_recon: 156248.140625, loss_pred: 0.261816
iteration 1427: loss: 156537.250000, loss_kl: 112.431465, loss_recon: 156509.062500, loss_pred: 0.270621
iteration 1428: loss: 160928.531250, loss_kl: 114.190842, loss_recon: 160902.140625, loss_pred: 0.252569
iteration 1429: loss: 158622.484375, loss_kl: 113.812775, loss_recon: 158587.703125, loss_pred: 0.336410
iteration 1430: loss: 159123.609375, loss_kl: 114.494995, loss_recon: 159090.953125, loss_pred: 0.315212
iteration 1431: loss: 160709.859375, loss_kl: 116.506142, loss_recon: 160686.171875, loss_pred: 0.225111
 80%|███████████████████████      | 159/200 [2:22:53<36:38, 53.62s/it]iteration 1432: loss: 159803.859375, loss_kl: 116.542229, loss_recon: 159766.000000, loss_pred: 0.366911
iteration 1433: loss: 159178.078125, loss_kl: 114.799835, loss_recon: 159148.406250, loss_pred: 0.285335
iteration 1434: loss: 159887.312500, loss_kl: 114.872185, loss_recon: 159865.921875, loss_pred: 0.202276
iteration 1435: loss: 158088.140625, loss_kl: 114.030586, loss_recon: 158062.093750, loss_pred: 0.249114
iteration 1436: loss: 159370.484375, loss_kl: 115.487389, loss_recon: 159345.093750, loss_pred: 0.242311
iteration 1437: loss: 159942.625000, loss_kl: 114.796143, loss_recon: 159916.765625, loss_pred: 0.247237
iteration 1438: loss: 159547.593750, loss_kl: 115.999748, loss_recon: 159527.953125, loss_pred: 0.184875
iteration 1439: loss: 155915.718750, loss_kl: 116.070496, loss_recon: 155886.812500, loss_pred: 0.277533
iteration 1440: loss: 155619.406250, loss_kl: 117.553185, loss_recon: 155591.453125, loss_pred: 0.267877
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_159.pth
 80%|███████████████████████▏     | 160/200 [2:23:46<35:46, 53.65s/it]iteration 1441: loss: 157331.718750, loss_kl: 117.338837, loss_recon: 157305.718750, loss_pred: 0.248235
iteration 1442: loss: 160237.968750, loss_kl: 117.783340, loss_recon: 160204.937500, loss_pred: 0.318572
iteration 1443: loss: 158326.859375, loss_kl: 118.439552, loss_recon: 158301.359375, loss_pred: 0.243189
iteration 1444: loss: 157585.515625, loss_kl: 118.306030, loss_recon: 157561.046875, loss_pred: 0.232848
iteration 1445: loss: 158092.625000, loss_kl: 119.028267, loss_recon: 158070.000000, loss_pred: 0.214405
iteration 1446: loss: 162128.640625, loss_kl: 120.918373, loss_recon: 162099.796875, loss_pred: 0.276484
iteration 1447: loss: 155332.312500, loss_kl: 119.700058, loss_recon: 155302.453125, loss_pred: 0.286611
iteration 1448: loss: 158464.640625, loss_kl: 118.544418, loss_recon: 158440.796875, loss_pred: 0.226616
iteration 1449: loss: 159463.015625, loss_kl: 119.725197, loss_recon: 159439.062500, loss_pred: 0.227451
 80%|███████████████████████▎     | 161/200 [2:24:41<35:02, 53.90s/it]iteration 1450: loss: 158364.562500, loss_kl: 120.555710, loss_recon: 158342.328125, loss_pred: 0.210280
iteration 1451: loss: 154585.375000, loss_kl: 120.618599, loss_recon: 154558.546875, loss_pred: 0.256196
iteration 1452: loss: 155982.921875, loss_kl: 122.702469, loss_recon: 155963.281250, loss_pred: 0.184081
iteration 1453: loss: 159860.781250, loss_kl: 121.420799, loss_recon: 159834.234375, loss_pred: 0.253319
iteration 1454: loss: 157582.859375, loss_kl: 123.544785, loss_recon: 157556.734375, loss_pred: 0.248856
iteration 1455: loss: 161891.562500, loss_kl: 124.524384, loss_recon: 161857.031250, loss_pred: 0.332753
iteration 1456: loss: 162268.812500, loss_kl: 123.440887, loss_recon: 162244.468750, loss_pred: 0.231040
iteration 1457: loss: 159262.625000, loss_kl: 125.363182, loss_recon: 159237.156250, loss_pred: 0.242147
iteration 1458: loss: 157194.937500, loss_kl: 124.007339, loss_recon: 157170.718750, loss_pred: 0.229857
 81%|███████████████████████▍     | 162/200 [2:25:35<34:07, 53.88s/it]iteration 1459: loss: 156704.906250, loss_kl: 124.782211, loss_recon: 156681.265625, loss_pred: 0.223859
iteration 1460: loss: 159110.109375, loss_kl: 127.583412, loss_recon: 159076.062500, loss_pred: 0.327623
iteration 1461: loss: 160404.609375, loss_kl: 124.355286, loss_recon: 160377.828125, loss_pred: 0.255366
iteration 1462: loss: 159884.843750, loss_kl: 123.821724, loss_recon: 159860.718750, loss_pred: 0.228862
iteration 1463: loss: 159134.656250, loss_kl: 124.677315, loss_recon: 159097.625000, loss_pred: 0.357758
iteration 1464: loss: 161120.000000, loss_kl: 126.104477, loss_recon: 161093.703125, loss_pred: 0.250366
iteration 1465: loss: 155542.296875, loss_kl: 126.675644, loss_recon: 155514.968750, loss_pred: 0.260550
iteration 1466: loss: 157566.671875, loss_kl: 129.263489, loss_recon: 157523.359375, loss_pred: 0.420090
iteration 1467: loss: 157454.437500, loss_kl: 130.363358, loss_recon: 157427.796875, loss_pred: 0.253463
 82%|███████████████████████▋     | 163/200 [2:26:28<33:10, 53.81s/it]iteration 1468: loss: 159521.812500, loss_kl: 130.655838, loss_recon: 159478.750000, loss_pred: 0.365746
iteration 1469: loss: 154370.906250, loss_kl: 124.688423, loss_recon: 154319.765625, loss_pred: 0.449490
iteration 1470: loss: 155764.468750, loss_kl: 128.081497, loss_recon: 155719.250000, loss_pred: 0.388632
iteration 1471: loss: 158327.843750, loss_kl: 128.437805, loss_recon: 158254.250000, loss_pred: 0.672249
iteration 1472: loss: 158354.593750, loss_kl: 129.813354, loss_recon: 158326.531250, loss_pred: 0.216271
iteration 1473: loss: 160654.593750, loss_kl: 131.052795, loss_recon: 160594.812500, loss_pred: 0.532736
iteration 1474: loss: 159309.671875, loss_kl: 132.277618, loss_recon: 159245.515625, loss_pred: 0.575913
iteration 1475: loss: 158969.078125, loss_kl: 131.091827, loss_recon: 158927.765625, loss_pred: 0.348085
iteration 1476: loss: 162307.671875, loss_kl: 129.996536, loss_recon: 162247.890625, loss_pred: 0.533286
 82%|███████████████████████▊     | 164/200 [2:27:23<32:30, 54.18s/it]iteration 1477: loss: 158836.093750, loss_kl: 132.523941, loss_recon: 158760.796875, loss_pred: 0.634709
iteration 1478: loss: 160081.265625, loss_kl: 131.899353, loss_recon: 160039.703125, loss_pred: 0.297910
iteration 1479: loss: 155655.843750, loss_kl: 131.883743, loss_recon: 155594.593750, loss_pred: 0.494783
iteration 1480: loss: 159487.015625, loss_kl: 133.664154, loss_recon: 159422.796875, loss_pred: 0.523027
iteration 1481: loss: 159974.703125, loss_kl: 135.938721, loss_recon: 159924.062500, loss_pred: 0.385128
iteration 1482: loss: 157954.375000, loss_kl: 135.043076, loss_recon: 157911.203125, loss_pred: 0.311287
iteration 1483: loss: 157290.078125, loss_kl: 135.322678, loss_recon: 157233.968750, loss_pred: 0.440381
iteration 1484: loss: 159197.921875, loss_kl: 136.646591, loss_recon: 159141.234375, loss_pred: 0.445052
iteration 1485: loss: 158701.046875, loss_kl: 135.473724, loss_recon: 158663.328125, loss_pred: 0.256384
 82%|███████████████████████▉     | 165/200 [2:28:17<31:35, 54.14s/it]iteration 1486: loss: 158167.906250, loss_kl: 135.499405, loss_recon: 158118.703125, loss_pred: 0.317492
iteration 1487: loss: 157927.296875, loss_kl: 135.618225, loss_recon: 157873.109375, loss_pred: 0.367261
iteration 1488: loss: 158017.406250, loss_kl: 135.634369, loss_recon: 157977.312500, loss_pred: 0.226243
iteration 1489: loss: 156032.234375, loss_kl: 136.617279, loss_recon: 155986.468750, loss_pred: 0.281674
iteration 1490: loss: 156588.000000, loss_kl: 137.791153, loss_recon: 156540.593750, loss_pred: 0.296620
iteration 1491: loss: 157588.062500, loss_kl: 138.136398, loss_recon: 157545.453125, loss_pred: 0.248125
iteration 1492: loss: 160298.984375, loss_kl: 139.561798, loss_recon: 160245.421875, loss_pred: 0.355938
iteration 1493: loss: 161506.250000, loss_kl: 141.966904, loss_recon: 161457.500000, loss_pred: 0.304640
iteration 1494: loss: 160516.718750, loss_kl: 140.074265, loss_recon: 160464.921875, loss_pred: 0.337451
 83%|████████████████████████     | 166/200 [2:29:10<30:29, 53.81s/it]iteration 1495: loss: 160259.703125, loss_kl: 141.007507, loss_recon: 160204.421875, loss_pred: 0.315263
iteration 1496: loss: 159435.953125, loss_kl: 140.442307, loss_recon: 159379.781250, loss_pred: 0.325210
iteration 1497: loss: 156477.296875, loss_kl: 140.284348, loss_recon: 156427.890625, loss_pred: 0.257847
iteration 1498: loss: 154679.625000, loss_kl: 139.320404, loss_recon: 154631.484375, loss_pred: 0.246762
iteration 1499: loss: 157339.218750, loss_kl: 141.398102, loss_recon: 157296.593750, loss_pred: 0.188054
iteration 1500: loss: 156758.593750, loss_kl: 143.573746, loss_recon: 156712.687500, loss_pred: 0.217331
iteration 1501: loss: 162185.031250, loss_kl: 143.629242, loss_recon: 162131.375000, loss_pred: 0.294717
iteration 1502: loss: 161219.359375, loss_kl: 144.802002, loss_recon: 161162.578125, loss_pred: 0.323848
iteration 1503: loss: 158323.859375, loss_kl: 146.888107, loss_recon: 158271.578125, loss_pred: 0.275392
 84%|████████████████████████▏    | 167/200 [2:30:04<29:32, 53.72s/it]iteration 1504: loss: 160968.734375, loss_kl: 146.816483, loss_recon: 160912.859375, loss_pred: 0.253430
iteration 1505: loss: 158254.328125, loss_kl: 147.980911, loss_recon: 158201.046875, loss_pred: 0.225071
iteration 1506: loss: 158508.500000, loss_kl: 148.122787, loss_recon: 158452.500000, loss_pred: 0.251842
iteration 1507: loss: 159774.609375, loss_kl: 149.908310, loss_recon: 159722.375000, loss_pred: 0.210409
iteration 1508: loss: 158520.218750, loss_kl: 149.170959, loss_recon: 158462.265625, loss_pred: 0.269204
iteration 1509: loss: 156070.250000, loss_kl: 148.267685, loss_recon: 156009.984375, loss_pred: 0.294235
iteration 1510: loss: 159631.218750, loss_kl: 146.608002, loss_recon: 159564.218750, loss_pred: 0.364990
iteration 1511: loss: 155261.015625, loss_kl: 146.979324, loss_recon: 155204.171875, loss_pred: 0.262674
iteration 1512: loss: 159935.500000, loss_kl: 145.751495, loss_recon: 159868.343750, loss_pred: 0.368371
 84%|████████████████████████▎    | 168/200 [2:30:58<28:42, 53.84s/it]iteration 1513: loss: 155652.312500, loss_kl: 145.682556, loss_recon: 155585.718750, loss_pred: 0.305109
iteration 1514: loss: 157042.328125, loss_kl: 147.653137, loss_recon: 156973.140625, loss_pred: 0.326251
iteration 1515: loss: 160107.656250, loss_kl: 147.917252, loss_recon: 160039.421875, loss_pred: 0.316131
iteration 1516: loss: 156280.265625, loss_kl: 148.285522, loss_recon: 156209.546875, loss_pred: 0.340052
iteration 1517: loss: 158763.750000, loss_kl: 151.542023, loss_recon: 158693.859375, loss_pred: 0.323742
iteration 1518: loss: 162357.281250, loss_kl: 153.898026, loss_recon: 162292.250000, loss_pred: 0.269276
iteration 1519: loss: 161444.718750, loss_kl: 157.200974, loss_recon: 161375.265625, loss_pred: 0.305378
iteration 1520: loss: 156955.890625, loss_kl: 156.925339, loss_recon: 156886.375000, loss_pred: 0.306498
iteration 1521: loss: 158296.140625, loss_kl: 153.841400, loss_recon: 158227.687500, loss_pred: 0.303608
 84%|████████████████████████▌    | 169/200 [2:31:51<27:45, 53.71s/it]iteration 1522: loss: 156445.484375, loss_kl: 151.647293, loss_recon: 156363.140625, loss_pred: 0.387995
iteration 1523: loss: 160730.968750, loss_kl: 153.256088, loss_recon: 160661.906250, loss_pred: 0.250446
iteration 1524: loss: 160334.515625, loss_kl: 150.921219, loss_recon: 160259.625000, loss_pred: 0.315414
iteration 1525: loss: 158928.703125, loss_kl: 153.361343, loss_recon: 158846.406250, loss_pred: 0.382571
iteration 1526: loss: 160172.375000, loss_kl: 155.633881, loss_recon: 160087.921875, loss_pred: 0.397490
iteration 1527: loss: 156898.593750, loss_kl: 154.875580, loss_recon: 156805.265625, loss_pred: 0.488489
iteration 1528: loss: 156993.343750, loss_kl: 157.532364, loss_recon: 156925.062500, loss_pred: 0.230242
iteration 1529: loss: 159248.640625, loss_kl: 156.567184, loss_recon: 159164.578125, loss_pred: 0.390909
iteration 1530: loss: 157597.171875, loss_kl: 158.128418, loss_recon: 157480.687500, loss_pred: 0.710651
 85%|████████████████████████▋    | 170/200 [2:32:45<26:54, 53.81s/it]iteration 1531: loss: 162378.859375, loss_kl: 157.035065, loss_recon: 162290.578125, loss_pred: 0.369631
iteration 1532: loss: 157216.906250, loss_kl: 154.500412, loss_recon: 157128.109375, loss_pred: 0.383111
iteration 1533: loss: 157007.750000, loss_kl: 154.363647, loss_recon: 156902.140625, loss_pred: 0.551508
iteration 1534: loss: 159256.296875, loss_kl: 153.560852, loss_recon: 159172.078125, loss_pred: 0.340369
iteration 1535: loss: 158380.875000, loss_kl: 148.858124, loss_recon: 158289.421875, loss_pred: 0.428118
iteration 1536: loss: 158962.234375, loss_kl: 151.127686, loss_recon: 158851.421875, loss_pred: 0.614168
iteration 1537: loss: 157828.578125, loss_kl: 153.953049, loss_recon: 157739.750000, loss_pred: 0.385151
iteration 1538: loss: 158651.546875, loss_kl: 155.140198, loss_recon: 158552.390625, loss_pred: 0.484455
iteration 1539: loss: 157959.421875, loss_kl: 155.854874, loss_recon: 157849.203125, loss_pred: 0.592753
 86%|████████████████████████▊    | 171/200 [2:33:39<25:58, 53.73s/it]iteration 1540: loss: 158365.281250, loss_kl: 156.721176, loss_recon: 158268.718750, loss_pred: 0.391427
iteration 1541: loss: 156790.546875, loss_kl: 156.150711, loss_recon: 156695.953125, loss_pred: 0.373765
iteration 1542: loss: 159383.515625, loss_kl: 158.124710, loss_recon: 159285.484375, loss_pred: 0.400983
iteration 1543: loss: 156956.640625, loss_kl: 154.755310, loss_recon: 156842.953125, loss_pred: 0.569916
iteration 1544: loss: 159156.953125, loss_kl: 157.757278, loss_recon: 159075.500000, loss_pred: 0.236524
iteration 1545: loss: 161739.062500, loss_kl: 160.857986, loss_recon: 161649.484375, loss_pred: 0.306407
iteration 1546: loss: 158214.546875, loss_kl: 155.829193, loss_recon: 158129.812500, loss_pred: 0.276359
iteration 1547: loss: 158551.046875, loss_kl: 159.814346, loss_recon: 158466.015625, loss_pred: 0.264720
iteration 1548: loss: 157712.906250, loss_kl: 155.865005, loss_recon: 157631.093750, loss_pred: 0.247054
 86%|████████████████████████▉    | 172/200 [2:34:33<25:03, 53.68s/it]iteration 1549: loss: 157408.859375, loss_kl: 157.984131, loss_recon: 157320.625000, loss_pred: 0.240905
iteration 1550: loss: 157311.359375, loss_kl: 157.810745, loss_recon: 157221.031250, loss_pred: 0.262536
iteration 1551: loss: 159305.484375, loss_kl: 158.619263, loss_recon: 159216.984375, loss_pred: 0.240910
iteration 1552: loss: 158052.343750, loss_kl: 156.676529, loss_recon: 157950.375000, loss_pred: 0.383533
iteration 1553: loss: 157995.078125, loss_kl: 157.569733, loss_recon: 157899.687500, loss_pred: 0.314205
iteration 1554: loss: 157408.609375, loss_kl: 157.143967, loss_recon: 157313.296875, loss_pred: 0.315116
iteration 1555: loss: 156673.140625, loss_kl: 162.660599, loss_recon: 156573.937500, loss_pred: 0.331639
iteration 1556: loss: 159741.734375, loss_kl: 164.014877, loss_recon: 159641.484375, loss_pred: 0.336620
iteration 1557: loss: 162791.484375, loss_kl: 164.410370, loss_recon: 162699.890625, loss_pred: 0.248508
 86%|█████████████████████████    | 173/200 [2:35:26<24:09, 53.70s/it]iteration 1558: loss: 161097.625000, loss_kl: 164.376251, loss_recon: 160983.421875, loss_pred: 0.409533
iteration 1559: loss: 162132.500000, loss_kl: 166.182663, loss_recon: 162022.406250, loss_pred: 0.360467
iteration 1560: loss: 153100.921875, loss_kl: 161.738693, loss_recon: 152985.968750, loss_pred: 0.428677
iteration 1561: loss: 160575.703125, loss_kl: 163.328094, loss_recon: 160461.593750, loss_pred: 0.413285
iteration 1562: loss: 154226.671875, loss_kl: 164.985794, loss_recon: 154102.875000, loss_pred: 0.502890
iteration 1563: loss: 159899.562500, loss_kl: 164.229752, loss_recon: 159793.484375, loss_pred: 0.328910
iteration 1564: loss: 157519.125000, loss_kl: 164.407471, loss_recon: 157419.656250, loss_pred: 0.262098
iteration 1565: loss: 159190.406250, loss_kl: 164.067139, loss_recon: 159084.546875, loss_pred: 0.327508
iteration 1566: loss: 158876.609375, loss_kl: 162.213715, loss_recon: 158768.750000, loss_pred: 0.355833
 87%|█████████████████████████▏   | 174/200 [2:36:20<23:18, 53.79s/it]iteration 1567: loss: 162268.421875, loss_kl: 168.909149, loss_recon: 162154.484375, loss_pred: 0.319879
iteration 1568: loss: 159764.656250, loss_kl: 167.136383, loss_recon: 159651.093750, loss_pred: 0.324623
iteration 1569: loss: 156047.328125, loss_kl: 167.141464, loss_recon: 155937.718750, loss_pred: 0.285159
iteration 1570: loss: 159278.734375, loss_kl: 170.480469, loss_recon: 159169.640625, loss_pred: 0.263792
iteration 1571: loss: 158067.578125, loss_kl: 165.365372, loss_recon: 157955.031250, loss_pred: 0.323050
iteration 1572: loss: 156516.531250, loss_kl: 163.871780, loss_recon: 156404.812500, loss_pred: 0.322059
iteration 1573: loss: 156622.609375, loss_kl: 163.765411, loss_recon: 156508.296875, loss_pred: 0.348578
iteration 1574: loss: 159018.062500, loss_kl: 169.183533, loss_recon: 158909.890625, loss_pred: 0.260816
iteration 1575: loss: 159191.031250, loss_kl: 164.060150, loss_recon: 159075.609375, loss_pred: 0.358149
 88%|█████████████████████████▍   | 175/200 [2:37:13<22:17, 53.51s/it]iteration 1576: loss: 155958.375000, loss_kl: 166.680130, loss_recon: 155845.312500, loss_pred: 0.255947
iteration 1577: loss: 157600.046875, loss_kl: 164.035553, loss_recon: 157482.828125, loss_pred: 0.311432
iteration 1578: loss: 159431.953125, loss_kl: 168.675674, loss_recon: 159311.531250, loss_pred: 0.319001
iteration 1579: loss: 159612.640625, loss_kl: 170.127609, loss_recon: 159494.640625, loss_pred: 0.287211
iteration 1580: loss: 160688.984375, loss_kl: 171.775970, loss_recon: 160570.062500, loss_pred: 0.287855
iteration 1581: loss: 156571.859375, loss_kl: 169.400620, loss_recon: 156454.281250, loss_pred: 0.286675
iteration 1582: loss: 157885.015625, loss_kl: 165.804733, loss_recon: 157769.000000, loss_pred: 0.289924
iteration 1583: loss: 160844.937500, loss_kl: 170.528290, loss_recon: 160725.296875, loss_pred: 0.301360
iteration 1584: loss: 157960.546875, loss_kl: 169.617172, loss_recon: 157842.859375, loss_pred: 0.286707
 88%|█████████████████████████▌   | 176/200 [2:38:06<21:20, 53.36s/it]iteration 1585: loss: 157973.500000, loss_kl: 169.661636, loss_recon: 157850.640625, loss_pred: 0.271118
iteration 1586: loss: 160580.718750, loss_kl: 173.222382, loss_recon: 160452.968750, loss_pred: 0.299913
iteration 1587: loss: 159181.140625, loss_kl: 173.014481, loss_recon: 159043.703125, loss_pred: 0.397746
iteration 1588: loss: 156433.281250, loss_kl: 169.233734, loss_recon: 156306.921875, loss_pred: 0.308421
iteration 1589: loss: 157874.593750, loss_kl: 169.610077, loss_recon: 157744.593750, loss_pred: 0.342587
iteration 1590: loss: 160240.984375, loss_kl: 172.383377, loss_recon: 160096.750000, loss_pred: 0.469350
iteration 1591: loss: 159844.781250, loss_kl: 173.563354, loss_recon: 159718.250000, loss_pred: 0.285831
iteration 1592: loss: 157744.906250, loss_kl: 170.071213, loss_recon: 157609.875000, loss_pred: 0.390543
iteration 1593: loss: 156428.890625, loss_kl: 170.958527, loss_recon: 156301.796875, loss_pred: 0.306031
 88%|█████████████████████████▋   | 177/200 [2:39:00<20:33, 53.64s/it]iteration 1594: loss: 159664.281250, loss_kl: 172.303680, loss_recon: 159532.609375, loss_pred: 0.275940
iteration 1595: loss: 156043.281250, loss_kl: 171.571503, loss_recon: 155908.187500, loss_pred: 0.314671
iteration 1596: loss: 157441.671875, loss_kl: 171.972458, loss_recon: 157311.515625, loss_pred: 0.262735
iteration 1597: loss: 159802.515625, loss_kl: 174.861221, loss_recon: 159662.000000, loss_pred: 0.349083
iteration 1598: loss: 158843.468750, loss_kl: 173.194534, loss_recon: 158709.343750, loss_pred: 0.295203
iteration 1599: loss: 157354.140625, loss_kl: 173.060883, loss_recon: 157218.687500, loss_pred: 0.309205
iteration 1600: loss: 160315.609375, loss_kl: 174.671829, loss_recon: 160182.203125, loss_pred: 0.279062
iteration 1601: loss: 156289.546875, loss_kl: 173.102844, loss_recon: 156139.656250, loss_pred: 0.453438
iteration 1602: loss: 159907.671875, loss_kl: 173.639542, loss_recon: 159773.593750, loss_pred: 0.292021
 89%|█████████████████████████▊   | 178/200 [2:39:54<19:40, 53.66s/it]iteration 1603: loss: 159370.578125, loss_kl: 175.670425, loss_recon: 159226.921875, loss_pred: 0.305921
iteration 1604: loss: 159585.953125, loss_kl: 181.656509, loss_recon: 159444.640625, loss_pred: 0.243884
iteration 1605: loss: 157835.500000, loss_kl: 173.297134, loss_recon: 157680.375000, loss_pred: 0.435911
iteration 1606: loss: 155325.296875, loss_kl: 172.831696, loss_recon: 155181.796875, loss_pred: 0.322672
iteration 1607: loss: 158106.062500, loss_kl: 173.457764, loss_recon: 157967.203125, loss_pred: 0.272162
iteration 1608: loss: 158935.875000, loss_kl: 174.949890, loss_recon: 158785.156250, loss_pred: 0.381195
iteration 1609: loss: 159111.546875, loss_kl: 174.485123, loss_recon: 158967.265625, loss_pred: 0.319775
iteration 1610: loss: 155878.218750, loss_kl: 176.987198, loss_recon: 155734.984375, loss_pred: 0.293287
iteration 1611: loss: 161905.437500, loss_kl: 180.685837, loss_recon: 161757.687500, loss_pred: 0.314458
 90%|█████████████████████████▉   | 179/200 [2:40:47<18:44, 53.56s/it]iteration 1612: loss: 160817.609375, loss_kl: 183.420502, loss_recon: 160656.968750, loss_pred: 0.353263
iteration 1613: loss: 155143.796875, loss_kl: 176.872345, loss_recon: 154978.921875, loss_pred: 0.440277
iteration 1614: loss: 158669.640625, loss_kl: 177.290909, loss_recon: 158515.718750, loss_pred: 0.327896
iteration 1615: loss: 155765.921875, loss_kl: 175.949265, loss_recon: 155601.375000, loss_pred: 0.443370
iteration 1616: loss: 158049.625000, loss_kl: 174.348831, loss_recon: 157889.625000, loss_pred: 0.408922
iteration 1617: loss: 158653.250000, loss_kl: 180.699310, loss_recon: 158502.031250, loss_pred: 0.277723
iteration 1618: loss: 158485.312500, loss_kl: 182.517349, loss_recon: 158332.453125, loss_pred: 0.281637
iteration 1619: loss: 164202.484375, loss_kl: 187.927841, loss_recon: 164041.937500, loss_pred: 0.321631
iteration 1620: loss: 155969.656250, loss_kl: 180.313065, loss_recon: 155817.531250, loss_pred: 0.289399
 90%|██████████████████████████   | 180/200 [2:41:41<17:52, 53.64s/it]iteration 1621: loss: 158647.187500, loss_kl: 184.359436, loss_recon: 158473.234375, loss_pred: 0.407056
iteration 1622: loss: 160723.015625, loss_kl: 178.655334, loss_recon: 160556.921875, loss_pred: 0.369680
iteration 1623: loss: 157198.453125, loss_kl: 180.808899, loss_recon: 157033.343750, loss_pred: 0.344189
iteration 1624: loss: 158998.140625, loss_kl: 185.758163, loss_recon: 158804.656250, loss_pred: 0.592197
iteration 1625: loss: 158444.281250, loss_kl: 182.906830, loss_recon: 158264.281250, loss_pred: 0.477928
iteration 1626: loss: 160265.781250, loss_kl: 183.766663, loss_recon: 160098.281250, loss_pred: 0.346730
iteration 1627: loss: 156189.218750, loss_kl: 181.530472, loss_recon: 156004.343750, loss_pred: 0.536730
iteration 1628: loss: 157810.234375, loss_kl: 181.101822, loss_recon: 157628.000000, loss_pred: 0.513259
iteration 1629: loss: 157652.046875, loss_kl: 181.910492, loss_recon: 157491.390625, loss_pred: 0.291708
 90%|██████████████████████████▏  | 181/200 [2:42:35<16:59, 53.68s/it]iteration 1630: loss: 157084.437500, loss_kl: 183.502533, loss_recon: 156887.687500, loss_pred: 0.568430
iteration 1631: loss: 155716.750000, loss_kl: 182.865463, loss_recon: 155537.031250, loss_pred: 0.402971
iteration 1632: loss: 159886.828125, loss_kl: 184.375061, loss_recon: 159710.562500, loss_pred: 0.356987
iteration 1633: loss: 158597.656250, loss_kl: 187.019943, loss_recon: 158409.421875, loss_pred: 0.456530
iteration 1634: loss: 160219.187500, loss_kl: 182.663177, loss_recon: 160047.828125, loss_pred: 0.320882
iteration 1635: loss: 157482.500000, loss_kl: 184.205963, loss_recon: 157299.750000, loss_pred: 0.423086
iteration 1636: loss: 155998.593750, loss_kl: 183.387924, loss_recon: 155808.281250, loss_pred: 0.504976
iteration 1637: loss: 160389.656250, loss_kl: 190.259201, loss_recon: 160207.859375, loss_pred: 0.367426
iteration 1638: loss: 160742.218750, loss_kl: 189.306778, loss_recon: 160547.562500, loss_pred: 0.503234
 91%|██████████████████████████▍  | 182/200 [2:43:29<16:06, 53.68s/it]iteration 1639: loss: 161441.828125, loss_kl: 190.769516, loss_recon: 161247.203125, loss_pred: 0.416253
iteration 1640: loss: 158865.812500, loss_kl: 183.401474, loss_recon: 158686.859375, loss_pred: 0.318622
iteration 1641: loss: 156795.046875, loss_kl: 184.203094, loss_recon: 156612.312500, loss_pred: 0.350019
iteration 1642: loss: 161305.875000, loss_kl: 183.290588, loss_recon: 161129.281250, loss_pred: 0.295934
iteration 1643: loss: 155464.812500, loss_kl: 185.182373, loss_recon: 155284.828125, loss_pred: 0.314663
iteration 1644: loss: 157042.312500, loss_kl: 184.646652, loss_recon: 156855.968750, loss_pred: 0.382556
iteration 1645: loss: 159802.906250, loss_kl: 182.524780, loss_recon: 159623.109375, loss_pred: 0.333996
iteration 1646: loss: 158210.312500, loss_kl: 185.647217, loss_recon: 158028.109375, loss_pred: 0.333142
iteration 1647: loss: 157030.437500, loss_kl: 183.803543, loss_recon: 156848.687500, loss_pred: 0.343486
 92%|██████████████████████████▌  | 183/200 [2:44:22<15:12, 53.69s/it]iteration 1648: loss: 158789.609375, loss_kl: 189.467697, loss_recon: 158585.140625, loss_pred: 0.450098
iteration 1649: loss: 159187.109375, loss_kl: 185.588226, loss_recon: 158998.265625, loss_pred: 0.326543
iteration 1650: loss: 157407.062500, loss_kl: 191.839035, loss_recon: 157195.828125, loss_pred: 0.497839
iteration 1651: loss: 157934.921875, loss_kl: 188.486694, loss_recon: 157736.593750, loss_pred: 0.396963
iteration 1652: loss: 160766.968750, loss_kl: 187.807693, loss_recon: 160573.953125, loss_pred: 0.349482
iteration 1653: loss: 155637.171875, loss_kl: 189.387802, loss_recon: 155424.546875, loss_pred: 0.532371
iteration 1654: loss: 161747.890625, loss_kl: 191.093979, loss_recon: 161542.593750, loss_pred: 0.444633
iteration 1655: loss: 156267.718750, loss_kl: 185.974976, loss_recon: 156058.093750, loss_pred: 0.531021
iteration 1656: loss: 157539.656250, loss_kl: 185.558990, loss_recon: 157333.390625, loss_pred: 0.500899
 92%|██████████████████████████▋  | 184/200 [2:45:16<14:19, 53.69s/it]iteration 1657: loss: 155881.906250, loss_kl: 186.979355, loss_recon: 155669.500000, loss_pred: 0.476399
iteration 1658: loss: 158379.656250, loss_kl: 192.987366, loss_recon: 158167.156250, loss_pred: 0.424372
iteration 1659: loss: 160046.984375, loss_kl: 196.787613, loss_recon: 159820.140625, loss_pred: 0.534323
iteration 1660: loss: 157533.468750, loss_kl: 191.201614, loss_recon: 157322.171875, loss_pred: 0.428153
iteration 1661: loss: 158118.593750, loss_kl: 194.835312, loss_recon: 157909.875000, loss_pred: 0.370235
iteration 1662: loss: 161410.468750, loss_kl: 190.030746, loss_recon: 161192.093750, loss_pred: 0.509279
iteration 1663: loss: 156806.984375, loss_kl: 188.568619, loss_recon: 156601.062500, loss_pred: 0.397567
iteration 1664: loss: 157584.718750, loss_kl: 188.481049, loss_recon: 157379.687500, loss_pred: 0.389404
iteration 1665: loss: 159314.531250, loss_kl: 187.609375, loss_recon: 159104.750000, loss_pred: 0.444476
 92%|██████████████████████████▊  | 185/200 [2:46:10<13:25, 53.68s/it]iteration 1666: loss: 159695.265625, loss_kl: 190.298843, loss_recon: 159489.421875, loss_pred: 0.306018
iteration 1667: loss: 157753.546875, loss_kl: 187.170670, loss_recon: 157541.593750, loss_pred: 0.396161
iteration 1668: loss: 154917.171875, loss_kl: 188.076096, loss_recon: 154696.515625, loss_pred: 0.474671
iteration 1669: loss: 159111.484375, loss_kl: 195.264038, loss_recon: 158892.000000, loss_pred: 0.396903
iteration 1670: loss: 157116.906250, loss_kl: 198.161438, loss_recon: 156888.093750, loss_pred: 0.463394
iteration 1671: loss: 161557.312500, loss_kl: 198.870270, loss_recon: 161334.515625, loss_pred: 0.396776
iteration 1672: loss: 159219.765625, loss_kl: 195.385452, loss_recon: 159001.000000, loss_pred: 0.388628
iteration 1673: loss: 159457.828125, loss_kl: 189.154236, loss_recon: 159253.734375, loss_pred: 0.299281
iteration 1674: loss: 156235.984375, loss_kl: 195.239853, loss_recon: 156015.984375, loss_pred: 0.402113
 93%|██████████████████████████▉  | 186/200 [2:47:04<12:31, 53.70s/it]iteration 1675: loss: 159507.390625, loss_kl: 199.264725, loss_recon: 159275.843750, loss_pred: 0.401687
iteration 1676: loss: 161330.531250, loss_kl: 205.422867, loss_recon: 161094.359375, loss_pred: 0.388941
iteration 1677: loss: 154637.703125, loss_kl: 197.702194, loss_recon: 154408.406250, loss_pred: 0.394267
iteration 1678: loss: 158849.062500, loss_kl: 206.193924, loss_recon: 158609.203125, loss_pred: 0.418291
iteration 1679: loss: 160330.750000, loss_kl: 199.813797, loss_recon: 160107.140625, loss_pred: 0.317075
iteration 1680: loss: 156527.593750, loss_kl: 202.721130, loss_recon: 156296.937500, loss_pred: 0.359696
iteration 1681: loss: 158214.312500, loss_kl: 200.155029, loss_recon: 157981.687500, loss_pred: 0.403896
iteration 1682: loss: 158880.031250, loss_kl: 200.058563, loss_recon: 158653.062500, loss_pred: 0.348224
iteration 1683: loss: 155984.156250, loss_kl: 193.789291, loss_recon: 155764.687500, loss_pred: 0.333605
 94%|███████████████████████████  | 187/200 [2:47:57<11:37, 53.65s/it]iteration 1684: loss: 156606.875000, loss_kl: 195.298584, loss_recon: 156379.093750, loss_pred: 0.324858
iteration 1685: loss: 158154.546875, loss_kl: 198.050079, loss_recon: 157918.265625, loss_pred: 0.382283
iteration 1686: loss: 156870.484375, loss_kl: 199.582138, loss_recon: 156629.031250, loss_pred: 0.418761
iteration 1687: loss: 158260.218750, loss_kl: 199.612747, loss_recon: 158020.140625, loss_pred: 0.404717
iteration 1688: loss: 156131.703125, loss_kl: 202.177338, loss_recon: 155892.765625, loss_pred: 0.367689
iteration 1689: loss: 159766.796875, loss_kl: 204.062134, loss_recon: 159527.546875, loss_pred: 0.351870
iteration 1690: loss: 158987.218750, loss_kl: 199.454193, loss_recon: 158749.531250, loss_pred: 0.382310
iteration 1691: loss: 160915.625000, loss_kl: 209.442383, loss_recon: 160671.109375, loss_pred: 0.350840
iteration 1692: loss: 158273.187500, loss_kl: 207.456131, loss_recon: 158030.187500, loss_pred: 0.355436
 94%|███████████████████████████▎ | 188/200 [2:48:50<10:42, 53.50s/it]iteration 1693: loss: 157059.375000, loss_kl: 205.990463, loss_recon: 156802.890625, loss_pred: 0.505041
iteration 1694: loss: 157309.156250, loss_kl: 207.714676, loss_recon: 157048.562500, loss_pred: 0.528719
iteration 1695: loss: 160163.562500, loss_kl: 207.911057, loss_recon: 159920.171875, loss_pred: 0.354769
iteration 1696: loss: 159294.062500, loss_kl: 209.477798, loss_recon: 159045.265625, loss_pred: 0.393120
iteration 1697: loss: 160123.015625, loss_kl: 203.047348, loss_recon: 159871.093750, loss_pred: 0.488797
iteration 1698: loss: 156609.453125, loss_kl: 197.514023, loss_recon: 156373.546875, loss_pred: 0.383969
iteration 1699: loss: 157018.234375, loss_kl: 201.081833, loss_recon: 156776.609375, loss_pred: 0.405416
iteration 1700: loss: 157749.875000, loss_kl: 203.802063, loss_recon: 157508.968750, loss_pred: 0.371106
iteration 1701: loss: 157901.343750, loss_kl: 210.105469, loss_recon: 157653.609375, loss_pred: 0.376198
 94%|███████████████████████████▍ | 189/200 [2:49:44<09:47, 53.45s/it]iteration 1702: loss: 158182.109375, loss_kl: 206.984344, loss_recon: 157941.406250, loss_pred: 0.337224
iteration 1703: loss: 161446.296875, loss_kl: 214.828674, loss_recon: 161190.703125, loss_pred: 0.407604
iteration 1704: loss: 158613.890625, loss_kl: 212.759567, loss_recon: 158365.921875, loss_pred: 0.352029
iteration 1705: loss: 161081.859375, loss_kl: 214.315704, loss_recon: 160828.484375, loss_pred: 0.390587
iteration 1706: loss: 158176.500000, loss_kl: 212.258896, loss_recon: 157923.156250, loss_pred: 0.410743
iteration 1707: loss: 156214.828125, loss_kl: 207.308762, loss_recon: 155957.718750, loss_pred: 0.497970
iteration 1708: loss: 156436.750000, loss_kl: 203.614059, loss_recon: 156192.390625, loss_pred: 0.407502
iteration 1709: loss: 155771.109375, loss_kl: 209.352798, loss_recon: 155515.093750, loss_pred: 0.466606
iteration 1710: loss: 157378.515625, loss_kl: 215.472336, loss_recon: 157116.593750, loss_pred: 0.464486
 95%|███████████████████████████▌ | 190/200 [2:50:38<08:56, 53.65s/it]iteration 1711: loss: 159961.500000, loss_kl: 216.800812, loss_recon: 159702.093750, loss_pred: 0.426166
iteration 1712: loss: 157659.312500, loss_kl: 222.399368, loss_recon: 157386.375000, loss_pred: 0.505253
iteration 1713: loss: 158743.046875, loss_kl: 210.124588, loss_recon: 158491.937500, loss_pred: 0.409823
iteration 1714: loss: 157918.031250, loss_kl: 204.696091, loss_recon: 157670.484375, loss_pred: 0.428453
iteration 1715: loss: 155222.250000, loss_kl: 209.102264, loss_recon: 154971.921875, loss_pred: 0.412185
iteration 1716: loss: 158912.640625, loss_kl: 219.844971, loss_recon: 158656.546875, loss_pred: 0.362477
iteration 1717: loss: 157048.093750, loss_kl: 226.069336, loss_recon: 156776.437500, loss_pred: 0.455872
iteration 1718: loss: 160593.906250, loss_kl: 227.991608, loss_recon: 160321.921875, loss_pred: 0.439972
iteration 1719: loss: 156488.000000, loss_kl: 217.587830, loss_recon: 156225.171875, loss_pred: 0.452393
 96%|███████████████████████████▋ | 191/200 [2:51:31<08:01, 53.48s/it]iteration 1720: loss: 156950.296875, loss_kl: 220.700470, loss_recon: 156686.421875, loss_pred: 0.431747
iteration 1721: loss: 160063.968750, loss_kl: 226.746841, loss_recon: 159784.656250, loss_pred: 0.525614
iteration 1722: loss: 156837.765625, loss_kl: 223.758133, loss_recon: 156564.515625, loss_pred: 0.494822
iteration 1723: loss: 157515.515625, loss_kl: 225.616302, loss_recon: 157238.687500, loss_pred: 0.512122
iteration 1724: loss: 160454.953125, loss_kl: 235.897232, loss_recon: 160165.078125, loss_pred: 0.539886
iteration 1725: loss: 157319.906250, loss_kl: 228.680588, loss_recon: 157042.078125, loss_pred: 0.491348
iteration 1726: loss: 157858.406250, loss_kl: 226.054214, loss_recon: 157588.281250, loss_pred: 0.440850
iteration 1727: loss: 155744.171875, loss_kl: 217.374176, loss_recon: 155478.375000, loss_pred: 0.484233
iteration 1728: loss: 159302.484375, loss_kl: 221.982147, loss_recon: 159030.765625, loss_pred: 0.497409
 96%|███████████████████████████▊ | 192/200 [2:52:24<07:07, 53.42s/it]iteration 1729: loss: 155807.453125, loss_kl: 224.548737, loss_recon: 155541.593750, loss_pred: 0.413093
iteration 1730: loss: 155740.046875, loss_kl: 222.202682, loss_recon: 155470.093750, loss_pred: 0.477540
iteration 1731: loss: 160362.156250, loss_kl: 233.835114, loss_recon: 160075.265625, loss_pred: 0.530624
iteration 1732: loss: 159648.171875, loss_kl: 222.798920, loss_recon: 159378.765625, loss_pred: 0.466051
iteration 1733: loss: 157632.375000, loss_kl: 225.314484, loss_recon: 157367.171875, loss_pred: 0.398943
iteration 1734: loss: 161080.328125, loss_kl: 227.920929, loss_recon: 160803.937500, loss_pred: 0.484688
iteration 1735: loss: 157267.390625, loss_kl: 229.674454, loss_recon: 156971.796875, loss_pred: 0.659256
iteration 1736: loss: 159409.421875, loss_kl: 233.133850, loss_recon: 159132.484375, loss_pred: 0.438020
iteration 1737: loss: 155045.281250, loss_kl: 233.999649, loss_recon: 154731.171875, loss_pred: 0.801099
 96%|███████████████████████████▉ | 193/200 [2:53:18<06:14, 53.48s/it]iteration 1738: loss: 158207.609375, loss_kl: 234.057388, loss_recon: 157914.656250, loss_pred: 0.588896
iteration 1739: loss: 154897.640625, loss_kl: 234.245590, loss_recon: 154607.000000, loss_pred: 0.563970
iteration 1740: loss: 158918.468750, loss_kl: 246.679688, loss_recon: 158623.828125, loss_pred: 0.479628
iteration 1741: loss: 158526.546875, loss_kl: 234.616959, loss_recon: 158246.578125, loss_pred: 0.453543
iteration 1742: loss: 157447.484375, loss_kl: 240.675720, loss_recon: 157136.140625, loss_pred: 0.706714
iteration 1743: loss: 156839.281250, loss_kl: 232.404770, loss_recon: 156551.687500, loss_pred: 0.551926
iteration 1744: loss: 157554.562500, loss_kl: 225.449112, loss_recon: 157283.234375, loss_pred: 0.458787
iteration 1745: loss: 159562.140625, loss_kl: 235.440536, loss_recon: 159269.296875, loss_pred: 0.574052
iteration 1746: loss: 159675.375000, loss_kl: 240.581955, loss_recon: 159390.031250, loss_pred: 0.447718
 97%|████████████████████████████▏| 194/200 [2:54:11<05:20, 53.36s/it]iteration 1747: loss: 159917.203125, loss_kl: 254.345291, loss_recon: 159609.703125, loss_pred: 0.531614
iteration 1748: loss: 155945.937500, loss_kl: 252.671906, loss_recon: 155631.796875, loss_pred: 0.614746
iteration 1749: loss: 156607.953125, loss_kl: 236.103516, loss_recon: 156315.000000, loss_pred: 0.568475
iteration 1750: loss: 160153.562500, loss_kl: 247.343307, loss_recon: 159856.890625, loss_pred: 0.493323
iteration 1751: loss: 155502.296875, loss_kl: 235.505661, loss_recon: 155212.890625, loss_pred: 0.538990
iteration 1752: loss: 159679.781250, loss_kl: 248.524277, loss_recon: 159380.000000, loss_pred: 0.512550
iteration 1753: loss: 158848.109375, loss_kl: 253.484375, loss_recon: 158549.687500, loss_pred: 0.449451
iteration 1754: loss: 155783.515625, loss_kl: 236.749863, loss_recon: 155499.343750, loss_pred: 0.474233
iteration 1755: loss: 157815.281250, loss_kl: 247.741638, loss_recon: 157521.140625, loss_pred: 0.464109
 98%|████████████████████████████▎| 195/200 [2:55:04<04:26, 53.31s/it]iteration 1756: loss: 157008.500000, loss_kl: 248.969223, loss_recon: 156710.796875, loss_pred: 0.487381
iteration 1757: loss: 160740.546875, loss_kl: 242.850586, loss_recon: 160438.890625, loss_pred: 0.588051
iteration 1758: loss: 156392.281250, loss_kl: 239.545151, loss_recon: 156104.718750, loss_pred: 0.480130
iteration 1759: loss: 159389.906250, loss_kl: 249.616211, loss_recon: 159085.734375, loss_pred: 0.545552
iteration 1760: loss: 153111.765625, loss_kl: 248.661270, loss_recon: 152789.140625, loss_pred: 0.739619
iteration 1761: loss: 156161.203125, loss_kl: 263.685425, loss_recon: 155844.984375, loss_pred: 0.525323
iteration 1762: loss: 160929.890625, loss_kl: 264.882996, loss_recon: 160592.531250, loss_pred: 0.724649
iteration 1763: loss: 159216.734375, loss_kl: 260.857788, loss_recon: 158897.328125, loss_pred: 0.585397
iteration 1764: loss: 156150.875000, loss_kl: 251.287979, loss_recon: 155847.593750, loss_pred: 0.520009
 98%|████████████████████████████▍| 196/200 [2:55:58<03:33, 53.40s/it]iteration 1765: loss: 154784.140625, loss_kl: 244.814682, loss_recon: 154490.062500, loss_pred: 0.492592
iteration 1766: loss: 159716.343750, loss_kl: 252.345810, loss_recon: 159419.765625, loss_pred: 0.442376
iteration 1767: loss: 157466.171875, loss_kl: 248.106918, loss_recon: 157161.890625, loss_pred: 0.561664
iteration 1768: loss: 157680.484375, loss_kl: 252.563278, loss_recon: 157378.234375, loss_pred: 0.496910
iteration 1769: loss: 158914.734375, loss_kl: 247.053543, loss_recon: 158616.828125, loss_pred: 0.508582
iteration 1770: loss: 158172.718750, loss_kl: 265.628845, loss_recon: 157848.078125, loss_pred: 0.590145
iteration 1771: loss: 160018.843750, loss_kl: 273.460632, loss_recon: 159688.421875, loss_pred: 0.569610
iteration 1772: loss: 157446.031250, loss_kl: 269.718109, loss_recon: 157107.093750, loss_pred: 0.692217
iteration 1773: loss: 154673.718750, loss_kl: 262.630310, loss_recon: 154360.578125, loss_pred: 0.505219
 98%|████████████████████████████▌| 197/200 [2:56:51<02:40, 53.38s/it]iteration 1774: loss: 160501.734375, loss_kl: 251.597412, loss_recon: 160205.750000, loss_pred: 0.443926
iteration 1775: loss: 157031.406250, loss_kl: 250.481918, loss_recon: 156726.234375, loss_pred: 0.546841
iteration 1776: loss: 159042.687500, loss_kl: 271.433319, loss_recon: 158718.593750, loss_pred: 0.526618
iteration 1777: loss: 159140.796875, loss_kl: 271.241730, loss_recon: 158812.312500, loss_pred: 0.572448
iteration 1778: loss: 161255.250000, loss_kl: 281.397461, loss_recon: 160912.312500, loss_pred: 0.615542
iteration 1779: loss: 156457.296875, loss_kl: 270.385406, loss_recon: 156132.578125, loss_pred: 0.543234
iteration 1780: loss: 153234.093750, loss_kl: 261.406708, loss_recon: 152908.921875, loss_pred: 0.637686
iteration 1781: loss: 154670.015625, loss_kl: 264.981659, loss_recon: 154352.250000, loss_pred: 0.527777
iteration 1782: loss: 157327.937500, loss_kl: 274.581696, loss_recon: 156983.078125, loss_pred: 0.702770
 99%|████████████████████████████▋| 198/200 [2:57:45<01:47, 53.61s/it]iteration 1783: loss: 157158.234375, loss_kl: 269.313843, loss_recon: 156830.187500, loss_pred: 0.587321
iteration 1784: loss: 156457.078125, loss_kl: 266.115448, loss_recon: 156127.031250, loss_pred: 0.639313
iteration 1785: loss: 158940.609375, loss_kl: 281.955353, loss_recon: 158593.343750, loss_pred: 0.653148
iteration 1786: loss: 154694.328125, loss_kl: 264.019043, loss_recon: 154374.843750, loss_pred: 0.554696
iteration 1787: loss: 155687.125000, loss_kl: 260.674622, loss_recon: 155364.718750, loss_pred: 0.617385
iteration 1788: loss: 155281.296875, loss_kl: 266.684601, loss_recon: 154938.640625, loss_pred: 0.759741
iteration 1789: loss: 157445.734375, loss_kl: 294.561371, loss_recon: 157095.468750, loss_pred: 0.557030
iteration 1790: loss: 160950.687500, loss_kl: 301.398621, loss_recon: 160590.765625, loss_pred: 0.585111
iteration 1791: loss: 160433.000000, loss_kl: 292.760651, loss_recon: 160070.828125, loss_pred: 0.694043
100%|████████████████████████████▊| 199/200 [2:58:39<00:53, 53.81s/it]iteration 1792: loss: 153465.031250, loss_kl: 272.134979, loss_recon: 153122.859375, loss_pred: 0.700249
iteration 1793: loss: 158205.750000, loss_kl: 280.933136, loss_recon: 157867.718750, loss_pred: 0.570918
iteration 1794: loss: 160285.109375, loss_kl: 281.170990, loss_recon: 159940.593750, loss_pred: 0.633387
iteration 1795: loss: 156980.609375, loss_kl: 296.574829, loss_recon: 156617.421875, loss_pred: 0.666138
iteration 1796: loss: 156961.328125, loss_kl: 287.101440, loss_recon: 156608.687500, loss_pred: 0.655424
iteration 1797: loss: 159298.640625, loss_kl: 303.877930, loss_recon: 158926.343750, loss_pred: 0.684207
iteration 1798: loss: 156428.453125, loss_kl: 284.354797, loss_recon: 156074.343750, loss_pred: 0.697577
iteration 1799: loss: 156931.578125, loss_kl: 286.678741, loss_recon: 156590.796875, loss_pred: 0.541139
iteration 1800: loss: 157968.703125, loss_kl: 290.735657, loss_recon: 157626.031250, loss_pred: 0.519378
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_199.pth
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_199.pth
100%|████████████████████████████▊| 199/200 [2:59:33<00:54, 54.14s/it]
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design2', img_size=[64, 64, 64], vit_patches_size=[1, 1, 1], net_path=False, vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=35, base_lr=0.001, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=35, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, number_of_samplings=6, num_classes=2, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[64, 64, 64]', label_size=11, distributed=False)
TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234
7 test iterations per epoch
0it [00:00, ?it/s]0it [00:10, ?it/s]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 389, in <module>
    inferrer[dataset_name](args, net, test_save_path)
  File "/home/mhashemi/TransVNet/test.py", line 165, in inferrer_mat2
    name_batch, metric_batch = test_multiple_volumes_generative2(image_batch, label_batch, time_batch, model, name_batch, test_save_path, number_of_samplings)
  File "/home/mhashemi/TransVNet/utils.py", line 252, in test_multiple_volumes_generative2
    absolute_errors = 100*((predicted_labels_generative_best*std + mean) - (label_batch*std + mean)) / (label_batch*std + mean)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
