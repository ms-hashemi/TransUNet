Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[1, 1, 1], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=35, base_lr=0.001, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=1, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
9 iterations per epoch. 1800 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 277063.593750, loss_kl: 317.544373, loss_recon: 275291.937500, loss_pred: 17.399191
iteration 2: loss: 274337.937500, loss_kl: 681.613525, loss_recon: 272907.312500, loss_pred: 13.624776
iteration 3: loss: 272594.031250, loss_kl: 527.258362, loss_recon: 271207.375000, loss_pred: 13.339301
iteration 4: loss: 271050.531250, loss_kl: 394.925110, loss_recon: 269801.968750, loss_pred: 12.090574
iteration 5: loss: 270423.781250, loss_kl: 345.098816, loss_recon: 269334.812500, loss_pred: 10.544690
iteration 6: loss: 269229.781250, loss_kl: 374.038605, loss_recon: 268381.812500, loss_pred: 8.105498
iteration 7: loss: 268913.531250, loss_kl: 406.778198, loss_recon: 268041.031250, loss_pred: 8.318160
iteration 8: loss: 267820.781250, loss_kl: 362.578430, loss_recon: 267113.062500, loss_pred: 6.714640
iteration 9: loss: 267698.000000, loss_kl: 356.595032, loss_recon: 267102.343750, loss_pred: 5.600086
  0%|▏                              | 1/200 [01:36<5:21:41, 96.99s/it]iteration 10: loss: 267385.437500, loss_kl: 333.793274, loss_recon: 266914.843750, loss_pred: 4.372225
iteration 11: loss: 266332.875000, loss_kl: 341.025909, loss_recon: 265920.343750, loss_pred: 3.784503
iteration 12: loss: 266304.125000, loss_kl: 388.475677, loss_recon: 265819.375000, loss_pred: 4.459109
iteration 13: loss: 265668.500000, loss_kl: 384.438049, loss_recon: 265387.156250, loss_pred: 2.429081
iteration 14: loss: 267022.750000, loss_kl: 377.011322, loss_recon: 266755.500000, loss_pred: 2.295516
iteration 15: loss: 266570.343750, loss_kl: 356.413605, loss_recon: 266244.843750, loss_pred: 2.898319
iteration 16: loss: 266132.218750, loss_kl: 355.137360, loss_recon: 265777.500000, loss_pred: 3.192276
iteration 17: loss: 265601.437500, loss_kl: 339.332947, loss_recon: 265306.687500, loss_pred: 2.608129
iteration 18: loss: 265041.750000, loss_kl: 348.732605, loss_recon: 264869.125000, loss_pred: 1.377574
  1%|▎                              | 2/200 [02:33<4:01:24, 73.16s/it]iteration 19: loss: 265313.656250, loss_kl: 354.502350, loss_recon: 265124.812500, loss_pred: 1.534062
iteration 20: loss: 265219.687500, loss_kl: 342.060272, loss_recon: 264879.875000, loss_pred: 3.055855
iteration 21: loss: 264138.437500, loss_kl: 340.101318, loss_recon: 263925.312500, loss_pred: 1.791309
iteration 22: loss: 264254.093750, loss_kl: 332.678833, loss_recon: 264023.937500, loss_pred: 1.968745
iteration 23: loss: 264596.437500, loss_kl: 324.397888, loss_recon: 264399.937500, loss_pred: 1.640768
iteration 24: loss: 264107.968750, loss_kl: 315.590851, loss_recon: 263927.375000, loss_pred: 1.490455
iteration 25: loss: 263215.218750, loss_kl: 325.718994, loss_recon: 262992.812500, loss_pred: 1.898420
iteration 26: loss: 263169.500000, loss_kl: 329.083496, loss_recon: 262996.562500, loss_pred: 1.400226
iteration 27: loss: 263411.031250, loss_kl: 330.245422, loss_recon: 263169.625000, loss_pred: 2.083835
  2%|▍                              | 3/200 [03:29<3:34:42, 65.40s/it]iteration 28: loss: 263417.875000, loss_kl: 334.084869, loss_recon: 263201.062500, loss_pred: 1.834124
iteration 29: loss: 263321.843750, loss_kl: 333.738708, loss_recon: 263138.187500, loss_pred: 1.502813
iteration 30: loss: 261991.984375, loss_kl: 323.094269, loss_recon: 261851.031250, loss_pred: 1.086350
iteration 31: loss: 262634.000000, loss_kl: 318.961670, loss_recon: 262523.531250, loss_pred: 0.785687
iteration 32: loss: 262117.718750, loss_kl: 309.814758, loss_recon: 261966.031250, loss_pred: 1.206957
iteration 33: loss: 261774.640625, loss_kl: 309.797699, loss_recon: 261616.343750, loss_pred: 1.273190
iteration 34: loss: 261341.156250, loss_kl: 309.186615, loss_recon: 261099.484375, loss_pred: 2.107493
iteration 35: loss: 261692.187500, loss_kl: 316.902313, loss_recon: 261574.796875, loss_pred: 0.857098
iteration 36: loss: 261437.718750, loss_kl: 321.075378, loss_recon: 261275.859375, loss_pred: 1.297522
  2%|▌                              | 4/200 [04:24<3:19:36, 61.10s/it]iteration 37: loss: 261627.984375, loss_kl: 323.641083, loss_recon: 261438.687500, loss_pred: 1.569315
iteration 38: loss: 260381.265625, loss_kl: 326.141449, loss_recon: 260162.578125, loss_pred: 1.860847
iteration 39: loss: 260411.875000, loss_kl: 319.203522, loss_recon: 260299.968750, loss_pred: 0.799800
iteration 40: loss: 261403.750000, loss_kl: 318.061768, loss_recon: 261210.937500, loss_pred: 1.610041
iteration 41: loss: 259606.046875, loss_kl: 310.252136, loss_recon: 259491.796875, loss_pred: 0.832191
iteration 42: loss: 260137.640625, loss_kl: 309.263885, loss_recon: 259969.656250, loss_pred: 1.370689
iteration 43: loss: 259753.781250, loss_kl: 311.971100, loss_recon: 259603.281250, loss_pred: 1.192999
iteration 44: loss: 260699.812500, loss_kl: 312.036560, loss_recon: 260541.000000, loss_pred: 1.276087
iteration 45: loss: 259016.562500, loss_kl: 313.267426, loss_recon: 258902.109375, loss_pred: 0.831302
  2%|▊                              | 5/200 [05:20<3:12:43, 59.30s/it]iteration 46: loss: 259696.484375, loss_kl: 317.267303, loss_recon: 259601.234375, loss_pred: 0.635184
iteration 47: loss: 258812.953125, loss_kl: 314.291351, loss_recon: 258668.437500, loss_pred: 1.130921
iteration 48: loss: 260150.625000, loss_kl: 319.130463, loss_recon: 259977.375000, loss_pred: 1.413403
iteration 49: loss: 258447.218750, loss_kl: 323.766418, loss_recon: 258336.859375, loss_pred: 0.779828
iteration 50: loss: 257746.671875, loss_kl: 318.212952, loss_recon: 257631.656250, loss_pred: 0.831814
iteration 51: loss: 257811.531250, loss_kl: 319.023132, loss_recon: 257719.203125, loss_pred: 0.604151
iteration 52: loss: 258305.640625, loss_kl: 314.743927, loss_recon: 258155.343750, loss_pred: 1.188339
iteration 53: loss: 257413.328125, loss_kl: 306.283203, loss_recon: 257283.031250, loss_pred: 0.996686
iteration 54: loss: 257974.593750, loss_kl: 311.089508, loss_recon: 257872.593750, loss_pred: 0.708888
  3%|▉                              | 6/200 [06:14<3:06:25, 57.66s/it]iteration 55: loss: 257912.453125, loss_kl: 308.374054, loss_recon: 257828.093750, loss_pred: 0.535112
iteration 56: loss: 256507.984375, loss_kl: 309.923920, loss_recon: 256403.375000, loss_pred: 0.736077
iteration 57: loss: 256443.500000, loss_kl: 309.574341, loss_recon: 256308.828125, loss_pred: 1.037258
iteration 58: loss: 257263.812500, loss_kl: 312.306915, loss_recon: 257134.343750, loss_pred: 0.982297
iteration 59: loss: 257261.828125, loss_kl: 310.902191, loss_recon: 257166.171875, loss_pred: 0.645557
iteration 60: loss: 256286.375000, loss_kl: 313.468933, loss_recon: 256116.281250, loss_pred: 1.387535
iteration 61: loss: 256114.656250, loss_kl: 315.494812, loss_recon: 255994.718750, loss_pred: 0.883957
iteration 62: loss: 256023.281250, loss_kl: 315.670380, loss_recon: 255943.484375, loss_pred: 0.482391
iteration 63: loss: 256333.796875, loss_kl: 316.196259, loss_recon: 256194.000000, loss_pred: 1.081708
  4%|█                              | 7/200 [07:08<3:01:40, 56.48s/it]iteration 64: loss: 255313.218750, loss_kl: 319.492462, loss_recon: 255244.687500, loss_pred: 0.365783
iteration 65: loss: 255087.171875, loss_kl: 305.663391, loss_recon: 254949.859375, loss_pred: 1.067545
iteration 66: loss: 255456.921875, loss_kl: 313.637939, loss_recon: 255349.718750, loss_pred: 0.758430
iteration 67: loss: 254774.265625, loss_kl: 309.141449, loss_recon: 254652.109375, loss_pred: 0.912300
iteration 68: loss: 255288.500000, loss_kl: 310.939972, loss_recon: 255143.265625, loss_pred: 1.141340
iteration 69: loss: 255027.453125, loss_kl: 306.026581, loss_recon: 254940.890625, loss_pred: 0.559545
iteration 70: loss: 255259.343750, loss_kl: 302.881744, loss_recon: 255141.484375, loss_pred: 0.875721
iteration 71: loss: 254510.562500, loss_kl: 307.108856, loss_recon: 254364.062500, loss_pred: 1.158044
iteration 72: loss: 253303.734375, loss_kl: 311.062073, loss_recon: 253196.687500, loss_pred: 0.759314
  4%|█▏                             | 8/200 [08:05<3:00:45, 56.49s/it]iteration 73: loss: 253285.781250, loss_kl: 306.701508, loss_recon: 253170.406250, loss_pred: 0.846978
iteration 74: loss: 253385.687500, loss_kl: 314.530457, loss_recon: 253269.921875, loss_pred: 0.843117
iteration 75: loss: 253188.812500, loss_kl: 319.257111, loss_recon: 253077.625000, loss_pred: 0.792598
iteration 76: loss: 252571.781250, loss_kl: 318.799561, loss_recon: 252461.968750, loss_pred: 0.779299
iteration 77: loss: 253163.390625, loss_kl: 315.513763, loss_recon: 253024.453125, loss_pred: 1.073921
iteration 78: loss: 253136.953125, loss_kl: 312.048065, loss_recon: 252966.718750, loss_pred: 1.390335
iteration 79: loss: 253282.265625, loss_kl: 306.999542, loss_recon: 253089.921875, loss_pred: 1.616422
iteration 80: loss: 253643.187500, loss_kl: 302.103577, loss_recon: 253517.765625, loss_pred: 0.952172
iteration 81: loss: 252534.828125, loss_kl: 302.142883, loss_recon: 252370.171875, loss_pred: 1.344306
  4%|█▍                             | 9/200 [08:59<2:57:59, 55.91s/it]iteration 82: loss: 252071.093750, loss_kl: 292.594269, loss_recon: 251821.515625, loss_pred: 2.203063
iteration 83: loss: 251681.687500, loss_kl: 297.983521, loss_recon: 251544.031250, loss_pred: 1.078560
iteration 84: loss: 252764.015625, loss_kl: 305.278107, loss_recon: 252601.343750, loss_pred: 1.321334
iteration 85: loss: 251904.921875, loss_kl: 306.243317, loss_recon: 251752.765625, loss_pred: 1.215380
iteration 86: loss: 250562.031250, loss_kl: 304.722351, loss_recon: 250434.765625, loss_pred: 0.968043
iteration 87: loss: 251491.343750, loss_kl: 310.625824, loss_recon: 251385.140625, loss_pred: 0.751349
iteration 88: loss: 250943.906250, loss_kl: 309.188507, loss_recon: 250810.687500, loss_pred: 1.023008
iteration 89: loss: 250791.843750, loss_kl: 316.317322, loss_recon: 250585.546875, loss_pred: 1.746793
iteration 90: loss: 250398.140625, loss_kl: 315.618652, loss_recon: 250285.109375, loss_pred: 0.814659
  5%|█▌                            | 10/200 [09:54<2:55:58, 55.57s/it]iteration 91: loss: 250160.781250, loss_kl: 312.589905, loss_recon: 250070.859375, loss_pred: 0.586495
iteration 92: loss: 249534.484375, loss_kl: 307.953186, loss_recon: 249380.656250, loss_pred: 1.230368
iteration 93: loss: 249633.843750, loss_kl: 305.359558, loss_recon: 249532.546875, loss_pred: 0.707583
iteration 94: loss: 249229.734375, loss_kl: 297.531464, loss_recon: 249109.828125, loss_pred: 0.901621
iteration 95: loss: 250577.046875, loss_kl: 304.736084, loss_recon: 250418.453125, loss_pred: 1.281217
iteration 96: loss: 249826.328125, loss_kl: 300.956696, loss_recon: 249730.687500, loss_pred: 0.655480
iteration 97: loss: 249314.203125, loss_kl: 300.980133, loss_recon: 249229.687500, loss_pred: 0.544251
iteration 98: loss: 248616.187500, loss_kl: 302.971771, loss_recon: 248454.578125, loss_pred: 1.313189
iteration 99: loss: 249802.906250, loss_kl: 307.609253, loss_recon: 249675.593750, loss_pred: 0.965519
  6%|█▋                            | 11/200 [10:51<2:55:50, 55.83s/it]iteration 100: loss: 248987.531250, loss_kl: 309.842407, loss_recon: 248900.687500, loss_pred: 0.558552
iteration 101: loss: 248642.812500, loss_kl: 307.908051, loss_recon: 248494.281250, loss_pred: 1.177271
iteration 102: loss: 248320.375000, loss_kl: 298.265839, loss_recon: 248220.234375, loss_pred: 0.703071
iteration 103: loss: 248062.937500, loss_kl: 300.532867, loss_recon: 247988.859375, loss_pred: 0.440291
iteration 104: loss: 247021.562500, loss_kl: 301.884308, loss_recon: 246884.765625, loss_pred: 1.066146
iteration 105: loss: 247461.375000, loss_kl: 307.876587, loss_recon: 247368.000000, loss_pred: 0.625870
iteration 106: loss: 247414.406250, loss_kl: 307.239624, loss_recon: 247310.312500, loss_pred: 0.733803
iteration 107: loss: 247550.000000, loss_kl: 302.623993, loss_recon: 247458.578125, loss_pred: 0.611601
iteration 108: loss: 247554.359375, loss_kl: 300.760101, loss_recon: 247412.578125, loss_pred: 1.117013
  6%|█▊                            | 12/200 [11:46<2:54:20, 55.64s/it]iteration 109: loss: 246519.953125, loss_kl: 301.076294, loss_recon: 246345.593750, loss_pred: 1.442500
iteration 110: loss: 246896.562500, loss_kl: 305.700439, loss_recon: 246782.281250, loss_pred: 0.837168
iteration 111: loss: 246828.843750, loss_kl: 305.156921, loss_recon: 246732.546875, loss_pred: 0.657812
iteration 112: loss: 247208.250000, loss_kl: 304.455902, loss_recon: 247062.343750, loss_pred: 1.154583
iteration 113: loss: 245935.500000, loss_kl: 298.816345, loss_recon: 245846.859375, loss_pred: 0.587598
iteration 114: loss: 246282.921875, loss_kl: 292.175720, loss_recon: 246187.484375, loss_pred: 0.662173
iteration 115: loss: 246237.140625, loss_kl: 291.141602, loss_recon: 246150.453125, loss_pred: 0.575788
iteration 116: loss: 245246.843750, loss_kl: 291.797607, loss_recon: 245032.968750, loss_pred: 1.846844
iteration 117: loss: 244804.187500, loss_kl: 301.104065, loss_recon: 244611.828125, loss_pred: 1.622450
  6%|█▉                            | 13/200 [12:40<2:52:11, 55.25s/it]iteration 118: loss: 244960.046875, loss_kl: 310.606323, loss_recon: 244667.765625, loss_pred: 1.382115
iteration 119: loss: 244679.250000, loss_kl: 319.791840, loss_recon: 244321.375000, loss_pred: 1.992702
iteration 120: loss: 245417.000000, loss_kl: 316.106476, loss_recon: 245124.000000, loss_pred: 1.362136
iteration 121: loss: 244883.218750, loss_kl: 310.015228, loss_recon: 244608.859375, loss_pred: 1.206009
iteration 122: loss: 244799.156250, loss_kl: 299.149567, loss_recon: 244470.750000, loss_pred: 1.800383
iteration 123: loss: 244288.359375, loss_kl: 295.450653, loss_recon: 243922.921875, loss_pred: 2.188978
iteration 124: loss: 245242.718750, loss_kl: 290.944580, loss_recon: 245037.593750, loss_pred: 0.608059
iteration 125: loss: 243952.000000, loss_kl: 293.903076, loss_recon: 243719.484375, loss_pred: 0.867315
iteration 126: loss: 243901.125000, loss_kl: 295.008636, loss_recon: 243627.281250, loss_pred: 1.275178
  7%|██                            | 14/200 [13:34<2:49:54, 54.81s/it]iteration 127: loss: 243052.515625, loss_kl: 295.496918, loss_recon: 242701.093750, loss_pred: 0.878458
iteration 128: loss: 243183.218750, loss_kl: 291.605225, loss_recon: 242820.453125, loss_pred: 1.026636
iteration 129: loss: 244045.812500, loss_kl: 287.364624, loss_recon: 243654.453125, loss_pred: 1.350286
iteration 130: loss: 243235.671875, loss_kl: 287.724396, loss_recon: 242889.343750, loss_pred: 0.896699
iteration 131: loss: 242915.687500, loss_kl: 297.776245, loss_recon: 242487.921875, loss_pred: 1.621633
iteration 132: loss: 243354.734375, loss_kl: 291.308136, loss_recon: 242963.265625, loss_pred: 1.316251
iteration 133: loss: 242590.500000, loss_kl: 292.767456, loss_recon: 242200.062500, loss_pred: 1.292812
iteration 134: loss: 242873.281250, loss_kl: 295.913696, loss_recon: 242537.312500, loss_pred: 0.720181
iteration 135: loss: 242504.187500, loss_kl: 291.591309, loss_recon: 242094.234375, loss_pred: 1.498663
  8%|██▎                           | 15/200 [14:29<2:48:46, 54.74s/it]iteration 136: loss: 242433.296875, loss_kl: 285.507629, loss_recon: 241944.859375, loss_pred: 1.206979
iteration 137: loss: 241796.125000, loss_kl: 276.354065, loss_recon: 241362.859375, loss_pred: 0.773227
iteration 138: loss: 242086.875000, loss_kl: 272.875397, loss_recon: 241620.312500, loss_pred: 1.150965
iteration 139: loss: 241058.703125, loss_kl: 278.561279, loss_recon: 240448.093750, loss_pred: 2.518263
iteration 140: loss: 241407.375000, loss_kl: 277.772003, loss_recon: 240953.796875, loss_pred: 0.958126
iteration 141: loss: 241496.859375, loss_kl: 276.649353, loss_recon: 241078.093750, loss_pred: 0.624436
iteration 142: loss: 240962.765625, loss_kl: 277.905975, loss_recon: 240488.281250, loss_pred: 1.165525
iteration 143: loss: 241256.250000, loss_kl: 278.980408, loss_recon: 240703.890625, loss_pred: 1.930302
iteration 144: loss: 241364.593750, loss_kl: 274.250061, loss_recon: 240955.281250, loss_pred: 0.560706
  8%|██▍                           | 16/200 [15:25<2:49:00, 55.11s/it]iteration 145: loss: 239974.296875, loss_kl: 267.963776, loss_recon: 239387.750000, loss_pred: 1.353003
iteration 146: loss: 240742.390625, loss_kl: 265.008240, loss_recon: 240150.968750, loss_pred: 1.451464
iteration 147: loss: 240611.609375, loss_kl: 264.378693, loss_recon: 240106.062500, loss_pred: 0.603276
iteration 148: loss: 240028.421875, loss_kl: 259.929291, loss_recon: 239522.171875, loss_pred: 0.685323
iteration 149: loss: 240671.734375, loss_kl: 258.084198, loss_recon: 240140.796875, loss_pred: 0.963356
iteration 150: loss: 239263.796875, loss_kl: 257.387146, loss_recon: 238748.593750, loss_pred: 0.817623
iteration 151: loss: 238666.281250, loss_kl: 257.350433, loss_recon: 238172.765625, loss_pred: 0.601445
iteration 152: loss: 239421.156250, loss_kl: 257.304565, loss_recon: 238889.343750, loss_pred: 0.985122
iteration 153: loss: 240104.718750, loss_kl: 250.966125, loss_recon: 239597.671875, loss_pred: 0.844204
  8%|██▌                           | 17/200 [16:21<2:49:19, 55.51s/it]iteration 154: loss: 240278.140625, loss_kl: 252.597107, loss_recon: 239661.765625, loss_pred: 0.909764
iteration 155: loss: 239672.281250, loss_kl: 249.174301, loss_recon: 239051.890625, loss_pred: 1.021096
iteration 156: loss: 238967.046875, loss_kl: 242.019226, loss_recon: 238406.859375, loss_pred: 0.567762
iteration 157: loss: 237934.687500, loss_kl: 240.902649, loss_recon: 237312.437500, loss_pred: 1.211778
iteration 158: loss: 238175.281250, loss_kl: 239.180801, loss_recon: 237571.593750, loss_pred: 1.061946
iteration 159: loss: 237549.515625, loss_kl: 240.911194, loss_recon: 236983.937500, loss_pred: 0.644891
iteration 160: loss: 237913.453125, loss_kl: 241.826447, loss_recon: 237315.781250, loss_pred: 0.946786
iteration 161: loss: 238235.156250, loss_kl: 239.397888, loss_recon: 237670.703125, loss_pred: 0.664948
iteration 162: loss: 236689.734375, loss_kl: 239.534515, loss_recon: 236100.859375, loss_pred: 0.906401
  9%|██▋                           | 18/200 [17:15<2:46:41, 54.95s/it]iteration 163: loss: 237215.781250, loss_kl: 234.589172, loss_recon: 236522.796875, loss_pred: 1.121420
iteration 164: loss: 236670.750000, loss_kl: 233.538834, loss_recon: 236028.265625, loss_pred: 0.642448
iteration 165: loss: 237380.421875, loss_kl: 231.454361, loss_recon: 236721.031250, loss_pred: 0.863055
iteration 166: loss: 237115.937500, loss_kl: 230.681244, loss_recon: 236481.609375, loss_pred: 0.631569
iteration 167: loss: 236232.640625, loss_kl: 229.693085, loss_recon: 235617.765625, loss_pred: 0.461493
iteration 168: loss: 237546.515625, loss_kl: 224.591217, loss_recon: 236932.859375, loss_pred: 0.575572
iteration 169: loss: 237171.265625, loss_kl: 224.424500, loss_recon: 236532.062500, loss_pred: 0.835344
iteration 170: loss: 235437.140625, loss_kl: 224.445312, loss_recon: 234778.687500, loss_pred: 1.027170
iteration 171: loss: 236579.328125, loss_kl: 220.835251, loss_recon: 235981.812500, loss_pred: 0.507326
 10%|██▊                           | 19/200 [18:11<2:47:21, 55.48s/it]iteration 172: loss: 236282.406250, loss_kl: 218.753616, loss_recon: 235550.421875, loss_pred: 1.037214
iteration 173: loss: 235368.781250, loss_kl: 215.803848, loss_recon: 234657.187500, loss_pred: 0.918080
iteration 174: loss: 235697.750000, loss_kl: 216.452515, loss_recon: 235008.562500, loss_pred: 0.675256
iteration 175: loss: 235876.734375, loss_kl: 221.578308, loss_recon: 235138.718750, loss_pred: 1.016410
iteration 176: loss: 234836.812500, loss_kl: 217.747986, loss_recon: 234138.171875, loss_pred: 0.732700
iteration 177: loss: 236178.609375, loss_kl: 214.936737, loss_recon: 235513.765625, loss_pred: 0.475409
iteration 178: loss: 234737.609375, loss_kl: 213.922684, loss_recon: 234030.890625, loss_pred: 0.923265
iteration 179: loss: 234699.625000, loss_kl: 211.670441, loss_recon: 234041.390625, loss_pred: 0.503191
iteration 180: loss: 233720.375000, loss_kl: 209.018066, loss_recon: 233082.125000, loss_pred: 0.379579
 10%|███                           | 20/200 [19:06<2:45:47, 55.26s/it]iteration 181: loss: 234152.015625, loss_kl: 209.848083, loss_recon: 233388.531250, loss_pred: 0.777032
iteration 182: loss: 234182.906250, loss_kl: 209.039047, loss_recon: 233413.359375, loss_pred: 0.864010
iteration 183: loss: 234248.281250, loss_kl: 207.787201, loss_recon: 233488.578125, loss_pred: 0.806513
iteration 184: loss: 234457.562500, loss_kl: 204.355560, loss_recon: 233751.687500, loss_pred: 0.380491
iteration 185: loss: 233832.921875, loss_kl: 203.454788, loss_recon: 233130.093750, loss_pred: 0.379348
iteration 186: loss: 233325.781250, loss_kl: 203.569138, loss_recon: 232559.843750, loss_pred: 1.006778
iteration 187: loss: 233448.062500, loss_kl: 201.344666, loss_recon: 232728.390625, loss_pred: 0.616760
iteration 188: loss: 232771.609375, loss_kl: 199.134476, loss_recon: 232028.656250, loss_pred: 0.921940
iteration 189: loss: 233302.593750, loss_kl: 200.230835, loss_recon: 232541.250000, loss_pred: 1.069850
 10%|███▏                          | 21/200 [20:02<2:45:08, 55.36s/it]iteration 190: loss: 231837.031250, loss_kl: 198.593018, loss_recon: 231047.218750, loss_pred: 0.621721
iteration 191: loss: 232710.531250, loss_kl: 194.822464, loss_recon: 231941.046875, loss_pred: 0.556572
iteration 192: loss: 233220.109375, loss_kl: 195.303040, loss_recon: 232442.687500, loss_pred: 0.618324
iteration 193: loss: 232549.218750, loss_kl: 191.459381, loss_recon: 231793.687500, loss_pred: 0.540367
iteration 194: loss: 232243.890625, loss_kl: 191.214264, loss_recon: 231498.140625, loss_pred: 0.451366
iteration 195: loss: 232092.750000, loss_kl: 186.662033, loss_recon: 231352.093750, loss_pred: 0.567148
iteration 196: loss: 231816.125000, loss_kl: 188.247345, loss_recon: 231071.078125, loss_pred: 0.553079
iteration 197: loss: 231344.484375, loss_kl: 185.455887, loss_recon: 230602.218750, loss_pred: 0.627550
iteration 198: loss: 232000.250000, loss_kl: 186.836197, loss_recon: 231275.875000, loss_pred: 0.398164
 11%|███▎                          | 22/200 [20:56<2:43:43, 55.19s/it]iteration 199: loss: 231453.656250, loss_kl: 186.441803, loss_recon: 230640.437500, loss_pred: 0.562615
iteration 200: loss: 231244.968750, loss_kl: 183.533798, loss_recon: 230456.437500, loss_pred: 0.433870
iteration 201: loss: 231318.906250, loss_kl: 183.996323, loss_recon: 230519.218750, loss_pred: 0.526566
iteration 202: loss: 231268.453125, loss_kl: 180.280991, loss_recon: 230457.765625, loss_pred: 0.787504
iteration 203: loss: 230405.187500, loss_kl: 183.899689, loss_recon: 229599.578125, loss_pred: 0.589778
iteration 204: loss: 230120.000000, loss_kl: 179.616837, loss_recon: 229314.562500, loss_pred: 0.761911
iteration 205: loss: 230689.203125, loss_kl: 176.533478, loss_recon: 229926.406250, loss_pred: 0.460741
iteration 206: loss: 229896.609375, loss_kl: 175.671555, loss_recon: 229103.421875, loss_pred: 0.799658
iteration 207: loss: 229962.859375, loss_kl: 174.821548, loss_recon: 229208.015625, loss_pred: 0.450649
 12%|███▍                          | 23/200 [21:51<2:42:06, 54.95s/it]iteration 208: loss: 230361.375000, loss_kl: 173.470261, loss_recon: 229545.062500, loss_pred: 0.433306
iteration 209: loss: 230065.296875, loss_kl: 173.537338, loss_recon: 229240.718750, loss_pred: 0.513021
iteration 210: loss: 230169.453125, loss_kl: 172.677536, loss_recon: 229327.515625, loss_pred: 0.724827
iteration 211: loss: 228684.296875, loss_kl: 171.213470, loss_recon: 227881.250000, loss_pred: 0.401274
iteration 212: loss: 228793.078125, loss_kl: 168.503799, loss_recon: 227949.031250, loss_pred: 0.931878
iteration 213: loss: 228299.093750, loss_kl: 166.426819, loss_recon: 227510.437500, loss_pred: 0.470606
iteration 214: loss: 228641.093750, loss_kl: 166.733536, loss_recon: 227857.515625, loss_pred: 0.406053
iteration 215: loss: 229037.156250, loss_kl: 166.922287, loss_recon: 228251.984375, loss_pred: 0.413601
iteration 216: loss: 228832.015625, loss_kl: 165.453491, loss_recon: 228046.625000, loss_pred: 0.481283
 12%|███▌                          | 24/200 [22:45<2:40:02, 54.56s/it]iteration 217: loss: 227896.875000, loss_kl: 163.112076, loss_recon: 227070.062500, loss_pred: 0.353964
iteration 218: loss: 229163.750000, loss_kl: 162.300293, loss_recon: 228326.453125, loss_pred: 0.498201
iteration 219: loss: 228028.828125, loss_kl: 161.388474, loss_recon: 227203.171875, loss_pred: 0.425946
iteration 220: loss: 228752.890625, loss_kl: 161.006805, loss_recon: 227928.375000, loss_pred: 0.433161
iteration 221: loss: 228259.953125, loss_kl: 160.069214, loss_recon: 227459.453125, loss_pred: 0.238511
iteration 222: loss: 226079.171875, loss_kl: 159.023117, loss_recon: 225258.093750, loss_pred: 0.495008
iteration 223: loss: 227104.015625, loss_kl: 152.390594, loss_recon: 226264.203125, loss_pred: 1.004061
iteration 224: loss: 227234.125000, loss_kl: 155.110016, loss_recon: 226440.375000, loss_pred: 0.411512
iteration 225: loss: 226896.140625, loss_kl: 155.579254, loss_recon: 226052.437500, loss_pred: 0.888232
 12%|███▊                          | 25/200 [23:41<2:40:46, 55.13s/it]iteration 226: loss: 226519.703125, loss_kl: 153.337997, loss_recon: 225652.953125, loss_pred: 0.620256
iteration 227: loss: 226038.609375, loss_kl: 151.890884, loss_recon: 225170.109375, loss_pred: 0.713828
iteration 228: loss: 227217.562500, loss_kl: 149.738907, loss_recon: 226331.656250, loss_pred: 1.000793
iteration 229: loss: 226896.406250, loss_kl: 148.724503, loss_recon: 226083.375000, loss_pred: 0.325379
iteration 230: loss: 225961.984375, loss_kl: 148.602844, loss_recon: 225107.281250, loss_pred: 0.748218
iteration 231: loss: 225937.906250, loss_kl: 147.443970, loss_recon: 225121.828125, loss_pred: 0.423044
iteration 232: loss: 225822.015625, loss_kl: 146.947678, loss_recon: 224983.984375, loss_pred: 0.668379
iteration 233: loss: 225976.828125, loss_kl: 143.621201, loss_recon: 225189.296875, loss_pred: 0.338188
iteration 234: loss: 225781.234375, loss_kl: 142.211517, loss_recon: 224997.609375, loss_pred: 0.373003
 13%|███▉                          | 26/200 [24:35<2:39:03, 54.85s/it]iteration 235: loss: 225043.859375, loss_kl: 140.078491, loss_recon: 224152.921875, loss_pred: 1.003228
iteration 236: loss: 224905.984375, loss_kl: 138.771652, loss_recon: 224088.671875, loss_pred: 0.340852
iteration 237: loss: 225343.406250, loss_kl: 140.320236, loss_recon: 224480.265625, loss_pred: 0.711783
iteration 238: loss: 224706.078125, loss_kl: 137.931473, loss_recon: 223874.140625, loss_pred: 0.534496
iteration 239: loss: 225208.531250, loss_kl: 137.139984, loss_recon: 224392.812500, loss_pred: 0.417106
iteration 240: loss: 225196.250000, loss_kl: 133.155136, loss_recon: 224386.484375, loss_pred: 0.582362
iteration 241: loss: 224648.578125, loss_kl: 131.136948, loss_recon: 223845.000000, loss_pred: 0.634373
iteration 242: loss: 223867.312500, loss_kl: 130.202255, loss_recon: 223065.562500, loss_pred: 0.668904
iteration 243: loss: 224036.984375, loss_kl: 129.538589, loss_recon: 223239.343750, loss_pred: 0.665276
 14%|████                          | 27/200 [25:31<2:39:04, 55.17s/it]iteration 244: loss: 224468.203125, loss_kl: 129.848892, loss_recon: 223608.765625, loss_pred: 0.751565
iteration 245: loss: 224781.000000, loss_kl: 125.840958, loss_recon: 223965.515625, loss_pred: 0.554092
iteration 246: loss: 223304.500000, loss_kl: 122.926620, loss_recon: 222492.375000, loss_pred: 0.696406
iteration 247: loss: 224472.062500, loss_kl: 120.752243, loss_recon: 223663.937500, loss_pred: 0.787792
iteration 248: loss: 222265.984375, loss_kl: 120.783165, loss_recon: 221475.203125, loss_pred: 0.612445
iteration 249: loss: 222922.453125, loss_kl: 122.206696, loss_recon: 222112.375000, loss_pred: 0.719519
iteration 250: loss: 222770.671875, loss_kl: 118.839622, loss_recon: 221997.578125, loss_pred: 0.552895
iteration 251: loss: 223067.546875, loss_kl: 117.449837, loss_recon: 222297.437500, loss_pred: 0.607125
iteration 252: loss: 221632.156250, loss_kl: 114.011070, loss_recon: 220882.890625, loss_pred: 0.606341
 14%|████▏                         | 28/200 [26:25<2:36:55, 54.74s/it]iteration 253: loss: 223423.984375, loss_kl: 110.158226, loss_recon: 222659.093750, loss_pred: 0.558985
iteration 254: loss: 222487.437500, loss_kl: 110.077866, loss_recon: 221715.953125, loss_pred: 0.630118
iteration 255: loss: 221333.156250, loss_kl: 108.208565, loss_recon: 220596.546875, loss_pred: 0.401793
iteration 256: loss: 222454.937500, loss_kl: 107.666023, loss_recon: 221725.656250, loss_pred: 0.363467
iteration 257: loss: 222364.500000, loss_kl: 107.688293, loss_recon: 221598.656250, loss_pred: 0.727729
iteration 258: loss: 221524.500000, loss_kl: 103.904518, loss_recon: 220787.171875, loss_pred: 0.685934
iteration 259: loss: 221687.296875, loss_kl: 101.698242, loss_recon: 220980.593750, loss_pred: 0.521689
iteration 260: loss: 219762.734375, loss_kl: 102.095917, loss_recon: 219045.921875, loss_pred: 0.597250
iteration 261: loss: 221234.937500, loss_kl: 102.416389, loss_recon: 220536.515625, loss_pred: 0.392655
 14%|████▎                         | 29/200 [27:19<2:35:53, 54.70s/it]iteration 262: loss: 220531.265625, loss_kl: 100.860458, loss_recon: 219776.593750, loss_pred: 0.655917
iteration 263: loss: 220035.953125, loss_kl: 97.182175, loss_recon: 219307.875000, loss_pred: 0.641278
iteration 264: loss: 220795.437500, loss_kl: 97.381638, loss_recon: 220056.203125, loss_pred: 0.739144
iteration 265: loss: 220299.218750, loss_kl: 97.305016, loss_recon: 219554.140625, loss_pred: 0.802939
iteration 266: loss: 221356.500000, loss_kl: 92.873077, loss_recon: 220669.171875, loss_pred: 0.528109
iteration 267: loss: 221189.578125, loss_kl: 92.216782, loss_recon: 220489.406250, loss_pred: 0.701356
iteration 268: loss: 220717.187500, loss_kl: 89.025276, loss_recon: 220029.265625, loss_pred: 0.797092
iteration 269: loss: 218966.875000, loss_kl: 91.047638, loss_recon: 218257.687500, loss_pred: 0.871498
iteration 270: loss: 219536.828125, loss_kl: 91.044884, loss_recon: 218840.625000, loss_pred: 0.741920
 15%|████▌                         | 30/200 [28:16<2:36:28, 55.23s/it]iteration 271: loss: 220266.625000, loss_kl: 90.885757, loss_recon: 219508.968750, loss_pred: 1.007275
iteration 272: loss: 219804.734375, loss_kl: 88.886536, loss_recon: 219075.546875, loss_pred: 0.867121
iteration 273: loss: 219093.468750, loss_kl: 86.201828, loss_recon: 218364.484375, loss_pred: 1.059252
iteration 274: loss: 218738.765625, loss_kl: 85.231735, loss_recon: 217970.078125, loss_pred: 1.526247
iteration 275: loss: 218355.000000, loss_kl: 84.839943, loss_recon: 217648.781250, loss_pred: 0.929996
iteration 276: loss: 217296.484375, loss_kl: 85.497398, loss_recon: 216644.406250, loss_pred: 0.341051
iteration 277: loss: 219877.937500, loss_kl: 86.556526, loss_recon: 219072.359375, loss_pred: 1.799525
iteration 278: loss: 219586.781250, loss_kl: 84.239792, loss_recon: 218852.843750, loss_pred: 1.250428
iteration 279: loss: 218157.328125, loss_kl: 81.565948, loss_recon: 217458.578125, loss_pred: 1.091852
 16%|████▋                         | 31/200 [29:11<2:35:43, 55.28s/it]iteration 280: loss: 219779.656250, loss_kl: 79.533501, loss_recon: 219053.609375, loss_pred: 1.196828
iteration 281: loss: 217600.703125, loss_kl: 78.254997, loss_recon: 216773.281250, loss_pred: 2.308143
iteration 282: loss: 218898.031250, loss_kl: 77.560532, loss_recon: 218221.906250, loss_pred: 0.847955
iteration 283: loss: 217947.015625, loss_kl: 76.962402, loss_recon: 217312.859375, loss_pred: 0.473839
iteration 284: loss: 216293.453125, loss_kl: 77.672638, loss_recon: 215607.843750, loss_pred: 0.934375
iteration 285: loss: 216981.062500, loss_kl: 77.147598, loss_recon: 216321.140625, loss_pred: 0.717476
iteration 286: loss: 217225.703125, loss_kl: 75.411674, loss_recon: 216585.937500, loss_pred: 0.648341
iteration 287: loss: 216940.359375, loss_kl: 77.460915, loss_recon: 216269.921875, loss_pred: 0.798818
iteration 288: loss: 216879.421875, loss_kl: 73.034203, loss_recon: 216220.984375, loss_pred: 1.016174
 16%|████▊                         | 32/200 [30:05<2:33:41, 54.89s/it]iteration 289: loss: 217696.484375, loss_kl: 74.984978, loss_recon: 217007.593750, loss_pred: 0.875168
iteration 290: loss: 216706.000000, loss_kl: 71.292099, loss_recon: 216052.937500, loss_pred: 0.812959
iteration 291: loss: 216222.203125, loss_kl: 71.186462, loss_recon: 215518.515625, loss_pred: 1.327670
iteration 292: loss: 216373.109375, loss_kl: 68.601173, loss_recon: 215749.921875, loss_pred: 0.730002
iteration 293: loss: 215430.437500, loss_kl: 69.249649, loss_recon: 214814.109375, loss_pred: 0.609500
iteration 294: loss: 217301.484375, loss_kl: 68.202278, loss_recon: 216685.734375, loss_pred: 0.687632
iteration 295: loss: 215768.390625, loss_kl: 67.872444, loss_recon: 215168.156250, loss_pred: 0.558914
iteration 296: loss: 214866.515625, loss_kl: 67.669403, loss_recon: 214274.437500, loss_pred: 0.493816
iteration 297: loss: 215499.593750, loss_kl: 65.936157, loss_recon: 214913.046875, loss_pred: 0.577394
 16%|████▉                         | 33/200 [31:01<2:33:36, 55.19s/it]iteration 298: loss: 216791.812500, loss_kl: 65.081238, loss_recon: 216169.921875, loss_pred: 0.741765
iteration 299: loss: 214800.578125, loss_kl: 63.233929, loss_recon: 214191.062500, loss_pred: 0.773417
iteration 300: loss: 215073.656250, loss_kl: 61.609779, loss_recon: 214499.984375, loss_pred: 0.551563
iteration 301: loss: 215272.984375, loss_kl: 63.085476, loss_recon: 214680.453125, loss_pred: 0.616021
iteration 302: loss: 214231.937500, loss_kl: 61.751003, loss_recon: 213661.109375, loss_pred: 0.511273
iteration 303: loss: 214081.859375, loss_kl: 60.263874, loss_recon: 213475.906250, loss_pred: 0.987599
iteration 304: loss: 214933.078125, loss_kl: 60.367279, loss_recon: 214349.796875, loss_pred: 0.752303
iteration 305: loss: 214599.375000, loss_kl: 60.295479, loss_recon: 214051.421875, loss_pred: 0.405044
iteration 306: loss: 213662.281250, loss_kl: 60.693501, loss_recon: 213090.031250, loss_pred: 0.614523
 17%|█████                         | 34/200 [31:57<2:32:49, 55.24s/it]iteration 307: loss: 214552.296875, loss_kl: 57.811291, loss_recon: 213990.687500, loss_pred: 0.521663
iteration 308: loss: 212973.171875, loss_kl: 57.754387, loss_recon: 212416.484375, loss_pred: 0.477536
iteration 309: loss: 213124.515625, loss_kl: 56.487392, loss_recon: 212550.828125, loss_pred: 0.759152
iteration 310: loss: 213424.125000, loss_kl: 55.425396, loss_recon: 212887.734375, loss_pred: 0.479893
iteration 311: loss: 213577.140625, loss_kl: 54.456860, loss_recon: 213034.234375, loss_pred: 0.630356
iteration 312: loss: 214548.125000, loss_kl: 53.547916, loss_recon: 214002.718750, loss_pred: 0.735458
iteration 313: loss: 212387.515625, loss_kl: 53.029778, loss_recon: 211888.609375, loss_pred: 0.316129
iteration 314: loss: 213739.593750, loss_kl: 52.524097, loss_recon: 213245.812500, loss_pred: 0.309334
iteration 315: loss: 212856.468750, loss_kl: 52.942593, loss_recon: 212353.093750, loss_pred: 0.368428
 18%|█████▎                        | 35/200 [32:53<2:32:52, 55.59s/it]iteration 316: loss: 213421.015625, loss_kl: 50.495750, loss_recon: 212919.000000, loss_pred: 0.370423
iteration 317: loss: 212957.359375, loss_kl: 51.006004, loss_recon: 212444.281250, loss_pred: 0.434263
iteration 318: loss: 212714.718750, loss_kl: 50.593506, loss_recon: 212193.937500, loss_pred: 0.549245
iteration 319: loss: 213257.125000, loss_kl: 49.514954, loss_recon: 212766.234375, loss_pred: 0.349546
iteration 320: loss: 211912.125000, loss_kl: 49.979733, loss_recon: 211369.578125, loss_pred: 0.823314
iteration 321: loss: 211444.265625, loss_kl: 49.533245, loss_recon: 210933.656250, loss_pred: 0.545070
iteration 322: loss: 211631.859375, loss_kl: 48.117607, loss_recon: 211130.859375, loss_pred: 0.579363
iteration 323: loss: 209964.906250, loss_kl: 47.864159, loss_recon: 209456.937500, loss_pred: 0.672331
iteration 324: loss: 211932.578125, loss_kl: 45.632233, loss_recon: 211437.828125, loss_pred: 0.745621
 18%|█████▍                        | 36/200 [33:50<2:33:28, 56.15s/it]iteration 325: loss: 210823.312500, loss_kl: 45.787910, loss_recon: 210315.000000, loss_pred: 0.685628
iteration 326: loss: 210999.484375, loss_kl: 46.806641, loss_recon: 210499.437500, loss_pred: 0.505103
iteration 327: loss: 212286.093750, loss_kl: 44.358810, loss_recon: 211812.656250, loss_pred: 0.474184
iteration 328: loss: 210653.125000, loss_kl: 44.238457, loss_recon: 210188.203125, loss_pred: 0.400598
iteration 329: loss: 211434.046875, loss_kl: 42.735504, loss_recon: 210984.031250, loss_pred: 0.395746
iteration 330: loss: 210409.812500, loss_kl: 43.256531, loss_recon: 209959.937500, loss_pred: 0.344364
iteration 331: loss: 211124.890625, loss_kl: 41.849224, loss_recon: 210684.859375, loss_pred: 0.381068
iteration 332: loss: 211704.875000, loss_kl: 41.508835, loss_recon: 211258.093750, loss_pred: 0.481288
iteration 333: loss: 208079.640625, loss_kl: 45.002811, loss_recon: 207601.875000, loss_pred: 0.455594
 18%|█████▌                        | 37/200 [34:46<2:31:44, 55.86s/it]iteration 334: loss: 210251.578125, loss_kl: 43.074963, loss_recon: 209767.515625, loss_pred: 0.533084
iteration 335: loss: 209998.609375, loss_kl: 39.649380, loss_recon: 209560.968750, loss_pred: 0.411425
iteration 336: loss: 210356.093750, loss_kl: 40.305645, loss_recon: 209905.093750, loss_pred: 0.479440
iteration 337: loss: 208293.109375, loss_kl: 40.018253, loss_recon: 207825.359375, loss_pred: 0.675576
iteration 338: loss: 210955.296875, loss_kl: 37.585140, loss_recon: 210490.234375, loss_pred: 0.892134
iteration 339: loss: 207958.718750, loss_kl: 41.151337, loss_recon: 207487.265625, loss_pred: 0.599416
iteration 340: loss: 209708.890625, loss_kl: 38.640976, loss_recon: 209271.171875, loss_pred: 0.513149
iteration 341: loss: 209198.859375, loss_kl: 38.082829, loss_recon: 208768.515625, loss_pred: 0.495195
iteration 342: loss: 209358.281250, loss_kl: 36.690208, loss_recon: 208935.703125, loss_pred: 0.556679
 19%|█████▋                        | 38/200 [35:40<2:29:55, 55.53s/it]iteration 343: loss: 207789.640625, loss_kl: 37.648632, loss_recon: 207308.625000, loss_pred: 1.045337
iteration 344: loss: 208556.671875, loss_kl: 38.014843, loss_recon: 208100.968750, loss_pred: 0.755482
iteration 345: loss: 208853.687500, loss_kl: 36.516472, loss_recon: 208430.546875, loss_pred: 0.579750
iteration 346: loss: 209169.031250, loss_kl: 34.641666, loss_recon: 208759.781250, loss_pred: 0.628213
iteration 347: loss: 208430.828125, loss_kl: 34.383797, loss_recon: 208045.406250, loss_pred: 0.415747
iteration 348: loss: 208748.156250, loss_kl: 35.095360, loss_recon: 208318.687500, loss_pred: 0.785205
iteration 349: loss: 207586.984375, loss_kl: 34.594887, loss_recon: 207157.093750, loss_pred: 0.839365
iteration 350: loss: 207700.640625, loss_kl: 33.015800, loss_recon: 207343.171875, loss_pred: 0.273169
iteration 351: loss: 207846.218750, loss_kl: 33.551899, loss_recon: 207453.625000, loss_pred: 0.570808
 20%|█████▊                        | 39/200 [36:36<2:29:10, 55.59s/it]iteration 352: loss: 207728.296875, loss_kl: 33.099617, loss_recon: 207345.406250, loss_pred: 0.518933
iteration 353: loss: 206269.921875, loss_kl: 33.473373, loss_recon: 205853.828125, loss_pred: 0.813668
iteration 354: loss: 207607.171875, loss_kl: 32.650211, loss_recon: 207231.750000, loss_pred: 0.489144
iteration 355: loss: 206911.468750, loss_kl: 32.330021, loss_recon: 206550.187500, loss_pred: 0.379872
iteration 356: loss: 206468.843750, loss_kl: 32.851974, loss_recon: 206085.328125, loss_pred: 0.549940
iteration 357: loss: 206870.265625, loss_kl: 31.597336, loss_recon: 206506.000000, loss_pred: 0.482897
iteration 358: loss: 207550.015625, loss_kl: 31.925907, loss_recon: 207149.859375, loss_pred: 0.808940
iteration 359: loss: 207220.796875, loss_kl: 31.159473, loss_recon: 206873.859375, loss_pred: 0.353402
iteration 360: loss: 206654.218750, loss_kl: 30.496206, loss_recon: 206289.546875, loss_pred: 0.596975
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_39.pth
 20%|██████                        | 40/200 [37:32<2:28:50, 55.81s/it]iteration 361: loss: 207528.125000, loss_kl: 29.850859, loss_recon: 207125.000000, loss_pred: 1.046065
iteration 362: loss: 206407.484375, loss_kl: 28.576517, loss_recon: 206072.453125, loss_pred: 0.492615
iteration 363: loss: 206007.250000, loss_kl: 29.839878, loss_recon: 205677.312500, loss_pred: 0.315269
iteration 364: loss: 204566.921875, loss_kl: 30.864725, loss_recon: 204227.515625, loss_pred: 0.307667
iteration 365: loss: 206163.906250, loss_kl: 28.349867, loss_recon: 205847.265625, loss_pred: 0.331362
iteration 366: loss: 205902.875000, loss_kl: 28.476351, loss_recon: 205596.656250, loss_pred: 0.214535
iteration 367: loss: 205646.015625, loss_kl: 27.297775, loss_recon: 205317.859375, loss_pred: 0.551733
iteration 368: loss: 205159.515625, loss_kl: 28.273285, loss_recon: 204817.421875, loss_pred: 0.593575
iteration 369: loss: 204881.781250, loss_kl: 28.077127, loss_recon: 204542.500000, loss_pred: 0.585203
 20%|██████▏                       | 41/200 [38:27<2:26:52, 55.43s/it]iteration 370: loss: 205166.375000, loss_kl: 27.414307, loss_recon: 204856.140625, loss_pred: 0.360902
iteration 371: loss: 205766.609375, loss_kl: 26.607965, loss_recon: 205470.015625, loss_pred: 0.305137
iteration 372: loss: 204155.515625, loss_kl: 26.662951, loss_recon: 203862.656250, loss_pred: 0.262294
iteration 373: loss: 205103.046875, loss_kl: 26.628864, loss_recon: 204794.000000, loss_pred: 0.427604
iteration 374: loss: 203770.953125, loss_kl: 26.540245, loss_recon: 203478.281250, loss_pred: 0.272593
iteration 375: loss: 203281.296875, loss_kl: 26.305569, loss_recon: 202968.093750, loss_pred: 0.501370
iteration 376: loss: 203997.609375, loss_kl: 25.650747, loss_recon: 203709.968750, loss_pred: 0.311415
iteration 377: loss: 206752.296875, loss_kl: 24.888525, loss_recon: 206416.937500, loss_pred: 0.864705
iteration 378: loss: 203464.515625, loss_kl: 25.244621, loss_recon: 203154.171875, loss_pred: 0.578966
 21%|██████▎                       | 42/200 [39:23<2:26:05, 55.48s/it]iteration 379: loss: 203279.531250, loss_kl: 24.908436, loss_recon: 202958.953125, loss_pred: 0.715046
iteration 380: loss: 203658.078125, loss_kl: 27.284521, loss_recon: 203319.093750, loss_pred: 0.661391
iteration 381: loss: 202555.765625, loss_kl: 27.136345, loss_recon: 202165.312500, loss_pred: 1.190869
iteration 382: loss: 204026.000000, loss_kl: 27.465635, loss_recon: 203661.453125, loss_pred: 0.898893
iteration 383: loss: 205091.640625, loss_kl: 25.208790, loss_recon: 204785.140625, loss_pred: 0.544021
iteration 384: loss: 203832.609375, loss_kl: 24.844217, loss_recon: 203513.140625, loss_pred: 0.710330
iteration 385: loss: 203354.625000, loss_kl: 26.976274, loss_recon: 203021.093750, loss_pred: 0.637659
iteration 386: loss: 202313.781250, loss_kl: 25.111643, loss_recon: 201995.421875, loss_pred: 0.672518
iteration 387: loss: 203200.421875, loss_kl: 26.677187, loss_recon: 202873.593750, loss_pred: 0.600687
 22%|██████▍                       | 43/200 [40:19<2:25:34, 55.63s/it]iteration 388: loss: 203458.843750, loss_kl: 23.823435, loss_recon: 203160.937500, loss_pred: 0.596696
iteration 389: loss: 203528.171875, loss_kl: 23.740326, loss_recon: 203235.390625, loss_pred: 0.553675
iteration 390: loss: 202964.281250, loss_kl: 21.962524, loss_recon: 202683.375000, loss_pred: 0.612816
iteration 391: loss: 202135.984375, loss_kl: 21.109936, loss_recon: 201856.234375, loss_pred: 0.686612
iteration 392: loss: 201761.875000, loss_kl: 21.756504, loss_recon: 201504.234375, loss_pred: 0.400741
iteration 393: loss: 199550.109375, loss_kl: 22.783966, loss_recon: 199263.375000, loss_pred: 0.588954
iteration 394: loss: 202321.234375, loss_kl: 23.687450, loss_recon: 202023.796875, loss_pred: 0.605691
iteration 395: loss: 202933.375000, loss_kl: 22.492786, loss_recon: 202616.703125, loss_pred: 0.917529
iteration 396: loss: 202221.156250, loss_kl: 22.166395, loss_recon: 201927.000000, loss_pred: 0.725037
 22%|██████▌                       | 44/200 [41:15<2:24:55, 55.74s/it]iteration 397: loss: 200530.656250, loss_kl: 22.100994, loss_recon: 200238.171875, loss_pred: 0.714724
iteration 398: loss: 200961.703125, loss_kl: 24.105194, loss_recon: 200650.718750, loss_pred: 0.699449
iteration 399: loss: 201214.968750, loss_kl: 21.835896, loss_recon: 200938.437500, loss_pred: 0.581788
iteration 400: loss: 201952.843750, loss_kl: 23.122431, loss_recon: 201661.812500, loss_pred: 0.598135
iteration 401: loss: 200997.625000, loss_kl: 20.589869, loss_recon: 200760.937500, loss_pred: 0.307756
iteration 402: loss: 201159.765625, loss_kl: 21.303520, loss_recon: 200874.984375, loss_pred: 0.717498
iteration 403: loss: 200751.375000, loss_kl: 20.999403, loss_recon: 200490.406250, loss_pred: 0.509673
iteration 404: loss: 202862.406250, loss_kl: 19.154858, loss_recon: 202628.718750, loss_pred: 0.421366
iteration 405: loss: 200314.906250, loss_kl: 20.560703, loss_recon: 199986.031250, loss_pred: 1.232706
 22%|██████▊                       | 45/200 [42:08<2:22:28, 55.15s/it]iteration 406: loss: 200145.062500, loss_kl: 19.570940, loss_recon: 199905.546875, loss_pred: 0.438090
iteration 407: loss: 198416.343750, loss_kl: 22.429678, loss_recon: 198104.828125, loss_pred: 0.872204
iteration 408: loss: 201318.484375, loss_kl: 19.625189, loss_recon: 201083.890625, loss_pred: 0.383444
iteration 409: loss: 200775.843750, loss_kl: 20.576344, loss_recon: 200518.343750, loss_pred: 0.517364
iteration 410: loss: 201083.500000, loss_kl: 20.146135, loss_recon: 200835.828125, loss_pred: 0.462081
iteration 411: loss: 200132.890625, loss_kl: 18.628197, loss_recon: 199888.734375, loss_pred: 0.578727
iteration 412: loss: 200456.281250, loss_kl: 19.901033, loss_recon: 200193.718750, loss_pred: 0.635489
iteration 413: loss: 199932.484375, loss_kl: 19.280884, loss_recon: 199690.968750, loss_pred: 0.487048
iteration 414: loss: 198514.468750, loss_kl: 19.316011, loss_recon: 198287.171875, loss_pred: 0.341388
 23%|██████▉                       | 46/200 [43:05<2:22:44, 55.61s/it]iteration 415: loss: 199636.453125, loss_kl: 20.290869, loss_recon: 199398.281250, loss_pred: 0.352647
iteration 416: loss: 197911.046875, loss_kl: 20.115953, loss_recon: 197676.937500, loss_pred: 0.329602
iteration 417: loss: 200166.109375, loss_kl: 18.454985, loss_recon: 199906.828125, loss_pred: 0.747269
iteration 418: loss: 200201.796875, loss_kl: 16.990406, loss_recon: 199983.312500, loss_pred: 0.485856
iteration 419: loss: 198499.609375, loss_kl: 16.626266, loss_recon: 198160.265625, loss_pred: 1.730819
iteration 420: loss: 196823.687500, loss_kl: 16.881672, loss_recon: 196579.546875, loss_pred: 0.753345
iteration 421: loss: 198776.281250, loss_kl: 18.488735, loss_recon: 198536.718750, loss_pred: 0.546795
iteration 422: loss: 199066.437500, loss_kl: 18.572292, loss_recon: 198769.593750, loss_pred: 1.111237
iteration 423: loss: 200087.484375, loss_kl: 18.194159, loss_recon: 199864.656250, loss_pred: 0.408886
 24%|███████                       | 47/200 [44:00<2:21:43, 55.58s/it]iteration 424: loss: 197320.312500, loss_kl: 17.747284, loss_recon: 197010.937500, loss_pred: 1.318985
iteration 425: loss: 196567.562500, loss_kl: 16.933582, loss_recon: 196323.359375, loss_pred: 0.748804
iteration 426: loss: 198357.156250, loss_kl: 20.496393, loss_recon: 198034.062500, loss_pred: 1.181191
iteration 427: loss: 198405.875000, loss_kl: 16.587206, loss_recon: 198145.609375, loss_pred: 0.943829
iteration 428: loss: 197999.531250, loss_kl: 19.200157, loss_recon: 197733.234375, loss_pred: 0.742954
iteration 429: loss: 198107.515625, loss_kl: 20.557543, loss_recon: 197803.859375, loss_pred: 0.980825
iteration 430: loss: 198569.984375, loss_kl: 16.481882, loss_recon: 198353.406250, loss_pred: 0.517635
iteration 431: loss: 198639.156250, loss_kl: 16.875143, loss_recon: 198389.625000, loss_pred: 0.807737
iteration 432: loss: 197894.156250, loss_kl: 16.765848, loss_recon: 197668.515625, loss_pred: 0.579914
 24%|███████▏                      | 48/200 [44:55<2:19:50, 55.20s/it]iteration 433: loss: 196707.890625, loss_kl: 15.943395, loss_recon: 196482.234375, loss_pred: 0.662159
iteration 434: loss: 196332.109375, loss_kl: 15.503077, loss_recon: 196102.546875, loss_pred: 0.745377
iteration 435: loss: 197318.765625, loss_kl: 16.843641, loss_recon: 197112.203125, loss_pred: 0.381182
iteration 436: loss: 197532.281250, loss_kl: 16.460997, loss_recon: 197263.265625, loss_pred: 1.044070
iteration 437: loss: 196375.109375, loss_kl: 16.895330, loss_recon: 196171.000000, loss_pred: 0.351546
iteration 438: loss: 196428.312500, loss_kl: 14.819633, loss_recon: 196197.796875, loss_pred: 0.823185
iteration 439: loss: 195990.453125, loss_kl: 15.367021, loss_recon: 195754.281250, loss_pred: 0.825067
iteration 440: loss: 196912.796875, loss_kl: 15.917548, loss_recon: 196704.500000, loss_pred: 0.491288
iteration 441: loss: 198677.140625, loss_kl: 14.249157, loss_recon: 198459.828125, loss_pred: 0.748324
 24%|███████▎                      | 49/200 [45:49<2:18:18, 54.96s/it]iteration 442: loss: 196449.656250, loss_kl: 16.344509, loss_recon: 196254.890625, loss_pred: 0.313255
iteration 443: loss: 196065.171875, loss_kl: 15.570431, loss_recon: 195856.406250, loss_pred: 0.530566
iteration 444: loss: 196224.234375, loss_kl: 14.920665, loss_recon: 196045.343750, loss_pred: 0.296953
iteration 445: loss: 195140.734375, loss_kl: 15.424306, loss_recon: 194952.187500, loss_pred: 0.342956
iteration 446: loss: 196713.640625, loss_kl: 14.123808, loss_recon: 196529.343750, loss_pred: 0.430587
iteration 447: loss: 194699.625000, loss_kl: 14.971261, loss_recon: 194494.265625, loss_pred: 0.556351
iteration 448: loss: 195215.640625, loss_kl: 13.606258, loss_recon: 195047.078125, loss_pred: 0.325005
iteration 449: loss: 196309.484375, loss_kl: 14.574707, loss_recon: 196106.156250, loss_pred: 0.575826
iteration 450: loss: 195999.078125, loss_kl: 14.060908, loss_recon: 195828.890625, loss_pred: 0.295732
 25%|███████▌                      | 50/200 [46:44<2:16:58, 54.79s/it]iteration 451: loss: 193496.937500, loss_kl: 14.488389, loss_recon: 193450.484375, loss_pred: 0.449975
iteration 452: loss: 194525.500000, loss_kl: 15.344127, loss_recon: 194498.796875, loss_pred: 0.251655
iteration 453: loss: 194680.500000, loss_kl: 16.894691, loss_recon: 194653.859375, loss_pred: 0.249488
iteration 454: loss: 196285.078125, loss_kl: 17.168526, loss_recon: 196247.421875, loss_pred: 0.359340
iteration 455: loss: 195026.859375, loss_kl: 17.866928, loss_recon: 194963.921875, loss_pred: 0.611493
iteration 456: loss: 193799.921875, loss_kl: 18.985363, loss_recon: 193756.921875, loss_pred: 0.410922
iteration 457: loss: 196089.406250, loss_kl: 19.883911, loss_recon: 196035.390625, loss_pred: 0.520349
iteration 458: loss: 196162.687500, loss_kl: 19.613651, loss_recon: 196117.421875, loss_pred: 0.432954
iteration 459: loss: 192485.843750, loss_kl: 21.358503, loss_recon: 192444.687500, loss_pred: 0.390195
 26%|███████▋                      | 51/200 [47:38<2:15:47, 54.68s/it]iteration 460: loss: 193931.046875, loss_kl: 22.528393, loss_recon: 193891.750000, loss_pred: 0.370544
iteration 461: loss: 192544.765625, loss_kl: 25.590662, loss_recon: 192503.656250, loss_pred: 0.385520
iteration 462: loss: 194839.000000, loss_kl: 26.526670, loss_recon: 194809.609375, loss_pred: 0.267421
iteration 463: loss: 194290.484375, loss_kl: 26.469904, loss_recon: 194258.437500, loss_pred: 0.294100
iteration 464: loss: 194712.328125, loss_kl: 27.442432, loss_recon: 194690.484375, loss_pred: 0.190947
iteration 465: loss: 194679.187500, loss_kl: 26.347132, loss_recon: 194648.781250, loss_pred: 0.277674
iteration 466: loss: 194121.609375, loss_kl: 26.434725, loss_recon: 194075.015625, loss_pred: 0.439565
iteration 467: loss: 193554.421875, loss_kl: 26.725927, loss_recon: 193492.203125, loss_pred: 0.595509
iteration 468: loss: 191182.984375, loss_kl: 28.518095, loss_recon: 191145.953125, loss_pred: 0.341725
 26%|███████▊                      | 52/200 [48:34<2:15:49, 55.06s/it]iteration 469: loss: 193390.062500, loss_kl: 27.086538, loss_recon: 193362.375000, loss_pred: 0.249801
iteration 470: loss: 195286.875000, loss_kl: 26.602818, loss_recon: 195258.687500, loss_pred: 0.255239
iteration 471: loss: 191203.640625, loss_kl: 29.330465, loss_recon: 191163.687500, loss_pred: 0.370212
iteration 472: loss: 192203.562500, loss_kl: 29.287577, loss_recon: 192170.484375, loss_pred: 0.301577
iteration 473: loss: 192491.359375, loss_kl: 29.361547, loss_recon: 192463.578125, loss_pred: 0.248361
iteration 474: loss: 193363.265625, loss_kl: 29.790783, loss_recon: 193307.437500, loss_pred: 0.528471
iteration 475: loss: 191493.859375, loss_kl: 31.821701, loss_recon: 191467.890625, loss_pred: 0.227831
iteration 476: loss: 193044.515625, loss_kl: 31.174952, loss_recon: 193015.031250, loss_pred: 0.263593
iteration 477: loss: 193085.203125, loss_kl: 31.792126, loss_recon: 193053.140625, loss_pred: 0.288909
 26%|███████▉                      | 53/200 [49:28<2:14:01, 54.70s/it]iteration 478: loss: 193445.343750, loss_kl: 31.458521, loss_recon: 193404.281250, loss_pred: 0.379256
iteration 479: loss: 192330.765625, loss_kl: 31.368969, loss_recon: 192307.265625, loss_pred: 0.203553
iteration 480: loss: 191881.140625, loss_kl: 31.104639, loss_recon: 191834.765625, loss_pred: 0.432595
iteration 481: loss: 192814.125000, loss_kl: 31.065395, loss_recon: 192788.421875, loss_pred: 0.225882
iteration 482: loss: 190773.859375, loss_kl: 33.025043, loss_recon: 190716.375000, loss_pred: 0.541874
iteration 483: loss: 192906.312500, loss_kl: 32.960434, loss_recon: 192874.515625, loss_pred: 0.284998
iteration 484: loss: 190559.109375, loss_kl: 33.958927, loss_recon: 190520.593750, loss_pred: 0.351299
iteration 485: loss: 190620.828125, loss_kl: 34.198021, loss_recon: 190576.375000, loss_pred: 0.410319
iteration 486: loss: 191813.046875, loss_kl: 31.804838, loss_recon: 191773.890625, loss_pred: 0.359661
 27%|████████                      | 54/200 [50:22<2:12:27, 54.43s/it]iteration 487: loss: 192256.296875, loss_kl: 31.423962, loss_recon: 192230.093750, loss_pred: 0.230591
iteration 488: loss: 192007.500000, loss_kl: 30.356983, loss_recon: 191960.312500, loss_pred: 0.441499
iteration 489: loss: 190315.875000, loss_kl: 31.414488, loss_recon: 190273.718750, loss_pred: 0.390112
iteration 490: loss: 189853.875000, loss_kl: 33.195663, loss_recon: 189823.515625, loss_pred: 0.270498
iteration 491: loss: 190123.781250, loss_kl: 33.930222, loss_recon: 190098.625000, loss_pred: 0.217607
iteration 492: loss: 191246.953125, loss_kl: 33.628681, loss_recon: 191209.406250, loss_pred: 0.341929
iteration 493: loss: 191620.718750, loss_kl: 32.329792, loss_recon: 191598.765625, loss_pred: 0.187182
iteration 494: loss: 189968.921875, loss_kl: 31.547461, loss_recon: 189929.765625, loss_pred: 0.359923
iteration 495: loss: 191599.078125, loss_kl: 29.998337, loss_recon: 191567.593750, loss_pred: 0.284775
 28%|████████▎                     | 55/200 [51:18<2:12:58, 55.03s/it]iteration 496: loss: 190154.953125, loss_kl: 30.797981, loss_recon: 190110.312500, loss_pred: 0.415678
iteration 497: loss: 190359.359375, loss_kl: 30.266104, loss_recon: 190322.593750, loss_pred: 0.337400
iteration 498: loss: 190310.250000, loss_kl: 31.750429, loss_recon: 190271.968750, loss_pred: 0.351035
iteration 499: loss: 190263.546875, loss_kl: 32.072525, loss_recon: 190234.734375, loss_pred: 0.256062
iteration 500: loss: 191206.734375, loss_kl: 32.480808, loss_recon: 191161.000000, loss_pred: 0.424900
iteration 501: loss: 189241.218750, loss_kl: 32.876884, loss_recon: 189206.171875, loss_pred: 0.317674
iteration 502: loss: 188928.890625, loss_kl: 31.753351, loss_recon: 188894.421875, loss_pred: 0.312941
iteration 503: loss: 190000.812500, loss_kl: 30.295341, loss_recon: 189965.718750, loss_pred: 0.320636
iteration 504: loss: 190532.406250, loss_kl: 31.505796, loss_recon: 190499.796875, loss_pred: 0.294579
 28%|████████▍                     | 56/200 [52:12<2:11:27, 54.77s/it]iteration 505: loss: 190929.515625, loss_kl: 31.209780, loss_recon: 190901.234375, loss_pred: 0.251625
iteration 506: loss: 188494.343750, loss_kl: 32.453232, loss_recon: 188466.921875, loss_pred: 0.241659
iteration 507: loss: 187488.578125, loss_kl: 31.942572, loss_recon: 187465.390625, loss_pred: 0.200027
iteration 508: loss: 189913.593750, loss_kl: 30.331997, loss_recon: 189885.546875, loss_pred: 0.250171
iteration 509: loss: 188817.953125, loss_kl: 30.516769, loss_recon: 188769.312500, loss_pred: 0.455929
iteration 510: loss: 191227.187500, loss_kl: 29.382450, loss_recon: 191188.750000, loss_pred: 0.355067
iteration 511: loss: 189485.062500, loss_kl: 31.310560, loss_recon: 189447.968750, loss_pred: 0.339661
iteration 512: loss: 190036.515625, loss_kl: 32.022690, loss_recon: 189999.625000, loss_pred: 0.336801
iteration 513: loss: 186876.062500, loss_kl: 32.415794, loss_recon: 186847.375000, loss_pred: 0.254558
 28%|████████▌                     | 57/200 [53:08<2:11:16, 55.08s/it]iteration 514: loss: 189880.171875, loss_kl: 30.972929, loss_recon: 189852.890625, loss_pred: 0.241938
iteration 515: loss: 187936.328125, loss_kl: 30.883430, loss_recon: 187890.890625, loss_pred: 0.423458
iteration 516: loss: 188261.984375, loss_kl: 29.079479, loss_recon: 188226.093750, loss_pred: 0.329896
iteration 517: loss: 190298.296875, loss_kl: 27.458954, loss_recon: 190248.000000, loss_pred: 0.475423
iteration 518: loss: 188290.453125, loss_kl: 28.355957, loss_recon: 188252.109375, loss_pred: 0.355211
iteration 519: loss: 188791.078125, loss_kl: 29.000544, loss_recon: 188766.640625, loss_pred: 0.215357
iteration 520: loss: 188404.218750, loss_kl: 29.603996, loss_recon: 188370.234375, loss_pred: 0.310361
iteration 521: loss: 187015.703125, loss_kl: 30.699896, loss_recon: 186979.875000, loss_pred: 0.327702
iteration 522: loss: 186876.468750, loss_kl: 30.443241, loss_recon: 186850.765625, loss_pred: 0.226559
 29%|████████▋                     | 58/200 [54:03<2:10:37, 55.20s/it]iteration 523: loss: 188958.828125, loss_kl: 30.047701, loss_recon: 188936.265625, loss_pred: 0.195671
iteration 524: loss: 188021.140625, loss_kl: 31.124281, loss_recon: 187962.390625, loss_pred: 0.556465
iteration 525: loss: 184972.703125, loss_kl: 32.136814, loss_recon: 184932.109375, loss_pred: 0.373798
iteration 526: loss: 187027.796875, loss_kl: 30.300634, loss_recon: 186992.437500, loss_pred: 0.323346
iteration 527: loss: 187338.750000, loss_kl: 31.083733, loss_recon: 187306.546875, loss_pred: 0.290878
iteration 528: loss: 188063.796875, loss_kl: 27.856838, loss_recon: 188036.343750, loss_pred: 0.246765
iteration 529: loss: 188949.046875, loss_kl: 28.089449, loss_recon: 188921.546875, loss_pred: 0.246919
iteration 530: loss: 188591.250000, loss_kl: 28.836323, loss_recon: 188555.625000, loss_pred: 0.327284
iteration 531: loss: 186479.031250, loss_kl: 29.941183, loss_recon: 186439.906250, loss_pred: 0.361193
 30%|████████▊                     | 59/200 [54:59<2:10:01, 55.33s/it]iteration 532: loss: 186667.468750, loss_kl: 31.747171, loss_recon: 186636.265625, loss_pred: 0.280340
iteration 533: loss: 187116.171875, loss_kl: 29.981676, loss_recon: 187036.140625, loss_pred: 0.770238
iteration 534: loss: 187484.265625, loss_kl: 28.508816, loss_recon: 187444.625000, loss_pred: 0.367903
iteration 535: loss: 187506.484375, loss_kl: 28.942919, loss_recon: 187455.656250, loss_pred: 0.479408
iteration 536: loss: 185951.468750, loss_kl: 30.378313, loss_recon: 185892.140625, loss_pred: 0.563001
iteration 537: loss: 186167.984375, loss_kl: 33.133781, loss_recon: 186137.718750, loss_pred: 0.269507
iteration 538: loss: 188680.484375, loss_kl: 32.378418, loss_recon: 188607.453125, loss_pred: 0.697946
iteration 539: loss: 187206.015625, loss_kl: 31.267599, loss_recon: 187178.093750, loss_pred: 0.247913
iteration 540: loss: 184607.984375, loss_kl: 31.370062, loss_recon: 184557.125000, loss_pred: 0.477151
 30%|█████████                     | 60/200 [55:54<2:08:31, 55.08s/it]iteration 541: loss: 186409.031250, loss_kl: 29.530043, loss_recon: 186377.484375, loss_pred: 0.285926
iteration 542: loss: 186800.093750, loss_kl: 29.690464, loss_recon: 186770.593750, loss_pred: 0.265358
iteration 543: loss: 187495.343750, loss_kl: 30.251566, loss_recon: 187459.921875, loss_pred: 0.323864
iteration 544: loss: 185029.828125, loss_kl: 32.111870, loss_recon: 185002.109375, loss_pred: 0.245066
iteration 545: loss: 184069.703125, loss_kl: 33.380775, loss_recon: 184030.593750, loss_pred: 0.357644
iteration 546: loss: 186714.203125, loss_kl: 32.151157, loss_recon: 186691.843750, loss_pred: 0.191471
iteration 547: loss: 185106.296875, loss_kl: 31.822199, loss_recon: 185081.656250, loss_pred: 0.214560
iteration 548: loss: 185584.984375, loss_kl: 31.334013, loss_recon: 185540.265625, loss_pred: 0.415828
iteration 549: loss: 187116.640625, loss_kl: 30.580950, loss_recon: 187077.265625, loss_pred: 0.363059
 30%|█████████▏                    | 61/200 [56:48<2:06:57, 54.80s/it]iteration 550: loss: 185232.078125, loss_kl: 32.267315, loss_recon: 185188.890625, loss_pred: 0.399472
iteration 551: loss: 186397.203125, loss_kl: 32.305664, loss_recon: 186355.593750, loss_pred: 0.383703
iteration 552: loss: 184353.453125, loss_kl: 34.081551, loss_recon: 184317.531250, loss_pred: 0.325116
iteration 553: loss: 186744.796875, loss_kl: 33.160255, loss_recon: 186705.953125, loss_pred: 0.355337
iteration 554: loss: 183142.609375, loss_kl: 32.410324, loss_recon: 183109.031250, loss_pred: 0.303440
iteration 555: loss: 184836.406250, loss_kl: 30.697636, loss_recon: 184810.250000, loss_pred: 0.230931
iteration 556: loss: 183893.718750, loss_kl: 30.560841, loss_recon: 183859.093750, loss_pred: 0.315608
iteration 557: loss: 186699.609375, loss_kl: 29.343224, loss_recon: 186664.625000, loss_pred: 0.320522
iteration 558: loss: 186140.578125, loss_kl: 30.519463, loss_recon: 186111.000000, loss_pred: 0.265328
 31%|█████████▎                    | 62/200 [57:43<2:06:26, 54.98s/it]iteration 559: loss: 185290.953125, loss_kl: 33.121128, loss_recon: 185262.093750, loss_pred: 0.255478
iteration 560: loss: 183920.078125, loss_kl: 33.571808, loss_recon: 183895.468750, loss_pred: 0.212485
iteration 561: loss: 185405.562500, loss_kl: 32.508461, loss_recon: 185368.140625, loss_pred: 0.341746
iteration 562: loss: 184372.484375, loss_kl: 33.315529, loss_recon: 184341.250000, loss_pred: 0.279031
iteration 563: loss: 184852.468750, loss_kl: 30.770823, loss_recon: 184814.265625, loss_pred: 0.351221
iteration 564: loss: 184736.234375, loss_kl: 30.775576, loss_recon: 184711.546875, loss_pred: 0.216085
iteration 565: loss: 185778.234375, loss_kl: 29.293751, loss_recon: 185722.593750, loss_pred: 0.527117
iteration 566: loss: 181934.953125, loss_kl: 31.819796, loss_recon: 181899.421875, loss_pred: 0.323399
iteration 567: loss: 184744.406250, loss_kl: 31.401054, loss_recon: 184707.093750, loss_pred: 0.341778
 32%|█████████▍                    | 63/200 [58:38<2:05:26, 54.94s/it]iteration 568: loss: 182793.062500, loss_kl: 32.553097, loss_recon: 182751.562500, loss_pred: 0.253544
iteration 569: loss: 183087.406250, loss_kl: 32.898438, loss_recon: 183045.421875, loss_pred: 0.256724
iteration 570: loss: 185637.796875, loss_kl: 31.730385, loss_recon: 185598.578125, loss_pred: 0.234848
iteration 571: loss: 184398.984375, loss_kl: 30.962179, loss_recon: 184345.500000, loss_pred: 0.381179
iteration 572: loss: 181228.031250, loss_kl: 31.395906, loss_recon: 181183.265625, loss_pred: 0.291948
iteration 573: loss: 185565.812500, loss_kl: 29.333252, loss_recon: 185527.109375, loss_pred: 0.241636
iteration 574: loss: 183240.359375, loss_kl: 29.270802, loss_recon: 183198.546875, loss_pred: 0.272927
iteration 575: loss: 184476.640625, loss_kl: 27.323347, loss_recon: 184434.609375, loss_pred: 0.284766
iteration 576: loss: 183942.250000, loss_kl: 26.700191, loss_recon: 183900.921875, loss_pred: 0.280706
 32%|█████████▌                    | 64/200 [59:34<2:05:00, 55.15s/it]iteration 577: loss: 183128.218750, loss_kl: 27.008932, loss_recon: 183074.406250, loss_pred: 0.297202
iteration 578: loss: 184730.890625, loss_kl: 25.686184, loss_recon: 184685.609375, loss_pred: 0.223685
iteration 579: loss: 182252.125000, loss_kl: 26.163179, loss_recon: 182194.296875, loss_pred: 0.344774
iteration 580: loss: 183452.468750, loss_kl: 24.703979, loss_recon: 183374.062500, loss_pred: 0.563769
iteration 581: loss: 183256.640625, loss_kl: 23.332760, loss_recon: 183205.656250, loss_pred: 0.301765
iteration 582: loss: 181644.843750, loss_kl: 21.348402, loss_recon: 181587.171875, loss_pred: 0.386316
iteration 583: loss: 184601.296875, loss_kl: 20.534887, loss_recon: 184532.546875, loss_pred: 0.504330
iteration 584: loss: 183908.406250, loss_kl: 18.478611, loss_recon: 183862.078125, loss_pred: 0.298361
iteration 585: loss: 181220.171875, loss_kl: 21.702662, loss_recon: 181174.390625, loss_pred: 0.264199
 32%|█████████                   | 65/200 [1:00:29<2:04:29, 55.33s/it]iteration 586: loss: 182619.015625, loss_kl: 18.616636, loss_recon: 182571.312500, loss_pred: 0.237256
iteration 587: loss: 181744.109375, loss_kl: 17.828342, loss_recon: 181696.343750, loss_pred: 0.247970
iteration 588: loss: 181562.671875, loss_kl: 17.131432, loss_recon: 181506.375000, loss_pred: 0.342342
iteration 589: loss: 182321.234375, loss_kl: 16.373497, loss_recon: 182272.765625, loss_pred: 0.273684
iteration 590: loss: 184390.062500, loss_kl: 15.535043, loss_recon: 184331.593750, loss_pred: 0.384481
iteration 591: loss: 183034.250000, loss_kl: 14.862392, loss_recon: 182989.796875, loss_pred: 0.253149
iteration 592: loss: 180666.125000, loss_kl: 15.340723, loss_recon: 180605.953125, loss_pred: 0.404091
iteration 593: loss: 183434.859375, loss_kl: 15.075614, loss_recon: 183393.531250, loss_pred: 0.218999
iteration 594: loss: 182222.937500, loss_kl: 16.547325, loss_recon: 182162.468750, loss_pred: 0.391505
 33%|█████████▏                  | 66/200 [1:01:25<2:03:57, 55.51s/it]iteration 595: loss: 182510.812500, loss_kl: 15.919594, loss_recon: 182455.953125, loss_pred: 0.280445
iteration 596: loss: 180553.687500, loss_kl: 16.261795, loss_recon: 180505.406250, loss_pred: 0.208854
iteration 597: loss: 179875.671875, loss_kl: 16.243805, loss_recon: 179824.265625, loss_pred: 0.240492
iteration 598: loss: 184530.093750, loss_kl: 14.975056, loss_recon: 184470.890625, loss_pred: 0.339894
iteration 599: loss: 181075.125000, loss_kl: 15.444507, loss_recon: 181022.093750, loss_pred: 0.270126
iteration 600: loss: 182821.718750, loss_kl: 14.434059, loss_recon: 182773.062500, loss_pred: 0.243475
iteration 601: loss: 181088.656250, loss_kl: 15.155389, loss_recon: 181036.171875, loss_pred: 0.269717
iteration 602: loss: 181318.343750, loss_kl: 15.486044, loss_recon: 181269.765625, loss_pred: 0.225022
iteration 603: loss: 182460.437500, loss_kl: 15.360017, loss_recon: 182399.281250, loss_pred: 0.353031
 34%|█████████▍                  | 67/200 [1:02:19<2:01:46, 54.94s/it]iteration 604: loss: 183934.937500, loss_kl: 14.570728, loss_recon: 183865.734375, loss_pred: 0.388942
iteration 605: loss: 182016.390625, loss_kl: 14.432656, loss_recon: 181947.968750, loss_pred: 0.383997
iteration 606: loss: 179590.218750, loss_kl: 14.697404, loss_recon: 179523.750000, loss_pred: 0.358936
iteration 607: loss: 179917.000000, loss_kl: 15.541174, loss_recon: 179841.171875, loss_pred: 0.435017
iteration 608: loss: 181875.484375, loss_kl: 15.510666, loss_recon: 181793.203125, loss_pred: 0.500218
iteration 609: loss: 180273.203125, loss_kl: 16.079214, loss_recon: 180204.406250, loss_pred: 0.353624
iteration 610: loss: 180933.109375, loss_kl: 15.059587, loss_recon: 180870.640625, loss_pred: 0.311481
iteration 611: loss: 179755.609375, loss_kl: 15.491899, loss_recon: 179699.000000, loss_pred: 0.243947
iteration 612: loss: 182447.406250, loss_kl: 15.282467, loss_recon: 182386.750000, loss_pred: 0.288801
 34%|█████████▌                  | 68/200 [1:03:13<2:00:03, 54.58s/it]iteration 613: loss: 180412.468750, loss_kl: 15.075357, loss_recon: 180349.687500, loss_pred: 0.254607
iteration 614: loss: 182256.906250, loss_kl: 13.828238, loss_recon: 182195.750000, loss_pred: 0.269191
iteration 615: loss: 178721.062500, loss_kl: 15.163257, loss_recon: 178658.625000, loss_pred: 0.248908
iteration 616: loss: 183259.312500, loss_kl: 13.428926, loss_recon: 183202.406250, loss_pred: 0.236498
iteration 617: loss: 180225.187500, loss_kl: 15.379660, loss_recon: 180159.093750, loss_pred: 0.280106
iteration 618: loss: 181039.468750, loss_kl: 13.720982, loss_recon: 180969.218750, loss_pred: 0.362851
iteration 619: loss: 181019.687500, loss_kl: 14.191096, loss_recon: 180963.687500, loss_pred: 0.208555
iteration 620: loss: 177336.015625, loss_kl: 15.035192, loss_recon: 177273.796875, loss_pred: 0.249854
iteration 621: loss: 180718.078125, loss_kl: 14.176011, loss_recon: 180635.265625, loss_pred: 0.477123
 34%|█████████▋                  | 69/200 [1:04:08<1:59:40, 54.82s/it]iteration 622: loss: 183072.015625, loss_kl: 13.231868, loss_recon: 183010.968750, loss_pred: 0.230462
iteration 623: loss: 181966.203125, loss_kl: 12.775299, loss_recon: 181882.156250, loss_pred: 0.473622
iteration 624: loss: 179208.671875, loss_kl: 12.923971, loss_recon: 179088.828125, loss_pred: 0.827132
iteration 625: loss: 181031.062500, loss_kl: 13.946802, loss_recon: 180938.437500, loss_pred: 0.525657
iteration 626: loss: 179858.781250, loss_kl: 14.365792, loss_recon: 179784.375000, loss_pred: 0.331471
iteration 627: loss: 176952.796875, loss_kl: 14.050333, loss_recon: 176869.218750, loss_pred: 0.432175
iteration 628: loss: 180348.890625, loss_kl: 13.122224, loss_recon: 180283.265625, loss_pred: 0.279360
iteration 629: loss: 179270.015625, loss_kl: 13.828750, loss_recon: 179201.890625, loss_pred: 0.284124
iteration 630: loss: 178044.359375, loss_kl: 14.466035, loss_recon: 177950.984375, loss_pred: 0.518291
 35%|█████████▊                  | 70/200 [1:05:02<1:58:04, 54.50s/it]iteration 631: loss: 178505.406250, loss_kl: 14.838219, loss_recon: 178422.968750, loss_pred: 0.339576
iteration 632: loss: 182029.015625, loss_kl: 13.334713, loss_recon: 181958.734375, loss_pred: 0.266990
iteration 633: loss: 178548.421875, loss_kl: 13.048035, loss_recon: 178474.375000, loss_pred: 0.314030
iteration 634: loss: 178849.156250, loss_kl: 13.791360, loss_recon: 178746.765625, loss_pred: 0.573353
iteration 635: loss: 178082.656250, loss_kl: 13.577376, loss_recon: 178012.859375, loss_pred: 0.254168
iteration 636: loss: 179459.187500, loss_kl: 13.811575, loss_recon: 179352.296875, loss_pred: 0.617517
iteration 637: loss: 181376.671875, loss_kl: 13.155530, loss_recon: 181300.984375, loss_pred: 0.326919
iteration 638: loss: 179754.375000, loss_kl: 12.883118, loss_recon: 179663.562500, loss_pred: 0.486995
iteration 639: loss: 177825.812500, loss_kl: 13.535639, loss_recon: 177712.062500, loss_pred: 0.695145
 36%|█████████▉                  | 71/200 [1:05:56<1:56:47, 54.32s/it]iteration 640: loss: 179478.359375, loss_kl: 12.952356, loss_recon: 179405.062500, loss_pred: 0.258441
iteration 641: loss: 178466.156250, loss_kl: 13.995123, loss_recon: 178360.546875, loss_pred: 0.543340
iteration 642: loss: 177580.531250, loss_kl: 14.777066, loss_recon: 177484.234375, loss_pred: 0.421575
iteration 643: loss: 176961.484375, loss_kl: 14.907945, loss_recon: 176878.359375, loss_pred: 0.285025
iteration 644: loss: 179881.656250, loss_kl: 13.166627, loss_recon: 179783.203125, loss_pred: 0.501958
iteration 645: loss: 179829.640625, loss_kl: 12.780736, loss_recon: 179750.703125, loss_pred: 0.321094
iteration 646: loss: 176879.125000, loss_kl: 13.550819, loss_recon: 176806.828125, loss_pred: 0.226332
iteration 647: loss: 179241.468750, loss_kl: 13.422086, loss_recon: 179150.546875, loss_pred: 0.417467
iteration 648: loss: 180926.578125, loss_kl: 13.147112, loss_recon: 180843.234375, loss_pred: 0.351766
 36%|██████████                  | 72/200 [1:06:49<1:55:26, 54.11s/it]iteration 649: loss: 178061.203125, loss_kl: 12.559248, loss_recon: 177979.453125, loss_pred: 0.307645
iteration 650: loss: 180403.437500, loss_kl: 13.100653, loss_recon: 180289.750000, loss_pred: 0.605012
iteration 651: loss: 176425.906250, loss_kl: 13.300485, loss_recon: 176333.625000, loss_pred: 0.382867
iteration 652: loss: 178290.984375, loss_kl: 12.310210, loss_recon: 178203.593750, loss_pred: 0.374006
iteration 653: loss: 177569.718750, loss_kl: 12.827937, loss_recon: 177470.765625, loss_pred: 0.468727
iteration 654: loss: 178542.671875, loss_kl: 11.872289, loss_recon: 178470.078125, loss_pred: 0.243929
iteration 655: loss: 178334.062500, loss_kl: 11.899450, loss_recon: 178243.093750, loss_pred: 0.426569
iteration 656: loss: 178667.625000, loss_kl: 11.828675, loss_recon: 178591.890625, loss_pred: 0.276986
iteration 657: loss: 178100.718750, loss_kl: 12.760940, loss_recon: 178027.656250, loss_pred: 0.212509
 36%|██████████▏                 | 73/200 [1:07:45<1:55:43, 54.68s/it]iteration 658: loss: 176886.937500, loss_kl: 12.950006, loss_recon: 176786.765625, loss_pred: 0.424709
iteration 659: loss: 178884.015625, loss_kl: 11.676068, loss_recon: 178813.828125, loss_pred: 0.181634
iteration 660: loss: 177818.468750, loss_kl: 11.997679, loss_recon: 177720.000000, loss_pred: 0.449925
iteration 661: loss: 177717.484375, loss_kl: 11.416408, loss_recon: 177629.453125, loss_pred: 0.371531
iteration 662: loss: 176795.859375, loss_kl: 12.431569, loss_recon: 176717.312500, loss_pred: 0.231538
iteration 663: loss: 178225.421875, loss_kl: 12.963213, loss_recon: 178130.515625, loss_pred: 0.371481
iteration 664: loss: 176031.250000, loss_kl: 12.914343, loss_recon: 175952.234375, loss_pred: 0.214619
iteration 665: loss: 178274.921875, loss_kl: 12.031932, loss_recon: 178191.000000, loss_pred: 0.303088
iteration 666: loss: 178995.578125, loss_kl: 10.832852, loss_recon: 178920.578125, loss_pred: 0.267348
 37%|██████████▎                 | 74/200 [1:08:40<1:54:40, 54.61s/it]iteration 667: loss: 175526.312500, loss_kl: 11.811096, loss_recon: 175440.406250, loss_pred: 0.285908
iteration 668: loss: 177271.593750, loss_kl: 12.087903, loss_recon: 177173.890625, loss_pred: 0.390407
iteration 669: loss: 177505.796875, loss_kl: 12.180369, loss_recon: 177412.093750, loss_pred: 0.346102
iteration 670: loss: 176610.656250, loss_kl: 11.988906, loss_recon: 176525.796875, loss_pred: 0.266906
iteration 671: loss: 178094.203125, loss_kl: 11.568707, loss_recon: 178004.593750, loss_pred: 0.334870
iteration 672: loss: 176700.468750, loss_kl: 10.708687, loss_recon: 176581.359375, loss_pred: 0.671520
iteration 673: loss: 176239.578125, loss_kl: 11.464298, loss_recon: 176132.765625, loss_pred: 0.511846
iteration 674: loss: 178420.562500, loss_kl: 12.728852, loss_recon: 178330.281250, loss_pred: 0.285174
iteration 675: loss: 178861.593750, loss_kl: 11.698300, loss_recon: 178706.921875, loss_pred: 0.979067
 38%|██████████▌                 | 75/200 [1:09:34<1:53:45, 54.60s/it]iteration 676: loss: 176950.453125, loss_kl: 11.785845, loss_recon: 176863.281250, loss_pred: 0.253103
iteration 677: loss: 175072.921875, loss_kl: 12.940657, loss_recon: 174937.031250, loss_pred: 0.679829
iteration 678: loss: 178016.531250, loss_kl: 11.037596, loss_recon: 177919.109375, loss_pred: 0.395043
iteration 679: loss: 176474.500000, loss_kl: 11.205864, loss_recon: 176391.750000, loss_pred: 0.239365
iteration 680: loss: 177336.609375, loss_kl: 11.807047, loss_recon: 177245.046875, loss_pred: 0.295877
iteration 681: loss: 177673.578125, loss_kl: 10.220063, loss_recon: 177598.281250, loss_pred: 0.216522
iteration 682: loss: 178073.437500, loss_kl: 10.504436, loss_recon: 177983.062500, loss_pred: 0.352567
iteration 683: loss: 173160.468750, loss_kl: 11.651165, loss_recon: 173048.578125, loss_pred: 0.507531
iteration 684: loss: 177964.843750, loss_kl: 11.079386, loss_recon: 177873.562500, loss_pred: 0.331409
 38%|██████████▋                 | 76/200 [1:10:29<1:52:59, 54.67s/it]iteration 685: loss: 175421.000000, loss_kl: 11.959865, loss_recon: 175327.109375, loss_pred: 0.263883
iteration 686: loss: 178847.437500, loss_kl: 10.150112, loss_recon: 178763.765625, loss_pred: 0.263878
iteration 687: loss: 178542.046875, loss_kl: 10.229093, loss_recon: 178460.234375, loss_pred: 0.240750
iteration 688: loss: 175718.609375, loss_kl: 10.177333, loss_recon: 175614.687500, loss_pred: 0.464843
iteration 689: loss: 174468.875000, loss_kl: 10.006765, loss_recon: 174374.218750, loss_pred: 0.381726
iteration 690: loss: 177883.062500, loss_kl: 10.727936, loss_recon: 177798.265625, loss_pred: 0.242456
iteration 691: loss: 176723.500000, loss_kl: 10.541717, loss_recon: 176616.593750, loss_pred: 0.474036
iteration 692: loss: 175555.140625, loss_kl: 10.838634, loss_recon: 175467.328125, loss_pred: 0.266333
iteration 693: loss: 173115.750000, loss_kl: 11.557082, loss_recon: 173020.859375, loss_pred: 0.296506
 38%|██████████▊                 | 77/200 [1:11:23<1:51:43, 54.50s/it]iteration 694: loss: 177940.140625, loss_kl: 9.963794, loss_recon: 177857.921875, loss_pred: 0.220295
iteration 695: loss: 177121.328125, loss_kl: 10.523683, loss_recon: 177039.828125, loss_pred: 0.179343
iteration 696: loss: 175582.390625, loss_kl: 10.819965, loss_recon: 175485.234375, loss_pred: 0.317979
iteration 697: loss: 175887.156250, loss_kl: 10.384962, loss_recon: 175798.171875, loss_pred: 0.262710
iteration 698: loss: 173948.609375, loss_kl: 9.972288, loss_recon: 173859.265625, loss_pred: 0.291037
iteration 699: loss: 177007.625000, loss_kl: 9.229665, loss_recon: 176924.625000, loss_pred: 0.272553
iteration 700: loss: 174716.765625, loss_kl: 10.415547, loss_recon: 174628.000000, loss_pred: 0.258537
iteration 701: loss: 173580.406250, loss_kl: 10.198285, loss_recon: 173495.796875, loss_pred: 0.230186
iteration 702: loss: 176297.875000, loss_kl: 8.921778, loss_recon: 176222.421875, loss_pred: 0.215556
 39%|██████████▉                 | 78/200 [1:12:17<1:50:16, 54.23s/it]iteration 703: loss: 176802.125000, loss_kl: 9.609716, loss_recon: 176709.750000, loss_pred: 0.305321
iteration 704: loss: 176054.984375, loss_kl: 8.887190, loss_recon: 175975.546875, loss_pred: 0.222387
iteration 705: loss: 175507.187500, loss_kl: 9.957479, loss_recon: 175421.203125, loss_pred: 0.218847
iteration 706: loss: 175008.250000, loss_kl: 9.693713, loss_recon: 174922.343750, loss_pred: 0.235145
iteration 707: loss: 175768.500000, loss_kl: 8.910051, loss_recon: 175681.796875, loss_pred: 0.293630
iteration 708: loss: 173800.343750, loss_kl: 9.298208, loss_recon: 173697.859375, loss_pred: 0.426328
iteration 709: loss: 176138.625000, loss_kl: 9.018865, loss_recon: 176061.281250, loss_pred: 0.192945
iteration 710: loss: 175528.937500, loss_kl: 9.664237, loss_recon: 175438.656250, loss_pred: 0.280797
iteration 711: loss: 173456.890625, loss_kl: 9.742394, loss_recon: 173363.515625, loss_pred: 0.306692
 40%|███████████                 | 79/200 [1:13:11<1:49:24, 54.25s/it]iteration 712: loss: 175415.359375, loss_kl: 9.263920, loss_recon: 175323.656250, loss_pred: 0.284034
iteration 713: loss: 174081.078125, loss_kl: 9.282701, loss_recon: 173982.421875, loss_pred: 0.352397
iteration 714: loss: 172815.453125, loss_kl: 10.404006, loss_recon: 172724.171875, loss_pred: 0.201994
iteration 715: loss: 175546.437500, loss_kl: 9.370977, loss_recon: 175441.234375, loss_pred: 0.411939
iteration 716: loss: 175299.015625, loss_kl: 9.618380, loss_recon: 175202.140625, loss_pred: 0.311554
iteration 717: loss: 174057.156250, loss_kl: 9.666069, loss_recon: 173967.453125, loss_pred: 0.236789
iteration 718: loss: 176863.812500, loss_kl: 8.195730, loss_recon: 176777.000000, loss_pred: 0.308069
iteration 719: loss: 173535.406250, loss_kl: 9.385332, loss_recon: 173442.718750, loss_pred: 0.285661
iteration 720: loss: 176621.062500, loss_kl: 9.097084, loss_recon: 176535.609375, loss_pred: 0.232935
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_79.pth
 40%|███████████▏                | 80/200 [1:14:07<1:49:25, 54.71s/it]iteration 721: loss: 173259.343750, loss_kl: 9.156873, loss_recon: 173173.312500, loss_pred: 0.198426
iteration 722: loss: 173743.140625, loss_kl: 9.320578, loss_recon: 173653.656250, loss_pred: 0.221171
iteration 723: loss: 176570.687500, loss_kl: 8.907133, loss_recon: 176481.140625, loss_pred: 0.251676
iteration 724: loss: 176158.625000, loss_kl: 7.915921, loss_recon: 176068.875000, loss_pred: 0.325336
iteration 725: loss: 173609.171875, loss_kl: 8.055346, loss_recon: 173524.421875, loss_pred: 0.265273
iteration 726: loss: 175038.812500, loss_kl: 7.886088, loss_recon: 174950.750000, loss_pred: 0.310646
iteration 727: loss: 174567.765625, loss_kl: 7.727163, loss_recon: 174490.906250, loss_pred: 0.209951
iteration 728: loss: 173382.421875, loss_kl: 8.899978, loss_recon: 173297.140625, loss_pred: 0.209469
iteration 729: loss: 174326.531250, loss_kl: 8.422638, loss_recon: 174239.390625, loss_pred: 0.262655
 40%|███████████▎                | 81/200 [1:15:01<1:47:55, 54.42s/it]iteration 730: loss: 176672.921875, loss_kl: 7.493228, loss_recon: 176590.421875, loss_pred: 0.253818
iteration 731: loss: 174746.296875, loss_kl: 7.677724, loss_recon: 174661.171875, loss_pred: 0.265911
iteration 732: loss: 174094.281250, loss_kl: 8.507404, loss_recon: 173998.312500, loss_pred: 0.311136
iteration 733: loss: 174526.718750, loss_kl: 9.016915, loss_recon: 174429.468750, loss_pred: 0.285001
iteration 734: loss: 174019.781250, loss_kl: 8.981344, loss_recon: 173912.109375, loss_pred: 0.392007
iteration 735: loss: 172211.734375, loss_kl: 9.377715, loss_recon: 172106.734375, loss_pred: 0.334928
iteration 736: loss: 173982.671875, loss_kl: 7.968381, loss_recon: 173851.921875, loss_pred: 0.699977
iteration 737: loss: 172047.546875, loss_kl: 8.350415, loss_recon: 171953.984375, loss_pred: 0.299056
iteration 738: loss: 174868.937500, loss_kl: 8.740921, loss_recon: 174755.796875, loss_pred: 0.464974
 41%|███████████▍                | 82/200 [1:15:56<1:47:14, 54.53s/it]iteration 739: loss: 172204.343750, loss_kl: 8.981503, loss_recon: 172085.312500, loss_pred: 0.469958
iteration 740: loss: 176453.062500, loss_kl: 8.232216, loss_recon: 176350.078125, loss_pred: 0.369633
iteration 741: loss: 174581.796875, loss_kl: 7.577127, loss_recon: 174467.593750, loss_pred: 0.534339
iteration 742: loss: 175168.687500, loss_kl: 8.385728, loss_recon: 175078.890625, loss_pred: 0.225399
iteration 743: loss: 173753.484375, loss_kl: 8.941661, loss_recon: 173646.375000, loss_pred: 0.353901
iteration 744: loss: 174986.171875, loss_kl: 7.581907, loss_recon: 174896.421875, loss_pred: 0.289406
iteration 745: loss: 172098.796875, loss_kl: 8.979212, loss_recon: 171974.515625, loss_pred: 0.522695
iteration 746: loss: 172563.984375, loss_kl: 9.655437, loss_recon: 172451.859375, loss_pred: 0.346945
iteration 747: loss: 172255.421875, loss_kl: 8.975001, loss_recon: 172160.546875, loss_pred: 0.228841
 42%|███████████▌                | 83/200 [1:16:49<1:45:52, 54.30s/it]iteration 748: loss: 175038.796875, loss_kl: 7.459065, loss_recon: 174929.203125, loss_pred: 0.468173
iteration 749: loss: 173593.359375, loss_kl: 8.538885, loss_recon: 173498.406250, loss_pred: 0.230972
iteration 750: loss: 174764.343750, loss_kl: 7.141223, loss_recon: 174678.921875, loss_pred: 0.253232
iteration 751: loss: 171435.125000, loss_kl: 8.521815, loss_recon: 171333.484375, loss_pred: 0.299162
iteration 752: loss: 173176.078125, loss_kl: 7.669155, loss_recon: 173094.562500, loss_pred: 0.169622
iteration 753: loss: 172238.734375, loss_kl: 8.307015, loss_recon: 172150.078125, loss_pred: 0.187482
iteration 754: loss: 173277.890625, loss_kl: 7.147712, loss_recon: 173199.468750, loss_pred: 0.182703
iteration 755: loss: 171847.156250, loss_kl: 7.570184, loss_recon: 171750.921875, loss_pred: 0.325325
iteration 756: loss: 175080.078125, loss_kl: 7.312880, loss_recon: 174994.234375, loss_pred: 0.242956
 42%|███████████▊                | 84/200 [1:17:44<1:45:18, 54.47s/it]iteration 757: loss: 176028.593750, loss_kl: 6.922679, loss_recon: 175938.171875, loss_pred: 0.294147
iteration 758: loss: 171559.625000, loss_kl: 8.112818, loss_recon: 171462.750000, loss_pred: 0.253970
iteration 759: loss: 171454.171875, loss_kl: 7.204950, loss_recon: 171369.375000, loss_pred: 0.213060
iteration 760: loss: 175420.671875, loss_kl: 7.480425, loss_recon: 175330.375000, loss_pred: 0.243709
iteration 761: loss: 173753.890625, loss_kl: 6.781691, loss_recon: 173669.281250, loss_pred: 0.248490
iteration 762: loss: 171947.218750, loss_kl: 7.126478, loss_recon: 171840.593750, loss_pred: 0.438254
iteration 763: loss: 171961.187500, loss_kl: 6.783947, loss_recon: 171874.906250, loss_pred: 0.264984
iteration 764: loss: 172933.328125, loss_kl: 7.840591, loss_recon: 172832.312500, loss_pred: 0.319274
iteration 765: loss: 172351.250000, loss_kl: 7.249213, loss_recon: 172264.234375, loss_pred: 0.231346
 42%|███████████▉                | 85/200 [1:18:40<1:44:59, 54.78s/it]iteration 766: loss: 173478.859375, loss_kl: 7.419950, loss_recon: 173390.578125, loss_pred: 0.199461
iteration 767: loss: 169422.281250, loss_kl: 7.771953, loss_recon: 169306.921875, loss_pred: 0.438019
iteration 768: loss: 173458.125000, loss_kl: 7.273410, loss_recon: 173370.687500, loss_pred: 0.204652
iteration 769: loss: 173633.765625, loss_kl: 7.435821, loss_recon: 173540.828125, loss_pred: 0.244732
iteration 770: loss: 172423.343750, loss_kl: 6.720752, loss_recon: 172337.421875, loss_pred: 0.240316
iteration 771: loss: 171361.843750, loss_kl: 7.388535, loss_recon: 171273.156250, loss_pred: 0.206609
iteration 772: loss: 172123.343750, loss_kl: 6.487057, loss_recon: 172018.968750, loss_pred: 0.446393
iteration 773: loss: 175263.953125, loss_kl: 5.904757, loss_recon: 175185.046875, loss_pred: 0.245292
iteration 774: loss: 173040.578125, loss_kl: 5.805158, loss_recon: 172956.656250, loss_pred: 0.304733
 43%|████████████                | 86/200 [1:19:34<1:43:37, 54.54s/it]iteration 775: loss: 171410.375000, loss_kl: 5.864794, loss_recon: 171309.656250, loss_pred: 0.443935
iteration 776: loss: 170029.546875, loss_kl: 7.008300, loss_recon: 169931.140625, loss_pred: 0.311012
iteration 777: loss: 172962.921875, loss_kl: 6.938944, loss_recon: 172869.140625, loss_pred: 0.271415
iteration 778: loss: 175190.812500, loss_kl: 6.787243, loss_recon: 175105.968750, loss_pred: 0.196499
iteration 779: loss: 171243.312500, loss_kl: 7.026532, loss_recon: 171131.921875, loss_pred: 0.439054
iteration 780: loss: 173731.234375, loss_kl: 7.523177, loss_recon: 173636.484375, loss_pred: 0.225039
iteration 781: loss: 173064.625000, loss_kl: 7.600678, loss_recon: 172963.171875, loss_pred: 0.284483
iteration 782: loss: 171242.171875, loss_kl: 5.935941, loss_recon: 171166.890625, loss_pred: 0.182623
iteration 783: loss: 172415.718750, loss_kl: 7.489327, loss_recon: 172325.765625, loss_pred: 0.180320
 44%|████████████▏               | 87/200 [1:20:28<1:42:47, 54.58s/it]iteration 784: loss: 171838.937500, loss_kl: 6.609665, loss_recon: 171747.093750, loss_pred: 0.257510
iteration 785: loss: 174357.796875, loss_kl: 6.117240, loss_recon: 174268.734375, loss_pred: 0.278840
iteration 786: loss: 171900.406250, loss_kl: 5.944808, loss_recon: 171806.937500, loss_pred: 0.340112
iteration 787: loss: 173658.625000, loss_kl: 5.269331, loss_recon: 173565.968750, loss_pred: 0.399627
iteration 788: loss: 169491.234375, loss_kl: 5.955881, loss_recon: 169400.109375, loss_pred: 0.315559
iteration 789: loss: 171194.140625, loss_kl: 6.194701, loss_recon: 171108.250000, loss_pred: 0.239320
iteration 790: loss: 173008.859375, loss_kl: 5.903536, loss_recon: 172900.593750, loss_pred: 0.492267
iteration 791: loss: 169206.031250, loss_kl: 6.029793, loss_recon: 169103.890625, loss_pred: 0.418462
iteration 792: loss: 173989.562500, loss_kl: 4.919183, loss_recon: 173915.718750, loss_pred: 0.246630
 44%|████████████▎               | 88/200 [1:21:24<1:42:14, 54.77s/it]iteration 793: loss: 170716.515625, loss_kl: 5.854274, loss_recon: 170633.796875, loss_pred: 0.241647
iteration 794: loss: 172812.734375, loss_kl: 5.317720, loss_recon: 172705.906250, loss_pred: 0.536629
iteration 795: loss: 171709.750000, loss_kl: 5.988088, loss_recon: 171617.796875, loss_pred: 0.320784
iteration 796: loss: 172237.984375, loss_kl: 5.370535, loss_recon: 172157.000000, loss_pred: 0.272738
iteration 797: loss: 171905.906250, loss_kl: 5.739789, loss_recon: 171828.703125, loss_pred: 0.198174
iteration 798: loss: 172083.125000, loss_kl: 5.992305, loss_recon: 171999.921875, loss_pred: 0.232826
iteration 799: loss: 170865.343750, loss_kl: 5.566689, loss_recon: 170786.234375, loss_pred: 0.234321
iteration 800: loss: 173022.734375, loss_kl: 5.970358, loss_recon: 172937.484375, loss_pred: 0.255511
iteration 801: loss: 170362.078125, loss_kl: 5.366105, loss_recon: 170281.140625, loss_pred: 0.272825
 44%|████████████▍               | 89/200 [1:22:19<1:41:59, 55.13s/it]iteration 802: loss: 171151.500000, loss_kl: 5.465691, loss_recon: 171072.953125, loss_pred: 0.238916
iteration 803: loss: 174306.703125, loss_kl: 4.801352, loss_recon: 174236.453125, loss_pred: 0.222292
iteration 804: loss: 171647.843750, loss_kl: 5.628524, loss_recon: 171563.609375, loss_pred: 0.279574
iteration 805: loss: 174176.859375, loss_kl: 4.686588, loss_recon: 174106.234375, loss_pred: 0.237680
iteration 806: loss: 171807.937500, loss_kl: 5.467264, loss_recon: 171730.890625, loss_pred: 0.223762
iteration 807: loss: 166861.109375, loss_kl: 5.760927, loss_recon: 166771.875000, loss_pred: 0.316277
iteration 808: loss: 170594.015625, loss_kl: 5.105542, loss_recon: 170524.625000, loss_pred: 0.183205
iteration 809: loss: 172332.078125, loss_kl: 5.155018, loss_recon: 172242.578125, loss_pred: 0.379590
iteration 810: loss: 170255.828125, loss_kl: 4.727315, loss_recon: 170183.000000, loss_pred: 0.255691
 45%|████████████▌               | 90/200 [1:23:15<1:41:12, 55.20s/it]iteration 811: loss: 168838.609375, loss_kl: 4.908843, loss_recon: 168760.031250, loss_pred: 0.294877
iteration 812: loss: 172090.687500, loss_kl: 4.611795, loss_recon: 172021.609375, loss_pred: 0.229504
iteration 813: loss: 175135.375000, loss_kl: 4.150153, loss_recon: 175071.015625, loss_pred: 0.228602
iteration 814: loss: 169487.734375, loss_kl: 4.914516, loss_recon: 169407.593750, loss_pred: 0.310040
iteration 815: loss: 171039.859375, loss_kl: 4.903220, loss_recon: 170969.453125, loss_pred: 0.213824
iteration 816: loss: 169241.906250, loss_kl: 4.888025, loss_recon: 169173.796875, loss_pred: 0.192281
iteration 817: loss: 170036.515625, loss_kl: 5.053655, loss_recon: 169965.234375, loss_pred: 0.207526
iteration 818: loss: 172049.359375, loss_kl: 4.510028, loss_recon: 171974.953125, loss_pred: 0.293107
iteration 819: loss: 172583.500000, loss_kl: 4.486601, loss_recon: 172508.343750, loss_pred: 0.302998
 46%|████████████▋               | 91/200 [1:24:10<1:40:26, 55.29s/it]iteration 820: loss: 172432.781250, loss_kl: 4.737671, loss_recon: 172366.953125, loss_pred: 0.184567
iteration 821: loss: 174522.734375, loss_kl: 4.209921, loss_recon: 174457.250000, loss_pred: 0.233966
iteration 822: loss: 170022.937500, loss_kl: 4.534432, loss_recon: 169939.812500, loss_pred: 0.377750
iteration 823: loss: 168546.468750, loss_kl: 4.864629, loss_recon: 168468.718750, loss_pred: 0.291090
iteration 824: loss: 170918.328125, loss_kl: 5.019533, loss_recon: 170828.265625, loss_pred: 0.398555
iteration 825: loss: 170919.015625, loss_kl: 5.415456, loss_recon: 170847.031250, loss_pred: 0.178271
iteration 826: loss: 169344.468750, loss_kl: 4.714670, loss_recon: 169262.593750, loss_pred: 0.347271
iteration 827: loss: 172131.000000, loss_kl: 5.077749, loss_recon: 172052.062500, loss_pred: 0.281560
iteration 828: loss: 169400.312500, loss_kl: 5.354450, loss_recon: 169320.812500, loss_pred: 0.259489
 46%|████████████▉               | 92/200 [1:25:06<1:39:53, 55.50s/it]iteration 829: loss: 168753.062500, loss_kl: 5.728510, loss_recon: 168674.515625, loss_pred: 0.212630
iteration 830: loss: 170776.328125, loss_kl: 4.845249, loss_recon: 170706.906250, loss_pred: 0.209700
iteration 831: loss: 171543.609375, loss_kl: 5.647555, loss_recon: 171460.671875, loss_pred: 0.264639
iteration 832: loss: 170001.265625, loss_kl: 4.473641, loss_recon: 169934.968750, loss_pred: 0.215658
iteration 833: loss: 170198.765625, loss_kl: 5.296752, loss_recon: 170116.890625, loss_pred: 0.289030
iteration 834: loss: 171139.203125, loss_kl: 4.594881, loss_recon: 171051.109375, loss_pred: 0.421416
iteration 835: loss: 170993.562500, loss_kl: 4.607755, loss_recon: 170896.921875, loss_pred: 0.505674
iteration 836: loss: 173229.093750, loss_kl: 3.949804, loss_recon: 173160.984375, loss_pred: 0.286103
iteration 837: loss: 169636.968750, loss_kl: 4.441756, loss_recon: 169558.343750, loss_pred: 0.342034
 46%|█████████████               | 93/200 [1:26:01<1:38:15, 55.10s/it]iteration 838: loss: 168945.750000, loss_kl: 4.684536, loss_recon: 168878.093750, loss_pred: 0.208131
iteration 839: loss: 171834.328125, loss_kl: 4.416164, loss_recon: 171757.718750, loss_pred: 0.324584
iteration 840: loss: 171509.078125, loss_kl: 5.225650, loss_recon: 171425.296875, loss_pred: 0.315278
iteration 841: loss: 169559.984375, loss_kl: 4.336421, loss_recon: 169486.968750, loss_pred: 0.296525
iteration 842: loss: 172247.718750, loss_kl: 5.207530, loss_recon: 172169.593750, loss_pred: 0.260540
iteration 843: loss: 170418.531250, loss_kl: 5.581520, loss_recon: 170341.171875, loss_pred: 0.215510
iteration 844: loss: 168660.609375, loss_kl: 4.838573, loss_recon: 168587.937500, loss_pred: 0.242748
iteration 845: loss: 171386.328125, loss_kl: 4.817510, loss_recon: 171310.265625, loss_pred: 0.278918
iteration 846: loss: 169563.156250, loss_kl: 4.799692, loss_recon: 169477.140625, loss_pred: 0.380143
 47%|█████████████▏              | 94/200 [1:26:56<1:37:41, 55.29s/it]iteration 847: loss: 171322.515625, loss_kl: 3.972637, loss_recon: 171241.343750, loss_pred: 0.414573
iteration 848: loss: 169161.140625, loss_kl: 3.578067, loss_recon: 169055.156250, loss_pred: 0.702015
iteration 849: loss: 173427.421875, loss_kl: 4.922708, loss_recon: 173344.921875, loss_pred: 0.332656
iteration 850: loss: 169131.078125, loss_kl: 4.416186, loss_recon: 169062.171875, loss_pred: 0.247422
iteration 851: loss: 168942.937500, loss_kl: 6.201002, loss_recon: 168848.109375, loss_pred: 0.328115
iteration 852: loss: 171081.859375, loss_kl: 4.594710, loss_recon: 171006.562500, loss_pred: 0.293428
iteration 853: loss: 170424.640625, loss_kl: 5.738226, loss_recon: 170337.421875, loss_pred: 0.298391
iteration 854: loss: 168723.078125, loss_kl: 4.723827, loss_recon: 168646.578125, loss_pred: 0.292623
iteration 855: loss: 169767.453125, loss_kl: 4.392149, loss_recon: 169686.453125, loss_pred: 0.370851
 48%|█████████████▎              | 95/200 [1:27:52<1:36:53, 55.37s/it]iteration 856: loss: 169766.031250, loss_kl: 4.081555, loss_recon: 169684.875000, loss_pred: 0.403508
iteration 857: loss: 168257.984375, loss_kl: 4.858799, loss_recon: 168178.187500, loss_pred: 0.312045
iteration 858: loss: 168969.718750, loss_kl: 5.215944, loss_recon: 168888.875000, loss_pred: 0.286930
iteration 859: loss: 172340.062500, loss_kl: 4.026167, loss_recon: 172266.421875, loss_pred: 0.333678
iteration 860: loss: 168605.359375, loss_kl: 5.659651, loss_recon: 168493.656250, loss_pred: 0.551155
iteration 861: loss: 169031.828125, loss_kl: 3.659559, loss_recon: 168948.390625, loss_pred: 0.468454
iteration 862: loss: 169207.421875, loss_kl: 6.673954, loss_recon: 169083.031250, loss_pred: 0.576600
iteration 863: loss: 170605.406250, loss_kl: 5.149661, loss_recon: 170487.312500, loss_pred: 0.665959
iteration 864: loss: 173631.578125, loss_kl: 5.785994, loss_recon: 173531.765625, loss_pred: 0.419459
 48%|█████████████▍              | 96/200 [1:28:48<1:36:09, 55.48s/it]iteration 865: loss: 170850.078125, loss_kl: 6.099474, loss_recon: 170686.406250, loss_pred: 1.026648
iteration 866: loss: 169884.625000, loss_kl: 3.335067, loss_recon: 169823.171875, loss_pred: 0.281076
iteration 867: loss: 169393.437500, loss_kl: 6.384191, loss_recon: 169298.703125, loss_pred: 0.308926
iteration 868: loss: 169906.812500, loss_kl: 6.384939, loss_recon: 169787.421875, loss_pred: 0.555523
iteration 869: loss: 169611.375000, loss_kl: 4.142193, loss_recon: 169515.671875, loss_pred: 0.542845
iteration 870: loss: 167294.625000, loss_kl: 6.437010, loss_recon: 167186.640625, loss_pred: 0.436075
iteration 871: loss: 170261.125000, loss_kl: 5.029348, loss_recon: 170183.250000, loss_pred: 0.275769
iteration 872: loss: 171445.953125, loss_kl: 4.441335, loss_recon: 171353.578125, loss_pred: 0.479627
iteration 873: loss: 169998.593750, loss_kl: 5.348501, loss_recon: 169920.765625, loss_pred: 0.243391
 48%|█████████████▌              | 97/200 [1:29:42<1:34:58, 55.32s/it]iteration 874: loss: 168700.265625, loss_kl: 3.729766, loss_recon: 168633.953125, loss_pred: 0.290179
iteration 875: loss: 170398.843750, loss_kl: 4.194237, loss_recon: 170308.343750, loss_pred: 0.485665
iteration 876: loss: 169891.281250, loss_kl: 3.796580, loss_recon: 169824.750000, loss_pred: 0.285629
iteration 877: loss: 169035.140625, loss_kl: 4.139809, loss_recon: 168953.750000, loss_pred: 0.399977
iteration 878: loss: 168823.296875, loss_kl: 4.254289, loss_recon: 168750.125000, loss_pred: 0.306208
iteration 879: loss: 167966.640625, loss_kl: 3.793335, loss_recon: 167902.671875, loss_pred: 0.260286
iteration 880: loss: 170213.453125, loss_kl: 4.240114, loss_recon: 170142.296875, loss_pred: 0.287515
iteration 881: loss: 171810.125000, loss_kl: 3.505845, loss_recon: 171737.921875, loss_pred: 0.371428
iteration 882: loss: 169759.468750, loss_kl: 4.514744, loss_recon: 169679.468750, loss_pred: 0.348548
 49%|█████████████▋              | 98/200 [1:30:37<1:33:40, 55.10s/it]iteration 883: loss: 167337.500000, loss_kl: 4.305536, loss_recon: 167270.859375, loss_pred: 0.235726
iteration 884: loss: 169798.500000, loss_kl: 3.698573, loss_recon: 169733.968750, loss_pred: 0.275419
iteration 885: loss: 168318.359375, loss_kl: 4.696960, loss_recon: 168248.796875, loss_pred: 0.225925
iteration 886: loss: 170961.234375, loss_kl: 4.247114, loss_recon: 170894.500000, loss_pred: 0.242691
iteration 887: loss: 170223.687500, loss_kl: 3.352931, loss_recon: 170166.578125, loss_pred: 0.235758
iteration 888: loss: 168729.312500, loss_kl: 4.262165, loss_recon: 168643.031250, loss_pred: 0.436566
iteration 889: loss: 169763.359375, loss_kl: 4.343005, loss_recon: 169700.140625, loss_pred: 0.197769
iteration 890: loss: 169872.000000, loss_kl: 3.986458, loss_recon: 169795.578125, loss_pred: 0.365674
iteration 891: loss: 169679.000000, loss_kl: 4.462316, loss_recon: 169594.921875, loss_pred: 0.394519
 50%|█████████████▊              | 99/200 [1:31:32<1:32:47, 55.13s/it]iteration 892: loss: 168037.937500, loss_kl: 3.613052, loss_recon: 167979.875000, loss_pred: 0.219411
iteration 893: loss: 171301.515625, loss_kl: 3.604596, loss_recon: 171218.046875, loss_pred: 0.474228
iteration 894: loss: 167775.375000, loss_kl: 4.607648, loss_recon: 167678.921875, loss_pred: 0.503738
iteration 895: loss: 167873.546875, loss_kl: 3.885764, loss_recon: 167800.421875, loss_pred: 0.342599
iteration 896: loss: 170769.359375, loss_kl: 4.415861, loss_recon: 170701.859375, loss_pred: 0.233373
iteration 897: loss: 166801.000000, loss_kl: 5.559134, loss_recon: 166724.421875, loss_pred: 0.209918
iteration 898: loss: 167659.578125, loss_kl: 3.758326, loss_recon: 167583.375000, loss_pred: 0.386260
iteration 899: loss: 171373.234375, loss_kl: 4.946977, loss_recon: 171300.921875, loss_pred: 0.228466
iteration 900: loss: 171500.515625, loss_kl: 5.396346, loss_recon: 171425.078125, loss_pred: 0.214735
 50%|█████████████▌             | 100/200 [1:32:28<1:32:11, 55.32s/it]iteration 901: loss: 170031.812500, loss_kl: 3.589199, loss_recon: 170002.968750, loss_pred: 0.284857
iteration 902: loss: 168338.656250, loss_kl: 4.442549, loss_recon: 168313.296875, loss_pred: 0.249277
iteration 903: loss: 167464.296875, loss_kl: 6.079092, loss_recon: 167441.750000, loss_pred: 0.219438
iteration 904: loss: 168260.375000, loss_kl: 7.126882, loss_recon: 168238.687500, loss_pred: 0.209712
iteration 905: loss: 170528.984375, loss_kl: 8.043117, loss_recon: 170509.000000, loss_pred: 0.191948
iteration 906: loss: 171276.656250, loss_kl: 8.750824, loss_recon: 171257.093750, loss_pred: 0.186950
iteration 907: loss: 167788.171875, loss_kl: 9.674348, loss_recon: 167760.484375, loss_pred: 0.267224
iteration 908: loss: 170758.015625, loss_kl: 10.777792, loss_recon: 170730.578125, loss_pred: 0.263595
iteration 909: loss: 166881.312500, loss_kl: 11.143243, loss_recon: 166861.109375, loss_pred: 0.191003
 50%|█████████████▋             | 101/200 [1:33:25<1:32:00, 55.76s/it]iteration 910: loss: 166434.515625, loss_kl: 11.612004, loss_recon: 166400.625000, loss_pred: 0.327303
iteration 911: loss: 166526.031250, loss_kl: 12.496614, loss_recon: 166497.343750, loss_pred: 0.274350
iteration 912: loss: 168584.312500, loss_kl: 13.291931, loss_recon: 168565.625000, loss_pred: 0.173642
iteration 913: loss: 171487.609375, loss_kl: 14.232741, loss_recon: 171466.531250, loss_pred: 0.196596
iteration 914: loss: 166970.015625, loss_kl: 15.686705, loss_recon: 166932.828125, loss_pred: 0.356185
iteration 915: loss: 169689.750000, loss_kl: 15.536998, loss_recon: 169670.484375, loss_pred: 0.177185
iteration 916: loss: 169942.843750, loss_kl: 15.245128, loss_recon: 169924.750000, loss_pred: 0.165691
iteration 917: loss: 168877.890625, loss_kl: 15.490864, loss_recon: 168856.281250, loss_pred: 0.200665
iteration 918: loss: 171259.703125, loss_kl: 15.632269, loss_recon: 171236.687500, loss_pred: 0.214478
 51%|█████████████▊             | 102/200 [1:34:20<1:30:52, 55.63s/it]iteration 919: loss: 170041.734375, loss_kl: 15.945990, loss_recon: 170016.390625, loss_pred: 0.237568
iteration 920: loss: 169574.343750, loss_kl: 16.609386, loss_recon: 169542.703125, loss_pred: 0.299864
iteration 921: loss: 165426.859375, loss_kl: 17.349920, loss_recon: 165402.109375, loss_pred: 0.230233
iteration 922: loss: 171407.343750, loss_kl: 16.130060, loss_recon: 171376.453125, loss_pred: 0.292784
iteration 923: loss: 169025.937500, loss_kl: 16.845058, loss_recon: 168998.171875, loss_pred: 0.260713
iteration 924: loss: 169755.296875, loss_kl: 17.791243, loss_recon: 169730.812500, loss_pred: 0.227062
iteration 925: loss: 169715.375000, loss_kl: 19.021338, loss_recon: 169675.359375, loss_pred: 0.381136
iteration 926: loss: 168500.015625, loss_kl: 19.644167, loss_recon: 168452.265625, loss_pred: 0.457774
iteration 927: loss: 165153.343750, loss_kl: 19.272532, loss_recon: 165104.625000, loss_pred: 0.467932
 52%|█████████████▉             | 103/200 [1:35:16<1:30:04, 55.72s/it]iteration 928: loss: 171341.546875, loss_kl: 16.578583, loss_recon: 171303.171875, loss_pred: 0.367235
iteration 929: loss: 167972.281250, loss_kl: 15.874364, loss_recon: 167929.765625, loss_pred: 0.409268
iteration 930: loss: 170822.046875, loss_kl: 15.395002, loss_recon: 170782.921875, loss_pred: 0.375748
iteration 931: loss: 170204.828125, loss_kl: 16.461376, loss_recon: 170180.062500, loss_pred: 0.231229
iteration 932: loss: 165792.453125, loss_kl: 18.646585, loss_recon: 165760.468750, loss_pred: 0.301282
iteration 933: loss: 166700.328125, loss_kl: 19.478586, loss_recon: 166658.984375, loss_pred: 0.393898
iteration 934: loss: 167292.500000, loss_kl: 19.288864, loss_recon: 167261.265625, loss_pred: 0.293178
iteration 935: loss: 168898.875000, loss_kl: 19.246717, loss_recon: 168872.031250, loss_pred: 0.249290
iteration 936: loss: 168326.859375, loss_kl: 18.590389, loss_recon: 168295.281250, loss_pred: 0.297168
 52%|██████████████             | 104/200 [1:36:11<1:28:58, 55.61s/it]iteration 937: loss: 171542.312500, loss_kl: 18.733841, loss_recon: 171516.171875, loss_pred: 0.242603
iteration 938: loss: 168352.359375, loss_kl: 19.035809, loss_recon: 168326.453125, loss_pred: 0.239925
iteration 939: loss: 166905.109375, loss_kl: 19.641985, loss_recon: 166879.437500, loss_pred: 0.237031
iteration 940: loss: 172790.562500, loss_kl: 18.544249, loss_recon: 172748.921875, loss_pred: 0.397810
iteration 941: loss: 168422.921875, loss_kl: 18.207726, loss_recon: 168389.406250, loss_pred: 0.316893
iteration 942: loss: 169563.718750, loss_kl: 17.212347, loss_recon: 169541.937500, loss_pred: 0.200619
iteration 943: loss: 167287.953125, loss_kl: 16.367941, loss_recon: 167257.531250, loss_pred: 0.287791
iteration 944: loss: 166892.546875, loss_kl: 16.552187, loss_recon: 166868.687500, loss_pred: 0.222071
iteration 945: loss: 164184.562500, loss_kl: 17.759897, loss_recon: 164163.375000, loss_pred: 0.194117
 52%|██████████████▏            | 105/200 [1:37:07<1:28:10, 55.69s/it]iteration 946: loss: 168719.734375, loss_kl: 18.041866, loss_recon: 168695.968750, loss_pred: 0.219764
iteration 947: loss: 166166.625000, loss_kl: 18.689533, loss_recon: 166132.953125, loss_pred: 0.318040
iteration 948: loss: 167622.296875, loss_kl: 18.373795, loss_recon: 167596.015625, loss_pred: 0.244348
iteration 949: loss: 168039.187500, loss_kl: 17.477821, loss_recon: 168015.109375, loss_pred: 0.223272
iteration 950: loss: 168340.671875, loss_kl: 16.525171, loss_recon: 168322.453125, loss_pred: 0.165584
iteration 951: loss: 168928.390625, loss_kl: 15.760738, loss_recon: 168889.140625, loss_pred: 0.376752
iteration 952: loss: 169579.484375, loss_kl: 15.430860, loss_recon: 169553.390625, loss_pred: 0.245441
iteration 953: loss: 167913.265625, loss_kl: 16.777798, loss_recon: 167884.375000, loss_pred: 0.272198
iteration 954: loss: 169496.859375, loss_kl: 16.806906, loss_recon: 169471.484375, loss_pred: 0.236891
 53%|██████████████▎            | 106/200 [1:38:02<1:26:37, 55.29s/it]iteration 955: loss: 168945.312500, loss_kl: 16.466452, loss_recon: 168916.500000, loss_pred: 0.271770
iteration 956: loss: 170153.406250, loss_kl: 15.907310, loss_recon: 170136.609375, loss_pred: 0.152045
iteration 957: loss: 168276.046875, loss_kl: 15.844905, loss_recon: 168246.750000, loss_pred: 0.277209
iteration 958: loss: 163704.218750, loss_kl: 16.490543, loss_recon: 163680.453125, loss_pred: 0.221110
iteration 959: loss: 170502.328125, loss_kl: 16.089632, loss_recon: 170480.546875, loss_pred: 0.201725
iteration 960: loss: 164561.015625, loss_kl: 18.180525, loss_recon: 164535.484375, loss_pred: 0.237159
iteration 961: loss: 169038.640625, loss_kl: 17.391731, loss_recon: 168997.125000, loss_pred: 0.397764
iteration 962: loss: 169987.828125, loss_kl: 17.310246, loss_recon: 169970.281250, loss_pred: 0.158069
iteration 963: loss: 168601.796875, loss_kl: 17.898151, loss_recon: 168552.328125, loss_pred: 0.476742
 54%|██████████████▍            | 107/200 [1:38:55<1:24:59, 54.83s/it]iteration 964: loss: 165382.812500, loss_kl: 17.993282, loss_recon: 165337.937500, loss_pred: 0.430745
iteration 965: loss: 168990.984375, loss_kl: 16.594273, loss_recon: 168970.734375, loss_pred: 0.185923
iteration 966: loss: 167773.578125, loss_kl: 16.463560, loss_recon: 167732.593750, loss_pred: 0.393456
iteration 967: loss: 169132.125000, loss_kl: 16.605742, loss_recon: 169098.656250, loss_pred: 0.318121
iteration 968: loss: 171181.312500, loss_kl: 17.241978, loss_recon: 171152.281250, loss_pred: 0.273076
iteration 969: loss: 167904.453125, loss_kl: 18.097212, loss_recon: 167878.421875, loss_pred: 0.242189
iteration 970: loss: 167838.203125, loss_kl: 18.765970, loss_recon: 167814.343750, loss_pred: 0.219816
iteration 971: loss: 166199.109375, loss_kl: 19.724815, loss_recon: 166169.515625, loss_pred: 0.276247
iteration 972: loss: 168428.468750, loss_kl: 18.784281, loss_recon: 168402.156250, loss_pred: 0.244398
 54%|██████████████▌            | 108/200 [1:39:52<1:24:51, 55.34s/it]iteration 973: loss: 165570.500000, loss_kl: 18.747265, loss_recon: 165532.328125, loss_pred: 0.362989
iteration 974: loss: 166015.765625, loss_kl: 18.700155, loss_recon: 165962.984375, loss_pred: 0.509132
iteration 975: loss: 165663.406250, loss_kl: 17.601133, loss_recon: 165642.312500, loss_pred: 0.193355
iteration 976: loss: 168419.156250, loss_kl: 17.389736, loss_recon: 168369.187500, loss_pred: 0.482311
iteration 977: loss: 170933.953125, loss_kl: 17.567232, loss_recon: 170909.953125, loss_pred: 0.222431
iteration 978: loss: 170133.046875, loss_kl: 19.105627, loss_recon: 170113.562500, loss_pred: 0.175767
iteration 979: loss: 169963.062500, loss_kl: 20.197641, loss_recon: 169938.343750, loss_pred: 0.226955
iteration 980: loss: 168333.578125, loss_kl: 20.983486, loss_recon: 168309.515625, loss_pred: 0.219666
iteration 981: loss: 166875.828125, loss_kl: 20.626823, loss_recon: 166858.093750, loss_pred: 0.156765
 55%|██████████████▋            | 109/200 [1:40:46<1:23:24, 55.00s/it]iteration 982: loss: 167512.750000, loss_kl: 20.768293, loss_recon: 167465.718750, loss_pred: 0.449529
iteration 983: loss: 166842.781250, loss_kl: 20.123257, loss_recon: 166826.156250, loss_pred: 0.146049
iteration 984: loss: 166360.171875, loss_kl: 20.378418, loss_recon: 166330.625000, loss_pred: 0.275091
iteration 985: loss: 168309.046875, loss_kl: 19.853563, loss_recon: 168277.484375, loss_pred: 0.295764
iteration 986: loss: 165457.859375, loss_kl: 20.636799, loss_recon: 165431.281250, loss_pred: 0.245140
iteration 987: loss: 172012.593750, loss_kl: 19.217335, loss_recon: 171980.593750, loss_pred: 0.300816
iteration 988: loss: 166930.171875, loss_kl: 20.995316, loss_recon: 166889.718750, loss_pred: 0.383653
iteration 989: loss: 170316.843750, loss_kl: 21.208733, loss_recon: 170286.625000, loss_pred: 0.280927
iteration 990: loss: 167468.500000, loss_kl: 22.124195, loss_recon: 167401.250000, loss_pred: 0.650242
 55%|██████████████▊            | 110/200 [1:41:42<1:22:50, 55.22s/it]iteration 991: loss: 165543.890625, loss_kl: 22.101978, loss_recon: 165486.718750, loss_pred: 0.549700
iteration 992: loss: 167508.125000, loss_kl: 21.004454, loss_recon: 167486.156250, loss_pred: 0.198753
iteration 993: loss: 168713.265625, loss_kl: 19.878038, loss_recon: 168663.859375, loss_pred: 0.474172
iteration 994: loss: 168768.875000, loss_kl: 19.679504, loss_recon: 168705.312500, loss_pred: 0.615868
iteration 995: loss: 165530.843750, loss_kl: 20.574509, loss_recon: 165500.609375, loss_pred: 0.281679
iteration 996: loss: 167487.156250, loss_kl: 22.130829, loss_recon: 167441.406250, loss_pred: 0.435264
iteration 997: loss: 169929.921875, loss_kl: 23.271013, loss_recon: 169870.109375, loss_pred: 0.574855
iteration 998: loss: 170514.078125, loss_kl: 23.702139, loss_recon: 170486.265625, loss_pred: 0.254442
iteration 999: loss: 166280.250000, loss_kl: 24.823479, loss_recon: 166234.828125, loss_pred: 0.429322
 56%|██████████████▉            | 111/200 [1:42:37<1:21:41, 55.07s/it]iteration 1000: loss: 169712.140625, loss_kl: 22.324417, loss_recon: 169654.093750, loss_pred: 0.558079
iteration 1001: loss: 170160.328125, loss_kl: 20.847349, loss_recon: 170128.890625, loss_pred: 0.293549
iteration 1002: loss: 165956.546875, loss_kl: 19.908472, loss_recon: 165916.093750, loss_pred: 0.384627
iteration 1003: loss: 168231.187500, loss_kl: 20.943916, loss_recon: 168179.625000, loss_pred: 0.494735
iteration 1004: loss: 168730.125000, loss_kl: 22.057882, loss_recon: 168705.343750, loss_pred: 0.225755
iteration 1005: loss: 167335.140625, loss_kl: 23.615334, loss_recon: 167300.421875, loss_pred: 0.323587
iteration 1006: loss: 165603.328125, loss_kl: 25.407677, loss_recon: 165564.937500, loss_pred: 0.358383
iteration 1007: loss: 168666.281250, loss_kl: 24.327114, loss_recon: 168637.734375, loss_pred: 0.261106
iteration 1008: loss: 165146.765625, loss_kl: 23.705688, loss_recon: 165128.031250, loss_pred: 0.163672
 56%|███████████████            | 112/200 [1:43:32<1:20:56, 55.19s/it]iteration 1009: loss: 165840.390625, loss_kl: 23.064833, loss_recon: 165818.343750, loss_pred: 0.197270
iteration 1010: loss: 168012.015625, loss_kl: 21.955555, loss_recon: 167984.703125, loss_pred: 0.251156
iteration 1011: loss: 168535.531250, loss_kl: 22.213860, loss_recon: 168506.312500, loss_pred: 0.270036
iteration 1012: loss: 168133.453125, loss_kl: 22.642433, loss_recon: 168111.468750, loss_pred: 0.197172
iteration 1013: loss: 166398.984375, loss_kl: 24.358486, loss_recon: 166369.500000, loss_pred: 0.270496
iteration 1014: loss: 169753.062500, loss_kl: 24.118095, loss_recon: 169734.984375, loss_pred: 0.156677
iteration 1015: loss: 167393.656250, loss_kl: 24.284037, loss_recon: 167372.609375, loss_pred: 0.186257
iteration 1016: loss: 165593.265625, loss_kl: 23.966267, loss_recon: 165561.718750, loss_pred: 0.291625
iteration 1017: loss: 169024.000000, loss_kl: 23.198292, loss_recon: 169003.796875, loss_pred: 0.178928
 56%|███████████████▎           | 113/200 [1:44:29<1:20:55, 55.81s/it]iteration 1018: loss: 170265.937500, loss_kl: 22.865334, loss_recon: 170228.796875, loss_pred: 0.257988
iteration 1019: loss: 162870.671875, loss_kl: 23.497959, loss_recon: 162829.734375, loss_pred: 0.292759
iteration 1020: loss: 167483.406250, loss_kl: 21.364639, loss_recon: 167444.953125, loss_pred: 0.278550
iteration 1021: loss: 166589.531250, loss_kl: 21.636957, loss_recon: 166555.140625, loss_pred: 0.236616
iteration 1022: loss: 168395.125000, loss_kl: 21.200624, loss_recon: 168364.312500, loss_pred: 0.203047
iteration 1023: loss: 169894.390625, loss_kl: 20.123350, loss_recon: 169859.484375, loss_pred: 0.249239
iteration 1024: loss: 168921.140625, loss_kl: 20.086081, loss_recon: 168891.406250, loss_pred: 0.197598
iteration 1025: loss: 166158.968750, loss_kl: 20.766718, loss_recon: 166121.968750, loss_pred: 0.267081
iteration 1026: loss: 167441.593750, loss_kl: 19.717520, loss_recon: 167409.859375, loss_pred: 0.219568
 57%|███████████████▍           | 114/200 [1:45:23<1:19:06, 55.19s/it]iteration 1027: loss: 168515.781250, loss_kl: 17.936739, loss_recon: 168479.875000, loss_pred: 0.199072
iteration 1028: loss: 168234.265625, loss_kl: 16.440104, loss_recon: 168201.000000, loss_pred: 0.185950
iteration 1029: loss: 164911.218750, loss_kl: 14.958482, loss_recon: 164873.562500, loss_pred: 0.243140
iteration 1030: loss: 165624.875000, loss_kl: 13.573462, loss_recon: 165575.250000, loss_pred: 0.375137
iteration 1031: loss: 166918.406250, loss_kl: 11.745253, loss_recon: 166882.593750, loss_pred: 0.253289
iteration 1032: loss: 169149.109375, loss_kl: 10.354675, loss_recon: 169119.078125, loss_pred: 0.208031
iteration 1033: loss: 168892.578125, loss_kl: 10.211601, loss_recon: 168864.156250, loss_pred: 0.193060
iteration 1034: loss: 166811.750000, loss_kl: 9.503277, loss_recon: 166786.890625, loss_pred: 0.163744
iteration 1035: loss: 168034.640625, loss_kl: 8.069905, loss_recon: 168008.968750, loss_pred: 0.184733
 57%|███████████████▌           | 115/200 [1:46:17<1:17:51, 54.95s/it]iteration 1036: loss: 167710.484375, loss_kl: 6.947006, loss_recon: 167688.000000, loss_pred: 0.135327
iteration 1037: loss: 164081.968750, loss_kl: 7.463111, loss_recon: 164046.078125, loss_pred: 0.262786
iteration 1038: loss: 170546.203125, loss_kl: 6.218241, loss_recon: 170521.203125, loss_pred: 0.169810
iteration 1039: loss: 169233.859375, loss_kl: 6.466776, loss_recon: 169206.578125, loss_pred: 0.189476
iteration 1040: loss: 165527.500000, loss_kl: 7.113348, loss_recon: 165497.687500, loss_pred: 0.206611
iteration 1041: loss: 168310.312500, loss_kl: 6.987649, loss_recon: 168279.531250, loss_pred: 0.217845
iteration 1042: loss: 167958.218750, loss_kl: 6.941411, loss_recon: 167927.062500, loss_pred: 0.222214
iteration 1043: loss: 166589.515625, loss_kl: 6.788517, loss_recon: 166559.890625, loss_pred: 0.208720
iteration 1044: loss: 166471.609375, loss_kl: 6.299884, loss_recon: 166441.328125, loss_pred: 0.221738
 58%|███████████████▋           | 116/200 [1:47:12<1:16:48, 54.87s/it]iteration 1045: loss: 165152.984375, loss_kl: 5.612215, loss_recon: 165122.921875, loss_pred: 0.206016
iteration 1046: loss: 166401.265625, loss_kl: 5.536927, loss_recon: 166372.062500, loss_pred: 0.198691
iteration 1047: loss: 166816.656250, loss_kl: 6.127398, loss_recon: 166786.156250, loss_pred: 0.201928
iteration 1048: loss: 165818.312500, loss_kl: 5.678225, loss_recon: 165781.171875, loss_pred: 0.275827
iteration 1049: loss: 172371.187500, loss_kl: 4.976090, loss_recon: 172346.687500, loss_pred: 0.161209
iteration 1050: loss: 167498.968750, loss_kl: 5.091393, loss_recon: 167471.578125, loss_pred: 0.188070
iteration 1051: loss: 166856.046875, loss_kl: 5.271376, loss_recon: 166816.406250, loss_pred: 0.307649
iteration 1052: loss: 166308.546875, loss_kl: 5.600683, loss_recon: 166270.765625, loss_pred: 0.283487
iteration 1053: loss: 168638.031250, loss_kl: 5.335076, loss_recon: 168598.140625, loss_pred: 0.309036
 58%|███████████████▊           | 117/200 [1:48:09<1:16:44, 55.47s/it]iteration 1054: loss: 170322.593750, loss_kl: 5.457970, loss_recon: 170292.281250, loss_pred: 0.189595
iteration 1055: loss: 168447.203125, loss_kl: 6.141426, loss_recon: 168402.515625, loss_pred: 0.319053
iteration 1056: loss: 164081.312500, loss_kl: 6.173292, loss_recon: 164040.718750, loss_pred: 0.277494
iteration 1057: loss: 166460.031250, loss_kl: 5.462497, loss_recon: 166424.796875, loss_pred: 0.238713
iteration 1058: loss: 165398.859375, loss_kl: 5.994283, loss_recon: 165364.593750, loss_pred: 0.217930
iteration 1059: loss: 167518.203125, loss_kl: 5.512478, loss_recon: 167487.281250, loss_pred: 0.194585
iteration 1060: loss: 167481.000000, loss_kl: 4.793021, loss_recon: 167435.656250, loss_pred: 0.353739
iteration 1061: loss: 165512.062500, loss_kl: 5.217008, loss_recon: 165475.593750, loss_pred: 0.256320
iteration 1062: loss: 170147.890625, loss_kl: 4.776699, loss_recon: 170111.921875, loss_pred: 0.260256
 59%|███████████████▉           | 118/200 [1:49:04<1:15:32, 55.27s/it]iteration 1063: loss: 168930.062500, loss_kl: 5.073370, loss_recon: 168892.218750, loss_pred: 0.252843
iteration 1064: loss: 166889.796875, loss_kl: 5.234770, loss_recon: 166854.953125, loss_pred: 0.218777
iteration 1065: loss: 169672.078125, loss_kl: 5.033106, loss_recon: 169637.843750, loss_pred: 0.217681
iteration 1066: loss: 169827.781250, loss_kl: 4.786789, loss_recon: 169796.765625, loss_pred: 0.191594
iteration 1067: loss: 167723.875000, loss_kl: 4.775154, loss_recon: 167690.921875, loss_pred: 0.211247
iteration 1068: loss: 164699.859375, loss_kl: 4.989160, loss_recon: 164650.765625, loss_pred: 0.367397
iteration 1069: loss: 167933.812500, loss_kl: 4.837270, loss_recon: 167898.703125, loss_pred: 0.231250
iteration 1070: loss: 164082.484375, loss_kl: 6.098521, loss_recon: 164042.000000, loss_pred: 0.253923
iteration 1071: loss: 165207.078125, loss_kl: 5.918666, loss_recon: 165171.234375, loss_pred: 0.211899
 60%|████████████████           | 119/200 [1:49:58<1:14:16, 55.02s/it]iteration 1072: loss: 163966.484375, loss_kl: 6.003309, loss_recon: 163919.015625, loss_pred: 0.302367
iteration 1073: loss: 167605.828125, loss_kl: 5.002921, loss_recon: 167572.875000, loss_pred: 0.185747
iteration 1074: loss: 166214.953125, loss_kl: 5.135357, loss_recon: 166181.234375, loss_pred: 0.189631
iteration 1075: loss: 172352.218750, loss_kl: 4.554930, loss_recon: 172319.421875, loss_pred: 0.197192
iteration 1076: loss: 162811.531250, loss_kl: 5.329933, loss_recon: 162767.765625, loss_pred: 0.284463
iteration 1077: loss: 167831.406250, loss_kl: 4.591414, loss_recon: 167796.546875, loss_pred: 0.216787
iteration 1078: loss: 168513.234375, loss_kl: 4.417697, loss_recon: 168472.750000, loss_pred: 0.277898
iteration 1079: loss: 167457.296875, loss_kl: 4.862724, loss_recon: 167420.484375, loss_pred: 0.228493
iteration 1080: loss: 167602.781250, loss_kl: 4.425479, loss_recon: 167567.687500, loss_pred: 0.223871
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_119.pth
 60%|████████████████▏          | 120/200 [1:50:53<1:13:18, 54.98s/it]iteration 1081: loss: 169521.406250, loss_kl: 4.147854, loss_recon: 169483.937500, loss_pred: 0.238988
iteration 1082: loss: 166580.406250, loss_kl: 5.079818, loss_recon: 166543.656250, loss_pred: 0.201499
iteration 1083: loss: 166637.781250, loss_kl: 4.648251, loss_recon: 166604.203125, loss_pred: 0.183849
iteration 1084: loss: 168858.828125, loss_kl: 4.115541, loss_recon: 168811.062500, loss_pred: 0.343054
iteration 1085: loss: 168620.484375, loss_kl: 4.168240, loss_recon: 168585.921875, loss_pred: 0.209335
iteration 1086: loss: 167589.125000, loss_kl: 4.331193, loss_recon: 167555.421875, loss_pred: 0.195503
iteration 1087: loss: 166050.625000, loss_kl: 4.354334, loss_recon: 166014.687500, loss_pred: 0.217048
iteration 1088: loss: 166363.390625, loss_kl: 4.608771, loss_recon: 166326.312500, loss_pred: 0.220227
iteration 1089: loss: 163599.890625, loss_kl: 4.970477, loss_recon: 163562.281250, loss_pred: 0.213665
 60%|████████████████▎          | 121/200 [1:51:48<1:12:11, 54.82s/it]iteration 1090: loss: 167546.593750, loss_kl: 4.828305, loss_recon: 167508.484375, loss_pred: 0.204232
iteration 1091: loss: 165916.187500, loss_kl: 4.588644, loss_recon: 165874.750000, loss_pred: 0.246283
iteration 1092: loss: 165222.500000, loss_kl: 4.416139, loss_recon: 165192.343750, loss_pred: 0.139670
iteration 1093: loss: 166598.171875, loss_kl: 4.463673, loss_recon: 166561.359375, loss_pred: 0.204472
iteration 1094: loss: 169439.656250, loss_kl: 4.206538, loss_recon: 169406.578125, loss_pred: 0.176661
iteration 1095: loss: 165977.750000, loss_kl: 4.205185, loss_recon: 165941.765625, loss_pred: 0.205715
iteration 1096: loss: 167303.203125, loss_kl: 4.557668, loss_recon: 167262.687500, loss_pred: 0.238189
iteration 1097: loss: 167587.812500, loss_kl: 4.452213, loss_recon: 167551.359375, loss_pred: 0.201380
iteration 1098: loss: 167625.593750, loss_kl: 4.093822, loss_recon: 167594.234375, loss_pred: 0.163631
 61%|████████████████▍          | 122/200 [1:52:42<1:11:05, 54.69s/it]iteration 1099: loss: 170706.234375, loss_kl: 3.901257, loss_recon: 170670.250000, loss_pred: 0.201402
iteration 1100: loss: 167328.437500, loss_kl: 4.382656, loss_recon: 167284.812500, loss_pred: 0.258220
iteration 1101: loss: 167621.031250, loss_kl: 3.841271, loss_recon: 167586.343750, loss_pred: 0.190912
iteration 1102: loss: 170131.562500, loss_kl: 4.208083, loss_recon: 170089.953125, loss_pred: 0.245314
iteration 1103: loss: 166726.296875, loss_kl: 4.212049, loss_recon: 166690.375000, loss_pred: 0.188296
iteration 1104: loss: 165003.625000, loss_kl: 4.361046, loss_recon: 164966.656250, loss_pred: 0.192707
iteration 1105: loss: 166621.406250, loss_kl: 3.935887, loss_recon: 166575.593750, loss_pred: 0.298279
iteration 1106: loss: 162950.046875, loss_kl: 4.234960, loss_recon: 162910.156250, loss_pred: 0.226981
iteration 1107: loss: 165933.562500, loss_kl: 4.678787, loss_recon: 165854.578125, loss_pred: 0.599920
 62%|████████████████▌          | 123/200 [1:53:37<1:10:18, 54.78s/it]iteration 1108: loss: 163354.437500, loss_kl: 4.467980, loss_recon: 163307.750000, loss_pred: 0.267778
iteration 1109: loss: 168305.906250, loss_kl: 4.118478, loss_recon: 168261.218750, loss_pred: 0.263254
iteration 1110: loss: 164691.265625, loss_kl: 4.512080, loss_recon: 164648.812500, loss_pred: 0.223453
iteration 1111: loss: 168211.843750, loss_kl: 3.796339, loss_recon: 168165.375000, loss_pred: 0.295402
iteration 1112: loss: 168381.468750, loss_kl: 3.751173, loss_recon: 168345.484375, loss_pred: 0.192666
iteration 1113: loss: 166571.140625, loss_kl: 3.764634, loss_recon: 166514.421875, loss_pred: 0.399332
iteration 1114: loss: 167160.656250, loss_kl: 3.877318, loss_recon: 167127.406250, loss_pred: 0.159760
iteration 1115: loss: 168605.421875, loss_kl: 4.231856, loss_recon: 168543.093750, loss_pred: 0.434721
iteration 1116: loss: 167424.953125, loss_kl: 4.847523, loss_recon: 167380.562500, loss_pred: 0.227979
 62%|████████████████▋          | 124/200 [1:54:31<1:09:15, 54.68s/it]iteration 1117: loss: 169296.359375, loss_kl: 4.967779, loss_recon: 169250.062500, loss_pred: 0.221844
iteration 1118: loss: 164480.062500, loss_kl: 4.982694, loss_recon: 164407.343750, loss_pred: 0.485452
iteration 1119: loss: 165082.640625, loss_kl: 3.883300, loss_recon: 165044.531250, loss_pred: 0.192594
iteration 1120: loss: 165657.000000, loss_kl: 4.991840, loss_recon: 165598.015625, loss_pred: 0.347597
iteration 1121: loss: 164924.171875, loss_kl: 4.580861, loss_recon: 164881.968750, loss_pred: 0.199818
iteration 1122: loss: 170639.906250, loss_kl: 3.555750, loss_recon: 170600.546875, loss_pred: 0.221147
iteration 1123: loss: 164146.656250, loss_kl: 3.632299, loss_recon: 164080.796875, loss_pred: 0.482379
iteration 1124: loss: 168349.781250, loss_kl: 3.818326, loss_recon: 168313.093750, loss_pred: 0.181579
iteration 1125: loss: 169861.328125, loss_kl: 3.996239, loss_recon: 169794.578125, loss_pred: 0.473527
 62%|████████████████▉          | 125/200 [1:55:26<1:08:13, 54.58s/it]iteration 1126: loss: 167007.718750, loss_kl: 4.085494, loss_recon: 166952.453125, loss_pred: 0.338251
iteration 1127: loss: 167833.562500, loss_kl: 4.511156, loss_recon: 167786.421875, loss_pred: 0.234712
iteration 1128: loss: 166452.828125, loss_kl: 3.724036, loss_recon: 166412.796875, loss_pred: 0.204847
iteration 1129: loss: 164442.125000, loss_kl: 3.777458, loss_recon: 164384.093750, loss_pred: 0.382006
iteration 1130: loss: 166626.468750, loss_kl: 5.159885, loss_recon: 166584.265625, loss_pred: 0.151325
iteration 1131: loss: 167724.859375, loss_kl: 5.083273, loss_recon: 167669.203125, loss_pred: 0.289874
iteration 1132: loss: 166915.171875, loss_kl: 3.595496, loss_recon: 166869.765625, loss_pred: 0.265305
iteration 1133: loss: 166313.406250, loss_kl: 4.834919, loss_recon: 166254.859375, loss_pred: 0.331717
iteration 1134: loss: 168807.296875, loss_kl: 4.283393, loss_recon: 168756.593750, loss_pred: 0.282170
 63%|█████████████████          | 126/200 [1:56:19<1:06:56, 54.28s/it]iteration 1135: loss: 170869.015625, loss_kl: 3.555012, loss_recon: 170828.093750, loss_pred: 0.208521
iteration 1136: loss: 166239.406250, loss_kl: 4.705065, loss_recon: 166193.890625, loss_pred: 0.189463
iteration 1137: loss: 165635.437500, loss_kl: 4.465436, loss_recon: 165586.328125, loss_pred: 0.238988
iteration 1138: loss: 165882.296875, loss_kl: 4.064974, loss_recon: 165838.171875, loss_pred: 0.211804
iteration 1139: loss: 168256.312500, loss_kl: 4.329476, loss_recon: 168206.140625, loss_pred: 0.257385
iteration 1140: loss: 166641.765625, loss_kl: 4.903101, loss_recon: 166580.968750, loss_pred: 0.331261
iteration 1141: loss: 164535.187500, loss_kl: 4.878389, loss_recon: 164474.203125, loss_pred: 0.334607
iteration 1142: loss: 166696.375000, loss_kl: 3.884379, loss_recon: 166650.906250, loss_pred: 0.235448
iteration 1143: loss: 167261.218750, loss_kl: 3.645230, loss_recon: 167207.390625, loss_pred: 0.332456
 64%|█████████████████▏         | 127/200 [1:57:15<1:06:33, 54.71s/it]iteration 1144: loss: 166392.312500, loss_kl: 4.596355, loss_recon: 166338.937500, loss_pred: 0.256068
iteration 1145: loss: 167506.734375, loss_kl: 4.523575, loss_recon: 167454.562500, loss_pred: 0.248451
iteration 1146: loss: 165266.281250, loss_kl: 4.138706, loss_recon: 165205.906250, loss_pred: 0.353677
iteration 1147: loss: 170230.421875, loss_kl: 3.789137, loss_recon: 170188.546875, loss_pred: 0.189784
iteration 1148: loss: 163938.984375, loss_kl: 4.774673, loss_recon: 163847.375000, loss_pred: 0.627613
iteration 1149: loss: 167286.187500, loss_kl: 5.122532, loss_recon: 167216.031250, loss_pred: 0.392263
iteration 1150: loss: 169422.062500, loss_kl: 3.976513, loss_recon: 169364.593750, loss_pred: 0.334485
iteration 1151: loss: 168740.828125, loss_kl: 3.275883, loss_recon: 168683.265625, loss_pred: 0.377889
iteration 1152: loss: 163080.500000, loss_kl: 4.262341, loss_recon: 163014.406250, loss_pred: 0.403361
 64%|█████████████████▎         | 128/200 [1:58:09<1:05:28, 54.56s/it]iteration 1153: loss: 164453.281250, loss_kl: 4.781850, loss_recon: 164384.812500, loss_pred: 0.376837
iteration 1154: loss: 166936.890625, loss_kl: 4.105045, loss_recon: 166875.718750, loss_pred: 0.347573
iteration 1155: loss: 168318.093750, loss_kl: 4.717803, loss_recon: 168269.968750, loss_pred: 0.177624
iteration 1156: loss: 167523.781250, loss_kl: 4.321977, loss_recon: 167455.281250, loss_pred: 0.406813
iteration 1157: loss: 166426.406250, loss_kl: 3.405425, loss_recon: 166351.593750, loss_pred: 0.528960
iteration 1158: loss: 164526.281250, loss_kl: 4.966873, loss_recon: 164459.984375, loss_pred: 0.343239
iteration 1159: loss: 169311.375000, loss_kl: 4.469199, loss_recon: 169200.000000, loss_pred: 0.826088
iteration 1160: loss: 167716.484375, loss_kl: 3.611497, loss_recon: 167663.328125, loss_pred: 0.299001
iteration 1161: loss: 166385.421875, loss_kl: 5.143808, loss_recon: 166312.828125, loss_pred: 0.394793
 64%|█████████████████▍         | 129/200 [1:59:05<1:04:54, 54.85s/it]iteration 1162: loss: 166117.718750, loss_kl: 4.600377, loss_recon: 166018.890625, loss_pred: 0.673902
iteration 1163: loss: 167466.343750, loss_kl: 4.083291, loss_recon: 167408.156250, loss_pred: 0.303042
iteration 1164: loss: 167365.109375, loss_kl: 6.014733, loss_recon: 167264.453125, loss_pred: 0.595702
iteration 1165: loss: 167897.109375, loss_kl: 5.038291, loss_recon: 167809.312500, loss_pred: 0.533741
iteration 1166: loss: 165760.234375, loss_kl: 4.324697, loss_recon: 165679.812500, loss_pred: 0.508776
iteration 1167: loss: 166725.500000, loss_kl: 4.835351, loss_recon: 166634.718750, loss_pred: 0.577507
iteration 1168: loss: 167190.218750, loss_kl: 3.867368, loss_recon: 167111.859375, loss_pred: 0.519387
iteration 1169: loss: 165929.968750, loss_kl: 3.921295, loss_recon: 165876.062500, loss_pred: 0.271081
iteration 1170: loss: 167044.640625, loss_kl: 4.934706, loss_recon: 166971.234375, loss_pred: 0.396912
 65%|█████████████████▌         | 130/200 [2:00:01<1:04:28, 55.26s/it]iteration 1171: loss: 167184.562500, loss_kl: 3.624488, loss_recon: 167120.921875, loss_pred: 0.374363
iteration 1172: loss: 163190.968750, loss_kl: 4.920581, loss_recon: 163115.718750, loss_pred: 0.396860
iteration 1173: loss: 166266.156250, loss_kl: 4.646041, loss_recon: 166194.281250, loss_pred: 0.382937
iteration 1174: loss: 170613.718750, loss_kl: 3.389222, loss_recon: 170558.828125, loss_pred: 0.303947
iteration 1175: loss: 166212.578125, loss_kl: 4.840282, loss_recon: 166149.781250, loss_pred: 0.278145
iteration 1176: loss: 167358.687500, loss_kl: 4.478442, loss_recon: 167300.859375, loss_pred: 0.254579
iteration 1177: loss: 163251.562500, loss_kl: 3.886058, loss_recon: 163183.265625, loss_pred: 0.401984
iteration 1178: loss: 169423.781250, loss_kl: 3.867602, loss_recon: 169373.843750, loss_pred: 0.219915
iteration 1179: loss: 167567.515625, loss_kl: 4.366017, loss_recon: 167514.375000, loss_pred: 0.215855
 66%|█████████████████▋         | 131/200 [2:00:56<1:03:22, 55.11s/it]iteration 1180: loss: 170189.109375, loss_kl: 3.196522, loss_recon: 170140.937500, loss_pred: 0.238001
iteration 1181: loss: 166547.234375, loss_kl: 3.303030, loss_recon: 166490.921875, loss_pred: 0.311307
iteration 1182: loss: 165376.546875, loss_kl: 4.079348, loss_recon: 165314.312500, loss_pred: 0.311377
iteration 1183: loss: 167319.437500, loss_kl: 3.611566, loss_recon: 167238.781250, loss_pred: 0.531222
iteration 1184: loss: 168950.171875, loss_kl: 3.619760, loss_recon: 168900.968750, loss_pred: 0.216028
iteration 1185: loss: 164639.593750, loss_kl: 4.158676, loss_recon: 164573.937500, loss_pred: 0.339601
iteration 1186: loss: 166098.968750, loss_kl: 3.137857, loss_recon: 166038.375000, loss_pred: 0.366779
iteration 1187: loss: 170492.500000, loss_kl: 4.241709, loss_recon: 170437.453125, loss_pred: 0.226999
iteration 1188: loss: 161263.750000, loss_kl: 4.461754, loss_recon: 161197.203125, loss_pred: 0.325289
 66%|█████████████████▊         | 132/200 [2:01:49<1:01:56, 54.66s/it]iteration 1189: loss: 165372.015625, loss_kl: 3.269350, loss_recon: 165319.265625, loss_pred: 0.265274
iteration 1190: loss: 169316.453125, loss_kl: 4.007132, loss_recon: 169248.921875, loss_pred: 0.353868
iteration 1191: loss: 162276.859375, loss_kl: 3.619305, loss_recon: 162221.546875, loss_pred: 0.262747
iteration 1192: loss: 164677.062500, loss_kl: 3.611258, loss_recon: 164619.828125, loss_pred: 0.282712
iteration 1193: loss: 165182.906250, loss_kl: 4.051809, loss_recon: 165106.296875, loss_pred: 0.441059
iteration 1194: loss: 169329.125000, loss_kl: 3.302397, loss_recon: 169273.203125, loss_pred: 0.294298
iteration 1195: loss: 170091.890625, loss_kl: 3.548585, loss_recon: 170029.031250, loss_pred: 0.343991
iteration 1196: loss: 166706.062500, loss_kl: 3.203364, loss_recon: 166642.218750, loss_pred: 0.381577
iteration 1197: loss: 167523.968750, loss_kl: 4.575555, loss_recon: 167469.109375, loss_pred: 0.181619
 66%|█████████████████▉         | 133/200 [2:02:44<1:00:59, 54.62s/it]iteration 1198: loss: 166835.390625, loss_kl: 3.961128, loss_recon: 166783.515625, loss_pred: 0.185379
iteration 1199: loss: 168360.796875, loss_kl: 3.500523, loss_recon: 168300.906250, loss_pred: 0.304366
iteration 1200: loss: 168352.750000, loss_kl: 4.170434, loss_recon: 168296.437500, loss_pred: 0.212135
iteration 1201: loss: 164311.968750, loss_kl: 3.248642, loss_recon: 164250.859375, loss_pred: 0.337702
iteration 1202: loss: 167568.328125, loss_kl: 4.030373, loss_recon: 167506.109375, loss_pred: 0.282907
iteration 1203: loss: 167455.125000, loss_kl: 3.205773, loss_recon: 167392.265625, loss_pred: 0.358700
iteration 1204: loss: 167450.218750, loss_kl: 3.294257, loss_recon: 167400.718750, loss_pred: 0.217845
iteration 1205: loss: 164848.312500, loss_kl: 3.321210, loss_recon: 164801.125000, loss_pred: 0.192391
iteration 1206: loss: 165186.828125, loss_kl: 3.216310, loss_recon: 165141.203125, loss_pred: 0.185595
 67%|███████████████████▍         | 134/200 [2:03:38<59:46, 54.34s/it]iteration 1207: loss: 169519.500000, loss_kl: 2.927629, loss_recon: 169467.421875, loss_pred: 0.262773
iteration 1208: loss: 165015.500000, loss_kl: 3.171554, loss_recon: 164953.828125, loss_pred: 0.337134
iteration 1209: loss: 164616.250000, loss_kl: 3.216848, loss_recon: 164564.968750, loss_pred: 0.229307
iteration 1210: loss: 167647.937500, loss_kl: 3.158355, loss_recon: 167589.375000, loss_pred: 0.307397
iteration 1211: loss: 168048.093750, loss_kl: 2.961669, loss_recon: 168004.796875, loss_pred: 0.172024
iteration 1212: loss: 167277.718750, loss_kl: 3.416552, loss_recon: 167228.421875, loss_pred: 0.191942
iteration 1213: loss: 165301.609375, loss_kl: 3.095789, loss_recon: 165252.812500, loss_pred: 0.215147
iteration 1214: loss: 167555.562500, loss_kl: 3.084829, loss_recon: 167501.500000, loss_pred: 0.268821
iteration 1215: loss: 165241.015625, loss_kl: 2.896484, loss_recon: 165155.406250, loss_pred: 0.600853
 68%|███████████████████▌         | 135/200 [2:04:33<59:02, 54.50s/it]iteration 1216: loss: 165547.640625, loss_kl: 2.691451, loss_recon: 165498.093750, loss_pred: 0.247601
iteration 1217: loss: 167746.781250, loss_kl: 2.672644, loss_recon: 167702.765625, loss_pred: 0.194130
iteration 1218: loss: 164857.046875, loss_kl: 3.001647, loss_recon: 164804.093750, loss_pred: 0.253085
iteration 1219: loss: 168016.250000, loss_kl: 2.941900, loss_recon: 167959.625000, loss_pred: 0.295381
iteration 1220: loss: 166426.375000, loss_kl: 2.829176, loss_recon: 166357.140625, loss_pred: 0.431938
iteration 1221: loss: 164969.015625, loss_kl: 3.151573, loss_recon: 164922.484375, loss_pred: 0.175180
iteration 1222: loss: 166586.328125, loss_kl: 3.181906, loss_recon: 166536.921875, loss_pred: 0.201056
iteration 1223: loss: 168360.781250, loss_kl: 3.312492, loss_recon: 168305.296875, loss_pred: 0.249884
iteration 1224: loss: 167548.671875, loss_kl: 2.840777, loss_recon: 167497.328125, loss_pred: 0.251880
 68%|███████████████████▋         | 136/200 [2:05:27<58:02, 54.41s/it]iteration 1225: loss: 164291.312500, loss_kl: 2.771344, loss_recon: 164227.625000, loss_pred: 0.370815
iteration 1226: loss: 161724.453125, loss_kl: 3.408935, loss_recon: 161666.609375, loss_pred: 0.251050
iteration 1227: loss: 165476.328125, loss_kl: 2.801336, loss_recon: 165424.031250, loss_pred: 0.253965
iteration 1228: loss: 169599.515625, loss_kl: 3.215176, loss_recon: 169526.921875, loss_pred: 0.417203
iteration 1229: loss: 165083.578125, loss_kl: 3.849704, loss_recon: 165016.000000, loss_pred: 0.306090
iteration 1230: loss: 168767.515625, loss_kl: 2.609562, loss_recon: 168699.000000, loss_pred: 0.434530
iteration 1231: loss: 170851.234375, loss_kl: 3.303311, loss_recon: 170774.515625, loss_pred: 0.449996
iteration 1232: loss: 167957.406250, loss_kl: 3.882595, loss_recon: 167889.484375, loss_pred: 0.306438
iteration 1233: loss: 166259.187500, loss_kl: 2.862131, loss_recon: 166206.765625, loss_pred: 0.249384
 68%|███████████████████▊         | 137/200 [2:06:20<56:49, 54.12s/it]iteration 1234: loss: 167175.125000, loss_kl: 4.039862, loss_recon: 167085.765625, loss_pred: 0.489518
iteration 1235: loss: 168855.921875, loss_kl: 4.420806, loss_recon: 168788.921875, loss_pred: 0.228043
iteration 1236: loss: 164523.203125, loss_kl: 3.379274, loss_recon: 164448.093750, loss_pred: 0.413093
iteration 1237: loss: 169210.015625, loss_kl: 3.833429, loss_recon: 169143.312500, loss_pred: 0.283695
iteration 1238: loss: 164546.500000, loss_kl: 4.307825, loss_recon: 164475.000000, loss_pred: 0.284290
iteration 1239: loss: 165987.015625, loss_kl: 2.749403, loss_recon: 165930.281250, loss_pred: 0.292411
iteration 1240: loss: 166940.640625, loss_kl: 3.615925, loss_recon: 166877.000000, loss_pred: 0.274856
iteration 1241: loss: 166895.796875, loss_kl: 3.159400, loss_recon: 166846.062500, loss_pred: 0.181468
iteration 1242: loss: 165875.562500, loss_kl: 2.684513, loss_recon: 165832.406250, loss_pred: 0.163145
 69%|████████████████████         | 138/200 [2:07:14<55:45, 53.97s/it]iteration 1243: loss: 166300.953125, loss_kl: 3.526662, loss_recon: 166245.109375, loss_pred: 0.205708
iteration 1244: loss: 168336.343750, loss_kl: 2.988523, loss_recon: 168289.062500, loss_pred: 0.173867
iteration 1245: loss: 167476.140625, loss_kl: 3.173084, loss_recon: 167419.171875, loss_pred: 0.252272
iteration 1246: loss: 167146.843750, loss_kl: 3.585490, loss_recon: 167092.250000, loss_pred: 0.187307
iteration 1247: loss: 168168.765625, loss_kl: 3.087182, loss_recon: 168113.765625, loss_pred: 0.241302
iteration 1248: loss: 165248.640625, loss_kl: 3.037266, loss_recon: 165174.203125, loss_pred: 0.440647
iteration 1249: loss: 163611.562500, loss_kl: 3.384345, loss_recon: 163558.203125, loss_pred: 0.195154
iteration 1250: loss: 167877.640625, loss_kl: 2.708900, loss_recon: 167822.781250, loss_pred: 0.277677
iteration 1251: loss: 165441.421875, loss_kl: 3.492756, loss_recon: 165386.500000, loss_pred: 0.200030
 70%|████████████████████▏        | 139/200 [2:08:08<55:02, 54.14s/it]iteration 1252: loss: 169409.500000, loss_kl: 2.617887, loss_recon: 169362.171875, loss_pred: 0.211510
iteration 1253: loss: 168238.015625, loss_kl: 2.818433, loss_recon: 168189.796875, loss_pred: 0.200262
iteration 1254: loss: 168329.375000, loss_kl: 2.865594, loss_recon: 168278.625000, loss_pred: 0.220892
iteration 1255: loss: 165276.671875, loss_kl: 2.437745, loss_recon: 165231.281250, loss_pred: 0.210107
iteration 1256: loss: 168406.515625, loss_kl: 3.030651, loss_recon: 168355.921875, loss_pred: 0.202764
iteration 1257: loss: 166785.250000, loss_kl: 2.759037, loss_recon: 166728.328125, loss_pred: 0.293345
iteration 1258: loss: 164794.296875, loss_kl: 2.964625, loss_recon: 164734.421875, loss_pred: 0.302313
iteration 1259: loss: 165024.156250, loss_kl: 2.837988, loss_recon: 164959.531250, loss_pred: 0.362442
iteration 1260: loss: 163319.015625, loss_kl: 2.817335, loss_recon: 163259.468750, loss_pred: 0.313696
 70%|████████████████████▎        | 140/200 [2:09:03<54:20, 54.34s/it]iteration 1261: loss: 163764.093750, loss_kl: 2.811275, loss_recon: 163714.593750, loss_pred: 0.213932
iteration 1262: loss: 167318.625000, loss_kl: 2.941353, loss_recon: 167253.593750, loss_pred: 0.356211
iteration 1263: loss: 165559.546875, loss_kl: 2.399914, loss_recon: 165514.281250, loss_pred: 0.212591
iteration 1264: loss: 169761.328125, loss_kl: 2.520621, loss_recon: 169719.062500, loss_pred: 0.170593
iteration 1265: loss: 166871.265625, loss_kl: 2.259260, loss_recon: 166817.265625, loss_pred: 0.314133
iteration 1266: loss: 164342.406250, loss_kl: 2.258486, loss_recon: 164276.531250, loss_pred: 0.433016
iteration 1267: loss: 165411.968750, loss_kl: 2.694462, loss_recon: 165363.921875, loss_pred: 0.211114
iteration 1268: loss: 168178.093750, loss_kl: 2.324164, loss_recon: 168132.484375, loss_pred: 0.223756
iteration 1269: loss: 168157.671875, loss_kl: 3.016083, loss_recon: 168105.406250, loss_pred: 0.221138
 70%|████████████████████▍        | 141/200 [2:09:59<53:52, 54.79s/it]iteration 1270: loss: 168136.828125, loss_kl: 2.341270, loss_recon: 168096.250000, loss_pred: 0.171769
iteration 1271: loss: 169053.125000, loss_kl: 2.789936, loss_recon: 168999.578125, loss_pred: 0.256440
iteration 1272: loss: 162596.421875, loss_kl: 3.143980, loss_recon: 162534.437500, loss_pred: 0.305528
iteration 1273: loss: 166625.187500, loss_kl: 2.656517, loss_recon: 166567.828125, loss_pred: 0.307906
iteration 1274: loss: 166068.015625, loss_kl: 3.166013, loss_recon: 165990.937500, loss_pred: 0.454147
iteration 1275: loss: 167038.750000, loss_kl: 2.643580, loss_recon: 166979.656250, loss_pred: 0.326538
iteration 1276: loss: 169648.281250, loss_kl: 2.420527, loss_recon: 169590.218750, loss_pred: 0.338588
iteration 1277: loss: 161697.671875, loss_kl: 3.404670, loss_recon: 161625.234375, loss_pred: 0.383897
iteration 1278: loss: 168562.343750, loss_kl: 2.305568, loss_recon: 168514.109375, loss_pred: 0.251703
 71%|████████████████████▌        | 142/200 [2:10:55<53:19, 55.16s/it]iteration 1279: loss: 166475.140625, loss_kl: 3.323547, loss_recon: 166415.593750, loss_pred: 0.263134
iteration 1280: loss: 168011.609375, loss_kl: 3.384574, loss_recon: 167948.921875, loss_pred: 0.288493
iteration 1281: loss: 168192.578125, loss_kl: 2.096081, loss_recon: 168145.234375, loss_pred: 0.263933
iteration 1282: loss: 166007.218750, loss_kl: 3.529087, loss_recon: 165926.406250, loss_pred: 0.455185
iteration 1283: loss: 165195.875000, loss_kl: 4.268062, loss_recon: 165125.687500, loss_pred: 0.274946
iteration 1284: loss: 165065.625000, loss_kl: 2.905145, loss_recon: 164993.578125, loss_pred: 0.430067
iteration 1285: loss: 165803.000000, loss_kl: 4.224003, loss_recon: 165733.984375, loss_pred: 0.267761
iteration 1286: loss: 167100.218750, loss_kl: 5.759881, loss_recon: 167015.406250, loss_pred: 0.272126
iteration 1287: loss: 167584.312500, loss_kl: 3.928805, loss_recon: 167519.890625, loss_pred: 0.251335
 72%|████████████████████▋        | 143/200 [2:11:49<52:00, 54.75s/it]iteration 1288: loss: 168023.187500, loss_kl: 2.971174, loss_recon: 167972.859375, loss_pred: 0.206126
iteration 1289: loss: 164852.156250, loss_kl: 4.111304, loss_recon: 164769.078125, loss_pred: 0.419734
iteration 1290: loss: 165492.203125, loss_kl: 2.665899, loss_recon: 165428.828125, loss_pred: 0.367163
iteration 1291: loss: 167183.750000, loss_kl: 3.079642, loss_recon: 167118.203125, loss_pred: 0.347446
iteration 1292: loss: 166408.546875, loss_kl: 3.245543, loss_recon: 166347.562500, loss_pred: 0.285314
iteration 1293: loss: 166871.296875, loss_kl: 2.558876, loss_recon: 166820.468750, loss_pred: 0.252365
iteration 1294: loss: 166027.250000, loss_kl: 3.288661, loss_recon: 165940.421875, loss_pred: 0.539439
iteration 1295: loss: 166935.359375, loss_kl: 2.525663, loss_recon: 166884.093750, loss_pred: 0.260162
iteration 1296: loss: 167378.453125, loss_kl: 3.166684, loss_recon: 167312.187500, loss_pred: 0.345867
 72%|████████████████████▉        | 144/200 [2:12:44<51:13, 54.88s/it]iteration 1297: loss: 168423.281250, loss_kl: 2.776753, loss_recon: 168369.546875, loss_pred: 0.259739
iteration 1298: loss: 168606.609375, loss_kl: 2.626158, loss_recon: 168549.093750, loss_pred: 0.312487
iteration 1299: loss: 165021.187500, loss_kl: 2.730994, loss_recon: 164967.343750, loss_pred: 0.265242
iteration 1300: loss: 168537.734375, loss_kl: 2.129300, loss_recon: 168485.437500, loss_pred: 0.309955
iteration 1301: loss: 162811.062500, loss_kl: 2.730664, loss_recon: 162747.250000, loss_pred: 0.365032
iteration 1302: loss: 166940.062500, loss_kl: 2.246747, loss_recon: 166895.406250, loss_pred: 0.221832
iteration 1303: loss: 165875.531250, loss_kl: 2.656921, loss_recon: 165828.015625, loss_pred: 0.209578
iteration 1304: loss: 167039.468750, loss_kl: 2.434666, loss_recon: 166994.593750, loss_pred: 0.205268
iteration 1305: loss: 165809.343750, loss_kl: 2.537622, loss_recon: 165760.031250, loss_pred: 0.239373
 72%|█████████████████████        | 145/200 [2:13:40<50:32, 55.13s/it]iteration 1306: loss: 163859.015625, loss_kl: 2.527361, loss_recon: 163813.453125, loss_pred: 0.202888
iteration 1307: loss: 164364.687500, loss_kl: 2.482056, loss_recon: 164318.250000, loss_pred: 0.216088
iteration 1308: loss: 165055.421875, loss_kl: 2.291409, loss_recon: 165002.062500, loss_pred: 0.304304
iteration 1309: loss: 167548.562500, loss_kl: 2.189444, loss_recon: 167507.515625, loss_pred: 0.191503
iteration 1310: loss: 168594.296875, loss_kl: 1.999220, loss_recon: 168555.453125, loss_pred: 0.188440
iteration 1311: loss: 165691.921875, loss_kl: 2.221666, loss_recon: 165642.484375, loss_pred: 0.272214
iteration 1312: loss: 166659.453125, loss_kl: 2.238795, loss_recon: 166616.125000, loss_pred: 0.209349
iteration 1313: loss: 170812.437500, loss_kl: 2.199105, loss_recon: 170759.218750, loss_pred: 0.312357
iteration 1314: loss: 166298.906250, loss_kl: 2.407238, loss_recon: 166247.937500, loss_pred: 0.268946
 73%|█████████████████████▏       | 146/200 [2:14:35<49:33, 55.06s/it]iteration 1315: loss: 166120.640625, loss_kl: 2.345917, loss_recon: 166075.875000, loss_pred: 0.213088
iteration 1316: loss: 165626.703125, loss_kl: 2.302741, loss_recon: 165579.343750, loss_pred: 0.243246
iteration 1317: loss: 167634.109375, loss_kl: 2.177183, loss_recon: 167589.687500, loss_pred: 0.226584
iteration 1318: loss: 166927.406250, loss_kl: 2.397851, loss_recon: 166883.796875, loss_pred: 0.196189
iteration 1319: loss: 168924.953125, loss_kl: 2.046536, loss_recon: 168887.593750, loss_pred: 0.168874
iteration 1320: loss: 165867.343750, loss_kl: 2.247518, loss_recon: 165826.343750, loss_pred: 0.185303
iteration 1321: loss: 169900.218750, loss_kl: 2.013459, loss_recon: 169860.000000, loss_pred: 0.200733
iteration 1322: loss: 163488.718750, loss_kl: 2.294390, loss_recon: 163438.859375, loss_pred: 0.269221
iteration 1323: loss: 164309.062500, loss_kl: 2.204805, loss_recon: 164258.921875, loss_pred: 0.280954
 74%|█████████████████████▎       | 147/200 [2:15:29<48:24, 54.80s/it]iteration 1324: loss: 166464.187500, loss_kl: 2.273483, loss_recon: 166414.406250, loss_pred: 0.270420
iteration 1325: loss: 165206.187500, loss_kl: 2.441265, loss_recon: 165165.937500, loss_pred: 0.158389
iteration 1326: loss: 168964.843750, loss_kl: 1.990619, loss_recon: 168927.593750, loss_pred: 0.173363
iteration 1327: loss: 163886.125000, loss_kl: 2.608636, loss_recon: 163842.890625, loss_pred: 0.171464
iteration 1328: loss: 167422.968750, loss_kl: 2.373893, loss_recon: 167376.703125, loss_pred: 0.225296
iteration 1329: loss: 166911.140625, loss_kl: 2.036751, loss_recon: 166872.984375, loss_pred: 0.177842
iteration 1330: loss: 167059.968750, loss_kl: 2.203025, loss_recon: 167021.546875, loss_pred: 0.163952
iteration 1331: loss: 167959.125000, loss_kl: 2.086831, loss_recon: 167910.156250, loss_pred: 0.280902
iteration 1332: loss: 164555.218750, loss_kl: 1.994984, loss_recon: 164505.578125, loss_pred: 0.296914
 74%|█████████████████████▍       | 148/200 [2:16:23<47:27, 54.76s/it]iteration 1333: loss: 167610.765625, loss_kl: 1.853154, loss_recon: 167574.234375, loss_pred: 0.179981
iteration 1334: loss: 167149.578125, loss_kl: 1.972684, loss_recon: 167109.281250, loss_pred: 0.205603
iteration 1335: loss: 166268.015625, loss_kl: 1.982509, loss_recon: 166223.171875, loss_pred: 0.250194
iteration 1336: loss: 168675.125000, loss_kl: 2.207765, loss_recon: 168632.671875, loss_pred: 0.203767
iteration 1337: loss: 163884.265625, loss_kl: 2.391942, loss_recon: 163837.546875, loss_pred: 0.227943
iteration 1338: loss: 165456.703125, loss_kl: 2.099124, loss_recon: 165410.062500, loss_pred: 0.256631
iteration 1339: loss: 167404.203125, loss_kl: 2.053952, loss_recon: 167350.203125, loss_pred: 0.334595
iteration 1340: loss: 167601.812500, loss_kl: 2.104333, loss_recon: 167563.468750, loss_pred: 0.173038
iteration 1341: loss: 164545.859375, loss_kl: 2.184529, loss_recon: 164499.421875, loss_pred: 0.245923
 74%|█████████████████████▌       | 149/200 [2:17:18<46:25, 54.61s/it]iteration 1342: loss: 165953.437500, loss_kl: 2.393603, loss_recon: 165892.828125, loss_pred: 0.366729
iteration 1343: loss: 167836.765625, loss_kl: 2.437969, loss_recon: 167792.437500, loss_pred: 0.199538
iteration 1344: loss: 165939.187500, loss_kl: 1.815523, loss_recon: 165890.062500, loss_pred: 0.309620
iteration 1345: loss: 166417.546875, loss_kl: 2.647763, loss_recon: 166366.375000, loss_pred: 0.246902
iteration 1346: loss: 167745.328125, loss_kl: 2.498953, loss_recon: 167696.453125, loss_pred: 0.238926
iteration 1347: loss: 164845.187500, loss_kl: 2.217578, loss_recon: 164805.156250, loss_pred: 0.178659
iteration 1348: loss: 167718.328125, loss_kl: 2.835275, loss_recon: 167665.703125, loss_pred: 0.242641
iteration 1349: loss: 167841.593750, loss_kl: 1.948596, loss_recon: 167791.906250, loss_pred: 0.302005
iteration 1350: loss: 164364.031250, loss_kl: 2.933779, loss_recon: 164303.390625, loss_pred: 0.313023
 75%|█████████████████████▊       | 150/200 [2:18:12<45:30, 54.61s/it]iteration 1351: loss: 167323.921875, loss_kl: 3.186850, loss_recon: 167301.812500, loss_pred: 0.217975
iteration 1352: loss: 163493.906250, loss_kl: 3.939363, loss_recon: 163466.390625, loss_pred: 0.271266
iteration 1353: loss: 167736.406250, loss_kl: 4.661396, loss_recon: 167713.312500, loss_pred: 0.226179
iteration 1354: loss: 167206.015625, loss_kl: 4.565121, loss_recon: 167169.578125, loss_pred: 0.359889
iteration 1355: loss: 166331.531250, loss_kl: 3.635763, loss_recon: 166304.046875, loss_pred: 0.271197
iteration 1356: loss: 167574.031250, loss_kl: 2.726461, loss_recon: 167549.593750, loss_pred: 0.241705
iteration 1357: loss: 166984.468750, loss_kl: 2.558682, loss_recon: 166957.187500, loss_pred: 0.270295
iteration 1358: loss: 169069.625000, loss_kl: 3.112648, loss_recon: 169039.078125, loss_pred: 0.302332
iteration 1359: loss: 162677.093750, loss_kl: 4.105498, loss_recon: 162639.031250, loss_pred: 0.376526
 76%|█████████████████████▉       | 151/200 [2:19:08<44:58, 55.08s/it]iteration 1360: loss: 170448.640625, loss_kl: 4.073905, loss_recon: 170381.515625, loss_pred: 0.667182
iteration 1361: loss: 165408.500000, loss_kl: 4.503423, loss_recon: 165382.593750, loss_pred: 0.254526
iteration 1362: loss: 166718.781250, loss_kl: 5.059806, loss_recon: 166692.375000, loss_pred: 0.259034
iteration 1363: loss: 167107.281250, loss_kl: 5.741603, loss_recon: 167070.703125, loss_pred: 0.360050
iteration 1364: loss: 166804.859375, loss_kl: 6.173377, loss_recon: 166779.671875, loss_pred: 0.245662
iteration 1365: loss: 164618.671875, loss_kl: 6.616138, loss_recon: 164590.406250, loss_pred: 0.276073
iteration 1366: loss: 168594.828125, loss_kl: 5.877283, loss_recon: 168565.687500, loss_pred: 0.285529
iteration 1367: loss: 165498.812500, loss_kl: 5.531612, loss_recon: 165476.031250, loss_pred: 0.222292
iteration 1368: loss: 163217.109375, loss_kl: 5.046250, loss_recon: 163176.515625, loss_pred: 0.400991
 76%|██████████████████████       | 152/200 [2:20:04<44:17, 55.37s/it]iteration 1369: loss: 168164.171875, loss_kl: 4.458416, loss_recon: 168134.937500, loss_pred: 0.287841
iteration 1370: loss: 165865.093750, loss_kl: 4.735240, loss_recon: 165822.125000, loss_pred: 0.424926
iteration 1371: loss: 164649.921875, loss_kl: 6.469571, loss_recon: 164602.468750, loss_pred: 0.468148
iteration 1372: loss: 167780.500000, loss_kl: 8.753232, loss_recon: 167761.140625, loss_pred: 0.184793
iteration 1373: loss: 168234.406250, loss_kl: 10.430772, loss_recon: 168200.968750, loss_pred: 0.323908
iteration 1374: loss: 165748.515625, loss_kl: 11.141993, loss_recon: 165695.656250, loss_pred: 0.517552
iteration 1375: loss: 163643.312500, loss_kl: 9.968565, loss_recon: 163615.921875, loss_pred: 0.263873
iteration 1376: loss: 164706.484375, loss_kl: 8.622998, loss_recon: 164673.203125, loss_pred: 0.324222
iteration 1377: loss: 169671.484375, loss_kl: 7.472696, loss_recon: 169632.687500, loss_pred: 0.380395
 76%|██████████████████████▏      | 153/200 [2:21:00<43:29, 55.52s/it]iteration 1378: loss: 162903.593750, loss_kl: 7.922247, loss_recon: 162876.421875, loss_pred: 0.263752
iteration 1379: loss: 165551.203125, loss_kl: 8.402546, loss_recon: 165503.593750, loss_pred: 0.467625
iteration 1380: loss: 167296.468750, loss_kl: 8.850447, loss_recon: 167266.500000, loss_pred: 0.290725
iteration 1381: loss: 164219.703125, loss_kl: 8.849240, loss_recon: 164196.265625, loss_pred: 0.225430
iteration 1382: loss: 165782.984375, loss_kl: 8.261956, loss_recon: 165738.171875, loss_pred: 0.439813
iteration 1383: loss: 170335.343750, loss_kl: 7.775439, loss_recon: 170308.484375, loss_pred: 0.260835
iteration 1384: loss: 164502.750000, loss_kl: 8.191640, loss_recon: 164473.031250, loss_pred: 0.289051
iteration 1385: loss: 168361.859375, loss_kl: 8.470442, loss_recon: 168335.828125, loss_pred: 0.251946
iteration 1386: loss: 168891.906250, loss_kl: 8.768046, loss_recon: 168864.765625, loss_pred: 0.262725
 77%|██████████████████████▎      | 154/200 [2:21:55<42:22, 55.27s/it]iteration 1387: loss: 165715.234375, loss_kl: 9.061904, loss_recon: 165683.828125, loss_pred: 0.305023
iteration 1388: loss: 164315.312500, loss_kl: 9.061988, loss_recon: 164294.796875, loss_pred: 0.196027
iteration 1389: loss: 167529.468750, loss_kl: 8.324098, loss_recon: 167502.875000, loss_pred: 0.257685
iteration 1390: loss: 167951.343750, loss_kl: 8.556808, loss_recon: 167932.265625, loss_pred: 0.182129
iteration 1391: loss: 165842.359375, loss_kl: 8.986909, loss_recon: 165819.593750, loss_pred: 0.218628
iteration 1392: loss: 165652.859375, loss_kl: 9.025337, loss_recon: 165632.953125, loss_pred: 0.189963
iteration 1393: loss: 167689.843750, loss_kl: 9.101048, loss_recon: 167664.546875, loss_pred: 0.243971
iteration 1394: loss: 169686.281250, loss_kl: 9.188056, loss_recon: 169651.781250, loss_pred: 0.335730
iteration 1395: loss: 163516.906250, loss_kl: 9.693100, loss_recon: 163489.109375, loss_pred: 0.268266
 78%|██████████████████████▍      | 155/200 [2:22:50<41:16, 55.03s/it]iteration 1396: loss: 166671.015625, loss_kl: 8.907248, loss_recon: 166644.500000, loss_pred: 0.256314
iteration 1397: loss: 168440.000000, loss_kl: 8.309402, loss_recon: 168409.250000, loss_pred: 0.299166
iteration 1398: loss: 166043.078125, loss_kl: 8.558672, loss_recon: 166014.062500, loss_pred: 0.281529
iteration 1399: loss: 163492.531250, loss_kl: 9.759257, loss_recon: 163469.578125, loss_pred: 0.219884
iteration 1400: loss: 164226.046875, loss_kl: 10.643293, loss_recon: 164205.390625, loss_pred: 0.195986
iteration 1401: loss: 168006.515625, loss_kl: 11.130802, loss_recon: 167988.171875, loss_pred: 0.172283
iteration 1402: loss: 169199.359375, loss_kl: 11.706879, loss_recon: 169173.375000, loss_pred: 0.248075
iteration 1403: loss: 164969.609375, loss_kl: 11.464064, loss_recon: 164921.750000, loss_pred: 0.467248
iteration 1404: loss: 166877.937500, loss_kl: 10.386692, loss_recon: 166853.703125, loss_pred: 0.232070
 78%|██████████████████████▌      | 156/200 [2:23:45<40:26, 55.14s/it]iteration 1405: loss: 164770.843750, loss_kl: 9.180403, loss_recon: 164733.750000, loss_pred: 0.361749
iteration 1406: loss: 162954.828125, loss_kl: 9.261081, loss_recon: 162911.609375, loss_pred: 0.423007
iteration 1407: loss: 164962.484375, loss_kl: 10.577691, loss_recon: 164938.265625, loss_pred: 0.231560
iteration 1408: loss: 169491.562500, loss_kl: 11.546016, loss_recon: 169471.859375, loss_pred: 0.185477
iteration 1409: loss: 167763.843750, loss_kl: 12.118841, loss_recon: 167734.062500, loss_pred: 0.285593
iteration 1410: loss: 168763.750000, loss_kl: 11.644910, loss_recon: 168742.796875, loss_pred: 0.197763
iteration 1411: loss: 165631.796875, loss_kl: 11.377801, loss_recon: 165603.203125, loss_pred: 0.274496
iteration 1412: loss: 166672.406250, loss_kl: 11.028280, loss_recon: 166642.640625, loss_pred: 0.286530
iteration 1413: loss: 167025.062500, loss_kl: 10.761552, loss_recon: 166997.484375, loss_pred: 0.265002
 78%|██████████████████████▊      | 157/200 [2:24:39<39:21, 54.92s/it]iteration 1414: loss: 168622.687500, loss_kl: 10.271939, loss_recon: 168598.890625, loss_pred: 0.227720
iteration 1415: loss: 164289.265625, loss_kl: 10.002330, loss_recon: 164262.140625, loss_pred: 0.261192
iteration 1416: loss: 165318.968750, loss_kl: 10.181803, loss_recon: 165294.515625, loss_pred: 0.234451
iteration 1417: loss: 165533.265625, loss_kl: 9.921176, loss_recon: 165506.281250, loss_pred: 0.259928
iteration 1418: loss: 168020.906250, loss_kl: 9.883686, loss_recon: 167991.828125, loss_pred: 0.280866
iteration 1419: loss: 167937.656250, loss_kl: 9.882413, loss_recon: 167910.625000, loss_pred: 0.260448
iteration 1420: loss: 163830.218750, loss_kl: 10.396921, loss_recon: 163773.281250, loss_pred: 0.558869
iteration 1421: loss: 167170.140625, loss_kl: 9.720264, loss_recon: 167144.828125, loss_pred: 0.243366
iteration 1422: loss: 167127.640625, loss_kl: 9.603259, loss_recon: 167097.765625, loss_pred: 0.289145
 79%|██████████████████████▉      | 158/200 [2:25:35<38:31, 55.03s/it]iteration 1423: loss: 165703.640625, loss_kl: 9.410274, loss_recon: 165675.765625, loss_pred: 0.269383
iteration 1424: loss: 165675.203125, loss_kl: 9.370597, loss_recon: 165630.140625, loss_pred: 0.441178
iteration 1425: loss: 165692.453125, loss_kl: 10.056593, loss_recon: 165664.968750, loss_pred: 0.264916
iteration 1426: loss: 164047.734375, loss_kl: 11.032999, loss_recon: 164017.281250, loss_pred: 0.293475
iteration 1427: loss: 167048.625000, loss_kl: 11.227596, loss_recon: 167024.859375, loss_pred: 0.226370
iteration 1428: loss: 167143.109375, loss_kl: 11.858445, loss_recon: 167120.031250, loss_pred: 0.218909
iteration 1429: loss: 167478.218750, loss_kl: 11.683777, loss_recon: 167456.859375, loss_pred: 0.201918
iteration 1430: loss: 168044.296875, loss_kl: 11.206311, loss_recon: 168014.031250, loss_pred: 0.291472
iteration 1431: loss: 166759.250000, loss_kl: 11.612910, loss_recon: 166733.046875, loss_pred: 0.250403
 80%|███████████████████████      | 159/200 [2:26:30<37:45, 55.25s/it]iteration 1432: loss: 166623.156250, loss_kl: 12.190546, loss_recon: 166589.796875, loss_pred: 0.321452
iteration 1433: loss: 165907.828125, loss_kl: 12.028085, loss_recon: 165865.640625, loss_pred: 0.409866
iteration 1434: loss: 166999.656250, loss_kl: 11.637599, loss_recon: 166978.312500, loss_pred: 0.201841
iteration 1435: loss: 164093.890625, loss_kl: 11.190171, loss_recon: 164064.578125, loss_pred: 0.281888
iteration 1436: loss: 166331.609375, loss_kl: 11.460971, loss_recon: 166303.968750, loss_pred: 0.264962
iteration 1437: loss: 166007.500000, loss_kl: 11.743234, loss_recon: 165989.156250, loss_pred: 0.171755
iteration 1438: loss: 167670.890625, loss_kl: 12.158557, loss_recon: 167648.546875, loss_pred: 0.211296
iteration 1439: loss: 168682.421875, loss_kl: 11.569740, loss_recon: 168654.406250, loss_pred: 0.268631
iteration 1440: loss: 165395.703125, loss_kl: 11.058207, loss_recon: 165371.203125, loss_pred: 0.233967
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_159.pth
 80%|███████████████████████▏     | 160/200 [2:27:27<37:08, 55.72s/it]iteration 1441: loss: 166268.953125, loss_kl: 10.283634, loss_recon: 166237.234375, loss_pred: 0.306914
iteration 1442: loss: 167109.421875, loss_kl: 10.309471, loss_recon: 167083.234375, loss_pred: 0.251498
iteration 1443: loss: 165590.343750, loss_kl: 10.789795, loss_recon: 165572.062500, loss_pred: 0.171996
iteration 1444: loss: 165414.468750, loss_kl: 11.511902, loss_recon: 165389.109375, loss_pred: 0.242003
iteration 1445: loss: 162000.140625, loss_kl: 11.773654, loss_recon: 161966.921875, loss_pred: 0.320431
iteration 1446: loss: 171028.750000, loss_kl: 10.159380, loss_recon: 170993.656250, loss_pred: 0.340716
iteration 1447: loss: 167366.859375, loss_kl: 9.637373, loss_recon: 167309.171875, loss_pred: 0.567239
iteration 1448: loss: 168438.375000, loss_kl: 10.372580, loss_recon: 168396.265625, loss_pred: 0.410737
iteration 1449: loss: 164238.406250, loss_kl: 12.038665, loss_recon: 164188.796875, loss_pred: 0.484036
 80%|███████████████████████▎     | 161/200 [2:28:23<36:09, 55.64s/it]iteration 1450: loss: 162501.812500, loss_kl: 12.721359, loss_recon: 162449.750000, loss_pred: 0.507977
iteration 1451: loss: 170143.859375, loss_kl: 12.192823, loss_recon: 170125.078125, loss_pred: 0.175630
iteration 1452: loss: 167147.515625, loss_kl: 12.313795, loss_recon: 167124.343750, loss_pred: 0.219424
iteration 1453: loss: 167241.671875, loss_kl: 12.639289, loss_recon: 167209.515625, loss_pred: 0.308878
iteration 1454: loss: 168297.484375, loss_kl: 12.691933, loss_recon: 168274.656250, loss_pred: 0.215616
iteration 1455: loss: 166079.109375, loss_kl: 12.931635, loss_recon: 166054.375000, loss_pred: 0.234430
iteration 1456: loss: 164364.875000, loss_kl: 13.486826, loss_recon: 164328.546875, loss_pred: 0.349777
iteration 1457: loss: 167188.203125, loss_kl: 12.794277, loss_recon: 167163.265625, loss_pred: 0.236611
iteration 1458: loss: 164358.093750, loss_kl: 12.277109, loss_recon: 164330.640625, loss_pred: 0.262176
 81%|███████████████████████▍     | 162/200 [2:29:19<35:17, 55.72s/it]iteration 1459: loss: 166808.671875, loss_kl: 11.987662, loss_recon: 166789.593750, loss_pred: 0.178747
iteration 1460: loss: 166793.859375, loss_kl: 12.300288, loss_recon: 166773.375000, loss_pred: 0.192431
iteration 1461: loss: 166937.171875, loss_kl: 12.347739, loss_recon: 166919.250000, loss_pred: 0.166835
iteration 1462: loss: 166745.421875, loss_kl: 12.307197, loss_recon: 166718.140625, loss_pred: 0.260494
iteration 1463: loss: 166361.640625, loss_kl: 12.119838, loss_recon: 166330.968750, loss_pred: 0.294535
iteration 1464: loss: 167076.859375, loss_kl: 11.754420, loss_recon: 167051.093750, loss_pred: 0.245899
iteration 1465: loss: 167792.718750, loss_kl: 11.932461, loss_recon: 167763.843750, loss_pred: 0.276872
iteration 1466: loss: 163435.718750, loss_kl: 12.897610, loss_recon: 163403.625000, loss_pred: 0.307991
iteration 1467: loss: 165738.625000, loss_kl: 12.783270, loss_recon: 165708.171875, loss_pred: 0.291752
 82%|███████████████████████▋     | 163/200 [2:30:13<34:06, 55.30s/it]iteration 1468: loss: 167064.265625, loss_kl: 12.876131, loss_recon: 167016.046875, loss_pred: 0.418321
iteration 1469: loss: 165608.015625, loss_kl: 14.296117, loss_recon: 165581.921875, loss_pred: 0.190038
iteration 1470: loss: 164353.546875, loss_kl: 15.710495, loss_recon: 164313.578125, loss_pred: 0.321718
iteration 1471: loss: 166856.265625, loss_kl: 15.083950, loss_recon: 166823.859375, loss_pred: 0.249252
iteration 1472: loss: 167240.546875, loss_kl: 13.278896, loss_recon: 167216.437500, loss_pred: 0.175155
iteration 1473: loss: 166918.843750, loss_kl: 11.938808, loss_recon: 166877.484375, loss_pred: 0.354371
iteration 1474: loss: 167786.046875, loss_kl: 11.094732, loss_recon: 167742.187500, loss_pred: 0.383562
iteration 1475: loss: 165378.859375, loss_kl: 11.967866, loss_recon: 165349.046875, loss_pred: 0.238783
iteration 1476: loss: 166526.375000, loss_kl: 12.408572, loss_recon: 166485.515625, loss_pred: 0.347062
 82%|███████████████████████▊     | 164/200 [2:31:08<33:08, 55.24s/it]iteration 1477: loss: 164385.875000, loss_kl: 12.772259, loss_recon: 164324.984375, loss_pred: 0.494950
iteration 1478: loss: 168342.125000, loss_kl: 10.565647, loss_recon: 168310.046875, loss_pred: 0.226504
iteration 1479: loss: 166641.625000, loss_kl: 8.043249, loss_recon: 166611.875000, loss_pred: 0.225817
iteration 1480: loss: 167085.265625, loss_kl: 5.976942, loss_recon: 167046.109375, loss_pred: 0.338219
iteration 1481: loss: 167779.828125, loss_kl: 5.653227, loss_recon: 167741.406250, loss_pred: 0.333813
iteration 1482: loss: 166106.984375, loss_kl: 5.758192, loss_recon: 166074.828125, loss_pred: 0.270104
iteration 1483: loss: 165547.796875, loss_kl: 5.757251, loss_recon: 165523.953125, loss_pred: 0.186954
iteration 1484: loss: 165964.078125, loss_kl: 5.402736, loss_recon: 165927.031250, loss_pred: 0.322359
iteration 1485: loss: 165771.031250, loss_kl: 5.045794, loss_recon: 165740.718750, loss_pred: 0.258108
 82%|███████████████████████▉     | 165/200 [2:32:03<32:06, 55.04s/it]iteration 1486: loss: 167315.015625, loss_kl: 4.099256, loss_recon: 167285.875000, loss_pred: 0.238553
iteration 1487: loss: 169359.578125, loss_kl: 3.477719, loss_recon: 169327.953125, loss_pred: 0.271404
iteration 1488: loss: 168174.828125, loss_kl: 3.606833, loss_recon: 168148.578125, loss_pred: 0.216022
iteration 1489: loss: 164595.593750, loss_kl: 3.765747, loss_recon: 164565.593750, loss_pred: 0.251538
iteration 1490: loss: 162993.437500, loss_kl: 3.572890, loss_recon: 162952.203125, loss_pred: 0.366324
iteration 1491: loss: 169168.890625, loss_kl: 3.478516, loss_recon: 169144.796875, loss_pred: 0.196112
iteration 1492: loss: 163377.781250, loss_kl: 4.085536, loss_recon: 163334.828125, loss_pred: 0.376816
iteration 1493: loss: 163500.156250, loss_kl: 3.839443, loss_recon: 163430.515625, loss_pred: 0.647089
iteration 1494: loss: 169201.906250, loss_kl: 4.333636, loss_recon: 169171.593750, loss_pred: 0.247296
 83%|████████████████████████     | 166/200 [2:32:57<31:04, 54.83s/it]iteration 1495: loss: 166102.875000, loss_kl: 6.134887, loss_recon: 166065.640625, loss_pred: 0.269132
iteration 1496: loss: 166128.046875, loss_kl: 6.376551, loss_recon: 166064.187500, loss_pred: 0.531268
iteration 1497: loss: 166928.093750, loss_kl: 5.484118, loss_recon: 166889.484375, loss_pred: 0.293692
iteration 1498: loss: 167039.250000, loss_kl: 4.342911, loss_recon: 167009.406250, loss_pred: 0.225377
iteration 1499: loss: 168447.890625, loss_kl: 3.394955, loss_recon: 168417.171875, loss_pred: 0.250003
iteration 1500: loss: 164304.593750, loss_kl: 3.544405, loss_recon: 164270.015625, loss_pred: 0.286021
iteration 1501: loss: 165852.671875, loss_kl: 3.415749, loss_recon: 165819.812500, loss_pred: 0.271020
iteration 1502: loss: 167485.796875, loss_kl: 3.783826, loss_recon: 167453.265625, loss_pred: 0.261608
iteration 1503: loss: 165281.156250, loss_kl: 3.825738, loss_recon: 165249.937500, loss_pred: 0.247844
 84%|████████████████████████▏    | 167/200 [2:33:51<29:59, 54.54s/it]iteration 1504: loss: 162139.187500, loss_kl: 3.720017, loss_recon: 162103.406250, loss_pred: 0.280540
iteration 1505: loss: 168417.890625, loss_kl: 3.163433, loss_recon: 168389.062500, loss_pred: 0.222562
iteration 1506: loss: 170212.984375, loss_kl: 3.475734, loss_recon: 170176.718750, loss_pred: 0.290363
iteration 1507: loss: 165006.343750, loss_kl: 3.385052, loss_recon: 164973.859375, loss_pred: 0.254392
iteration 1508: loss: 164968.031250, loss_kl: 3.267404, loss_recon: 164929.968750, loss_pred: 0.312686
iteration 1509: loss: 166143.734375, loss_kl: 3.263475, loss_recon: 166116.906250, loss_pred: 0.200391
iteration 1510: loss: 165142.375000, loss_kl: 3.424195, loss_recon: 165098.140625, loss_pred: 0.371054
iteration 1511: loss: 167961.156250, loss_kl: 3.035250, loss_recon: 167930.406250, loss_pred: 0.244441
iteration 1512: loss: 167421.734375, loss_kl: 3.386708, loss_recon: 167375.921875, loss_pred: 0.387701
 84%|████████████████████████▎    | 168/200 [2:34:46<29:11, 54.72s/it]iteration 1513: loss: 165422.765625, loss_kl: 2.849156, loss_recon: 165390.453125, loss_pred: 0.252626
iteration 1514: loss: 164256.453125, loss_kl: 3.508214, loss_recon: 164226.937500, loss_pred: 0.208295
iteration 1515: loss: 165522.203125, loss_kl: 4.192785, loss_recon: 165495.265625, loss_pred: 0.165663
iteration 1516: loss: 167132.046875, loss_kl: 4.235011, loss_recon: 167084.453125, loss_pred: 0.371017
iteration 1517: loss: 166634.296875, loss_kl: 3.065591, loss_recon: 166597.484375, loss_pred: 0.292191
iteration 1518: loss: 166565.625000, loss_kl: 3.235205, loss_recon: 166534.265625, loss_pred: 0.233439
iteration 1519: loss: 166571.953125, loss_kl: 5.252695, loss_recon: 166535.312500, loss_pred: 0.236396
iteration 1520: loss: 167171.281250, loss_kl: 5.342300, loss_recon: 167126.546875, loss_pred: 0.314950
iteration 1521: loss: 167815.500000, loss_kl: 3.800646, loss_recon: 167779.718750, loss_pred: 0.263713
 84%|████████████████████████▌    | 169/200 [2:35:39<28:04, 54.33s/it]iteration 1522: loss: 166226.859375, loss_kl: 2.932017, loss_recon: 166191.828125, loss_pred: 0.266134
iteration 1523: loss: 169078.140625, loss_kl: 3.597154, loss_recon: 169043.515625, loss_pred: 0.242996
iteration 1524: loss: 166470.281250, loss_kl: 3.606061, loss_recon: 166435.171875, loss_pred: 0.247564
iteration 1525: loss: 166592.812500, loss_kl: 2.819461, loss_recon: 166562.265625, loss_pred: 0.224536
iteration 1526: loss: 164040.781250, loss_kl: 2.893055, loss_recon: 164011.000000, loss_pred: 0.214756
iteration 1527: loss: 162231.109375, loss_kl: 3.233542, loss_recon: 162192.375000, loss_pred: 0.294453
iteration 1528: loss: 169004.078125, loss_kl: 2.520074, loss_recon: 168977.765625, loss_pred: 0.190827
iteration 1529: loss: 167954.734375, loss_kl: 2.760676, loss_recon: 167927.750000, loss_pred: 0.190667
iteration 1530: loss: 165680.218750, loss_kl: 3.436258, loss_recon: 165646.625000, loss_pred: 0.237165
 85%|████████████████████████▋    | 170/200 [2:36:34<27:15, 54.53s/it]iteration 1531: loss: 162061.218750, loss_kl: 3.405939, loss_recon: 162033.000000, loss_pred: 0.170972
iteration 1532: loss: 165128.312500, loss_kl: 2.861046, loss_recon: 165099.687500, loss_pred: 0.192818
iteration 1533: loss: 167702.781250, loss_kl: 2.977856, loss_recon: 167665.250000, loss_pred: 0.277920
iteration 1534: loss: 167957.765625, loss_kl: 2.671906, loss_recon: 167924.515625, loss_pred: 0.245110
iteration 1535: loss: 166024.562500, loss_kl: 2.667187, loss_recon: 165988.734375, loss_pred: 0.271047
iteration 1536: loss: 170605.109375, loss_kl: 2.652696, loss_recon: 170575.625000, loss_pred: 0.208057
iteration 1537: loss: 166441.734375, loss_kl: 2.541760, loss_recon: 166411.921875, loss_pred: 0.214941
iteration 1538: loss: 164364.906250, loss_kl: 2.962900, loss_recon: 164338.593750, loss_pred: 0.166255
iteration 1539: loss: 166989.890625, loss_kl: 3.096174, loss_recon: 166953.281250, loss_pred: 0.264817
 86%|████████████████████████▊    | 171/200 [2:37:29<26:22, 54.56s/it]iteration 1540: loss: 167263.281250, loss_kl: 2.919934, loss_recon: 167234.046875, loss_pred: 0.185371
iteration 1541: loss: 163283.859375, loss_kl: 2.783643, loss_recon: 163247.265625, loss_pred: 0.263972
iteration 1542: loss: 165544.562500, loss_kl: 2.810731, loss_recon: 165515.578125, loss_pred: 0.186903
iteration 1543: loss: 170388.390625, loss_kl: 2.479275, loss_recon: 170351.578125, loss_pred: 0.277411
iteration 1544: loss: 164561.218750, loss_kl: 2.633643, loss_recon: 164528.890625, loss_pred: 0.226716
iteration 1545: loss: 167597.890625, loss_kl: 2.461491, loss_recon: 167566.093750, loss_pred: 0.227797
iteration 1546: loss: 166908.781250, loss_kl: 2.484776, loss_recon: 166876.171875, loss_pred: 0.234993
iteration 1547: loss: 163905.109375, loss_kl: 2.492268, loss_recon: 163871.765625, loss_pred: 0.242209
iteration 1548: loss: 167750.015625, loss_kl: 2.548379, loss_recon: 167709.140625, loss_pred: 0.315354
 86%|████████████████████████▉    | 172/200 [2:38:25<25:37, 54.91s/it]iteration 1549: loss: 166004.281250, loss_kl: 2.806522, loss_recon: 165969.890625, loss_pred: 0.229955
iteration 1550: loss: 166703.406250, loss_kl: 2.858959, loss_recon: 166652.359375, loss_pred: 0.394349
iteration 1551: loss: 165225.281250, loss_kl: 2.689381, loss_recon: 165165.968750, loss_pred: 0.483981
iteration 1552: loss: 167047.125000, loss_kl: 3.239646, loss_recon: 167009.515625, loss_pred: 0.244531
iteration 1553: loss: 165280.437500, loss_kl: 3.759293, loss_recon: 165241.000000, loss_pred: 0.241684
iteration 1554: loss: 166636.312500, loss_kl: 3.349843, loss_recon: 166589.234375, loss_pred: 0.334820
iteration 1555: loss: 166195.156250, loss_kl: 3.021894, loss_recon: 166162.703125, loss_pred: 0.201845
iteration 1556: loss: 163887.906250, loss_kl: 3.470265, loss_recon: 163849.625000, loss_pred: 0.241909
iteration 1557: loss: 170402.312500, loss_kl: 3.029643, loss_recon: 170368.234375, loss_pred: 0.217864
 86%|█████████████████████████    | 173/200 [2:39:20<24:46, 55.05s/it]iteration 1558: loss: 165938.218750, loss_kl: 2.768453, loss_recon: 165902.218750, loss_pred: 0.236562
iteration 1559: loss: 165993.500000, loss_kl: 2.672522, loss_recon: 165961.546875, loss_pred: 0.200506
iteration 1560: loss: 165627.171875, loss_kl: 2.830880, loss_recon: 165595.484375, loss_pred: 0.190758
iteration 1561: loss: 166874.312500, loss_kl: 2.371892, loss_recon: 166839.968750, loss_pred: 0.237738
iteration 1562: loss: 163475.625000, loss_kl: 2.669273, loss_recon: 163444.937500, loss_pred: 0.187896
iteration 1563: loss: 164651.531250, loss_kl: 2.724955, loss_recon: 164616.578125, loss_pred: 0.228189
iteration 1564: loss: 170647.937500, loss_kl: 2.300584, loss_recon: 170618.656250, loss_pred: 0.190370
iteration 1565: loss: 166214.234375, loss_kl: 2.683370, loss_recon: 166175.031250, loss_pred: 0.272533
iteration 1566: loss: 167553.265625, loss_kl: 3.224753, loss_recon: 167515.937500, loss_pred: 0.229480
 87%|█████████████████████████▏   | 174/200 [2:40:16<23:56, 55.25s/it]iteration 1567: loss: 166481.921875, loss_kl: 3.530758, loss_recon: 166440.062500, loss_pred: 0.247352
iteration 1568: loss: 165168.015625, loss_kl: 2.734428, loss_recon: 165116.406250, loss_pred: 0.383449
iteration 1569: loss: 169542.437500, loss_kl: 2.829451, loss_recon: 169504.140625, loss_pred: 0.245652
iteration 1570: loss: 165901.531250, loss_kl: 4.003474, loss_recon: 165860.421875, loss_pred: 0.216870
iteration 1571: loss: 169425.625000, loss_kl: 3.650197, loss_recon: 169373.484375, loss_pred: 0.344304
iteration 1572: loss: 166346.500000, loss_kl: 2.498206, loss_recon: 166311.453125, loss_pred: 0.229260
iteration 1573: loss: 163450.765625, loss_kl: 3.366078, loss_recon: 163404.375000, loss_pred: 0.300675
iteration 1574: loss: 166088.218750, loss_kl: 3.895415, loss_recon: 166047.546875, loss_pred: 0.217604
iteration 1575: loss: 164873.531250, loss_kl: 3.328123, loss_recon: 164839.031250, loss_pred: 0.183585
 88%|█████████████████████████▍   | 175/200 [2:41:11<23:03, 55.33s/it]iteration 1576: loss: 162348.953125, loss_kl: 2.870052, loss_recon: 162310.234375, loss_pred: 0.236542
iteration 1577: loss: 162011.328125, loss_kl: 3.363108, loss_recon: 161971.578125, loss_pred: 0.220943
iteration 1578: loss: 168114.875000, loss_kl: 2.994002, loss_recon: 168073.671875, loss_pred: 0.254842
iteration 1579: loss: 168704.875000, loss_kl: 2.656224, loss_recon: 168665.578125, loss_pred: 0.253566
iteration 1580: loss: 167879.031250, loss_kl: 2.423715, loss_recon: 167840.515625, loss_pred: 0.258034
iteration 1581: loss: 166741.359375, loss_kl: 2.550974, loss_recon: 166706.093750, loss_pred: 0.218694
iteration 1582: loss: 165483.375000, loss_kl: 2.449684, loss_recon: 165438.296875, loss_pred: 0.322206
iteration 1583: loss: 169561.031250, loss_kl: 2.285806, loss_recon: 169524.234375, loss_pred: 0.247932
iteration 1584: loss: 166192.546875, loss_kl: 2.977850, loss_recon: 166151.781250, loss_pred: 0.251459
 88%|█████████████████████████▌   | 176/200 [2:42:07<22:08, 55.36s/it]iteration 1585: loss: 167924.843750, loss_kl: 2.443953, loss_recon: 167892.281250, loss_pred: 0.187635
iteration 1586: loss: 166750.312500, loss_kl: 2.350309, loss_recon: 166710.296875, loss_pred: 0.267562
iteration 1587: loss: 163658.765625, loss_kl: 2.989323, loss_recon: 163613.906250, loss_pred: 0.279769
iteration 1588: loss: 165307.000000, loss_kl: 3.138893, loss_recon: 165260.578125, loss_pred: 0.287088
iteration 1589: loss: 168865.781250, loss_kl: 3.002527, loss_recon: 168817.296875, loss_pred: 0.315254
iteration 1590: loss: 167144.437500, loss_kl: 3.403588, loss_recon: 167086.984375, loss_pred: 0.382496
iteration 1591: loss: 164706.000000, loss_kl: 3.240121, loss_recon: 164662.328125, loss_pred: 0.253855
iteration 1592: loss: 167560.343750, loss_kl: 3.535315, loss_recon: 167519.437500, loss_pred: 0.209547
iteration 1593: loss: 165280.484375, loss_kl: 3.521731, loss_recon: 165234.625000, loss_pred: 0.259811
 88%|█████████████████████████▋   | 177/200 [2:43:01<21:06, 55.05s/it]iteration 1594: loss: 166318.609375, loss_kl: 3.223426, loss_recon: 166276.968750, loss_pred: 0.221686
iteration 1595: loss: 164513.343750, loss_kl: 2.692311, loss_recon: 164473.265625, loss_pred: 0.238133
iteration 1596: loss: 166873.968750, loss_kl: 3.286379, loss_recon: 166834.515625, loss_pred: 0.196153
iteration 1597: loss: 168105.906250, loss_kl: 2.935699, loss_recon: 168067.234375, loss_pred: 0.209384
iteration 1598: loss: 167311.984375, loss_kl: 2.229656, loss_recon: 167276.656250, loss_pred: 0.218541
iteration 1599: loss: 166397.046875, loss_kl: 2.592225, loss_recon: 166361.750000, loss_pred: 0.196377
iteration 1600: loss: 166826.921875, loss_kl: 2.357445, loss_recon: 166785.718750, loss_pred: 0.269644
iteration 1601: loss: 164842.968750, loss_kl: 2.304395, loss_recon: 164805.750000, loss_pred: 0.232987
iteration 1602: loss: 165254.140625, loss_kl: 2.482950, loss_recon: 165219.515625, loss_pred: 0.196219
 89%|█████████████████████████▊   | 178/200 [2:43:55<20:01, 54.61s/it]iteration 1603: loss: 166751.390625, loss_kl: 2.174562, loss_recon: 166718.718750, loss_pred: 0.186751
iteration 1604: loss: 166177.359375, loss_kl: 2.396361, loss_recon: 166138.937500, loss_pred: 0.229953
iteration 1605: loss: 166876.843750, loss_kl: 2.137285, loss_recon: 166843.765625, loss_pred: 0.193268
iteration 1606: loss: 163893.734375, loss_kl: 2.536201, loss_recon: 163829.531250, loss_pred: 0.478684
iteration 1607: loss: 164213.640625, loss_kl: 2.173358, loss_recon: 164177.937500, loss_pred: 0.217163
iteration 1608: loss: 167238.953125, loss_kl: 2.777371, loss_recon: 167196.375000, loss_pred: 0.247078
iteration 1609: loss: 168276.859375, loss_kl: 2.288671, loss_recon: 168233.656250, loss_pred: 0.284706
iteration 1610: loss: 166663.171875, loss_kl: 3.174206, loss_recon: 166620.046875, loss_pred: 0.227043
iteration 1611: loss: 166710.718750, loss_kl: 2.904660, loss_recon: 166668.859375, loss_pred: 0.231790
 90%|█████████████████████████▉   | 179/200 [2:44:48<18:58, 54.24s/it]iteration 1612: loss: 166307.343750, loss_kl: 2.200210, loss_recon: 166275.343750, loss_pred: 0.169753
iteration 1613: loss: 163629.218750, loss_kl: 3.037642, loss_recon: 163575.937500, loss_pred: 0.325390
iteration 1614: loss: 168394.359375, loss_kl: 2.305125, loss_recon: 168356.937500, loss_pred: 0.216698
iteration 1615: loss: 163468.390625, loss_kl: 2.676526, loss_recon: 163425.968750, loss_pred: 0.241432
iteration 1616: loss: 164348.703125, loss_kl: 2.314641, loss_recon: 164306.765625, loss_pred: 0.261304
iteration 1617: loss: 168034.687500, loss_kl: 2.855358, loss_recon: 167994.640625, loss_pred: 0.205504
iteration 1618: loss: 164762.828125, loss_kl: 2.902038, loss_recon: 164721.171875, loss_pred: 0.218212
iteration 1619: loss: 167515.031250, loss_kl: 2.119379, loss_recon: 167483.171875, loss_pred: 0.173827
iteration 1620: loss: 170077.296875, loss_kl: 2.400450, loss_recon: 170036.296875, loss_pred: 0.245911
 90%|██████████████████████████   | 180/200 [2:45:43<18:11, 54.58s/it]iteration 1621: loss: 162277.546875, loss_kl: 2.277389, loss_recon: 162232.515625, loss_pred: 0.285632
iteration 1622: loss: 166041.437500, loss_kl: 3.013424, loss_recon: 165997.031250, loss_pred: 0.226271
iteration 1623: loss: 164821.468750, loss_kl: 2.473694, loss_recon: 164775.375000, loss_pred: 0.282128
iteration 1624: loss: 167920.421875, loss_kl: 2.702292, loss_recon: 167880.937500, loss_pred: 0.199489
iteration 1625: loss: 163341.687500, loss_kl: 3.347845, loss_recon: 163297.515625, loss_pred: 0.199656
iteration 1626: loss: 165456.312500, loss_kl: 2.338055, loss_recon: 165419.062500, loss_pred: 0.203460
iteration 1627: loss: 170903.921875, loss_kl: 3.167933, loss_recon: 170858.640625, loss_pred: 0.223836
iteration 1628: loss: 167669.625000, loss_kl: 2.595607, loss_recon: 167627.515625, loss_pred: 0.233484
iteration 1629: loss: 168159.953125, loss_kl: 2.560311, loss_recon: 168118.718750, loss_pred: 0.227361
 90%|██████████████████████████▏  | 181/200 [2:46:39<17:23, 54.90s/it]iteration 1630: loss: 169033.531250, loss_kl: 2.796927, loss_recon: 168991.984375, loss_pred: 0.202228
iteration 1631: loss: 164930.484375, loss_kl: 1.915609, loss_recon: 164888.531250, loss_pred: 0.273408
iteration 1632: loss: 169974.265625, loss_kl: 3.200942, loss_recon: 169931.796875, loss_pred: 0.180701
iteration 1633: loss: 163136.515625, loss_kl: 2.599836, loss_recon: 163077.359375, loss_pred: 0.393280
iteration 1634: loss: 166942.140625, loss_kl: 3.258814, loss_recon: 166891.656250, loss_pred: 0.256382
iteration 1635: loss: 166938.953125, loss_kl: 3.678446, loss_recon: 166889.609375, loss_pred: 0.212918
iteration 1636: loss: 164513.656250, loss_kl: 2.299732, loss_recon: 164466.171875, loss_pred: 0.299605
iteration 1637: loss: 164847.734375, loss_kl: 3.909189, loss_recon: 164790.234375, loss_pred: 0.277044
iteration 1638: loss: 166252.859375, loss_kl: 4.023142, loss_recon: 166192.937500, loss_pred: 0.292523
 91%|██████████████████████████▍  | 182/200 [2:47:33<16:25, 54.73s/it]iteration 1639: loss: 166639.187500, loss_kl: 3.192451, loss_recon: 166588.203125, loss_pred: 0.253762
iteration 1640: loss: 168496.390625, loss_kl: 3.317172, loss_recon: 168441.953125, loss_pred: 0.278276
iteration 1641: loss: 166381.234375, loss_kl: 3.145058, loss_recon: 166316.796875, loss_pred: 0.392115
iteration 1642: loss: 164190.406250, loss_kl: 2.774105, loss_recon: 164138.765625, loss_pred: 0.293925
iteration 1643: loss: 165969.859375, loss_kl: 3.215761, loss_recon: 165886.531250, loss_pred: 0.575315
iteration 1644: loss: 166153.968750, loss_kl: 2.301144, loss_recon: 166104.343750, loss_pred: 0.311714
iteration 1645: loss: 165463.453125, loss_kl: 2.586968, loss_recon: 165379.000000, loss_pred: 0.636955
iteration 1646: loss: 165670.953125, loss_kl: 2.693118, loss_recon: 165621.031250, loss_pred: 0.283265
iteration 1647: loss: 168052.750000, loss_kl: 4.050303, loss_recon: 167968.265625, loss_pred: 0.520067
 92%|██████████████████████████▌  | 183/200 [2:48:29<15:33, 54.88s/it]iteration 1648: loss: 165389.140625, loss_kl: 2.508546, loss_recon: 165318.765625, loss_pred: 0.492692
iteration 1649: loss: 169810.343750, loss_kl: 4.544483, loss_recon: 169742.312500, loss_pred: 0.297861
iteration 1650: loss: 169528.843750, loss_kl: 5.571572, loss_recon: 169445.765625, loss_pred: 0.361932
iteration 1651: loss: 163878.234375, loss_kl: 3.709864, loss_recon: 163795.718750, loss_pred: 0.512955
iteration 1652: loss: 167241.718750, loss_kl: 3.660805, loss_recon: 167180.281250, loss_pred: 0.306191
iteration 1653: loss: 166537.578125, loss_kl: 5.788846, loss_recon: 166417.640625, loss_pred: 0.712249
iteration 1654: loss: 161940.296875, loss_kl: 3.592460, loss_recon: 161868.859375, loss_pred: 0.412058
iteration 1655: loss: 166728.453125, loss_kl: 3.252444, loss_recon: 166661.000000, loss_pred: 0.400848
iteration 1656: loss: 165792.250000, loss_kl: 4.250710, loss_recon: 165710.890625, loss_pred: 0.455755
 92%|██████████████████████████▋  | 184/200 [2:49:25<14:43, 55.23s/it]iteration 1657: loss: 165120.546875, loss_kl: 2.586897, loss_recon: 165065.375000, loss_pred: 0.323821
iteration 1658: loss: 164527.562500, loss_kl: 3.394393, loss_recon: 164473.921875, loss_pred: 0.237418
iteration 1659: loss: 168177.453125, loss_kl: 4.500801, loss_recon: 168097.078125, loss_pred: 0.407184
iteration 1660: loss: 164854.875000, loss_kl: 3.580902, loss_recon: 164793.843750, loss_pred: 0.294612
iteration 1661: loss: 165535.875000, loss_kl: 3.145562, loss_recon: 165477.140625, loss_pred: 0.310144
iteration 1662: loss: 167460.234375, loss_kl: 3.575094, loss_recon: 167402.421875, loss_pred: 0.263195
iteration 1663: loss: 167039.281250, loss_kl: 2.860490, loss_recon: 166987.218750, loss_pred: 0.268576
iteration 1664: loss: 168033.437500, loss_kl: 2.670403, loss_recon: 167984.171875, loss_pred: 0.257304
iteration 1665: loss: 166023.828125, loss_kl: 3.017476, loss_recon: 165970.625000, loss_pred: 0.266115
 92%|██████████████████████████▊  | 185/200 [2:50:18<13:41, 54.77s/it]iteration 1666: loss: 165070.812500, loss_kl: 2.698027, loss_recon: 165021.703125, loss_pred: 0.242649
iteration 1667: loss: 166984.156250, loss_kl: 2.388218, loss_recon: 166935.593750, loss_pred: 0.265796
iteration 1668: loss: 169885.671875, loss_kl: 2.806985, loss_recon: 169836.312500, loss_pred: 0.235152
iteration 1669: loss: 166097.781250, loss_kl: 2.271028, loss_recon: 166036.093750, loss_pred: 0.407888
iteration 1670: loss: 168733.046875, loss_kl: 2.517201, loss_recon: 168671.375000, loss_pred: 0.385011
iteration 1671: loss: 163626.312500, loss_kl: 2.822363, loss_recon: 163576.921875, loss_pred: 0.234035
iteration 1672: loss: 169513.718750, loss_kl: 2.062268, loss_recon: 169461.046875, loss_pred: 0.336839
iteration 1673: loss: 162422.453125, loss_kl: 2.834822, loss_recon: 162368.109375, loss_pred: 0.282407
iteration 1674: loss: 164205.234375, loss_kl: 3.010545, loss_recon: 164152.296875, loss_pred: 0.252132
 93%|██████████████████████████▉  | 186/200 [2:51:12<12:43, 54.55s/it]iteration 1675: loss: 164959.421875, loss_kl: 1.872135, loss_recon: 164916.062500, loss_pred: 0.253733
iteration 1676: loss: 167095.046875, loss_kl: 3.020638, loss_recon: 167038.890625, loss_pred: 0.271333
iteration 1677: loss: 162685.984375, loss_kl: 2.750910, loss_recon: 162637.687500, loss_pred: 0.218735
iteration 1678: loss: 168037.187500, loss_kl: 2.221084, loss_recon: 167982.234375, loss_pred: 0.336209
iteration 1679: loss: 165350.546875, loss_kl: 3.120658, loss_recon: 165298.078125, loss_pred: 0.224928
iteration 1680: loss: 165101.484375, loss_kl: 2.250890, loss_recon: 165059.578125, loss_pred: 0.202744
iteration 1681: loss: 169416.703125, loss_kl: 2.389823, loss_recon: 169374.312500, loss_pred: 0.194412
iteration 1682: loss: 164818.328125, loss_kl: 2.626784, loss_recon: 164770.562500, loss_pred: 0.225329
iteration 1683: loss: 168673.578125, loss_kl: 1.857777, loss_recon: 168636.718750, loss_pred: 0.190184
 94%|███████████████████████████  | 187/200 [2:52:07<11:48, 54.50s/it]iteration 1684: loss: 170181.343750, loss_kl: 1.908761, loss_recon: 170138.593750, loss_pred: 0.236486
iteration 1685: loss: 164602.562500, loss_kl: 2.171895, loss_recon: 164561.593750, loss_pred: 0.192435
iteration 1686: loss: 165727.625000, loss_kl: 1.953128, loss_recon: 165683.468750, loss_pred: 0.246262
iteration 1687: loss: 169347.609375, loss_kl: 1.951690, loss_recon: 169302.718750, loss_pred: 0.253723
iteration 1688: loss: 168246.765625, loss_kl: 1.892067, loss_recon: 168205.359375, loss_pred: 0.224875
iteration 1689: loss: 162734.812500, loss_kl: 2.153687, loss_recon: 162687.656250, loss_pred: 0.256173
iteration 1690: loss: 166241.000000, loss_kl: 2.010170, loss_recon: 166195.265625, loss_pred: 0.256299
iteration 1691: loss: 166945.265625, loss_kl: 2.077635, loss_recon: 166901.765625, loss_pred: 0.227160
iteration 1692: loss: 162238.484375, loss_kl: 2.395470, loss_recon: 162186.281250, loss_pred: 0.282436
 94%|███████████████████████████▎ | 188/200 [2:53:03<11:01, 55.09s/it]iteration 1693: loss: 168812.000000, loss_kl: 1.955203, loss_recon: 168770.515625, loss_pred: 0.219357
iteration 1694: loss: 162778.140625, loss_kl: 2.118788, loss_recon: 162725.406250, loss_pred: 0.315423
iteration 1695: loss: 169035.984375, loss_kl: 1.676037, loss_recon: 169001.203125, loss_pred: 0.180147
iteration 1696: loss: 168253.640625, loss_kl: 1.935634, loss_recon: 168215.000000, loss_pred: 0.192797
iteration 1697: loss: 165227.796875, loss_kl: 1.778060, loss_recon: 165192.406250, loss_pred: 0.176159
iteration 1698: loss: 164770.375000, loss_kl: 2.037446, loss_recon: 164726.296875, loss_pred: 0.237039
iteration 1699: loss: 164404.187500, loss_kl: 1.627506, loss_recon: 164363.593750, loss_pred: 0.243194
iteration 1700: loss: 167030.578125, loss_kl: 1.963625, loss_recon: 166988.578125, loss_pred: 0.223670
iteration 1701: loss: 165710.781250, loss_kl: 1.840528, loss_recon: 165671.921875, loss_pred: 0.204582
 94%|███████████████████████████▍ | 189/200 [2:53:58<10:03, 54.85s/it]iteration 1702: loss: 165880.078125, loss_kl: 1.856764, loss_recon: 165842.593750, loss_pred: 0.189229
iteration 1703: loss: 168122.687500, loss_kl: 1.727672, loss_recon: 168083.109375, loss_pred: 0.222972
iteration 1704: loss: 167604.343750, loss_kl: 1.636607, loss_recon: 167566.593750, loss_pred: 0.213935
iteration 1705: loss: 165756.359375, loss_kl: 1.886081, loss_recon: 165713.265625, loss_pred: 0.242406
iteration 1706: loss: 162480.671875, loss_kl: 2.013285, loss_recon: 162433.562500, loss_pred: 0.269676
iteration 1707: loss: 164718.625000, loss_kl: 2.284535, loss_recon: 164676.421875, loss_pred: 0.193542
iteration 1708: loss: 165806.796875, loss_kl: 1.870518, loss_recon: 165763.203125, loss_pred: 0.248840
iteration 1709: loss: 168786.593750, loss_kl: 2.249313, loss_recon: 168743.125000, loss_pred: 0.209635
iteration 1710: loss: 166900.562500, loss_kl: 2.055922, loss_recon: 166853.750000, loss_pred: 0.262442
 95%|███████████████████████████▌ | 190/200 [2:54:53<09:11, 55.16s/it]iteration 1711: loss: 164922.343750, loss_kl: 1.782595, loss_recon: 164879.546875, loss_pred: 0.249632
iteration 1712: loss: 167044.281250, loss_kl: 1.924970, loss_recon: 167000.265625, loss_pred: 0.247733
iteration 1713: loss: 169069.546875, loss_kl: 1.540675, loss_recon: 169032.406250, loss_pred: 0.217272
iteration 1714: loss: 164843.765625, loss_kl: 2.287856, loss_recon: 164787.531250, loss_pred: 0.333641
iteration 1715: loss: 166683.921875, loss_kl: 1.620747, loss_recon: 166642.312500, loss_pred: 0.254042
iteration 1716: loss: 168977.078125, loss_kl: 1.941755, loss_recon: 168928.343750, loss_pred: 0.293162
iteration 1717: loss: 166969.546875, loss_kl: 1.565584, loss_recon: 166908.078125, loss_pred: 0.458182
iteration 1718: loss: 161900.515625, loss_kl: 2.645729, loss_recon: 161817.593750, loss_pred: 0.564762
iteration 1719: loss: 165999.812500, loss_kl: 2.501709, loss_recon: 165941.250000, loss_pred: 0.335402
 96%|███████████████████████████▋ | 191/200 [2:55:50<08:20, 55.64s/it]iteration 1720: loss: 168052.796875, loss_kl: 2.268751, loss_recon: 167979.562500, loss_pred: 0.505471
iteration 1721: loss: 165882.390625, loss_kl: 3.531241, loss_recon: 165827.437500, loss_pred: 0.196361
iteration 1722: loss: 166052.250000, loss_kl: 2.862306, loss_recon: 165988.359375, loss_pred: 0.352630
iteration 1723: loss: 164808.625000, loss_kl: 2.120735, loss_recon: 164752.937500, loss_pred: 0.344831
iteration 1724: loss: 161379.640625, loss_kl: 4.236007, loss_recon: 161309.421875, loss_pred: 0.278550
iteration 1725: loss: 168652.234375, loss_kl: 3.491862, loss_recon: 168555.937500, loss_pred: 0.613672
iteration 1726: loss: 164720.734375, loss_kl: 1.862292, loss_recon: 164676.343750, loss_pred: 0.257650
iteration 1727: loss: 167877.796875, loss_kl: 3.638493, loss_recon: 167814.750000, loss_pred: 0.266507
iteration 1728: loss: 168731.328125, loss_kl: 2.863719, loss_recon: 168662.031250, loss_pred: 0.406599
 96%|███████████████████████████▊ | 192/200 [2:56:45<07:23, 55.42s/it]iteration 1729: loss: 164803.500000, loss_kl: 1.913925, loss_recon: 164751.515625, loss_pred: 0.328464
iteration 1730: loss: 163839.031250, loss_kl: 3.527417, loss_recon: 163776.093750, loss_pred: 0.276590
iteration 1731: loss: 165392.484375, loss_kl: 3.062260, loss_recon: 165325.750000, loss_pred: 0.361129
iteration 1732: loss: 164719.812500, loss_kl: 2.276192, loss_recon: 164672.828125, loss_pred: 0.242226
iteration 1733: loss: 164775.656250, loss_kl: 3.654999, loss_recon: 164705.421875, loss_pred: 0.336814
iteration 1734: loss: 170290.218750, loss_kl: 2.214368, loss_recon: 170244.000000, loss_pred: 0.240789
iteration 1735: loss: 168324.156250, loss_kl: 2.389237, loss_recon: 168277.093750, loss_pred: 0.231779
iteration 1736: loss: 169348.078125, loss_kl: 2.852751, loss_recon: 169289.828125, loss_pred: 0.297262
iteration 1737: loss: 164833.375000, loss_kl: 1.871958, loss_recon: 164792.921875, loss_pred: 0.217335
 96%|███████████████████████████▉ | 193/200 [2:57:41<06:28, 55.49s/it]iteration 1738: loss: 164824.015625, loss_kl: 2.392630, loss_recon: 164768.968750, loss_pred: 0.311228
iteration 1739: loss: 163037.734375, loss_kl: 2.406765, loss_recon: 162971.796875, loss_pred: 0.418711
iteration 1740: loss: 165212.093750, loss_kl: 1.796323, loss_recon: 165162.484375, loss_pred: 0.316386
iteration 1741: loss: 166280.734375, loss_kl: 1.948815, loss_recon: 166230.828125, loss_pred: 0.304234
iteration 1742: loss: 168725.093750, loss_kl: 1.544691, loss_recon: 168683.046875, loss_pred: 0.265953
iteration 1743: loss: 166536.578125, loss_kl: 2.342212, loss_recon: 166477.312500, loss_pred: 0.358418
iteration 1744: loss: 167429.625000, loss_kl: 1.944622, loss_recon: 167384.343750, loss_pred: 0.258281
iteration 1745: loss: 166172.531250, loss_kl: 2.181831, loss_recon: 166126.593750, loss_pred: 0.241236
iteration 1746: loss: 167736.671875, loss_kl: 2.382334, loss_recon: 167686.984375, loss_pred: 0.258587
 97%|████████████████████████████▏| 194/200 [2:58:35<05:30, 55.12s/it]iteration 1747: loss: 164807.375000, loss_kl: 2.315012, loss_recon: 164761.968750, loss_pred: 0.222551
iteration 1748: loss: 162830.968750, loss_kl: 2.058257, loss_recon: 162784.531250, loss_pred: 0.258591
iteration 1749: loss: 166061.593750, loss_kl: 2.343523, loss_recon: 166004.000000, loss_pred: 0.341604
iteration 1750: loss: 166975.671875, loss_kl: 1.848107, loss_recon: 166932.078125, loss_pred: 0.251048
iteration 1751: loss: 166141.359375, loss_kl: 2.303003, loss_recon: 166094.062500, loss_pred: 0.242731
iteration 1752: loss: 170077.687500, loss_kl: 2.480432, loss_recon: 170025.718750, loss_pred: 0.271671
iteration 1753: loss: 165805.421875, loss_kl: 2.108606, loss_recon: 165756.921875, loss_pred: 0.273999
iteration 1754: loss: 163873.734375, loss_kl: 2.235890, loss_recon: 163829.000000, loss_pred: 0.223774
iteration 1755: loss: 169299.125000, loss_kl: 2.782725, loss_recon: 169242.687500, loss_pred: 0.286119
 98%|████████████████████████████▎| 195/200 [2:59:29<04:34, 54.87s/it]iteration 1756: loss: 166211.687500, loss_kl: 3.123726, loss_recon: 166149.500000, loss_pred: 0.309586
iteration 1757: loss: 168006.859375, loss_kl: 2.303296, loss_recon: 167963.281250, loss_pred: 0.205469
iteration 1758: loss: 162363.000000, loss_kl: 2.531815, loss_recon: 162308.031250, loss_pred: 0.296546
iteration 1759: loss: 164851.406250, loss_kl: 3.310394, loss_recon: 164793.031250, loss_pred: 0.252719
iteration 1760: loss: 167854.453125, loss_kl: 2.933977, loss_recon: 167800.843750, loss_pred: 0.242638
iteration 1761: loss: 166425.968750, loss_kl: 1.650771, loss_recon: 166387.937500, loss_pred: 0.215302
iteration 1762: loss: 165280.296875, loss_kl: 3.321979, loss_recon: 165221.015625, loss_pred: 0.260580
iteration 1763: loss: 166354.218750, loss_kl: 3.386482, loss_recon: 166291.203125, loss_pred: 0.291632
iteration 1764: loss: 168583.296875, loss_kl: 2.136537, loss_recon: 168530.406250, loss_pred: 0.315246
 98%|████████████████████████████▍| 196/200 [3:00:25<03:40, 55.24s/it]iteration 1765: loss: 165769.937500, loss_kl: 2.382969, loss_recon: 165721.406250, loss_pred: 0.246957
iteration 1766: loss: 165669.187500, loss_kl: 3.187251, loss_recon: 165613.421875, loss_pred: 0.238948
iteration 1767: loss: 168232.812500, loss_kl: 2.703600, loss_recon: 168183.078125, loss_pred: 0.226997
iteration 1768: loss: 166083.843750, loss_kl: 1.755304, loss_recon: 166040.500000, loss_pred: 0.258030
iteration 1769: loss: 161470.015625, loss_kl: 2.789381, loss_recon: 161397.031250, loss_pred: 0.450873
iteration 1770: loss: 167242.734375, loss_kl: 3.220171, loss_recon: 167186.640625, loss_pred: 0.238891
iteration 1771: loss: 166919.953125, loss_kl: 2.272649, loss_recon: 166874.156250, loss_pred: 0.230842
iteration 1772: loss: 169020.250000, loss_kl: 1.780464, loss_recon: 168966.796875, loss_pred: 0.356576
iteration 1773: loss: 165521.640625, loss_kl: 3.016772, loss_recon: 165460.703125, loss_pred: 0.307717
 98%|████████████████████████████▌| 197/200 [3:01:20<02:45, 55.19s/it]iteration 1774: loss: 169777.640625, loss_kl: 2.208807, loss_recon: 169732.000000, loss_pred: 0.235465
iteration 1775: loss: 165475.359375, loss_kl: 1.746300, loss_recon: 165433.093750, loss_pred: 0.247959
iteration 1776: loss: 169423.312500, loss_kl: 2.429751, loss_recon: 169372.890625, loss_pred: 0.261242
iteration 1777: loss: 163975.812500, loss_kl: 2.007772, loss_recon: 163927.859375, loss_pred: 0.278706
iteration 1778: loss: 162460.078125, loss_kl: 1.876043, loss_recon: 162416.687500, loss_pred: 0.246183
iteration 1779: loss: 165400.890625, loss_kl: 2.027853, loss_recon: 165358.890625, loss_pred: 0.217128
iteration 1780: loss: 166321.890625, loss_kl: 2.327226, loss_recon: 166267.484375, loss_pred: 0.311396
iteration 1781: loss: 168168.375000, loss_kl: 1.760067, loss_recon: 168118.000000, loss_pred: 0.327866
iteration 1782: loss: 164752.859375, loss_kl: 2.244665, loss_recon: 164707.250000, loss_pred: 0.231535
 99%|████████████████████████████▋| 198/200 [3:02:16<01:50, 55.16s/it]iteration 1783: loss: 169281.921875, loss_kl: 2.376604, loss_recon: 169231.015625, loss_pred: 0.271456
iteration 1784: loss: 165263.703125, loss_kl: 1.761582, loss_recon: 165211.796875, loss_pred: 0.342950
iteration 1785: loss: 167209.515625, loss_kl: 1.656530, loss_recon: 167158.765625, loss_pred: 0.341802
iteration 1786: loss: 164498.000000, loss_kl: 1.818147, loss_recon: 164451.171875, loss_pred: 0.286416
iteration 1787: loss: 162642.140625, loss_kl: 2.015368, loss_recon: 162595.265625, loss_pred: 0.267170
iteration 1788: loss: 165572.500000, loss_kl: 1.616239, loss_recon: 165531.000000, loss_pred: 0.253400
iteration 1789: loss: 165405.546875, loss_kl: 1.496719, loss_recon: 165348.968750, loss_pred: 0.416070
iteration 1790: loss: 170282.421875, loss_kl: 2.022130, loss_recon: 170217.765625, loss_pred: 0.444337
iteration 1791: loss: 166029.968750, loss_kl: 1.900904, loss_recon: 165973.421875, loss_pred: 0.375258
100%|████████████████████████████▊| 199/200 [3:03:11<00:55, 55.20s/it]iteration 1792: loss: 165918.921875, loss_kl: 1.537999, loss_recon: 165870.343750, loss_pred: 0.332008
iteration 1793: loss: 168130.531250, loss_kl: 1.750636, loss_recon: 168068.343750, loss_pred: 0.446827
iteration 1794: loss: 164651.093750, loss_kl: 2.221244, loss_recon: 164590.531250, loss_pred: 0.383455
iteration 1795: loss: 165671.343750, loss_kl: 2.074575, loss_recon: 165625.484375, loss_pred: 0.251061
iteration 1796: loss: 167075.609375, loss_kl: 1.556649, loss_recon: 167027.890625, loss_pred: 0.321538
iteration 1797: loss: 165924.953125, loss_kl: 1.645386, loss_recon: 165882.171875, loss_pred: 0.263220
iteration 1798: loss: 164648.171875, loss_kl: 1.855851, loss_recon: 164602.421875, loss_pred: 0.271826
iteration 1799: loss: 165318.671875, loss_kl: 1.709166, loss_recon: 165274.140625, loss_pred: 0.274429
iteration 1800: loss: 168347.531250, loss_kl: 1.472001, loss_recon: 168307.171875, loss_pred: 0.256345
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_199.pth
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234/epoch_199.pth
100%|████████████████████████████▊| 199/200 [3:04:07<00:55, 55.51s/it]
Namespace(dataset='Design2', img_size=[64, 64, 64], vit_patches_size=[1, 1, 1], net_path=False, vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=35, base_lr=0.001, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=35, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, number_of_samplings=6, num_classes=1, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[64, 64, 64]', label_size=11, distributed=False)
TVG_Conv-ViT-Gen2-B_16_vitpatch[1, 1, 1]_epo200_bs35_lr0.001_seed1234
7 test iterations per epoch
0it [00:00, ?it/s]0it [00:10, ?it/s]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 389, in <module>
    inferrer[dataset_name](args, net, test_save_path)
  File "/home/mhashemi/TransVNet/test.py", line 165, in inferrer_mat2
    name_batch, metric_batch = test_multiple_volumes_generative2(image_batch, label_batch, time_batch, model, name_batch, test_save_path, number_of_samplings)
  File "/home/mhashemi/TransVNet/utils.py", line 224, in test_multiple_volumes_generative2
    mu2, log_variance2, predicted_labels_generative, features = net.module.encoder(generative_output.unsqueeze(1), torch.tensor([-1]).cuda())
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mhashemi/TransVNet/networks/TransVNet_modeling.py", line 759, in forward
    x = x.repeat(1,3,1,1,1)
RuntimeError: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor
