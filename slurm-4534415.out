/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[160, 160, 160], vit_name='Conv-ViT-Gen-B_16', pretrained_net_path=False, is_encoder_pretrained=True, vit_patches_size=[8, 8, 8], deterministic=1, max_epochs=100, batch_size=32, base_lr=0.01, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[160, 160, 160]', distributed=False)
10 iterations per epoch. 1000 max iterations 
  0%|                                         | 0/100 [00:00<?, ?it/s]iteration 1: loss: 0.700154, loss_kl: 1.172940, loss_recon: 0.693142, loss_pred: 1.663034
iteration 2: loss: 0.881269, loss_kl: 68.887024, loss_recon: 0.691462, loss_pred: 7.876262
iteration 3: loss: 1.284802, loss_kl: 88.831589, loss_recon: 0.691731, loss_pred: 151.023315
iteration 4: loss: 1.445733, loss_kl: 96.701920, loss_recon: 0.688610, loss_pred: 209.500565
iteration 5: loss: 0.903990, loss_kl: 78.735023, loss_recon: 0.685475, loss_pred: 9.638768
iteration 6: loss: 0.799219, loss_kl: 43.011215, loss_recon: 0.688935, loss_pred: 1.590776
iteration 7: loss: 0.756767, loss_kl: 26.126987, loss_recon: 0.687694, loss_pred: 1.808136
iteration 8: loss: 0.785597, loss_kl: 39.213104, loss_recon: 0.684378, loss_pred: 1.722677
iteration 9: loss: 0.795659, loss_kl: 38.832645, loss_recon: 0.695848, loss_pred: 1.534050
iteration 10: loss: 0.803477, loss_kl: 45.616772, loss_recon: 0.687225, loss_pred: 1.398912
  1%|▎                            | 1/100 [06:23<10:32:51, 383.55s/it]iteration 11: loss: 0.970467, loss_kl: 41.797478, loss_recon: 0.690516, loss_pred: 1.726510
iteration 12: loss: 1.030990, loss_kl: 51.780472, loss_recon: 0.686008, loss_pred: 1.854009
iteration 13: loss: 1.008857, loss_kl: 48.326126, loss_recon: 0.689872, loss_pred: 1.266498
iteration 14: loss: 1.030943, loss_kl: 51.476402, loss_recon: 0.689724, loss_pred: 1.572975
iteration 15: loss: 1.032110, loss_kl: 51.909088, loss_recon: 0.689164, loss_pred: 1.408764
iteration 16: loss: 0.995730, loss_kl: 46.672752, loss_recon: 0.687007, loss_pred: 1.324500
iteration 17: loss: 1.027349, loss_kl: 51.223557, loss_recon: 0.688682, loss_pred: 1.428954
iteration 18: loss: 1.022453, loss_kl: 50.689430, loss_recon: 0.687009, loss_pred: 1.462123
iteration 19: loss: 0.992848, loss_kl: 46.260830, loss_recon: 0.685056, loss_pred: 1.591561
iteration 20: loss: 1.030434, loss_kl: 51.586323, loss_recon: 0.688423, loss_pred: 1.586104
  2%|▌                            | 2/100 [12:26<10:06:30, 371.33s/it]iteration 21: loss: 1.489527, loss_kl: 46.879242, loss_recon: 0.682291, loss_pred: 1.672315
iteration 22: loss: 1.479065, loss_kl: 46.131088, loss_recon: 0.686653, loss_pred: 1.528929
iteration 23: loss: 1.659573, loss_kl: 57.372021, loss_recon: 0.683479, loss_pred: 1.335636
iteration 24: loss: 1.558284, loss_kl: 50.743889, loss_recon: 0.686093, loss_pred: 1.714505
iteration 25: loss: 1.676664, loss_kl: 57.538292, loss_recon: 0.691290, loss_pred: 1.727490
iteration 26: loss: 1.603113, loss_kl: 53.229080, loss_recon: 0.696412, loss_pred: 1.304865
iteration 27: loss: 1.545402, loss_kl: 49.670067, loss_recon: 0.691894, loss_pred: 1.664611
iteration 28: loss: 1.543060, loss_kl: 50.029846, loss_recon: 0.688314, loss_pred: 1.379228
iteration 29: loss: 1.502075, loss_kl: 47.198940, loss_recon: 0.691756, loss_pred: 1.538097
iteration 30: loss: 1.430459, loss_kl: 44.080082, loss_recon: 0.682417, loss_pred: 0.911248
  3%|▉                             | 3/100 [18:29<9:54:25, 367.69s/it]iteration 31: loss: 2.581985, loss_kl: 43.511318, loss_recon: 0.691698, loss_pred: 1.187118
iteration 32: loss: 2.967671, loss_kl: 52.881569, loss_recon: 0.689709, loss_pred: 0.983969
iteration 33: loss: 3.021263, loss_kl: 53.608654, loss_recon: 0.688086, loss_pred: 1.562527
iteration 34: loss: 2.945664, loss_kl: 52.191120, loss_recon: 0.687782, loss_pred: 1.199623
iteration 35: loss: 2.837064, loss_kl: 49.539696, loss_recon: 0.686372, loss_pred: 1.316392
iteration 36: loss: 2.704449, loss_kl: 46.569614, loss_recon: 0.686383, loss_pred: 1.150343
iteration 37: loss: 2.570225, loss_kl: 43.460377, loss_recon: 0.684549, loss_pred: 1.129036
iteration 38: loss: 2.542459, loss_kl: 42.940880, loss_recon: 0.688986, loss_pred: 0.887056
iteration 39: loss: 2.345879, loss_kl: 38.252460, loss_recon: 0.688944, loss_pred: 0.928059
iteration 40: loss: 2.241961, loss_kl: 35.827789, loss_recon: 0.687113, loss_pred: 0.938741
  4%|█▏                            | 4/100 [24:32<9:45:02, 365.65s/it]iteration 41: loss: 4.668829, loss_kl: 37.950600, loss_recon: 0.684220, loss_pred: 0.585103
iteration 42: loss: 4.679898, loss_kl: 38.174461, loss_recon: 0.681188, loss_pred: 0.497613
iteration 43: loss: 5.766423, loss_kl: 48.124649, loss_recon: 0.675148, loss_pred: 1.113771
iteration 44: loss: 5.677633, loss_kl: 47.716602, loss_recon: 0.679264, loss_pred: 0.623313
iteration 45: loss: 4.938262, loss_kl: 40.451775, loss_recon: 0.668794, loss_pred: 0.838839
iteration 46: loss: 4.829043, loss_kl: 39.537960, loss_recon: 0.660025, loss_pred: 0.781188
iteration 47: loss: 4.352521, loss_kl: 35.183556, loss_recon: 0.659866, loss_pred: 0.528618
iteration 48: loss: 4.485620, loss_kl: 36.570072, loss_recon: 0.648485, loss_pred: 0.539387
iteration 49: loss: 3.964054, loss_kl: 31.620592, loss_recon: 0.652876, loss_pred: 0.402265
iteration 50: loss: 4.278507, loss_kl: 34.360771, loss_recon: 0.637064, loss_pred: 0.856117
  5%|█▌                            | 5/100 [30:35<9:37:24, 364.68s/it]iteration 51: loss: 7.967977, loss_kl: 31.187439, loss_recon: 0.646570, loss_pred: 0.441898
iteration 52: loss: 8.262613, loss_kl: 32.309364, loss_recon: 0.670045, loss_pred: 0.491419
iteration 53: loss: 7.277475, loss_kl: 28.136208, loss_recon: 0.650388, loss_pred: 0.493581
iteration 54: loss: 7.186438, loss_kl: 27.826736, loss_recon: 0.658385, loss_pred: 0.375216
iteration 55: loss: 6.218349, loss_kl: 23.818615, loss_recon: 0.643568, loss_pred: 0.265089
iteration 56: loss: 5.587481, loss_kl: 21.140221, loss_recon: 0.632915, loss_pred: 0.264086
iteration 57: loss: 5.229737, loss_kl: 19.536982, loss_recon: 0.649317, loss_pred: 0.250968
iteration 58: loss: 4.935272, loss_kl: 18.382463, loss_recon: 0.646584, loss_pred: 0.145168
iteration 59: loss: 4.575699, loss_kl: 16.812702, loss_recon: 0.639086, loss_pred: 0.193926
iteration 60: loss: 4.236614, loss_kl: 15.262400, loss_recon: 0.653240, loss_pred: 0.218196
  6%|█▊                            | 6/100 [36:39<9:30:57, 364.44s/it]iteration 61: loss: 6.822057, loss_kl: 13.287016, loss_recon: 0.646662, loss_pred: 0.738846
iteration 62: loss: 10.269169, loss_kl: 20.368887, loss_recon: 0.661212, loss_pred: 1.453178
iteration 63: loss: 5.529114, loss_kl: 10.804347, loss_recon: 0.637958, loss_pred: 0.304687
iteration 64: loss: 6.828028, loss_kl: 13.608837, loss_recon: 0.635935, loss_pred: 0.454949
iteration 65: loss: 7.209816, loss_kl: 14.697247, loss_recon: 0.633367, loss_pred: 0.239509
iteration 66: loss: 5.773404, loss_kl: 10.904027, loss_recon: 0.635794, loss_pred: 0.764764
iteration 67: loss: 3.065814, loss_kl: 5.131580, loss_recon: 0.633231, loss_pred: 0.393424
iteration 68: loss: 3.680707, loss_kl: 6.323784, loss_recon: 0.634728, loss_pred: 0.594394
iteration 69: loss: 2.619169, loss_kl: 3.858427, loss_recon: 0.636736, loss_pred: 0.644173
iteration 70: loss: 2.279258, loss_kl: 3.429516, loss_recon: 0.646199, loss_pred: 0.279570
  7%|██                            | 7/100 [42:42<9:24:32, 364.22s/it]iteration 71: loss: 4.855838, loss_kl: 5.881295, loss_recon: 0.647724, loss_pred: 0.375127
iteration 72: loss: 5.853520, loss_kl: 7.243714, loss_recon: 0.640902, loss_pred: 0.506158
iteration 73: loss: 5.282566, loss_kl: 6.425250, loss_recon: 0.653838, loss_pred: 0.456521
iteration 74: loss: 3.542996, loss_kl: 3.817765, loss_recon: 0.643694, loss_pred: 0.492778
iteration 75: loss: 3.525687, loss_kl: 3.852686, loss_recon: 0.638651, loss_pred: 0.439621
iteration 76: loss: 3.670810, loss_kl: 3.575971, loss_recon: 0.626963, loss_pred: 0.949475
iteration 77: loss: 3.376958, loss_kl: 3.879562, loss_recon: 0.643496, loss_pred: 0.184419
iteration 78: loss: 2.963301, loss_kl: 3.043618, loss_recon: 0.625183, loss_pred: 0.432585
iteration 79: loss: 3.820501, loss_kl: 4.494734, loss_recon: 0.623018, loss_pred: 0.259132
iteration 80: loss: 2.730846, loss_kl: 2.906417, loss_recon: 0.646658, loss_pred: 0.192255
  8%|██▍                           | 8/100 [48:46<9:18:18, 364.12s/it]iteration 81: loss: 4.269321, loss_kl: 3.935932, loss_recon: 0.620964, loss_pred: 0.392384
iteration 82: loss: 5.214137, loss_kl: 4.251336, loss_recon: 0.627655, loss_pred: 1.189947
iteration 83: loss: 3.224617, loss_kl: 2.750679, loss_recon: 0.632276, loss_pred: 0.324808
iteration 84: loss: 3.734623, loss_kl: 3.321956, loss_recon: 0.645769, loss_pred: 0.342580
iteration 85: loss: 4.201773, loss_kl: 3.852775, loss_recon: 0.623131, loss_pred: 0.392833
iteration 86: loss: 3.918241, loss_kl: 3.692759, loss_recon: 0.628188, loss_pred: 0.210474
iteration 87: loss: 4.142383, loss_kl: 3.628530, loss_recon: 0.632797, loss_pred: 0.535152
iteration 88: loss: 2.626689, loss_kl: 2.194081, loss_recon: 0.625003, loss_pred: 0.180668
iteration 89: loss: 2.828774, loss_kl: 2.227579, loss_recon: 0.628715, loss_pred: 0.382514
iteration 90: loss: 4.481754, loss_kl: 4.210419, loss_recon: 0.624298, loss_pred: 0.365966
  9%|██▋                           | 9/100 [54:49<9:11:44, 363.79s/it]iteration 91: loss: 4.592333, loss_kl: 3.947240, loss_recon: 0.623744, loss_pred: 0.304552
iteration 92: loss: 3.531617, loss_kl: 2.709557, loss_recon: 0.632895, loss_pred: 0.396021
iteration 93: loss: 3.646447, loss_kl: 3.021559, loss_recon: 0.630683, loss_pred: 0.209414
iteration 94: loss: 3.277210, loss_kl: 2.694691, loss_recon: 0.622045, loss_pred: 0.149950
iteration 95: loss: 2.971266, loss_kl: 2.408198, loss_recon: 0.625147, loss_pred: 0.105343
iteration 96: loss: 3.367287, loss_kl: 2.789472, loss_recon: 0.626170, loss_pred: 0.147255
iteration 97: loss: 3.666973, loss_kl: 3.157197, loss_recon: 0.629773, loss_pred: 0.096741
iteration 98: loss: 2.992217, loss_kl: 2.389522, loss_recon: 0.636184, loss_pred: 0.134642
iteration 99: loss: 3.245631, loss_kl: 2.509073, loss_recon: 0.622222, loss_pred: 0.301546
iteration 100: loss: 2.354224, loss_kl: 1.577797, loss_recon: 0.621527, loss_pred: 0.278548
 10%|██▋                        | 10/100 [1:00:52<9:05:15, 363.51s/it]iteration 101: loss: 3.261898, loss_kl: 2.306376, loss_recon: 0.623924, loss_pred: 0.403677
iteration 102: loss: 2.503403, loss_kl: 1.731312, loss_recon: 0.622795, loss_pred: 0.200682
iteration 103: loss: 2.349932, loss_kl: 1.514024, loss_recon: 0.619776, loss_pred: 0.263406
iteration 104: loss: 2.875759, loss_kl: 2.183901, loss_recon: 0.621631, loss_pred: 0.131818
iteration 105: loss: 2.389060, loss_kl: 1.694416, loss_recon: 0.616033, loss_pred: 0.127057
iteration 106: loss: 2.845371, loss_kl: 2.188699, loss_recon: 0.620670, loss_pred: 0.096789
iteration 107: loss: 2.906185, loss_kl: 1.949469, loss_recon: 0.623149, loss_pred: 0.395948
iteration 108: loss: 1.596842, loss_kl: 0.871543, loss_recon: 0.616166, loss_pred: 0.135929
iteration 109: loss: 3.298863, loss_kl: 2.263942, loss_recon: 0.630357, loss_pred: 0.477478
iteration 110: loss: 2.014231, loss_kl: 1.247270, loss_recon: 0.635591, loss_pred: 0.169040
 11%|██▉                        | 11/100 [1:06:55<8:58:54, 363.31s/it]iteration 111: loss: 3.465534, loss_kl: 2.102114, loss_recon: 0.620326, loss_pred: 0.772861
iteration 112: loss: 2.901595, loss_kl: 2.154843, loss_recon: 0.609492, loss_pred: 0.161240
iteration 113: loss: 3.261149, loss_kl: 1.918473, loss_recon: 0.634392, loss_pred: 0.735765
iteration 114: loss: 2.983594, loss_kl: 2.093148, loss_recon: 0.616222, loss_pred: 0.298992
iteration 115: loss: 3.045971, loss_kl: 2.175211, loss_recon: 0.631190, loss_pred: 0.264833
iteration 116: loss: 2.053585, loss_kl: 1.079000, loss_recon: 0.629227, loss_pred: 0.360259
iteration 117: loss: 2.559954, loss_kl: 1.689273, loss_recon: 0.622065, loss_pred: 0.268891
iteration 118: loss: 2.524275, loss_kl: 1.732253, loss_recon: 0.630616, loss_pred: 0.181218
iteration 119: loss: 3.245718, loss_kl: 2.429631, loss_recon: 0.622478, loss_pred: 0.221053
iteration 120: loss: 2.330806, loss_kl: 1.617953, loss_recon: 0.629207, loss_pred: 0.101448
 12%|███▏                       | 12/100 [1:12:58<8:52:43, 363.22s/it]iteration 121: loss: 3.206389, loss_kl: 2.444395, loss_recon: 0.620287, loss_pred: 0.152066
iteration 122: loss: 2.705083, loss_kl: 1.965908, loss_recon: 0.627997, loss_pred: 0.119499
iteration 123: loss: 2.197484, loss_kl: 1.401742, loss_recon: 0.615249, loss_pred: 0.186831
iteration 124: loss: 2.440440, loss_kl: 1.620297, loss_recon: 0.620420, loss_pred: 0.207013
iteration 125: loss: 1.623911, loss_kl: 0.857560, loss_recon: 0.633481, loss_pred: 0.136838
iteration 126: loss: 2.526018, loss_kl: 1.787856, loss_recon: 0.609359, loss_pred: 0.136481
iteration 127: loss: 2.048572, loss_kl: 1.296753, loss_recon: 0.626806, loss_pred: 0.130708
iteration 128: loss: 2.284726, loss_kl: 1.585747, loss_recon: 0.617436, loss_pred: 0.088222
iteration 129: loss: 2.164060, loss_kl: 1.427619, loss_recon: 0.627124, loss_pred: 0.115473
iteration 130: loss: 2.190724, loss_kl: 1.441996, loss_recon: 0.649639, loss_pred: 0.105264
 13%|███▌                       | 13/100 [1:19:02<8:46:49, 363.33s/it]iteration 131: loss: 2.161547, loss_kl: 1.134373, loss_recon: 0.615068, loss_pred: 0.412106
iteration 132: loss: 2.128515, loss_kl: 1.309216, loss_recon: 0.620909, loss_pred: 0.198389
iteration 133: loss: 2.168105, loss_kl: 1.427899, loss_recon: 0.618667, loss_pred: 0.121540
iteration 134: loss: 2.584453, loss_kl: 1.766098, loss_recon: 0.609382, loss_pred: 0.208972
iteration 135: loss: 1.901213, loss_kl: 1.207962, loss_recon: 0.611256, loss_pred: 0.081995
iteration 136: loss: 2.085966, loss_kl: 1.354882, loss_recon: 0.624516, loss_pred: 0.106568
iteration 137: loss: 2.026544, loss_kl: 1.242967, loss_recon: 0.629120, loss_pred: 0.154456
iteration 138: loss: 2.033601, loss_kl: 1.310764, loss_recon: 0.624680, loss_pred: 0.098156
iteration 139: loss: 1.913987, loss_kl: 1.173105, loss_recon: 0.624254, loss_pred: 0.116627
iteration 140: loss: 2.014822, loss_kl: 1.266520, loss_recon: 0.646463, loss_pred: 0.101840
 14%|███▊                       | 14/100 [1:25:05<8:40:36, 363.22s/it]iteration 141: loss: 2.341349, loss_kl: 1.509968, loss_recon: 0.616830, loss_pred: 0.214552
iteration 142: loss: 2.442686, loss_kl: 1.720265, loss_recon: 0.624338, loss_pred: 0.098083
iteration 143: loss: 1.951806, loss_kl: 1.202188, loss_recon: 0.624711, loss_pred: 0.124907
iteration 144: loss: 2.055093, loss_kl: 1.303836, loss_recon: 0.614399, loss_pred: 0.136858
iteration 145: loss: 1.731767, loss_kl: 1.024256, loss_recon: 0.614304, loss_pred: 0.093207
iteration 146: loss: 1.853927, loss_kl: 1.151890, loss_recon: 0.619381, loss_pred: 0.082656
iteration 147: loss: 1.967409, loss_kl: 1.266815, loss_recon: 0.614001, loss_pred: 0.086594
iteration 148: loss: 1.534882, loss_kl: 0.846417, loss_recon: 0.620843, loss_pred: 0.067623
iteration 149: loss: 2.051984, loss_kl: 1.337087, loss_recon: 0.631972, loss_pred: 0.082925
iteration 150: loss: 2.097947, loss_kl: 1.234534, loss_recon: 0.635187, loss_pred: 0.228227
 15%|████                       | 15/100 [1:31:08<8:34:29, 363.18s/it]iteration 151: loss: 2.173535, loss_kl: 1.379109, loss_recon: 0.621578, loss_pred: 0.172849
iteration 152: loss: 2.685560, loss_kl: 1.784467, loss_recon: 0.622968, loss_pred: 0.278125
iteration 153: loss: 2.152171, loss_kl: 1.452050, loss_recon: 0.623351, loss_pred: 0.076770
iteration 154: loss: 2.855191, loss_kl: 1.865746, loss_recon: 0.614520, loss_pred: 0.374925
iteration 155: loss: 1.890517, loss_kl: 1.185958, loss_recon: 0.604693, loss_pred: 0.099866
iteration 156: loss: 2.026383, loss_kl: 1.210332, loss_recon: 0.632162, loss_pred: 0.183888
iteration 157: loss: 1.688896, loss_kl: 0.911203, loss_recon: 0.627504, loss_pred: 0.150190
iteration 158: loss: 2.446151, loss_kl: 1.657493, loss_recon: 0.620482, loss_pred: 0.168177
iteration 159: loss: 1.679076, loss_kl: 0.975911, loss_recon: 0.620716, loss_pred: 0.082449
iteration 160: loss: 1.973495, loss_kl: 1.178313, loss_recon: 0.634534, loss_pred: 0.160647
 16%|████▎                      | 16/100 [1:37:11<8:28:37, 363.31s/it]iteration 161: loss: 2.389138, loss_kl: 1.711836, loss_recon: 0.621770, loss_pred: 0.055532
iteration 162: loss: 2.434510, loss_kl: 1.697596, loss_recon: 0.610388, loss_pred: 0.126526
iteration 163: loss: 2.082913, loss_kl: 1.384925, loss_recon: 0.609299, loss_pred: 0.088689
iteration 164: loss: 2.207854, loss_kl: 1.530279, loss_recon: 0.613386, loss_pred: 0.064189
iteration 165: loss: 2.200292, loss_kl: 1.515553, loss_recon: 0.616373, loss_pred: 0.068367
iteration 166: loss: 1.341845, loss_kl: 0.662592, loss_recon: 0.617011, loss_pred: 0.062242
iteration 167: loss: 1.727864, loss_kl: 1.018437, loss_recon: 0.615045, loss_pred: 0.094382
iteration 168: loss: 2.211024, loss_kl: 1.529402, loss_recon: 0.616268, loss_pred: 0.065354
iteration 169: loss: 1.593320, loss_kl: 0.904437, loss_recon: 0.626804, loss_pred: 0.062078
iteration 170: loss: 1.824790, loss_kl: 1.081988, loss_recon: 0.632983, loss_pred: 0.109818
 17%|████▌                      | 17/100 [1:43:15<8:22:42, 363.40s/it]iteration 171: loss: 1.821131, loss_kl: 1.122488, loss_recon: 0.610714, loss_pred: 0.087929
iteration 172: loss: 1.896395, loss_kl: 1.155850, loss_recon: 0.627203, loss_pred: 0.113341
iteration 173: loss: 1.874656, loss_kl: 1.176353, loss_recon: 0.623410, loss_pred: 0.074893
iteration 174: loss: 1.651680, loss_kl: 0.933085, loss_recon: 0.617892, loss_pred: 0.100703
iteration 175: loss: 1.608293, loss_kl: 0.912950, loss_recon: 0.615983, loss_pred: 0.079359
iteration 176: loss: 1.704295, loss_kl: 1.004574, loss_recon: 0.607817, loss_pred: 0.091904
iteration 177: loss: 1.511281, loss_kl: 0.815640, loss_recon: 0.615622, loss_pred: 0.080020
iteration 178: loss: 1.832843, loss_kl: 1.089007, loss_recon: 0.620491, loss_pred: 0.123345
iteration 179: loss: 1.856668, loss_kl: 1.155080, loss_recon: 0.610579, loss_pred: 0.091009
iteration 180: loss: 1.481966, loss_kl: 0.732893, loss_recon: 0.630011, loss_pred: 0.119062
 18%|████▊                      | 18/100 [1:49:18<8:16:34, 363.35s/it]iteration 181: loss: 1.673851, loss_kl: 0.985984, loss_recon: 0.616591, loss_pred: 0.071276
iteration 182: loss: 2.135813, loss_kl: 1.431127, loss_recon: 0.618897, loss_pred: 0.085790
iteration 183: loss: 1.775311, loss_kl: 1.090400, loss_recon: 0.618450, loss_pred: 0.066462
iteration 184: loss: 1.949989, loss_kl: 1.280653, loss_recon: 0.599551, loss_pred: 0.069785
iteration 185: loss: 1.301107, loss_kl: 0.626225, loss_recon: 0.611387, loss_pred: 0.063495
iteration 186: loss: 1.728145, loss_kl: 1.033395, loss_recon: 0.616853, loss_pred: 0.077897
iteration 187: loss: 1.980554, loss_kl: 1.271203, loss_recon: 0.615673, loss_pred: 0.093679
iteration 188: loss: 1.507626, loss_kl: 0.781850, loss_recon: 0.618973, loss_pred: 0.106802
iteration 189: loss: 1.456905, loss_kl: 0.752879, loss_recon: 0.619843, loss_pred: 0.084183
iteration 190: loss: 1.723140, loss_kl: 1.018966, loss_recon: 0.642787, loss_pred: 0.061388
 19%|█████▏                     | 19/100 [1:55:21<8:10:27, 363.31s/it]iteration 191: loss: 2.061896, loss_kl: 1.350852, loss_recon: 0.627664, loss_pred: 0.083380
iteration 192: loss: 2.331408, loss_kl: 1.645838, loss_recon: 0.625506, loss_pred: 0.060064
iteration 193: loss: 2.023637, loss_kl: 1.332683, loss_recon: 0.623000, loss_pred: 0.067953
iteration 194: loss: 1.488961, loss_kl: 0.812240, loss_recon: 0.614676, loss_pred: 0.062044
iteration 195: loss: 2.201619, loss_kl: 1.477339, loss_recon: 0.611277, loss_pred: 0.113003
iteration 196: loss: 1.654466, loss_kl: 0.986055, loss_recon: 0.605754, loss_pred: 0.062656
iteration 197: loss: 1.513107, loss_kl: 0.826088, loss_recon: 0.626231, loss_pred: 0.060788
iteration 198: loss: 1.969723, loss_kl: 1.279307, loss_recon: 0.620512, loss_pred: 0.069904
iteration 199: loss: 1.961448, loss_kl: 1.285204, loss_recon: 0.604150, loss_pred: 0.072094
iteration 200: loss: 1.956269, loss_kl: 1.182732, loss_recon: 0.619563, loss_pred: 0.153973
 20%|█████▍                     | 20/100 [2:01:25<8:04:40, 363.50s/it]iteration 201: loss: 1.861162, loss_kl: 1.167947, loss_recon: 0.602454, loss_pred: 0.090761
iteration 202: loss: 1.679457, loss_kl: 0.964858, loss_recon: 0.624467, loss_pred: 0.090132
iteration 203: loss: 2.200260, loss_kl: 1.428555, loss_recon: 0.628651, loss_pred: 0.143054
iteration 204: loss: 2.120520, loss_kl: 1.263155, loss_recon: 0.628515, loss_pred: 0.228850
iteration 205: loss: 1.788005, loss_kl: 1.067898, loss_recon: 0.617370, loss_pred: 0.102737
iteration 206: loss: 2.032742, loss_kl: 1.262706, loss_recon: 0.609561, loss_pred: 0.160475
iteration 207: loss: 1.831798, loss_kl: 1.069442, loss_recon: 0.620025, loss_pred: 0.142331
iteration 208: loss: 2.242062, loss_kl: 1.339086, loss_recon: 0.611229, loss_pred: 0.291746
iteration 209: loss: 1.353404, loss_kl: 0.631942, loss_recon: 0.614510, loss_pred: 0.106952
iteration 210: loss: 1.753036, loss_kl: 0.992018, loss_recon: 0.612705, loss_pred: 0.148313
 21%|█████▋                     | 21/100 [2:07:28<7:58:21, 363.31s/it]iteration 211: loss: 1.634007, loss_kl: 0.892276, loss_recon: 0.618865, loss_pred: 0.122866
iteration 212: loss: 1.767338, loss_kl: 1.000935, loss_recon: 0.619400, loss_pred: 0.147003
iteration 213: loss: 1.684019, loss_kl: 0.941871, loss_recon: 0.614960, loss_pred: 0.127188
iteration 214: loss: 1.791127, loss_kl: 1.047073, loss_recon: 0.614374, loss_pred: 0.129680
iteration 215: loss: 1.631007, loss_kl: 0.770157, loss_recon: 0.620220, loss_pred: 0.240630
iteration 216: loss: 1.569299, loss_kl: 0.799160, loss_recon: 0.613957, loss_pred: 0.156183
iteration 217: loss: 1.610548, loss_kl: 0.795592, loss_recon: 0.612296, loss_pred: 0.202660
iteration 218: loss: 2.001225, loss_kl: 1.279004, loss_recon: 0.614687, loss_pred: 0.107534
iteration 219: loss: 1.875537, loss_kl: 1.079301, loss_recon: 0.619281, loss_pred: 0.176955
iteration 220: loss: 1.380641, loss_kl: 0.677014, loss_recon: 0.615702, loss_pred: 0.087925
 22%|█████▉                     | 22/100 [2:13:32<7:52:25, 363.41s/it]iteration 221: loss: 1.959889, loss_kl: 1.235497, loss_recon: 0.611285, loss_pred: 0.113107
iteration 222: loss: 2.297496, loss_kl: 1.569736, loss_recon: 0.612688, loss_pred: 0.115072
iteration 223: loss: 1.711403, loss_kl: 0.996489, loss_recon: 0.616397, loss_pred: 0.098518
iteration 224: loss: 1.826556, loss_kl: 1.151692, loss_recon: 0.608947, loss_pred: 0.065918
iteration 225: loss: 1.614101, loss_kl: 0.910365, loss_recon: 0.619247, loss_pred: 0.084489
iteration 226: loss: 1.949212, loss_kl: 1.231410, loss_recon: 0.619475, loss_pred: 0.098327
iteration 227: loss: 2.497153, loss_kl: 1.814123, loss_recon: 0.615794, loss_pred: 0.067236
iteration 228: loss: 1.746918, loss_kl: 1.071162, loss_recon: 0.610424, loss_pred: 0.065332
iteration 229: loss: 1.568074, loss_kl: 0.902286, loss_recon: 0.630084, loss_pred: 0.035703
iteration 230: loss: 1.750274, loss_kl: 0.999521, loss_recon: 0.601189, loss_pred: 0.149565
 23%|██████▏                    | 23/100 [2:19:35<7:46:24, 363.44s/it]iteration 231: loss: 1.671563, loss_kl: 0.935138, loss_recon: 0.612105, loss_pred: 0.124320
iteration 232: loss: 1.654487, loss_kl: 0.997676, loss_recon: 0.604037, loss_pred: 0.052775
iteration 233: loss: 1.730278, loss_kl: 1.050342, loss_recon: 0.616716, loss_pred: 0.063221
iteration 234: loss: 1.844782, loss_kl: 1.139121, loss_recon: 0.612276, loss_pred: 0.093386
iteration 235: loss: 1.786751, loss_kl: 1.117633, loss_recon: 0.620388, loss_pred: 0.048730
iteration 236: loss: 1.846137, loss_kl: 1.179671, loss_recon: 0.615155, loss_pred: 0.051311
iteration 237: loss: 1.835958, loss_kl: 1.174672, loss_recon: 0.613410, loss_pred: 0.047876
iteration 238: loss: 1.605944, loss_kl: 0.939458, loss_recon: 0.631422, loss_pred: 0.035065
iteration 239: loss: 1.522363, loss_kl: 0.841607, loss_recon: 0.614312, loss_pred: 0.066444
iteration 240: loss: 2.271727, loss_kl: 1.551199, loss_recon: 0.627752, loss_pred: 0.092775
 24%|██████▍                    | 24/100 [2:25:38<7:40:12, 363.32s/it]iteration 241: loss: 2.097252, loss_kl: 1.346462, loss_recon: 0.618265, loss_pred: 0.132525
iteration 242: loss: 1.995224, loss_kl: 1.305080, loss_recon: 0.610853, loss_pred: 0.079292
iteration 243: loss: 1.756062, loss_kl: 1.035380, loss_recon: 0.625342, loss_pred: 0.095340
iteration 244: loss: 2.135320, loss_kl: 1.455434, loss_recon: 0.615380, loss_pred: 0.064506
iteration 245: loss: 1.771745, loss_kl: 1.063288, loss_recon: 0.619607, loss_pred: 0.088850
iteration 246: loss: 1.527187, loss_kl: 0.873628, loss_recon: 0.606180, loss_pred: 0.047379
iteration 247: loss: 2.059962, loss_kl: 1.395554, loss_recon: 0.613489, loss_pred: 0.050918
iteration 248: loss: 2.287122, loss_kl: 1.570944, loss_recon: 0.619713, loss_pred: 0.096464
iteration 249: loss: 1.505395, loss_kl: 0.825792, loss_recon: 0.608592, loss_pred: 0.071010
iteration 250: loss: 1.763926, loss_kl: 1.057206, loss_recon: 0.630098, loss_pred: 0.076621
 25%|██████▊                    | 25/100 [2:31:42<7:34:12, 363.37s/it]iteration 251: loss: 0.624341, loss_kl: 0.865677, loss_recon: 0.621861, loss_pred: 0.137306
iteration 252: loss: 0.611875, loss_kl: 1.680256, loss_recon: 0.607468, loss_pred: 0.102077
iteration 253: loss: 0.618591, loss_kl: 1.977790, loss_recon: 0.613399, loss_pred: 0.122200
iteration 254: loss: 0.613766, loss_kl: 3.033561, loss_recon: 0.605831, loss_pred: 0.175365
iteration 255: loss: 0.627439, loss_kl: 3.177715, loss_recon: 0.619123, loss_pred: 0.185316
iteration 256: loss: 0.623759, loss_kl: 4.438061, loss_recon: 0.611907, loss_pred: 0.355229
iteration 257: loss: 0.643443, loss_kl: 5.860409, loss_recon: 0.628147, loss_pred: 0.325876
iteration 258: loss: 0.633370, loss_kl: 7.045355, loss_recon: 0.615092, loss_pred: 0.346531
iteration 259: loss: 0.626309, loss_kl: 7.710581, loss_recon: 0.606007, loss_pred: 0.500064
iteration 260: loss: 0.652393, loss_kl: 8.297518, loss_recon: 0.630965, loss_pred: 0.368500
 26%|███████                    | 26/100 [2:37:45<7:27:58, 363.23s/it]iteration 261: loss: 0.664438, loss_kl: 7.895649, loss_recon: 0.610341, loss_pred: 0.514713
iteration 262: loss: 0.678909, loss_kl: 8.335909, loss_recon: 0.622416, loss_pred: 0.447100
iteration 263: loss: 0.671190, loss_kl: 8.374811, loss_recon: 0.613856, loss_pred: 0.538898
iteration 264: loss: 0.668773, loss_kl: 9.023336, loss_recon: 0.607537, loss_pred: 0.497152
iteration 265: loss: 0.668417, loss_kl: 9.482937, loss_recon: 0.603703, loss_pred: 0.578080
iteration 266: loss: 0.695238, loss_kl: 10.110595, loss_recon: 0.627935, loss_pred: 0.353016
iteration 267: loss: 0.669393, loss_kl: 7.986636, loss_recon: 0.615605, loss_pred: 0.375806
iteration 268: loss: 0.675651, loss_kl: 9.717873, loss_recon: 0.610749, loss_pred: 0.372344
iteration 269: loss: 0.659017, loss_kl: 9.165013, loss_recon: 0.597205, loss_pred: 0.444943
iteration 270: loss: 0.686613, loss_kl: 8.949047, loss_recon: 0.627134, loss_pred: 0.298167
 27%|███████▎                   | 27/100 [2:43:49<7:22:12, 363.46s/it]iteration 271: loss: 0.748028, loss_kl: 8.361298, loss_recon: 0.603794, loss_pred: 0.313708
iteration 272: loss: 0.764442, loss_kl: 8.235941, loss_recon: 0.624582, loss_pred: 0.175957
iteration 273: loss: 0.751589, loss_kl: 9.064338, loss_recon: 0.596430, loss_pred: 0.267789
iteration 274: loss: 0.738935, loss_kl: 7.899045, loss_recon: 0.603902, loss_pred: 0.222584
iteration 275: loss: 0.728783, loss_kl: 6.704911, loss_recon: 0.614262, loss_pred: 0.183062
iteration 276: loss: 0.728842, loss_kl: 6.640536, loss_recon: 0.615750, loss_pred: 0.161441
iteration 277: loss: 0.704113, loss_kl: 5.368680, loss_recon: 0.612275, loss_pred: 0.154969
iteration 278: loss: 0.704417, loss_kl: 5.036128, loss_recon: 0.618289, loss_pred: 0.144102
iteration 279: loss: 0.701936, loss_kl: 4.545990, loss_recon: 0.623404, loss_pred: 0.177363
iteration 280: loss: 0.693495, loss_kl: 4.826680, loss_recon: 0.609218, loss_pred: 0.242210
 28%|███████▌                   | 28/100 [2:49:53<7:16:23, 363.67s/it]iteration 281: loss: 0.782875, loss_kl: 4.044612, loss_recon: 0.605827, loss_pred: 0.141935
iteration 282: loss: 0.736728, loss_kl: 2.632683, loss_recon: 0.619255, loss_pred: 0.145123
iteration 283: loss: 0.761108, loss_kl: 3.456317, loss_recon: 0.610467, loss_pred: 0.105786
iteration 284: loss: 0.703627, loss_kl: 2.174714, loss_recon: 0.606350, loss_pred: 0.125535
iteration 285: loss: 0.695592, loss_kl: 1.776893, loss_recon: 0.614854, loss_pred: 0.132262
iteration 286: loss: 0.703407, loss_kl: 2.059545, loss_recon: 0.611563, loss_pred: 0.112217
iteration 287: loss: 0.677802, loss_kl: 1.515748, loss_recon: 0.609473, loss_pred: 0.099974
iteration 288: loss: 0.654448, loss_kl: 0.897240, loss_recon: 0.611601, loss_pred: 0.115931
iteration 289: loss: 0.661213, loss_kl: 0.972422, loss_recon: 0.617051, loss_pred: 0.071832
iteration 290: loss: 0.657131, loss_kl: 0.636666, loss_recon: 0.625805, loss_pred: 0.104061
 29%|███████▊                   | 29/100 [2:55:57<7:10:26, 363.75s/it]iteration 291: loss: 0.706541, loss_kl: 0.820821, loss_recon: 0.615594, loss_pred: 0.058736
iteration 292: loss: 0.682272, loss_kl: 0.745332, loss_recon: 0.598779, loss_pred: 0.062143
iteration 293: loss: 0.719812, loss_kl: 0.937385, loss_recon: 0.616452, loss_pred: 0.062227
iteration 294: loss: 0.694228, loss_kl: 0.721253, loss_recon: 0.614092, loss_pred: 0.053749
iteration 295: loss: 0.702476, loss_kl: 0.878878, loss_recon: 0.607419, loss_pred: 0.040428
iteration 296: loss: 0.684777, loss_kl: 0.624496, loss_recon: 0.613811, loss_pred: 0.061829
iteration 297: loss: 0.660520, loss_kl: 0.417889, loss_recon: 0.612610, loss_pred: 0.045452
iteration 298: loss: 0.665714, loss_kl: 0.521638, loss_recon: 0.606865, loss_pred: 0.047500
iteration 299: loss: 0.725876, loss_kl: 1.058532, loss_recon: 0.613077, loss_pred: 0.032356
iteration 300: loss: 0.709257, loss_kl: 0.774347, loss_recon: 0.616282, loss_pred: 0.124827
 30%|████████                   | 30/100 [3:02:00<7:04:15, 363.64s/it]iteration 301: loss: 0.760721, loss_kl: 0.630712, loss_recon: 0.605461, loss_pred: 0.040030
iteration 302: loss: 0.715010, loss_kl: 0.425609, loss_recon: 0.607395, loss_pred: 0.039302
iteration 303: loss: 0.771249, loss_kl: 0.627100, loss_recon: 0.613384, loss_pred: 0.054897
iteration 304: loss: 0.827074, loss_kl: 0.845215, loss_recon: 0.622310, loss_pred: 0.039387
iteration 305: loss: 0.842312, loss_kl: 0.980179, loss_recon: 0.606095, loss_pred: 0.040308
iteration 306: loss: 0.818981, loss_kl: 0.827726, loss_recon: 0.619188, loss_pred: 0.035401
iteration 307: loss: 0.836921, loss_kl: 0.917059, loss_recon: 0.616785, loss_pred: 0.033955
iteration 308: loss: 0.832816, loss_kl: 0.890239, loss_recon: 0.616809, loss_pred: 0.042940
iteration 309: loss: 0.813460, loss_kl: 0.826768, loss_recon: 0.614561, loss_pred: 0.032498
iteration 310: loss: 0.741455, loss_kl: 0.575409, loss_recon: 0.597926, loss_pred: 0.044651
 31%|████████▎                  | 31/100 [3:08:04<6:58:04, 363.55s/it]iteration 311: loss: 1.126224, loss_kl: 1.071086, loss_recon: 0.622867, loss_pred: 0.072164
iteration 312: loss: 1.096638, loss_kl: 1.042671, loss_recon: 0.615242, loss_pred: 0.050700
iteration 313: loss: 0.964346, loss_kl: 0.773813, loss_recon: 0.608660, loss_pred: 0.034038
iteration 314: loss: 1.046016, loss_kl: 0.932684, loss_recon: 0.604324, loss_pred: 0.070508
iteration 315: loss: 0.939057, loss_kl: 0.715403, loss_recon: 0.609877, loss_pred: 0.032245
iteration 316: loss: 1.073946, loss_kl: 0.990482, loss_recon: 0.610527, loss_pred: 0.062058
iteration 317: loss: 1.087921, loss_kl: 1.042824, loss_recon: 0.614264, loss_pred: 0.032971
iteration 318: loss: 1.060364, loss_kl: 0.991662, loss_recon: 0.610250, loss_pred: 0.030659
iteration 319: loss: 0.958435, loss_kl: 0.690405, loss_recon: 0.615719, loss_pred: 0.087990
iteration 320: loss: 0.944047, loss_kl: 0.642721, loss_recon: 0.622117, loss_pred: 0.088463
 32%|████████▋                  | 32/100 [3:14:07<6:51:51, 363.41s/it]iteration 321: loss: 1.393888, loss_kl: 1.099265, loss_recon: 0.619999, loss_pred: 0.051317
iteration 322: loss: 1.361010, loss_kl: 0.997710, loss_recon: 0.611658, loss_pred: 0.116391
iteration 323: loss: 1.272687, loss_kl: 0.951292, loss_recon: 0.611136, loss_pred: 0.032271
iteration 324: loss: 1.051583, loss_kl: 0.516712, loss_recon: 0.618232, loss_pred: 0.127574
iteration 325: loss: 1.146722, loss_kl: 0.686384, loss_recon: 0.607839, loss_pred: 0.114803
iteration 326: loss: 1.121073, loss_kl: 0.640841, loss_recon: 0.612620, loss_pred: 0.115103
iteration 327: loss: 1.072005, loss_kl: 0.584610, loss_recon: 0.604101, loss_pred: 0.111047
iteration 328: loss: 1.220670, loss_kl: 0.776599, loss_recon: 0.626980, loss_pred: 0.106070
iteration 329: loss: 1.061415, loss_kl: 0.555792, loss_recon: 0.610423, loss_pred: 0.114722
iteration 330: loss: 1.328357, loss_kl: 1.016964, loss_recon: 0.608188, loss_pred: 0.053750
 33%|████████▉                  | 33/100 [3:20:10<6:45:51, 363.46s/it]iteration 331: loss: 1.608583, loss_kl: 0.954912, loss_recon: 0.605257, loss_pred: 0.235408
iteration 332: loss: 1.282845, loss_kl: 0.719070, loss_recon: 0.618016, loss_pred: 0.069665
iteration 333: loss: 1.206320, loss_kl: 0.540999, loss_recon: 0.628697, loss_pred: 0.144279
iteration 334: loss: 1.358774, loss_kl: 0.665841, loss_recon: 0.608837, loss_pred: 0.223865
iteration 335: loss: 1.414873, loss_kl: 0.846749, loss_recon: 0.628069, loss_pred: 0.086695
iteration 336: loss: 1.364978, loss_kl: 0.769510, loss_recon: 0.606755, loss_pred: 0.130026
iteration 337: loss: 1.400380, loss_kl: 0.876604, loss_recon: 0.614186, loss_pred: 0.056117
iteration 338: loss: 1.444060, loss_kl: 0.904055, loss_recon: 0.599627, loss_pred: 0.097758
iteration 339: loss: 1.513820, loss_kl: 0.991376, loss_recon: 0.604944, loss_pred: 0.086890
iteration 340: loss: 1.205525, loss_kl: 0.471671, loss_recon: 0.668251, loss_pred: 0.165737
 34%|█████████▏                 | 34/100 [3:26:13<6:39:41, 363.35s/it]iteration 341: loss: 1.885236, loss_kl: 1.285024, loss_recon: 0.626528, loss_pred: 0.063508
iteration 342: loss: 2.483517, loss_kl: 1.917556, loss_recon: 0.613598, loss_pred: 0.085803
iteration 343: loss: 1.685483, loss_kl: 1.046221, loss_recon: 0.616629, loss_pred: 0.098907
iteration 344: loss: 1.568391, loss_kl: 1.001542, loss_recon: 0.597592, loss_pred: 0.038535
iteration 345: loss: 2.355220, loss_kl: 1.758073, loss_recon: 0.603683, loss_pred: 0.118455
iteration 346: loss: 1.815133, loss_kl: 1.219038, loss_recon: 0.621434, loss_pred: 0.059845
iteration 347: loss: 1.633535, loss_kl: 1.005290, loss_recon: 0.613728, loss_pred: 0.087291
iteration 348: loss: 1.313400, loss_kl: 0.674259, loss_recon: 0.621305, loss_pred: 0.067224
iteration 349: loss: 1.234203, loss_kl: 0.609271, loss_recon: 0.614170, loss_pred: 0.055008
iteration 350: loss: 1.268011, loss_kl: 0.615025, loss_recon: 0.597916, loss_pred: 0.102888
 35%|█████████▍                 | 35/100 [3:32:17<6:33:41, 363.40s/it]iteration 351: loss: 1.541710, loss_kl: 0.886288, loss_recon: 0.611064, loss_pred: 0.069787
iteration 352: loss: 1.909361, loss_kl: 1.281868, loss_recon: 0.626026, loss_pred: 0.036533
iteration 353: loss: 1.629726, loss_kl: 0.911126, loss_recon: 0.622587, loss_pred: 0.123532
iteration 354: loss: 1.789864, loss_kl: 1.052233, loss_recon: 0.605509, loss_pred: 0.164483
iteration 355: loss: 1.420023, loss_kl: 0.702546, loss_recon: 0.612597, loss_pred: 0.126942
iteration 356: loss: 1.710160, loss_kl: 1.031278, loss_recon: 0.614991, loss_pred: 0.093815
iteration 357: loss: 1.871511, loss_kl: 1.224839, loss_recon: 0.610832, loss_pred: 0.070286
iteration 358: loss: 1.614619, loss_kl: 0.917794, loss_recon: 0.622032, loss_pred: 0.101914
iteration 359: loss: 1.600551, loss_kl: 0.975460, loss_recon: 0.609152, loss_pred: 0.043028
iteration 360: loss: 0.967398, loss_kl: 0.297630, loss_recon: 0.603674, loss_pred: 0.076032
 36%|█████████▋                 | 36/100 [3:38:21<6:27:48, 363.57s/it]iteration 361: loss: 1.837496, loss_kl: 1.120453, loss_recon: 0.604120, loss_pred: 0.125826
iteration 362: loss: 1.635064, loss_kl: 0.972513, loss_recon: 0.607269, loss_pred: 0.066035
iteration 363: loss: 2.146315, loss_kl: 1.329691, loss_recon: 0.605651, loss_pred: 0.227091
iteration 364: loss: 1.932783, loss_kl: 1.272771, loss_recon: 0.608370, loss_pred: 0.065498
iteration 365: loss: 1.935702, loss_kl: 1.157228, loss_recon: 0.619927, loss_pred: 0.172313
iteration 366: loss: 1.601058, loss_kl: 0.921139, loss_recon: 0.611585, loss_pred: 0.078685
iteration 367: loss: 1.667024, loss_kl: 0.889081, loss_recon: 0.617633, loss_pred: 0.171288
iteration 368: loss: 1.562182, loss_kl: 0.878822, loss_recon: 0.632319, loss_pred: 0.060769
iteration 369: loss: 2.307930, loss_kl: 1.605394, loss_recon: 0.615533, loss_pred: 0.104709
iteration 370: loss: 1.806978, loss_kl: 1.142245, loss_recon: 0.618541, loss_pred: 0.058626
 37%|█████████▉                 | 37/100 [3:44:25<6:21:49, 363.64s/it]iteration 371: loss: 1.438049, loss_kl: 0.777417, loss_recon: 0.600354, loss_pred: 0.063634
iteration 372: loss: 1.318887, loss_kl: 0.647575, loss_recon: 0.607602, loss_pred: 0.066560
iteration 373: loss: 2.176980, loss_kl: 1.482096, loss_recon: 0.602869, loss_pred: 0.098320
iteration 374: loss: 1.837186, loss_kl: 1.170150, loss_recon: 0.619956, loss_pred: 0.051955
iteration 375: loss: 1.595653, loss_kl: 0.943903, loss_recon: 0.620202, loss_pred: 0.035456
iteration 376: loss: 1.805295, loss_kl: 1.154099, loss_recon: 0.612509, loss_pred: 0.043465
iteration 377: loss: 1.760016, loss_kl: 1.087868, loss_recon: 0.619646, loss_pred: 0.057070
iteration 378: loss: 2.342360, loss_kl: 1.667124, loss_recon: 0.614601, loss_pred: 0.067556
iteration 379: loss: 1.638488, loss_kl: 0.945142, loss_recon: 0.619396, loss_pred: 0.078032
iteration 380: loss: 2.389421, loss_kl: 1.695280, loss_recon: 0.638035, loss_pred: 0.063121
 38%|██████████▎                | 38/100 [3:50:27<6:15:25, 363.32s/it]iteration 381: loss: 1.639401, loss_kl: 0.871211, loss_recon: 0.609713, loss_pred: 0.158476
iteration 382: loss: 1.782741, loss_kl: 1.082444, loss_recon: 0.617235, loss_pred: 0.083062
iteration 383: loss: 1.799912, loss_kl: 1.133204, loss_recon: 0.609864, loss_pred: 0.056844
iteration 384: loss: 1.633074, loss_kl: 0.912719, loss_recon: 0.611224, loss_pred: 0.109131
iteration 385: loss: 1.762966, loss_kl: 1.092916, loss_recon: 0.602895, loss_pred: 0.067155
iteration 386: loss: 1.550577, loss_kl: 0.875827, loss_recon: 0.605612, loss_pred: 0.069139
iteration 387: loss: 1.758386, loss_kl: 1.096687, loss_recon: 0.620168, loss_pred: 0.041531
iteration 388: loss: 1.849905, loss_kl: 1.198675, loss_recon: 0.612882, loss_pred: 0.038348
iteration 389: loss: 1.988655, loss_kl: 1.330713, loss_recon: 0.617824, loss_pred: 0.040118
iteration 390: loss: 1.928789, loss_kl: 1.257358, loss_recon: 0.636826, loss_pred: 0.034604
 39%|██████████▌                | 39/100 [3:56:30<6:09:14, 363.19s/it]iteration 391: loss: 1.513100, loss_kl: 0.826008, loss_recon: 0.609836, loss_pred: 0.077257
iteration 392: loss: 1.923705, loss_kl: 1.215249, loss_recon: 0.616508, loss_pred: 0.091947
iteration 393: loss: 1.514892, loss_kl: 0.802561, loss_recon: 0.632098, loss_pred: 0.080233
iteration 394: loss: 1.880453, loss_kl: 1.223458, loss_recon: 0.604048, loss_pred: 0.052947
iteration 395: loss: 1.378250, loss_kl: 0.727292, loss_recon: 0.603541, loss_pred: 0.047417
iteration 396: loss: 1.308665, loss_kl: 0.637071, loss_recon: 0.619877, loss_pred: 0.051717
iteration 397: loss: 2.223764, loss_kl: 1.555672, loss_recon: 0.620484, loss_pred: 0.047608
iteration 398: loss: 1.789160, loss_kl: 1.118461, loss_recon: 0.608922, loss_pred: 0.061777
iteration 399: loss: 1.584902, loss_kl: 0.914540, loss_recon: 0.613878, loss_pred: 0.056484
iteration 400: loss: 0.990257, loss_kl: 0.295596, loss_recon: 0.599327, loss_pred: 0.095334
 40%|██████████▊                | 40/100 [4:02:33<6:03:09, 363.15s/it]iteration 401: loss: 1.727229, loss_kl: 1.068825, loss_recon: 0.624195, loss_pred: 0.034209
iteration 402: loss: 1.625020, loss_kl: 0.969766, loss_recon: 0.607799, loss_pred: 0.047455
iteration 403: loss: 1.708818, loss_kl: 1.061847, loss_recon: 0.614269, loss_pred: 0.032701
iteration 404: loss: 1.594613, loss_kl: 0.959880, loss_recon: 0.602032, loss_pred: 0.032702
iteration 405: loss: 1.638083, loss_kl: 0.935559, loss_recon: 0.622666, loss_pred: 0.079859
iteration 406: loss: 1.674763, loss_kl: 1.003566, loss_recon: 0.616850, loss_pred: 0.054347
iteration 407: loss: 1.566009, loss_kl: 0.848074, loss_recon: 0.605286, loss_pred: 0.112648
iteration 408: loss: 1.574495, loss_kl: 0.907454, loss_recon: 0.615591, loss_pred: 0.051451
iteration 409: loss: 1.645201, loss_kl: 0.951803, loss_recon: 0.611541, loss_pred: 0.081857
iteration 410: loss: 1.788844, loss_kl: 0.883951, loss_recon: 0.615326, loss_pred: 0.289567
 41%|███████████                | 41/100 [4:08:37<5:57:10, 363.23s/it]iteration 411: loss: 1.457511, loss_kl: 0.760318, loss_recon: 0.608492, loss_pred: 0.088701
iteration 412: loss: 1.763636, loss_kl: 1.063460, loss_recon: 0.613858, loss_pred: 0.086317
iteration 413: loss: 1.680205, loss_kl: 1.013161, loss_recon: 0.605714, loss_pred: 0.061331
iteration 414: loss: 1.974306, loss_kl: 1.261742, loss_recon: 0.611401, loss_pred: 0.101163
iteration 415: loss: 1.709932, loss_kl: 1.027164, loss_recon: 0.612488, loss_pred: 0.070280
iteration 416: loss: 1.752080, loss_kl: 1.086353, loss_recon: 0.622570, loss_pred: 0.043157
iteration 417: loss: 1.460472, loss_kl: 0.768770, loss_recon: 0.620657, loss_pred: 0.071045
iteration 418: loss: 1.913705, loss_kl: 1.261648, loss_recon: 0.612393, loss_pred: 0.039664
iteration 419: loss: 2.079693, loss_kl: 1.377115, loss_recon: 0.619213, loss_pred: 0.083366
iteration 420: loss: 1.794102, loss_kl: 1.120594, loss_recon: 0.618262, loss_pred: 0.055246
 42%|███████████▎               | 42/100 [4:14:40<5:51:14, 363.35s/it]iteration 421: loss: 1.681160, loss_kl: 1.028185, loss_recon: 0.614248, loss_pred: 0.038727
iteration 422: loss: 2.144789, loss_kl: 1.472271, loss_recon: 0.604415, loss_pred: 0.068102
iteration 423: loss: 1.671311, loss_kl: 1.000117, loss_recon: 0.605684, loss_pred: 0.065510
iteration 424: loss: 1.686363, loss_kl: 0.980556, loss_recon: 0.621842, loss_pred: 0.083966
iteration 425: loss: 1.832135, loss_kl: 1.149904, loss_recon: 0.615459, loss_pred: 0.066773
iteration 426: loss: 1.718689, loss_kl: 1.068360, loss_recon: 0.606605, loss_pred: 0.043724
iteration 427: loss: 1.496400, loss_kl: 0.820338, loss_recon: 0.617584, loss_pred: 0.058478
iteration 428: loss: 1.954717, loss_kl: 1.263136, loss_recon: 0.611956, loss_pred: 0.079625
iteration 429: loss: 1.410807, loss_kl: 0.731199, loss_recon: 0.621460, loss_pred: 0.058148
iteration 430: loss: 1.585993, loss_kl: 0.919957, loss_recon: 0.613342, loss_pred: 0.052694
 43%|███████████▌               | 43/100 [4:20:44<5:45:12, 363.38s/it]iteration 431: loss: 1.506410, loss_kl: 0.851037, loss_recon: 0.617778, loss_pred: 0.037595
iteration 432: loss: 1.837354, loss_kl: 1.179699, loss_recon: 0.613589, loss_pred: 0.044066
iteration 433: loss: 2.020741, loss_kl: 1.346804, loss_recon: 0.621559, loss_pred: 0.052378
iteration 434: loss: 1.288003, loss_kl: 0.631271, loss_recon: 0.616859, loss_pred: 0.039873
iteration 435: loss: 1.619236, loss_kl: 0.975496, loss_recon: 0.598645, loss_pred: 0.045094
iteration 436: loss: 1.891987, loss_kl: 1.211892, loss_recon: 0.603830, loss_pred: 0.076265
iteration 437: loss: 1.836733, loss_kl: 1.164042, loss_recon: 0.601111, loss_pred: 0.071581
iteration 438: loss: 1.415006, loss_kl: 0.753391, loss_recon: 0.626196, loss_pred: 0.035419
iteration 439: loss: 1.559062, loss_kl: 0.848726, loss_recon: 0.605450, loss_pred: 0.104886
iteration 440: loss: 1.848462, loss_kl: 1.126517, loss_recon: 0.669785, loss_pred: 0.052161
 44%|███████████▉               | 44/100 [4:26:47<5:39:16, 363.50s/it]iteration 441: loss: 1.703443, loss_kl: 1.009410, loss_recon: 0.605568, loss_pred: 0.088466
iteration 442: loss: 1.786387, loss_kl: 1.125049, loss_recon: 0.611415, loss_pred: 0.049923
iteration 443: loss: 2.133482, loss_kl: 1.476203, loss_recon: 0.608888, loss_pred: 0.048391
iteration 444: loss: 1.746590, loss_kl: 1.091216, loss_recon: 0.606725, loss_pred: 0.048650
iteration 445: loss: 1.924417, loss_kl: 1.273745, loss_recon: 0.606259, loss_pred: 0.044414
iteration 446: loss: 1.497040, loss_kl: 0.829390, loss_recon: 0.604601, loss_pred: 0.063048
iteration 447: loss: 1.643286, loss_kl: 0.963946, loss_recon: 0.620190, loss_pred: 0.059151
iteration 448: loss: 1.960276, loss_kl: 1.309254, loss_recon: 0.616895, loss_pred: 0.034128
iteration 449: loss: 1.329701, loss_kl: 0.648508, loss_recon: 0.620664, loss_pred: 0.060529
iteration 450: loss: 1.766695, loss_kl: 1.072075, loss_recon: 0.655675, loss_pred: 0.038944
 45%|████████████▏              | 45/100 [4:32:51<5:33:11, 363.48s/it]iteration 451: loss: 1.288808, loss_kl: 0.629721, loss_recon: 0.604695, loss_pred: 0.054392
iteration 452: loss: 1.873448, loss_kl: 1.190616, loss_recon: 0.616530, loss_pred: 0.066301
iteration 453: loss: 1.575803, loss_kl: 0.888255, loss_recon: 0.620231, loss_pred: 0.067317
iteration 454: loss: 1.830747, loss_kl: 1.172241, loss_recon: 0.622179, loss_pred: 0.036327
iteration 455: loss: 1.277355, loss_kl: 0.612561, loss_recon: 0.610052, loss_pred: 0.054743
iteration 456: loss: 1.268724, loss_kl: 0.574330, loss_recon: 0.619781, loss_pred: 0.074613
iteration 457: loss: 1.298403, loss_kl: 0.624726, loss_recon: 0.617015, loss_pred: 0.056663
iteration 458: loss: 1.615513, loss_kl: 0.902308, loss_recon: 0.607822, loss_pred: 0.105384
iteration 459: loss: 1.543391, loss_kl: 0.860710, loss_recon: 0.605308, loss_pred: 0.077373
iteration 460: loss: 1.221789, loss_kl: 0.526025, loss_recon: 0.618539, loss_pred: 0.077225
 46%|████████████▍              | 46/100 [4:38:54<5:27:05, 363.44s/it]iteration 461: loss: 1.832905, loss_kl: 1.120804, loss_recon: 0.600289, loss_pred: 0.111812
iteration 462: loss: 1.821845, loss_kl: 1.153102, loss_recon: 0.621233, loss_pred: 0.047510
iteration 463: loss: 2.010345, loss_kl: 1.334512, loss_recon: 0.606132, loss_pred: 0.069701
iteration 464: loss: 1.693778, loss_kl: 0.998018, loss_recon: 0.633848, loss_pred: 0.061912
iteration 465: loss: 1.622148, loss_kl: 0.926471, loss_recon: 0.616788, loss_pred: 0.078889
iteration 466: loss: 1.808681, loss_kl: 1.144336, loss_recon: 0.597162, loss_pred: 0.067182
iteration 467: loss: 1.832645, loss_kl: 1.151080, loss_recon: 0.612306, loss_pred: 0.069259
iteration 468: loss: 1.662269, loss_kl: 0.998378, loss_recon: 0.615050, loss_pred: 0.048840
iteration 469: loss: 1.777174, loss_kl: 1.119812, loss_recon: 0.614027, loss_pred: 0.043335
iteration 470: loss: 1.099903, loss_kl: 0.388305, loss_recon: 0.629360, loss_pred: 0.082238
 47%|████████████▋              | 47/100 [4:44:58<5:21:00, 363.40s/it]iteration 471: loss: 1.790654, loss_kl: 1.054951, loss_recon: 0.608469, loss_pred: 0.127234
iteration 472: loss: 1.539562, loss_kl: 0.845284, loss_recon: 0.620211, loss_pred: 0.074068
iteration 473: loss: 1.157648, loss_kl: 0.497441, loss_recon: 0.607122, loss_pred: 0.053084
iteration 474: loss: 1.777039, loss_kl: 1.040492, loss_recon: 0.616870, loss_pred: 0.119677
iteration 475: loss: 1.552921, loss_kl: 0.873221, loss_recon: 0.601604, loss_pred: 0.078095
iteration 476: loss: 1.592740, loss_kl: 0.875977, loss_recon: 0.627161, loss_pred: 0.089602
iteration 477: loss: 1.624850, loss_kl: 0.906045, loss_recon: 0.609280, loss_pred: 0.109525
iteration 478: loss: 1.266872, loss_kl: 0.592733, loss_recon: 0.610427, loss_pred: 0.063712
iteration 479: loss: 1.560547, loss_kl: 0.839376, loss_recon: 0.613813, loss_pred: 0.107358
iteration 480: loss: 1.601349, loss_kl: 0.753092, loss_recon: 0.615335, loss_pred: 0.232922
 48%|████████████▉              | 48/100 [4:51:01<5:14:57, 363.41s/it]iteration 481: loss: 1.687849, loss_kl: 1.017276, loss_recon: 0.608988, loss_pred: 0.061586
iteration 482: loss: 1.775338, loss_kl: 1.035048, loss_recon: 0.613457, loss_pred: 0.126832
iteration 483: loss: 2.255556, loss_kl: 1.516220, loss_recon: 0.612511, loss_pred: 0.126825
iteration 484: loss: 1.701448, loss_kl: 0.968407, loss_recon: 0.624587, loss_pred: 0.108454
iteration 485: loss: 1.681644, loss_kl: 1.009538, loss_recon: 0.617272, loss_pred: 0.054834
iteration 486: loss: 1.960537, loss_kl: 1.164978, loss_recon: 0.612867, loss_pred: 0.182693
iteration 487: loss: 1.790989, loss_kl: 1.107224, loss_recon: 0.617576, loss_pred: 0.066189
iteration 488: loss: 1.695588, loss_kl: 0.966186, loss_recon: 0.599819, loss_pred: 0.129584
iteration 489: loss: 1.200826, loss_kl: 0.462955, loss_recon: 0.621343, loss_pred: 0.116528
iteration 490: loss: 1.314824, loss_kl: 0.649344, loss_recon: 0.620429, loss_pred: 0.045052
 49%|█████████████▏             | 49/100 [4:57:04<5:08:46, 363.27s/it]iteration 491: loss: 1.880344, loss_kl: 1.143927, loss_recon: 0.614360, loss_pred: 0.122056
iteration 492: loss: 2.010710, loss_kl: 1.327915, loss_recon: 0.606359, loss_pred: 0.076436
iteration 493: loss: 1.727277, loss_kl: 1.045907, loss_recon: 0.620521, loss_pred: 0.060850
iteration 494: loss: 1.566198, loss_kl: 0.912035, loss_recon: 0.601183, loss_pred: 0.052979
iteration 495: loss: 1.521803, loss_kl: 0.852746, loss_recon: 0.601268, loss_pred: 0.067790
iteration 496: loss: 1.877873, loss_kl: 1.211583, loss_recon: 0.614931, loss_pred: 0.051358
iteration 497: loss: 1.641848, loss_kl: 0.976259, loss_recon: 0.614909, loss_pred: 0.050680
iteration 498: loss: 1.288586, loss_kl: 0.626525, loss_recon: 0.611519, loss_pred: 0.050542
iteration 499: loss: 1.577892, loss_kl: 0.903052, loss_recon: 0.621786, loss_pred: 0.053054
iteration 500: loss: 1.748451, loss_kl: 1.036713, loss_recon: 0.634263, loss_pred: 0.077475
 50%|█████████████▌             | 50/100 [5:03:07<5:02:46, 363.33s/it]iteration 501: loss: 0.607085, loss_kl: 0.673857, loss_recon: 0.605152, loss_pred: 0.107998
iteration 502: loss: 0.627952, loss_kl: 1.456611, loss_recon: 0.623566, loss_pred: 0.317274
iteration 503: loss: 0.629014, loss_kl: 2.433091, loss_recon: 0.621348, loss_pred: 0.667056
iteration 504: loss: 0.620934, loss_kl: 3.106673, loss_recon: 0.610846, loss_pred: 0.973443
iteration 505: loss: 0.624928, loss_kl: 3.275486, loss_recon: 0.613798, loss_pred: 1.225761
iteration 506: loss: 0.617712, loss_kl: 3.661032, loss_recon: 0.605292, loss_pred: 1.361995
iteration 507: loss: 0.625630, loss_kl: 4.824841, loss_recon: 0.609591, loss_pred: 1.661659
iteration 508: loss: 0.645469, loss_kl: 5.148430, loss_recon: 0.627517, loss_pred: 2.111622
iteration 509: loss: 0.630500, loss_kl: 6.796104, loss_recon: 0.609031, loss_pred: 1.886690
iteration 510: loss: 0.643990, loss_kl: 7.241301, loss_recon: 0.620634, loss_pred: 2.204641
 51%|█████████████▊             | 51/100 [5:09:10<4:56:38, 363.24s/it]iteration 511: loss: 0.677313, loss_kl: 8.363408, loss_recon: 0.610981, loss_pred: 1.949255
iteration 512: loss: 0.686764, loss_kl: 8.172665, loss_recon: 0.620455, loss_pred: 2.136320
iteration 513: loss: 0.679674, loss_kl: 8.671241, loss_recon: 0.613371, loss_pred: 1.636933
iteration 514: loss: 0.681635, loss_kl: 8.564621, loss_recon: 0.616863, loss_pred: 1.505480
iteration 515: loss: 0.680427, loss_kl: 8.252779, loss_recon: 0.619137, loss_pred: 1.275999
iteration 516: loss: 0.668345, loss_kl: 8.491557, loss_recon: 0.607110, loss_pred: 1.028671
iteration 517: loss: 0.670980, loss_kl: 7.273071, loss_recon: 0.619097, loss_pred: 0.793091
iteration 518: loss: 0.660020, loss_kl: 8.008951, loss_recon: 0.604302, loss_pred: 0.653457
iteration 519: loss: 0.664291, loss_kl: 8.993809, loss_recon: 0.603306, loss_pred: 0.487615
iteration 520: loss: 0.684870, loss_kl: 8.985540, loss_recon: 0.623568, loss_pred: 0.544967
 52%|██████████████             | 52/100 [5:15:14<4:50:37, 363.29s/it]iteration 521: loss: 0.744371, loss_kl: 7.944554, loss_recon: 0.605269, loss_pred: 0.421758
iteration 522: loss: 0.751450, loss_kl: 8.062943, loss_recon: 0.612251, loss_pred: 0.309250
iteration 523: loss: 0.706902, loss_kl: 5.761237, loss_recon: 0.606090, loss_pred: 0.302141
iteration 524: loss: 0.707810, loss_kl: 5.914871, loss_recon: 0.603128, loss_pred: 0.381257
iteration 525: loss: 0.719192, loss_kl: 5.835795, loss_recon: 0.617150, loss_pred: 0.301525
iteration 526: loss: 0.699107, loss_kl: 4.736309, loss_recon: 0.615579, loss_pred: 0.287508
iteration 527: loss: 0.709126, loss_kl: 5.058489, loss_recon: 0.621358, loss_pred: 0.220377
iteration 528: loss: 0.674398, loss_kl: 4.148099, loss_recon: 0.600367, loss_pred: 0.304540
iteration 529: loss: 0.682054, loss_kl: 3.209126, loss_recon: 0.624864, loss_pred: 0.230547
iteration 530: loss: 0.671599, loss_kl: 2.616896, loss_recon: 0.623494, loss_pred: 0.276400
 53%|██████████████▎            | 53/100 [5:21:17<4:44:39, 363.39s/it]iteration 531: loss: 0.746513, loss_kl: 3.170750, loss_recon: 0.602276, loss_pred: 0.239937
iteration 532: loss: 0.708833, loss_kl: 2.024563, loss_recon: 0.612902, loss_pred: 0.243839
iteration 533: loss: 0.707547, loss_kl: 2.074862, loss_recon: 0.610450, loss_pred: 0.221127
iteration 534: loss: 0.669189, loss_kl: 1.358839, loss_recon: 0.603538, loss_pred: 0.193570
iteration 535: loss: 0.706666, loss_kl: 1.797330, loss_recon: 0.622540, loss_pred: 0.191936
iteration 536: loss: 0.658571, loss_kl: 0.979804, loss_recon: 0.610135, loss_pred: 0.165549
iteration 537: loss: 0.640247, loss_kl: 0.606084, loss_recon: 0.607204, loss_pred: 0.175259
iteration 538: loss: 0.641238, loss_kl: 0.536947, loss_recon: 0.612230, loss_pred: 0.148997
iteration 539: loss: 0.650728, loss_kl: 0.719334, loss_recon: 0.613920, loss_pred: 0.151032
iteration 540: loss: 0.673189, loss_kl: 0.727869, loss_recon: 0.634059, loss_pred: 0.197423
 54%|██████████████▌            | 54/100 [5:27:21<4:38:32, 363.32s/it]iteration 541: loss: 0.664075, loss_kl: 0.503730, loss_recon: 0.600951, loss_pred: 0.106749
iteration 542: loss: 0.673495, loss_kl: 0.511231, loss_recon: 0.608895, loss_pred: 0.113527
iteration 543: loss: 0.673864, loss_kl: 0.509260, loss_recon: 0.613038, loss_pred: 0.078994
iteration 544: loss: 0.676997, loss_kl: 0.503258, loss_recon: 0.614947, loss_pred: 0.096845
iteration 545: loss: 0.679517, loss_kl: 0.545105, loss_recon: 0.614277, loss_pred: 0.085839
iteration 546: loss: 0.648925, loss_kl: 0.362994, loss_recon: 0.600141, loss_pred: 0.108801
iteration 547: loss: 0.666445, loss_kl: 0.484264, loss_recon: 0.609957, loss_pred: 0.062040
iteration 548: loss: 0.677403, loss_kl: 0.533729, loss_recon: 0.613952, loss_pred: 0.079917
iteration 549: loss: 0.705084, loss_kl: 0.729140, loss_recon: 0.621558, loss_pred: 0.078651
iteration 550: loss: 0.726010, loss_kl: 0.919953, loss_recon: 0.616298, loss_pred: 0.141087
 55%|██████████████▊            | 55/100 [5:33:24<4:32:27, 363.28s/it]iteration 551: loss: 0.770910, loss_kl: 0.575697, loss_recon: 0.620473, loss_pred: 0.074211
iteration 552: loss: 0.756577, loss_kl: 0.476522, loss_recon: 0.613642, loss_pred: 0.140974
iteration 553: loss: 0.751648, loss_kl: 0.533681, loss_recon: 0.612585, loss_pred: 0.067087
iteration 554: loss: 0.850115, loss_kl: 1.028290, loss_recon: 0.599421, loss_pred: 0.054737
iteration 555: loss: 0.808059, loss_kl: 0.788013, loss_recon: 0.607017, loss_pred: 0.080513
iteration 556: loss: 0.811358, loss_kl: 0.834891, loss_recon: 0.607328, loss_pred: 0.046542
iteration 557: loss: 0.754145, loss_kl: 0.582424, loss_recon: 0.609109, loss_pred: 0.044149
iteration 558: loss: 0.741478, loss_kl: 0.443299, loss_recon: 0.623380, loss_pred: 0.066898
iteration 559: loss: 0.799280, loss_kl: 0.755167, loss_recon: 0.612556, loss_pred: 0.051501
iteration 560: loss: 0.876544, loss_kl: 0.990522, loss_recon: 0.632432, loss_pred: 0.064070
 56%|███████████████            | 56/100 [5:39:28<4:26:35, 363.53s/it]iteration 561: loss: 0.968772, loss_kl: 0.774474, loss_recon: 0.608500, loss_pred: 0.043793
iteration 562: loss: 0.935890, loss_kl: 0.669650, loss_recon: 0.614885, loss_pred: 0.059431
iteration 563: loss: 0.890198, loss_kl: 0.551394, loss_recon: 0.616240, loss_pred: 0.070832
iteration 564: loss: 0.976136, loss_kl: 0.705809, loss_recon: 0.614818, loss_pred: 0.114835
iteration 565: loss: 0.970220, loss_kl: 0.758549, loss_recon: 0.614907, loss_pred: 0.048455
iteration 566: loss: 1.015308, loss_kl: 0.860491, loss_recon: 0.600855, loss_pred: 0.080835
iteration 567: loss: 0.869638, loss_kl: 0.538437, loss_recon: 0.612052, loss_pred: 0.046605
iteration 568: loss: 0.886670, loss_kl: 0.524891, loss_recon: 0.620747, loss_pred: 0.079086
iteration 569: loss: 0.952962, loss_kl: 0.700061, loss_recon: 0.600927, loss_pred: 0.099498
iteration 570: loss: 0.778564, loss_kl: 0.298821, loss_recon: 0.622058, loss_pred: 0.056645
 57%|███████████████▍           | 57/100 [5:45:31<4:20:24, 363.37s/it]iteration 571: loss: 1.185646, loss_kl: 0.773630, loss_recon: 0.605704, loss_pred: 0.088600
iteration 572: loss: 0.993645, loss_kl: 0.533608, loss_recon: 0.610954, loss_pred: 0.035358
iteration 573: loss: 1.077063, loss_kl: 0.589800, loss_recon: 0.613700, loss_pred: 0.099107
iteration 574: loss: 1.047395, loss_kl: 0.626429, loss_recon: 0.596228, loss_pred: 0.044344
iteration 575: loss: 1.148590, loss_kl: 0.677886, loss_recon: 0.617508, loss_pred: 0.111701
iteration 576: loss: 1.186904, loss_kl: 0.796538, loss_recon: 0.618249, loss_pred: 0.048911
iteration 577: loss: 0.981465, loss_kl: 0.471267, loss_recon: 0.624995, loss_pred: 0.058716
iteration 578: loss: 1.176516, loss_kl: 0.810254, loss_recon: 0.607262, loss_pred: 0.036087
iteration 579: loss: 1.342743, loss_kl: 1.038346, loss_recon: 0.612749, loss_pred: 0.046974
iteration 580: loss: 1.098062, loss_kl: 0.580876, loss_recon: 0.650887, loss_pred: 0.083962
 58%|███████████████▋           | 58/100 [5:51:34<4:14:22, 363.40s/it]iteration 581: loss: 1.126296, loss_kl: 0.577260, loss_recon: 0.614590, loss_pred: 0.029815
iteration 582: loss: 1.386770, loss_kl: 0.890180, loss_recon: 0.602200, loss_pred: 0.040614
iteration 583: loss: 1.240335, loss_kl: 0.681830, loss_recon: 0.618956, loss_pred: 0.055358
iteration 584: loss: 1.523492, loss_kl: 1.027943, loss_recon: 0.620597, loss_pred: 0.043227
iteration 585: loss: 1.373121, loss_kl: 0.831158, loss_recon: 0.615321, loss_pred: 0.067878
iteration 586: loss: 1.263743, loss_kl: 0.721391, loss_recon: 0.613846, loss_pred: 0.049630
iteration 587: loss: 1.109008, loss_kl: 0.472656, loss_recon: 0.621174, loss_pred: 0.106097
iteration 588: loss: 1.073336, loss_kl: 0.518051, loss_recon: 0.609707, loss_pred: 0.031985
iteration 589: loss: 1.062480, loss_kl: 0.415706, loss_recon: 0.604019, loss_pred: 0.128201
iteration 590: loss: 1.291212, loss_kl: 0.778104, loss_recon: 0.610119, loss_pred: 0.029927
 59%|███████████████▉           | 59/100 [5:57:38<4:08:25, 363.54s/it]iteration 591: loss: 1.412636, loss_kl: 0.683471, loss_recon: 0.619355, loss_pred: 0.166420
iteration 592: loss: 1.157982, loss_kl: 0.537529, loss_recon: 0.625704, loss_pred: 0.032733
iteration 593: loss: 1.384328, loss_kl: 0.659230, loss_recon: 0.610406, loss_pred: 0.169920
iteration 594: loss: 1.161933, loss_kl: 0.530393, loss_recon: 0.606140, loss_pred: 0.065062
iteration 595: loss: 1.567875, loss_kl: 0.842996, loss_recon: 0.630058, loss_pred: 0.161745
iteration 596: loss: 1.294470, loss_kl: 0.690853, loss_recon: 0.610036, loss_pred: 0.042423
iteration 597: loss: 1.601367, loss_kl: 0.944648, loss_recon: 0.618961, loss_pred: 0.107864
iteration 598: loss: 1.377752, loss_kl: 0.770847, loss_recon: 0.597261, loss_pred: 0.065340
iteration 599: loss: 1.102372, loss_kl: 0.353715, loss_recon: 0.606777, loss_pred: 0.177246
iteration 600: loss: 1.323538, loss_kl: 0.668285, loss_recon: 0.591249, loss_pred: 0.116261
 60%|████████████████▏          | 60/100 [6:03:42<4:02:19, 363.48s/it]iteration 601: loss: 1.504134, loss_kl: 0.822862, loss_recon: 0.615241, loss_pred: 0.090319
iteration 602: loss: 1.809331, loss_kl: 1.139112, loss_recon: 0.613070, loss_pred: 0.089835
iteration 603: loss: 1.290508, loss_kl: 0.640538, loss_recon: 0.603664, loss_pred: 0.065074
iteration 604: loss: 1.516546, loss_kl: 0.818707, loss_recon: 0.608515, loss_pred: 0.114135
iteration 605: loss: 1.269137, loss_kl: 0.593836, loss_recon: 0.622349, loss_pred: 0.070625
iteration 606: loss: 1.073891, loss_kl: 0.364801, loss_recon: 0.611372, loss_pred: 0.110356
iteration 607: loss: 1.630556, loss_kl: 0.989405, loss_recon: 0.608046, loss_pred: 0.061044
iteration 608: loss: 1.548207, loss_kl: 0.895590, loss_recon: 0.619672, loss_pred: 0.058317
iteration 609: loss: 1.772429, loss_kl: 1.123118, loss_recon: 0.608889, loss_pred: 0.072214
iteration 610: loss: 2.055561, loss_kl: 1.429288, loss_recon: 0.619828, loss_pred: 0.045675
 61%|████████████████▍          | 61/100 [6:09:45<3:56:16, 363.51s/it]iteration 611: loss: 1.369210, loss_kl: 0.634391, loss_recon: 0.608660, loss_pred: 0.134115
iteration 612: loss: 2.220031, loss_kl: 1.554771, loss_recon: 0.625064, loss_pred: 0.056882
iteration 613: loss: 1.728969, loss_kl: 1.059023, loss_recon: 0.601259, loss_pred: 0.080485
iteration 614: loss: 1.377298, loss_kl: 0.730092, loss_recon: 0.614480, loss_pred: 0.040706
iteration 615: loss: 1.309860, loss_kl: 0.648046, loss_recon: 0.611532, loss_pred: 0.057588
iteration 616: loss: 1.623722, loss_kl: 0.962718, loss_recon: 0.605261, loss_pred: 0.066398
iteration 617: loss: 1.653928, loss_kl: 0.956916, loss_recon: 0.617927, loss_pred: 0.089925
iteration 618: loss: 1.394324, loss_kl: 0.731542, loss_recon: 0.623683, loss_pred: 0.047162
iteration 619: loss: 1.318548, loss_kl: 0.654148, loss_recon: 0.611581, loss_pred: 0.060216
iteration 620: loss: 2.066872, loss_kl: 1.364671, loss_recon: 0.601315, loss_pred: 0.116219
 62%|████████████████▋          | 62/100 [6:15:48<3:50:07, 363.36s/it]iteration 621: loss: 1.481841, loss_kl: 0.749818, loss_recon: 0.613915, loss_pred: 0.121584
iteration 622: loss: 1.752161, loss_kl: 1.112995, loss_recon: 0.604019, loss_pred: 0.039746
iteration 623: loss: 1.972652, loss_kl: 1.278839, loss_recon: 0.610846, loss_pred: 0.088422
iteration 624: loss: 1.986705, loss_kl: 1.326999, loss_recon: 0.622785, loss_pred: 0.042384
iteration 625: loss: 1.761439, loss_kl: 1.092853, loss_recon: 0.615182, loss_pred: 0.057997
iteration 626: loss: 1.690364, loss_kl: 1.044934, loss_recon: 0.610665, loss_pred: 0.039089
iteration 627: loss: 1.447357, loss_kl: 0.782908, loss_recon: 0.615530, loss_pred: 0.052251
iteration 628: loss: 1.954528, loss_kl: 1.291776, loss_recon: 0.606283, loss_pred: 0.061871
iteration 629: loss: 1.400575, loss_kl: 0.751344, loss_recon: 0.608609, loss_pred: 0.043794
iteration 630: loss: 2.013803, loss_kl: 1.320568, loss_recon: 0.643864, loss_pred: 0.054859
 63%|█████████████████          | 63/100 [6:21:52<3:44:11, 363.54s/it]iteration 631: loss: 1.772315, loss_kl: 1.125908, loss_recon: 0.606249, loss_pred: 0.040157
iteration 632: loss: 1.600122, loss_kl: 0.930101, loss_recon: 0.619133, loss_pred: 0.050889
iteration 633: loss: 1.581365, loss_kl: 0.907074, loss_recon: 0.610117, loss_pred: 0.064174
iteration 634: loss: 1.733655, loss_kl: 1.070545, loss_recon: 0.621620, loss_pred: 0.041490
iteration 635: loss: 1.433878, loss_kl: 0.751237, loss_recon: 0.613677, loss_pred: 0.068964
iteration 636: loss: 1.369563, loss_kl: 0.723395, loss_recon: 0.595578, loss_pred: 0.050590
iteration 637: loss: 1.834283, loss_kl: 1.165408, loss_recon: 0.616732, loss_pred: 0.052143
iteration 638: loss: 1.759194, loss_kl: 1.110498, loss_recon: 0.612439, loss_pred: 0.036256
iteration 639: loss: 2.096092, loss_kl: 1.418420, loss_recon: 0.619712, loss_pred: 0.057960
iteration 640: loss: 1.830493, loss_kl: 1.186293, loss_recon: 0.613602, loss_pred: 0.030598
save model to ../model/TVG_Design[160, 160, 160]/TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_63.pth
 64%|█████████████████▎         | 64/100 [6:27:56<3:38:12, 363.68s/it]iteration 641: loss: 1.808959, loss_kl: 1.146758, loss_recon: 0.621447, loss_pred: 0.040755
iteration 642: loss: 1.995086, loss_kl: 1.331080, loss_recon: 0.605906, loss_pred: 0.058099
iteration 643: loss: 1.392762, loss_kl: 0.731439, loss_recon: 0.602518, loss_pred: 0.058804
iteration 644: loss: 1.425729, loss_kl: 0.731895, loss_recon: 0.633694, loss_pred: 0.060140
iteration 645: loss: 1.686278, loss_kl: 1.039604, loss_recon: 0.603257, loss_pred: 0.043417
iteration 646: loss: 1.472908, loss_kl: 0.807989, loss_recon: 0.615547, loss_pred: 0.049371
iteration 647: loss: 1.563957, loss_kl: 0.905034, loss_recon: 0.606974, loss_pred: 0.051950
iteration 648: loss: 1.638295, loss_kl: 0.988348, loss_recon: 0.617253, loss_pred: 0.032694
iteration 649: loss: 1.513969, loss_kl: 0.872205, loss_recon: 0.618934, loss_pred: 0.022830
iteration 650: loss: 1.286901, loss_kl: 0.594062, loss_recon: 0.603539, loss_pred: 0.089299
 65%|█████████████████▌         | 65/100 [6:34:00<3:32:10, 363.72s/it]iteration 651: loss: 1.923082, loss_kl: 1.203375, loss_recon: 0.622863, loss_pred: 0.096843
iteration 652: loss: 1.621651, loss_kl: 0.978316, loss_recon: 0.604782, loss_pred: 0.038554
iteration 653: loss: 1.825412, loss_kl: 1.111352, loss_recon: 0.616608, loss_pred: 0.097452
iteration 654: loss: 1.602499, loss_kl: 0.947080, loss_recon: 0.625055, loss_pred: 0.030364
iteration 655: loss: 1.791878, loss_kl: 1.111471, loss_recon: 0.610936, loss_pred: 0.069470
iteration 656: loss: 1.581469, loss_kl: 0.941033, loss_recon: 0.608243, loss_pred: 0.032193
iteration 657: loss: 1.336811, loss_kl: 0.653803, loss_recon: 0.615524, loss_pred: 0.067484
iteration 658: loss: 1.519460, loss_kl: 0.872103, loss_recon: 0.610426, loss_pred: 0.036930
iteration 659: loss: 1.804164, loss_kl: 1.166200, loss_recon: 0.602124, loss_pred: 0.035841
iteration 660: loss: 1.822825, loss_kl: 1.084595, loss_recon: 0.623794, loss_pred: 0.114436
 66%|█████████████████▊         | 66/100 [6:40:03<3:26:04, 363.67s/it]iteration 661: loss: 1.605207, loss_kl: 0.931288, loss_recon: 0.623788, loss_pred: 0.050131
iteration 662: loss: 1.427631, loss_kl: 0.757504, loss_recon: 0.624931, loss_pred: 0.045196
iteration 663: loss: 1.562901, loss_kl: 0.902503, loss_recon: 0.598473, loss_pred: 0.061925
iteration 664: loss: 1.587322, loss_kl: 0.936848, loss_recon: 0.602764, loss_pred: 0.047710
iteration 665: loss: 1.745836, loss_kl: 1.068454, loss_recon: 0.611957, loss_pred: 0.065426
iteration 666: loss: 1.411076, loss_kl: 0.750396, loss_recon: 0.615501, loss_pred: 0.045179
iteration 667: loss: 1.549170, loss_kl: 0.887006, loss_recon: 0.614249, loss_pred: 0.047916
iteration 668: loss: 1.819307, loss_kl: 1.170960, loss_recon: 0.610323, loss_pred: 0.038024
iteration 669: loss: 1.454116, loss_kl: 0.816268, loss_recon: 0.614633, loss_pred: 0.023214
iteration 670: loss: 2.378855, loss_kl: 1.705986, loss_recon: 0.630866, loss_pred: 0.042003
 67%|██████████████████         | 67/100 [6:46:06<3:19:53, 363.44s/it]iteration 671: loss: 1.694836, loss_kl: 1.047711, loss_recon: 0.610939, loss_pred: 0.036186
iteration 672: loss: 1.542943, loss_kl: 0.889437, loss_recon: 0.622027, loss_pred: 0.031480
iteration 673: loss: 1.503159, loss_kl: 0.833225, loss_recon: 0.623273, loss_pred: 0.046661
iteration 674: loss: 1.759059, loss_kl: 1.109701, loss_recon: 0.606670, loss_pred: 0.042688
iteration 675: loss: 1.355899, loss_kl: 0.702237, loss_recon: 0.611370, loss_pred: 0.042292
iteration 676: loss: 1.429445, loss_kl: 0.773313, loss_recon: 0.603584, loss_pred: 0.052548
iteration 677: loss: 1.691931, loss_kl: 1.035526, loss_recon: 0.621483, loss_pred: 0.034922
iteration 678: loss: 1.643473, loss_kl: 0.988967, loss_recon: 0.610503, loss_pred: 0.044003
iteration 679: loss: 1.553751, loss_kl: 0.912064, loss_recon: 0.607149, loss_pred: 0.034538
iteration 680: loss: 1.617981, loss_kl: 0.808475, loss_recon: 0.607970, loss_pred: 0.201536
 68%|██████████████████▎        | 68/100 [6:52:11<3:13:58, 363.69s/it]iteration 681: loss: 1.564119, loss_kl: 0.747369, loss_recon: 0.615558, loss_pred: 0.201193
iteration 682: loss: 1.607559, loss_kl: 0.940328, loss_recon: 0.612319, loss_pred: 0.054913
iteration 683: loss: 1.793638, loss_kl: 1.041292, loss_recon: 0.604432, loss_pred: 0.147915
iteration 684: loss: 1.478929, loss_kl: 0.790743, loss_recon: 0.609510, loss_pred: 0.078676
iteration 685: loss: 1.591379, loss_kl: 0.867541, loss_recon: 0.610722, loss_pred: 0.113116
iteration 686: loss: 1.560286, loss_kl: 0.899732, loss_recon: 0.601998, loss_pred: 0.058556
iteration 687: loss: 1.505530, loss_kl: 0.799178, loss_recon: 0.624281, loss_pred: 0.082071
iteration 688: loss: 1.645245, loss_kl: 0.967143, loss_recon: 0.619783, loss_pred: 0.058318
iteration 689: loss: 1.228495, loss_kl: 0.550061, loss_recon: 0.617021, loss_pred: 0.061413
iteration 690: loss: 2.200526, loss_kl: 1.516780, loss_recon: 0.612203, loss_pred: 0.071543
 69%|██████████████████▋        | 69/100 [6:58:13<3:07:44, 363.36s/it]iteration 691: loss: 1.416124, loss_kl: 0.763275, loss_recon: 0.624514, loss_pred: 0.028336
iteration 692: loss: 1.441946, loss_kl: 0.765319, loss_recon: 0.601274, loss_pred: 0.075354
iteration 693: loss: 1.266592, loss_kl: 0.583523, loss_recon: 0.619185, loss_pred: 0.063883
iteration 694: loss: 1.381934, loss_kl: 0.716519, loss_recon: 0.612405, loss_pred: 0.053010
iteration 695: loss: 1.567778, loss_kl: 0.917054, loss_recon: 0.599004, loss_pred: 0.051721
iteration 696: loss: 1.556369, loss_kl: 0.891582, loss_recon: 0.623090, loss_pred: 0.041697
iteration 697: loss: 1.239659, loss_kl: 0.598203, loss_recon: 0.608109, loss_pred: 0.033347
iteration 698: loss: 1.573395, loss_kl: 0.917233, loss_recon: 0.609558, loss_pred: 0.046604
iteration 699: loss: 1.795326, loss_kl: 1.115601, loss_recon: 0.609885, loss_pred: 0.069839
iteration 700: loss: 1.833070, loss_kl: 1.155629, loss_recon: 0.613389, loss_pred: 0.064051
 70%|██████████████████▉        | 70/100 [7:04:16<3:01:38, 363.28s/it]iteration 701: loss: 1.354973, loss_kl: 0.681409, loss_recon: 0.617833, loss_pred: 0.055731
iteration 702: loss: 1.677726, loss_kl: 1.010010, loss_recon: 0.607034, loss_pred: 0.060682
iteration 703: loss: 1.441234, loss_kl: 0.776740, loss_recon: 0.609743, loss_pred: 0.054750
iteration 704: loss: 1.565690, loss_kl: 0.907732, loss_recon: 0.611211, loss_pred: 0.046747
iteration 705: loss: 1.434637, loss_kl: 0.789864, loss_recon: 0.615481, loss_pred: 0.029293
iteration 706: loss: 1.332171, loss_kl: 0.685383, loss_recon: 0.614018, loss_pred: 0.032769
iteration 707: loss: 1.645128, loss_kl: 0.977175, loss_recon: 0.611834, loss_pred: 0.056119
iteration 708: loss: 1.954868, loss_kl: 1.302341, loss_recon: 0.607329, loss_pred: 0.045198
iteration 709: loss: 1.664647, loss_kl: 1.009717, loss_recon: 0.612837, loss_pred: 0.042093
iteration 710: loss: 1.746067, loss_kl: 1.105270, loss_recon: 0.603809, loss_pred: 0.036988
 71%|███████████████████▏       | 71/100 [7:10:20<2:55:37, 363.36s/it]iteration 711: loss: 1.572925, loss_kl: 0.912174, loss_recon: 0.600789, loss_pred: 0.059963
iteration 712: loss: 1.681164, loss_kl: 1.027219, loss_recon: 0.610966, loss_pred: 0.042979
iteration 713: loss: 1.400257, loss_kl: 0.723139, loss_recon: 0.625788, loss_pred: 0.051330
iteration 714: loss: 1.627028, loss_kl: 0.985281, loss_recon: 0.607487, loss_pred: 0.034260
iteration 715: loss: 1.594515, loss_kl: 0.955392, loss_recon: 0.609977, loss_pred: 0.029147
iteration 716: loss: 1.337906, loss_kl: 0.688433, loss_recon: 0.610158, loss_pred: 0.039316
iteration 717: loss: 1.544974, loss_kl: 0.886823, loss_recon: 0.609138, loss_pred: 0.049013
iteration 718: loss: 1.377093, loss_kl: 0.722595, loss_recon: 0.610636, loss_pred: 0.043862
iteration 719: loss: 1.715614, loss_kl: 1.046535, loss_recon: 0.629960, loss_pred: 0.039119
iteration 720: loss: 1.522880, loss_kl: 0.734480, loss_recon: 0.625504, loss_pred: 0.162895
 72%|███████████████████▍       | 72/100 [7:16:23<2:49:31, 363.28s/it]iteration 721: loss: 1.802878, loss_kl: 1.134558, loss_recon: 0.617756, loss_pred: 0.050564
iteration 722: loss: 1.645059, loss_kl: 0.972461, loss_recon: 0.608735, loss_pred: 0.063863
iteration 723: loss: 1.581442, loss_kl: 0.892724, loss_recon: 0.614343, loss_pred: 0.074375
iteration 724: loss: 1.475305, loss_kl: 0.726577, loss_recon: 0.616396, loss_pred: 0.132333
iteration 725: loss: 1.677876, loss_kl: 1.023657, loss_recon: 0.615686, loss_pred: 0.038533
iteration 726: loss: 1.489378, loss_kl: 0.819628, loss_recon: 0.601936, loss_pred: 0.067813
iteration 727: loss: 1.535648, loss_kl: 0.855630, loss_recon: 0.601235, loss_pred: 0.078783
iteration 728: loss: 1.542241, loss_kl: 0.850270, loss_recon: 0.621323, loss_pred: 0.070648
iteration 729: loss: 1.548982, loss_kl: 0.866639, loss_recon: 0.604912, loss_pred: 0.077431
iteration 730: loss: 1.142299, loss_kl: 0.393451, loss_recon: 0.671924, loss_pred: 0.076924
 73%|███████████████████▋       | 73/100 [7:22:27<2:43:31, 363.40s/it]iteration 731: loss: 1.796062, loss_kl: 1.144066, loss_recon: 0.619539, loss_pred: 0.032458
iteration 732: loss: 1.714325, loss_kl: 1.051426, loss_recon: 0.611357, loss_pred: 0.051543
iteration 733: loss: 2.129566, loss_kl: 1.421117, loss_recon: 0.616953, loss_pred: 0.091496
iteration 734: loss: 1.393387, loss_kl: 0.749678, loss_recon: 0.606226, loss_pred: 0.037483
iteration 735: loss: 1.243143, loss_kl: 0.595767, loss_recon: 0.617857, loss_pred: 0.029519
iteration 736: loss: 1.327426, loss_kl: 0.659661, loss_recon: 0.612538, loss_pred: 0.055226
iteration 737: loss: 1.291000, loss_kl: 0.603735, loss_recon: 0.602326, loss_pred: 0.084940
iteration 738: loss: 1.719328, loss_kl: 1.057561, loss_recon: 0.607011, loss_pred: 0.054756
iteration 739: loss: 1.482632, loss_kl: 0.823521, loss_recon: 0.617861, loss_pred: 0.041250
iteration 740: loss: 1.170740, loss_kl: 0.470513, loss_recon: 0.621942, loss_pred: 0.078285
 74%|███████████████████▉       | 74/100 [7:28:31<2:37:33, 363.59s/it]iteration 741: loss: 1.677771, loss_kl: 1.029341, loss_recon: 0.616403, loss_pred: 0.032026
iteration 742: loss: 1.924377, loss_kl: 1.263989, loss_recon: 0.605394, loss_pred: 0.054994
iteration 743: loss: 1.456896, loss_kl: 0.812045, loss_recon: 0.607638, loss_pred: 0.037213
iteration 744: loss: 1.541163, loss_kl: 0.897216, loss_recon: 0.617876, loss_pred: 0.026071
iteration 745: loss: 1.392769, loss_kl: 0.752759, loss_recon: 0.609884, loss_pred: 0.030126
iteration 746: loss: 1.286414, loss_kl: 0.619585, loss_recon: 0.630531, loss_pred: 0.036298
iteration 747: loss: 1.363967, loss_kl: 0.721356, loss_recon: 0.608321, loss_pred: 0.034290
iteration 748: loss: 1.563799, loss_kl: 0.914420, loss_recon: 0.613399, loss_pred: 0.035981
iteration 749: loss: 1.582632, loss_kl: 0.942885, loss_recon: 0.607345, loss_pred: 0.032402
iteration 750: loss: 1.573456, loss_kl: 0.943050, loss_recon: 0.584680, loss_pred: 0.045726
 75%|████████████████████▎      | 75/100 [7:34:34<2:31:24, 363.39s/it]iteration 751: loss: 0.603424, loss_kl: 0.987174, loss_recon: 0.600853, loss_pred: 0.052542
iteration 752: loss: 0.614247, loss_kl: 1.531253, loss_recon: 0.610275, loss_pred: 0.075304
iteration 753: loss: 0.621369, loss_kl: 2.003531, loss_recon: 0.616044, loss_pred: 0.150153
iteration 754: loss: 0.627780, loss_kl: 2.158081, loss_recon: 0.621997, loss_pred: 0.180852
iteration 755: loss: 0.625879, loss_kl: 2.825711, loss_recon: 0.618357, loss_pred: 0.216766
iteration 756: loss: 0.633975, loss_kl: 3.681939, loss_recon: 0.624263, loss_pred: 0.245813
iteration 757: loss: 0.622343, loss_kl: 3.634205, loss_recon: 0.612672, loss_pred: 0.276941
iteration 758: loss: 0.614671, loss_kl: 4.031635, loss_recon: 0.604022, loss_pred: 0.275061
iteration 759: loss: 0.628960, loss_kl: 4.999666, loss_recon: 0.615879, loss_pred: 0.290967
iteration 760: loss: 0.637698, loss_kl: 5.355989, loss_recon: 0.623734, loss_pred: 0.291738
 76%|████████████████████▌      | 76/100 [7:40:37<2:25:19, 363.30s/it]iteration 761: loss: 0.633422, loss_kl: 4.704943, loss_recon: 0.601241, loss_pred: 0.298333
iteration 762: loss: 0.668694, loss_kl: 6.637152, loss_recon: 0.624221, loss_pred: 0.277037
iteration 763: loss: 0.653433, loss_kl: 6.046053, loss_recon: 0.612943, loss_pred: 0.248855
iteration 764: loss: 0.652599, loss_kl: 6.238637, loss_recon: 0.610931, loss_pred: 0.239519
iteration 765: loss: 0.655897, loss_kl: 6.230889, loss_recon: 0.614476, loss_pred: 0.208809
iteration 766: loss: 0.646019, loss_kl: 6.126080, loss_recon: 0.605374, loss_pred: 0.193041
iteration 767: loss: 0.667161, loss_kl: 7.301912, loss_recon: 0.619024, loss_pred: 0.181881
iteration 768: loss: 0.652143, loss_kl: 6.261899, loss_recon: 0.610948, loss_pred: 0.142711
iteration 769: loss: 0.647537, loss_kl: 6.328889, loss_recon: 0.606053, loss_pred: 0.120686
iteration 770: loss: 0.665450, loss_kl: 7.237114, loss_recon: 0.618254, loss_pred: 0.100395
 77%|████████████████████▊      | 77/100 [7:46:40<2:19:13, 363.19s/it]iteration 771: loss: 0.703595, loss_kl: 5.575740, loss_recon: 0.609336, loss_pred: 0.093521
iteration 772: loss: 0.708558, loss_kl: 6.137096, loss_recon: 0.605005, loss_pred: 0.091110
iteration 773: loss: 0.717339, loss_kl: 6.040707, loss_recon: 0.615622, loss_pred: 0.077127
iteration 774: loss: 0.719043, loss_kl: 6.733978, loss_recon: 0.605855, loss_pred: 0.073725
iteration 775: loss: 0.724172, loss_kl: 6.066165, loss_recon: 0.622385, loss_pred: 0.055828
iteration 776: loss: 0.707502, loss_kl: 5.283858, loss_recon: 0.618619, loss_pred: 0.062035
iteration 777: loss: 0.672176, loss_kl: 4.463098, loss_recon: 0.596594, loss_pred: 0.082769
iteration 778: loss: 0.693211, loss_kl: 4.797079, loss_recon: 0.612168, loss_pred: 0.077297
iteration 779: loss: 0.690793, loss_kl: 4.472398, loss_recon: 0.615434, loss_pred: 0.060073
iteration 780: loss: 0.642561, loss_kl: 2.496375, loss_recon: 0.599586, loss_pred: 0.088347
 78%|█████████████████████      | 78/100 [7:52:44<2:13:15, 363.44s/it]iteration 781: loss: 0.774558, loss_kl: 4.068053, loss_recon: 0.599950, loss_pred: 0.060803
iteration 782: loss: 0.753703, loss_kl: 3.310862, loss_recon: 0.611263, loss_pred: 0.057315
iteration 783: loss: 0.735058, loss_kl: 2.856433, loss_recon: 0.612184, loss_pred: 0.049107
iteration 784: loss: 0.708361, loss_kl: 2.263375, loss_recon: 0.611054, loss_pred: 0.037588
iteration 785: loss: 0.709268, loss_kl: 2.125955, loss_recon: 0.617856, loss_pred: 0.035621
iteration 786: loss: 0.676125, loss_kl: 1.408922, loss_recon: 0.615033, loss_pred: 0.035689
iteration 787: loss: 0.654416, loss_kl: 1.168251, loss_recon: 0.603519, loss_pred: 0.035276
iteration 788: loss: 0.674122, loss_kl: 1.478700, loss_recon: 0.610070, loss_pred: 0.035892
iteration 789: loss: 0.647526, loss_kl: 0.744866, loss_recon: 0.614679, loss_pred: 0.031849
iteration 790: loss: 0.670994, loss_kl: 0.915652, loss_recon: 0.630698, loss_pred: 0.037191
 79%|█████████████████████▎     | 79/100 [7:58:47<2:07:13, 363.50s/it]iteration 791: loss: 0.676944, loss_kl: 0.690903, loss_recon: 0.601947, loss_pred: 0.034410
iteration 792: loss: 0.703806, loss_kl: 0.910175, loss_recon: 0.606133, loss_pred: 0.034434
iteration 793: loss: 0.682431, loss_kl: 0.624041, loss_recon: 0.614505, loss_pred: 0.032878
iteration 794: loss: 0.686519, loss_kl: 0.692723, loss_recon: 0.611971, loss_pred: 0.028246
iteration 795: loss: 0.681245, loss_kl: 0.589129, loss_recon: 0.617651, loss_pred: 0.025898
iteration 796: loss: 0.674080, loss_kl: 0.595551, loss_recon: 0.609882, loss_pred: 0.025316
iteration 797: loss: 0.679133, loss_kl: 0.555029, loss_recon: 0.618259, loss_pred: 0.033692
iteration 798: loss: 0.642023, loss_kl: 0.308746, loss_recon: 0.607129, loss_pred: 0.028721
iteration 799: loss: 0.690212, loss_kl: 0.797256, loss_recon: 0.604976, loss_pred: 0.027072
iteration 800: loss: 0.740975, loss_kl: 1.117222, loss_recon: 0.623099, loss_pred: 0.022781
save model to ../model/TVG_Design[160, 160, 160]/TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_79.pth
 80%|█████████████████████▌     | 80/100 [8:04:52<2:01:17, 363.89s/it]iteration 801: loss: 0.733934, loss_kl: 0.488579, loss_recon: 0.615121, loss_pred: 0.024709
iteration 802: loss: 0.741696, loss_kl: 0.515201, loss_recon: 0.616990, loss_pred: 0.023545
iteration 803: loss: 0.755235, loss_kl: 0.599712, loss_recon: 0.611850, loss_pred: 0.019724
iteration 804: loss: 0.716162, loss_kl: 0.479032, loss_recon: 0.598519, loss_pred: 0.029201
iteration 805: loss: 0.751107, loss_kl: 0.619087, loss_recon: 0.602663, loss_pred: 0.022208
iteration 806: loss: 0.840440, loss_kl: 0.989179, loss_recon: 0.604893, loss_pred: 0.028414
iteration 807: loss: 0.789344, loss_kl: 0.752002, loss_recon: 0.609292, loss_pred: 0.025842
iteration 808: loss: 0.763499, loss_kl: 0.618838, loss_recon: 0.612933, loss_pred: 0.031624
iteration 809: loss: 0.821673, loss_kl: 0.872789, loss_recon: 0.615290, loss_pred: 0.018808
iteration 810: loss: 0.813954, loss_kl: 0.705002, loss_recon: 0.642380, loss_pred: 0.036217
 81%|█████████████████████▊     | 81/100 [8:10:55<1:55:08, 363.61s/it]iteration 811: loss: 1.065377, loss_kl: 1.024187, loss_recon: 0.603332, loss_pred: 0.025233
iteration 812: loss: 0.979518, loss_kl: 0.834871, loss_recon: 0.600280, loss_pred: 0.026475
iteration 813: loss: 0.874219, loss_kl: 0.587282, loss_recon: 0.607075, loss_pred: 0.019470
iteration 814: loss: 0.938687, loss_kl: 0.729544, loss_recon: 0.605934, loss_pred: 0.026220
iteration 815: loss: 1.105076, loss_kl: 1.040507, loss_recon: 0.633470, loss_pred: 0.030629
iteration 816: loss: 0.980352, loss_kl: 0.820766, loss_recon: 0.608810, loss_pred: 0.023097
iteration 817: loss: 0.976282, loss_kl: 0.788828, loss_recon: 0.619204, loss_pred: 0.022184
iteration 818: loss: 0.940433, loss_kl: 0.728497, loss_recon: 0.609637, loss_pred: 0.022823
iteration 819: loss: 0.902114, loss_kl: 0.619401, loss_recon: 0.616506, loss_pred: 0.029286
iteration 820: loss: 0.931418, loss_kl: 0.715490, loss_recon: 0.605131, loss_pred: 0.025590
 82%|██████████████████████▏    | 82/100 [8:16:58<1:49:03, 363.54s/it]iteration 821: loss: 1.226699, loss_kl: 0.905812, loss_recon: 0.598183, loss_pred: 0.028636
iteration 822: loss: 1.192130, loss_kl: 0.850417, loss_recon: 0.603010, loss_pred: 0.025460
iteration 823: loss: 1.286214, loss_kl: 0.961066, loss_recon: 0.620673, loss_pred: 0.028428
iteration 824: loss: 1.082219, loss_kl: 0.700566, loss_recon: 0.590250, loss_pred: 0.030870
iteration 825: loss: 0.979082, loss_kl: 0.504118, loss_recon: 0.624203, loss_pred: 0.023498
iteration 826: loss: 1.026474, loss_kl: 0.581445, loss_recon: 0.612423, loss_pred: 0.034147
iteration 827: loss: 0.990707, loss_kl: 0.521403, loss_recon: 0.623726, loss_pred: 0.024208
iteration 828: loss: 0.962192, loss_kl: 0.474237, loss_recon: 0.608733, loss_pred: 0.051270
iteration 829: loss: 0.988998, loss_kl: 0.520443, loss_recon: 0.616963, loss_pred: 0.032682
iteration 830: loss: 1.080680, loss_kl: 0.657464, loss_recon: 0.596475, loss_pred: 0.062429
 83%|██████████████████████▍    | 83/100 [8:23:02<1:42:58, 363.43s/it]iteration 831: loss: 1.513193, loss_kl: 0.988611, loss_recon: 0.616248, loss_pred: 0.075500
iteration 832: loss: 1.402166, loss_kl: 0.901020, loss_recon: 0.618036, loss_pred: 0.029252
iteration 833: loss: 1.360689, loss_kl: 0.826910, loss_recon: 0.604347, loss_pred: 0.070394
iteration 834: loss: 1.186570, loss_kl: 0.651147, loss_recon: 0.604770, loss_pred: 0.039085
iteration 835: loss: 1.453973, loss_kl: 0.953361, loss_recon: 0.610281, loss_pred: 0.047573
iteration 836: loss: 1.218183, loss_kl: 0.687035, loss_recon: 0.607333, loss_pred: 0.037662
iteration 837: loss: 1.269215, loss_kl: 0.706095, loss_recon: 0.616017, loss_pred: 0.068843
iteration 838: loss: 0.907928, loss_kl: 0.309906, loss_recon: 0.610781, loss_pred: 0.042621
iteration 839: loss: 1.459059, loss_kl: 0.962418, loss_recon: 0.623633, loss_pred: 0.028710
iteration 840: loss: 1.236898, loss_kl: 0.720647, loss_recon: 0.603311, loss_pred: 0.031024
 84%|██████████████████████▋    | 84/100 [8:29:06<1:36:58, 363.63s/it]iteration 841: loss: 1.046273, loss_kl: 0.417781, loss_recon: 0.605337, loss_pred: 0.054622
iteration 842: loss: 1.604135, loss_kl: 1.007432, loss_recon: 0.619588, loss_pred: 0.047374
iteration 843: loss: 1.446469, loss_kl: 0.876527, loss_recon: 0.603259, loss_pred: 0.026856
iteration 844: loss: 1.362468, loss_kl: 0.731418, loss_recon: 0.638389, loss_pred: 0.044332
iteration 845: loss: 0.896610, loss_kl: 0.268314, loss_recon: 0.607446, loss_pred: 0.041485
iteration 846: loss: 1.568528, loss_kl: 0.958221, loss_recon: 0.629423, loss_pred: 0.047899
iteration 847: loss: 1.436595, loss_kl: 0.869526, loss_recon: 0.599874, loss_pred: 0.026905
iteration 848: loss: 1.408268, loss_kl: 0.811888, loss_recon: 0.605041, loss_pred: 0.048659
iteration 849: loss: 1.069363, loss_kl: 0.473578, loss_recon: 0.603373, loss_pred: 0.025665
iteration 850: loss: 1.840530, loss_kl: 1.301692, loss_recon: 0.588785, loss_pred: 0.039379
 85%|██████████████████████▉    | 85/100 [8:35:10<1:30:55, 363.72s/it]iteration 851: loss: 1.629398, loss_kl: 1.014268, loss_recon: 0.613092, loss_pred: 0.029808
iteration 852: loss: 1.547014, loss_kl: 0.916873, loss_recon: 0.611060, loss_pred: 0.044655
iteration 853: loss: 1.946010, loss_kl: 1.337938, loss_recon: 0.612121, loss_pred: 0.032398
iteration 854: loss: 1.586866, loss_kl: 0.976064, loss_recon: 0.604100, loss_pred: 0.033555
iteration 855: loss: 1.521468, loss_kl: 0.898495, loss_recon: 0.612415, loss_pred: 0.035396
iteration 856: loss: 1.678495, loss_kl: 1.055234, loss_recon: 0.610944, loss_pred: 0.041486
iteration 857: loss: 1.543900, loss_kl: 0.927041, loss_recon: 0.608568, loss_pred: 0.033848
iteration 858: loss: 1.695756, loss_kl: 1.066774, loss_recon: 0.610375, loss_pred: 0.048264
iteration 859: loss: 1.434524, loss_kl: 0.810798, loss_recon: 0.615550, loss_pred: 0.030553
iteration 860: loss: 1.498626, loss_kl: 0.880072, loss_recon: 0.620571, loss_pred: 0.021975
 86%|███████████████████████▏   | 86/100 [8:41:13<1:24:52, 363.72s/it]iteration 861: loss: 1.746300, loss_kl: 1.112442, loss_recon: 0.615354, loss_pred: 0.030336
iteration 862: loss: 1.797812, loss_kl: 1.176746, loss_recon: 0.610546, loss_pred: 0.022941
iteration 863: loss: 1.526882, loss_kl: 0.908037, loss_recon: 0.605635, loss_pred: 0.022848
iteration 864: loss: 1.618701, loss_kl: 1.004960, loss_recon: 0.602627, loss_pred: 0.021743
iteration 865: loss: 1.666992, loss_kl: 1.030443, loss_recon: 0.623396, loss_pred: 0.024072
iteration 866: loss: 2.063733, loss_kl: 1.446316, loss_recon: 0.606807, loss_pred: 0.025852
iteration 867: loss: 1.331336, loss_kl: 0.683135, loss_recon: 0.618092, loss_pred: 0.037571
iteration 868: loss: 1.479017, loss_kl: 0.850889, loss_recon: 0.610241, loss_pred: 0.026976
iteration 869: loss: 1.291375, loss_kl: 0.645381, loss_recon: 0.605595, loss_pred: 0.047574
iteration 870: loss: 2.254009, loss_kl: 1.610594, loss_recon: 0.626540, loss_pred: 0.033902
 87%|███████████████████████▍   | 87/100 [8:47:17<1:18:46, 363.57s/it]iteration 871: loss: 1.692740, loss_kl: 1.060508, loss_recon: 0.612592, loss_pred: 0.023966
iteration 872: loss: 1.782082, loss_kl: 1.163866, loss_recon: 0.598600, loss_pred: 0.024356
iteration 873: loss: 1.522549, loss_kl: 0.886380, loss_recon: 0.602845, loss_pred: 0.037007
iteration 874: loss: 1.541783, loss_kl: 0.878111, loss_recon: 0.624245, loss_pred: 0.043103
iteration 875: loss: 1.470997, loss_kl: 0.812527, loss_recon: 0.617889, loss_pred: 0.043999
iteration 876: loss: 1.237070, loss_kl: 0.594828, loss_recon: 0.616682, loss_pred: 0.028045
iteration 877: loss: 1.326872, loss_kl: 0.690593, loss_recon: 0.614504, loss_pred: 0.024628
iteration 878: loss: 1.499182, loss_kl: 0.856852, loss_recon: 0.611458, loss_pred: 0.034428
iteration 879: loss: 1.286090, loss_kl: 0.652269, loss_recon: 0.609925, loss_pred: 0.026605
iteration 880: loss: 2.349699, loss_kl: 1.698286, loss_recon: 0.611466, loss_pred: 0.046910
 88%|███████████████████████▊   | 88/100 [8:53:20<1:12:41, 363.43s/it]iteration 881: loss: 1.577388, loss_kl: 0.936741, loss_recon: 0.620745, loss_pred: 0.019901
iteration 882: loss: 1.663684, loss_kl: 1.013495, loss_recon: 0.626479, loss_pred: 0.023711
iteration 883: loss: 1.316934, loss_kl: 0.684949, loss_recon: 0.604413, loss_pred: 0.027572
iteration 884: loss: 1.700995, loss_kl: 1.065959, loss_recon: 0.609064, loss_pred: 0.025972
iteration 885: loss: 1.637583, loss_kl: 0.991431, loss_recon: 0.615373, loss_pred: 0.030779
iteration 886: loss: 1.112579, loss_kl: 0.459713, loss_recon: 0.600632, loss_pred: 0.052234
iteration 887: loss: 1.121005, loss_kl: 0.469157, loss_recon: 0.623733, loss_pred: 0.028114
iteration 888: loss: 1.536481, loss_kl: 0.912501, loss_recon: 0.604610, loss_pred: 0.019371
iteration 889: loss: 1.804640, loss_kl: 1.155548, loss_recon: 0.604557, loss_pred: 0.044535
iteration 890: loss: 1.630381, loss_kl: 1.005196, loss_recon: 0.608496, loss_pred: 0.016689
 89%|████████████████████████   | 89/100 [8:59:23<1:06:37, 363.41s/it]iteration 891: loss: 1.529883, loss_kl: 0.869102, loss_recon: 0.617143, loss_pred: 0.043638
iteration 892: loss: 1.490706, loss_kl: 0.846188, loss_recon: 0.608771, loss_pred: 0.035746
iteration 893: loss: 1.795225, loss_kl: 1.154953, loss_recon: 0.611570, loss_pred: 0.028702
iteration 894: loss: 1.703438, loss_kl: 1.067039, loss_recon: 0.615820, loss_pred: 0.020578
iteration 895: loss: 1.029196, loss_kl: 0.405520, loss_recon: 0.594260, loss_pred: 0.029416
iteration 896: loss: 1.109684, loss_kl: 0.458358, loss_recon: 0.621783, loss_pred: 0.029543
iteration 897: loss: 1.625391, loss_kl: 0.981150, loss_recon: 0.601701, loss_pred: 0.042541
iteration 898: loss: 1.449367, loss_kl: 0.810215, loss_recon: 0.605031, loss_pred: 0.034121
iteration 899: loss: 1.483822, loss_kl: 0.840388, loss_recon: 0.615910, loss_pred: 0.027524
iteration 900: loss: 1.400418, loss_kl: 0.732279, loss_recon: 0.614379, loss_pred: 0.053760
 90%|████████████████████████▎  | 90/100 [9:05:27<1:00:35, 363.51s/it]iteration 901: loss: 1.883164, loss_kl: 1.200962, loss_recon: 0.621595, loss_pred: 0.060608
iteration 902: loss: 2.013094, loss_kl: 1.385825, loss_recon: 0.599181, loss_pred: 0.028089
iteration 903: loss: 1.927908, loss_kl: 1.258943, loss_recon: 0.613758, loss_pred: 0.055208
iteration 904: loss: 1.608047, loss_kl: 0.965029, loss_recon: 0.617823, loss_pred: 0.025195
iteration 905: loss: 1.431815, loss_kl: 0.761805, loss_recon: 0.605263, loss_pred: 0.064748
iteration 906: loss: 1.549596, loss_kl: 0.913657, loss_recon: 0.607091, loss_pred: 0.028848
iteration 907: loss: 1.198690, loss_kl: 0.521725, loss_recon: 0.625241, loss_pred: 0.051724
iteration 908: loss: 1.120171, loss_kl: 0.490105, loss_recon: 0.597267, loss_pred: 0.032799
iteration 909: loss: 1.544710, loss_kl: 0.839817, loss_recon: 0.603392, loss_pred: 0.101501
iteration 910: loss: 1.459363, loss_kl: 0.783154, loss_recon: 0.640116, loss_pred: 0.036093
 91%|██████████████████████████▍  | 91/100 [9:11:30<54:29, 363.27s/it]iteration 911: loss: 1.339715, loss_kl: 0.634507, loss_recon: 0.612101, loss_pred: 0.093106
iteration 912: loss: 1.910754, loss_kl: 1.236765, loss_recon: 0.608885, loss_pred: 0.065104
iteration 913: loss: 1.901575, loss_kl: 1.210779, loss_recon: 0.604163, loss_pred: 0.086634
iteration 914: loss: 1.601527, loss_kl: 0.925783, loss_recon: 0.617577, loss_pred: 0.058168
iteration 915: loss: 1.998082, loss_kl: 1.341933, loss_recon: 0.608835, loss_pred: 0.047315
iteration 916: loss: 1.314205, loss_kl: 0.628853, loss_recon: 0.608602, loss_pred: 0.076750
iteration 917: loss: 1.375150, loss_kl: 0.713235, loss_recon: 0.623364, loss_pred: 0.038552
iteration 918: loss: 1.475000, loss_kl: 0.790491, loss_recon: 0.618149, loss_pred: 0.066360
iteration 919: loss: 1.415504, loss_kl: 0.763469, loss_recon: 0.613362, loss_pred: 0.038673
iteration 920: loss: 1.353066, loss_kl: 0.662299, loss_recon: 0.617672, loss_pred: 0.073095
 92%|██████████████████████████▋  | 92/100 [9:17:33<48:26, 363.33s/it]iteration 921: loss: 1.561576, loss_kl: 0.937754, loss_recon: 0.604002, loss_pred: 0.019820
iteration 922: loss: 1.444079, loss_kl: 0.745460, loss_recon: 0.624512, loss_pred: 0.074108
iteration 923: loss: 1.196602, loss_kl: 0.554278, loss_recon: 0.608631, loss_pred: 0.033692
iteration 924: loss: 1.554791, loss_kl: 0.863980, loss_recon: 0.616693, loss_pred: 0.074119
iteration 925: loss: 1.199152, loss_kl: 0.560396, loss_recon: 0.607913, loss_pred: 0.030843
iteration 926: loss: 1.438470, loss_kl: 0.746843, loss_recon: 0.621794, loss_pred: 0.069833
iteration 927: loss: 1.499417, loss_kl: 0.852027, loss_recon: 0.606417, loss_pred: 0.040972
iteration 928: loss: 1.483489, loss_kl: 0.841139, loss_recon: 0.590045, loss_pred: 0.052305
iteration 929: loss: 1.580813, loss_kl: 0.917577, loss_recon: 0.628423, loss_pred: 0.034814
iteration 930: loss: 1.732835, loss_kl: 1.067214, loss_recon: 0.609140, loss_pred: 0.056482
 93%|██████████████████████████▉  | 93/100 [9:23:36<42:21, 363.11s/it]iteration 931: loss: 1.410935, loss_kl: 0.760570, loss_recon: 0.598984, loss_pred: 0.051381
iteration 932: loss: 1.647676, loss_kl: 0.994284, loss_recon: 0.610878, loss_pred: 0.042514
iteration 933: loss: 1.502603, loss_kl: 0.857941, loss_recon: 0.609588, loss_pred: 0.035074
iteration 934: loss: 1.316879, loss_kl: 0.662701, loss_recon: 0.609726, loss_pred: 0.044451
iteration 935: loss: 1.489922, loss_kl: 0.825917, loss_recon: 0.612620, loss_pred: 0.051385
iteration 936: loss: 1.464815, loss_kl: 0.786630, loss_recon: 0.619298, loss_pred: 0.058887
iteration 937: loss: 1.158464, loss_kl: 0.512460, loss_recon: 0.618664, loss_pred: 0.027339
iteration 938: loss: 1.378285, loss_kl: 0.716546, loss_recon: 0.611713, loss_pred: 0.050026
iteration 939: loss: 1.463580, loss_kl: 0.821125, loss_recon: 0.614873, loss_pred: 0.027582
iteration 940: loss: 1.907176, loss_kl: 1.227346, loss_recon: 0.631001, loss_pred: 0.048829
 94%|███████████████████████████▎ | 94/100 [9:29:39<36:19, 363.30s/it]iteration 941: loss: 1.464502, loss_kl: 0.777975, loss_recon: 0.622192, loss_pred: 0.064335
iteration 942: loss: 1.512239, loss_kl: 0.843508, loss_recon: 0.607242, loss_pred: 0.061490
iteration 943: loss: 2.095937, loss_kl: 1.418800, loss_recon: 0.621057, loss_pred: 0.056080
iteration 944: loss: 1.592068, loss_kl: 0.941512, loss_recon: 0.604894, loss_pred: 0.045662
iteration 945: loss: 1.743153, loss_kl: 1.057786, loss_recon: 0.605538, loss_pred: 0.079829
iteration 946: loss: 1.599702, loss_kl: 0.944374, loss_recon: 0.609019, loss_pred: 0.046309
iteration 947: loss: 1.681258, loss_kl: 1.003778, loss_recon: 0.625344, loss_pred: 0.052136
iteration 948: loss: 1.177766, loss_kl: 0.544977, loss_recon: 0.606625, loss_pred: 0.026165
iteration 949: loss: 1.728994, loss_kl: 1.065279, loss_recon: 0.607921, loss_pred: 0.055794
iteration 950: loss: 1.784495, loss_kl: 1.156042, loss_recon: 0.602634, loss_pred: 0.025818
 95%|███████████████████████████▌ | 95/100 [9:35:43<30:16, 363.40s/it]iteration 951: loss: 1.231585, loss_kl: 0.554377, loss_recon: 0.607057, loss_pred: 0.070150
iteration 952: loss: 1.821332, loss_kl: 1.176520, loss_recon: 0.597877, loss_pred: 0.046936
iteration 953: loss: 1.563520, loss_kl: 0.870812, loss_recon: 0.619717, loss_pred: 0.072991
iteration 954: loss: 1.653400, loss_kl: 1.019603, loss_recon: 0.604535, loss_pred: 0.029262
iteration 955: loss: 1.468623, loss_kl: 0.788517, loss_recon: 0.609789, loss_pred: 0.070318
iteration 956: loss: 1.453727, loss_kl: 0.793986, loss_recon: 0.605816, loss_pred: 0.053925
iteration 957: loss: 1.089407, loss_kl: 0.384710, loss_recon: 0.625313, loss_pred: 0.079384
iteration 958: loss: 1.228866, loss_kl: 0.553053, loss_recon: 0.617571, loss_pred: 0.058242
iteration 959: loss: 1.348840, loss_kl: 0.676448, loss_recon: 0.610223, loss_pred: 0.062169
iteration 960: loss: 1.531678, loss_kl: 0.837671, loss_recon: 0.634628, loss_pred: 0.059378
save model to ../model/TVG_Design[160, 160, 160]/TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_95.pth
 96%|███████████████████████████▊ | 96/100 [9:41:48<24:15, 363.86s/it]iteration 961: loss: 1.918565, loss_kl: 1.275480, loss_recon: 0.616429, loss_pred: 0.026656
iteration 962: loss: 1.530365, loss_kl: 0.878347, loss_recon: 0.609773, loss_pred: 0.042245
iteration 963: loss: 1.098606, loss_kl: 0.454811, loss_recon: 0.616511, loss_pred: 0.027285
iteration 964: loss: 1.694561, loss_kl: 1.006597, loss_recon: 0.619798, loss_pred: 0.068166
iteration 965: loss: 1.218923, loss_kl: 0.576162, loss_recon: 0.609895, loss_pred: 0.032867
iteration 966: loss: 1.434739, loss_kl: 0.770070, loss_recon: 0.613148, loss_pred: 0.051521
iteration 967: loss: 1.343174, loss_kl: 0.677478, loss_recon: 0.602089, loss_pred: 0.063606
iteration 968: loss: 1.253539, loss_kl: 0.619465, loss_recon: 0.609590, loss_pred: 0.024484
iteration 969: loss: 1.578886, loss_kl: 0.873196, loss_recon: 0.612162, loss_pred: 0.093528
iteration 970: loss: 1.420667, loss_kl: 0.754469, loss_recon: 0.617746, loss_pred: 0.048453
 97%|████████████████████████████▏| 97/100 [9:47:52<18:11, 363.87s/it]iteration 971: loss: 1.517999, loss_kl: 0.859324, loss_recon: 0.612293, loss_pred: 0.046383
iteration 972: loss: 1.350373, loss_kl: 0.714494, loss_recon: 0.601766, loss_pred: 0.034113
iteration 973: loss: 1.666545, loss_kl: 1.016785, loss_recon: 0.607318, loss_pred: 0.042441
iteration 974: loss: 1.421500, loss_kl: 0.772870, loss_recon: 0.621097, loss_pred: 0.027532
iteration 975: loss: 1.406171, loss_kl: 0.772239, loss_recon: 0.608905, loss_pred: 0.025028
iteration 976: loss: 1.177598, loss_kl: 0.546456, loss_recon: 0.608070, loss_pred: 0.023072
iteration 977: loss: 1.364634, loss_kl: 0.721618, loss_recon: 0.618398, loss_pred: 0.024618
iteration 978: loss: 1.265340, loss_kl: 0.629635, loss_recon: 0.610043, loss_pred: 0.025662
iteration 979: loss: 1.455799, loss_kl: 0.813197, loss_recon: 0.615080, loss_pred: 0.027523
iteration 980: loss: 1.288141, loss_kl: 0.665034, loss_recon: 0.594814, loss_pred: 0.028293
 98%|████████████████████████████▍| 98/100 [9:53:55<12:07, 363.61s/it]iteration 981: loss: 1.545544, loss_kl: 0.910202, loss_recon: 0.609315, loss_pred: 0.026027
iteration 982: loss: 1.673807, loss_kl: 1.020702, loss_recon: 0.621286, loss_pred: 0.031819
iteration 983: loss: 1.638577, loss_kl: 0.994323, loss_recon: 0.613416, loss_pred: 0.030838
iteration 984: loss: 1.518686, loss_kl: 0.891103, loss_recon: 0.602156, loss_pred: 0.025427
iteration 985: loss: 1.191502, loss_kl: 0.548426, loss_recon: 0.612541, loss_pred: 0.030536
iteration 986: loss: 1.468847, loss_kl: 0.839201, loss_recon: 0.605225, loss_pred: 0.024420
iteration 987: loss: 1.449869, loss_kl: 0.824936, loss_recon: 0.601581, loss_pred: 0.023351
iteration 988: loss: 1.407188, loss_kl: 0.761888, loss_recon: 0.619786, loss_pred: 0.025514
iteration 989: loss: 1.474133, loss_kl: 0.825929, loss_recon: 0.609832, loss_pred: 0.038372
iteration 990: loss: 0.975566, loss_kl: 0.321970, loss_recon: 0.628311, loss_pred: 0.025285
 99%|████████████████████████████▋| 99/100 [9:59:58<06:03, 363.45s/it]iteration 991: loss: 1.528812, loss_kl: 0.881788, loss_recon: 0.613102, loss_pred: 0.033922
iteration 992: loss: 1.499626, loss_kl: 0.855980, loss_recon: 0.614154, loss_pred: 0.029492
iteration 993: loss: 1.507954, loss_kl: 0.878326, loss_recon: 0.606046, loss_pred: 0.023583
iteration 994: loss: 1.650457, loss_kl: 1.008135, loss_recon: 0.617161, loss_pred: 0.025161
iteration 995: loss: 1.539017, loss_kl: 0.901204, loss_recon: 0.616209, loss_pred: 0.021605
iteration 996: loss: 1.293604, loss_kl: 0.669102, loss_recon: 0.601897, loss_pred: 0.022605
iteration 997: loss: 1.494162, loss_kl: 0.851923, loss_recon: 0.613569, loss_pred: 0.028670
iteration 998: loss: 1.593452, loss_kl: 0.970423, loss_recon: 0.601323, loss_pred: 0.021707
iteration 999: loss: 1.092829, loss_kl: 0.461976, loss_recon: 0.607807, loss_pred: 0.023047
iteration 1000: loss: 1.342047, loss_kl: 0.683353, loss_recon: 0.611963, loss_pred: 0.046731
save model to ../model/TVG_Design[160, 160, 160]/TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_99.pth
 99%|███████████████████████████▋| 99/100 [10:06:01<06:07, 367.29s/it]
