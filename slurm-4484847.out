/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1666643016022/work/aten/src/ATen/native/TensorShape.cpp:3190.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(root_path='/work/sheidaei/mhashemi/data/mat', dataset='Design', list_dir='./lists/lists_Design', num_classes=2, max_iterations=288, max_epochs=100, batch_size=24, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, deterministic=1, base_lr=0.01, img_size=[160, 160, 160], seed=1234, n_skip=4, vit_name='Conv-ViT-Gen-B_16', vit_patches_size=[8, 8, 8], is_pretrain=True, exp='TV_Design[160, 160, 160]', distributed=False)
13 iterations per epoch. 1300 max iterations 
  0%|                                         | 0/100 [00:00<?, ?it/s]iteration 1: loss: 2.600917, loss_kl: 1.072385, loss_recon: -1.183211, loss_pred: 1.415054
iteration 2: loss: 7.958699, loss_kl: 62.013992, loss_recon: -1.193639, loss_pred: 6.611723
iteration 3: loss: 291.693970, loss_kl: 97.021111, loss_recon: -1.162464, loss_pred: 290.291595
iteration 4: loss: 2.594035, loss_kl: 3.134319, loss_recon: -1.148131, loss_pred: 1.438154
iteration 5: loss: 3.215706, loss_kl: 33.591003, loss_recon: -1.136351, loss_pred: 1.996297
iteration 6: loss: 3.158249, loss_kl: 33.947769, loss_recon: -1.129864, loss_pred: 1.944445
iteration 7: loss: 2.943499, loss_kl: 35.336323, loss_recon: -1.100067, loss_pred: 1.756059
iteration 8: loss: 3.167063, loss_kl: 38.853020, loss_recon: -1.084445, loss_pred: 1.986549
iteration 9: loss: 3.189085, loss_kl: 45.781384, loss_recon: -1.077372, loss_pred: 1.998513
iteration 10: loss: 3.269330, loss_kl: 46.973003, loss_recon: -1.062201, loss_pred: 2.090982
iteration 11: loss: 2.622126, loss_kl: 48.814327, loss_recon: -1.054576, loss_pred: 1.446851
iteration 12: loss: 2.512883, loss_kl: 45.442913, loss_recon: -1.042910, loss_pred: 1.357610
iteration 13: loss: 2.414540, loss_kl: 46.306118, loss_recon: -1.042176, loss_pred: 1.257867
  1%|▎                            | 1/100 [09:32<15:45:10, 572.84s/it]iteration 14: loss: 2.760506, loss_kl: 51.645256, loss_recon: -1.037690, loss_pred: 1.390629
iteration 15: loss: 3.281971, loss_kl: 52.751690, loss_recon: -1.034117, loss_pred: 1.908550
iteration 16: loss: 2.920694, loss_kl: 53.751312, loss_recon: -1.034403, loss_pred: 1.540557
iteration 17: loss: 2.580359, loss_kl: 58.302834, loss_recon: -1.035094, loss_pred: 1.170255
iteration 18: loss: 2.536027, loss_kl: 61.885662, loss_recon: -1.035836, loss_pred: 1.102136
iteration 19: loss: 2.535580, loss_kl: 61.024376, loss_recon: -1.029621, loss_pred: 1.113443
iteration 20: loss: 2.167536, loss_kl: 59.486786, loss_recon: -1.031265, loss_pred: 0.753646
iteration 21: loss: 2.035206, loss_kl: 57.876617, loss_recon: -1.029987, loss_pred: 0.632950
iteration 22: loss: 2.111336, loss_kl: 62.116661, loss_recon: -1.028145, loss_pred: 0.683650
iteration 23: loss: 1.997547, loss_kl: 59.237465, loss_recon: -1.025719, loss_pred: 0.590806
iteration 24: loss: 2.131215, loss_kl: 61.502914, loss_recon: -1.018370, loss_pred: 0.717252
iteration 25: loss: 2.024546, loss_kl: 59.563530, loss_recon: -1.015577, loss_pred: 0.625850
iteration 26: loss: 2.003230, loss_kl: 60.551167, loss_recon: -1.019613, loss_pred: 0.594145
  2%|▌                            | 2/100 [14:56<11:36:19, 426.32s/it]iteration 27: loss: 2.621471, loss_kl: 62.807083, loss_recon: -1.018957, loss_pred: 0.558260
iteration 28: loss: 2.670858, loss_kl: 62.243195, loss_recon: -1.012841, loss_pred: 0.623140
iteration 29: loss: 2.623708, loss_kl: 63.843506, loss_recon: -1.020108, loss_pred: 0.542115
iteration 30: loss: 2.638758, loss_kl: 61.799282, loss_recon: -1.012258, loss_pred: 0.599003
iteration 31: loss: 2.764532, loss_kl: 65.533264, loss_recon: -1.009518, loss_pred: 0.665434
iteration 32: loss: 2.519175, loss_kl: 61.965240, loss_recon: -1.014769, loss_pred: 0.474149
iteration 33: loss: 2.536088, loss_kl: 65.399292, loss_recon: -1.013719, loss_pred: 0.435017
iteration 34: loss: 2.380342, loss_kl: 63.845226, loss_recon: -1.008408, loss_pred: 0.310420
iteration 35: loss: 2.505554, loss_kl: 61.632172, loss_recon: -1.005304, loss_pred: 0.475532
iteration 36: loss: 2.638056, loss_kl: 67.176765, loss_recon: -1.008726, loss_pred: 0.512425
iteration 37: loss: 2.369651, loss_kl: 64.486572, loss_recon: -1.002189, loss_pred: 0.295285
iteration 38: loss: 2.474421, loss_kl: 65.551147, loss_recon: -1.003725, loss_pred: 0.380819
iteration 39: loss: 2.546190, loss_kl: 69.709435, loss_recon: -1.011853, loss_pred: 0.375323
  3%|▊                            | 3/100 [20:19<10:13:06, 379.24s/it]iteration 40: loss: 4.156771, loss_kl: 65.564697, loss_recon: -1.003146, loss_pred: 0.380909
iteration 41: loss: 4.143840, loss_kl: 70.130020, loss_recon: -1.005954, loss_pred: 0.172103
iteration 42: loss: 4.804255, loss_kl: 73.363716, loss_recon: -0.999601, loss_pred: 0.702119
iteration 43: loss: 4.705781, loss_kl: 76.253860, loss_recon: -0.999272, loss_pred: 0.481750
iteration 44: loss: 4.219723, loss_kl: 70.675377, loss_recon: -1.001267, loss_pred: 0.229610
iteration 45: loss: 4.726774, loss_kl: 72.434898, loss_recon: -0.999170, loss_pred: 0.664348
iteration 46: loss: 4.203239, loss_kl: 69.259666, loss_recon: -0.998928, loss_pred: 0.275335
iteration 47: loss: 4.777943, loss_kl: 73.535500, loss_recon: -0.997813, loss_pred: 0.670331
iteration 48: loss: 4.372101, loss_kl: 74.766922, loss_recon: -0.996686, loss_pred: 0.213539
iteration 49: loss: 4.875527, loss_kl: 75.137451, loss_recon: -0.994362, loss_pred: 0.703619
iteration 50: loss: 4.488215, loss_kl: 73.875267, loss_recon: -0.992234, loss_pred: 0.371813
iteration 51: loss: 4.315185, loss_kl: 72.194023, loss_recon: -0.993883, loss_pred: 0.268233
iteration 52: loss: 4.606768, loss_kl: 77.586945, loss_recon: -0.986827, loss_pred: 0.338808
  4%|█▏                            | 4/100 [25:43<9:31:44, 357.34s/it]iteration 53: loss: 8.833978, loss_kl: 72.628006, loss_recon: -0.992252, loss_pred: 0.331957
iteration 54: loss: 9.201313, loss_kl: 75.985886, loss_recon: -0.992638, loss_pred: 0.351700
iteration 55: loss: 8.962641, loss_kl: 75.353355, loss_recon: -0.988435, loss_pred: 0.182636
iteration 56: loss: 9.124244, loss_kl: 75.742310, loss_recon: -0.989888, loss_pred: 0.302568
iteration 57: loss: 9.131126, loss_kl: 77.270599, loss_recon: -0.989066, loss_pred: 0.152245
iteration 58: loss: 9.073746, loss_kl: 75.858978, loss_recon: -0.989893, loss_pred: 0.240000
iteration 59: loss: 9.253969, loss_kl: 77.612640, loss_recon: -0.984289, loss_pred: 0.244498
iteration 60: loss: 9.018379, loss_kl: 75.244263, loss_recon: -0.988398, loss_pred: 0.249690
iteration 61: loss: 9.189284, loss_kl: 77.873642, loss_recon: -0.983982, loss_pred: 0.153132
iteration 62: loss: 8.978756, loss_kl: 75.090622, loss_recon: -0.980820, loss_pred: 0.233532
iteration 63: loss: 8.991071, loss_kl: 75.115891, loss_recon: -0.979128, loss_pred: 0.244926
iteration 64: loss: 8.795187, loss_kl: 74.626694, loss_recon: -0.980883, loss_pred: 0.097870
iteration 65: loss: 9.073084, loss_kl: 72.375450, loss_recon: -0.983369, loss_pred: 0.606060
  5%|█▌                            | 5/100 [31:07<9:06:26, 345.12s/it]iteration 66: loss: 18.937263, loss_kl: 76.885063, loss_recon: -0.978831, loss_pred: 0.161446
iteration 67: loss: 18.685814, loss_kl: 74.848465, loss_recon: -0.979701, loss_pred: 0.380547
iteration 68: loss: 18.474831, loss_kl: 74.634583, loss_recon: -0.978776, loss_pred: 0.219999
iteration 69: loss: 18.239660, loss_kl: 73.452942, loss_recon: -0.978502, loss_pred: 0.258621
iteration 70: loss: 18.316515, loss_kl: 73.486252, loss_recon: -0.975827, loss_pred: 0.330442
iteration 71: loss: 18.642614, loss_kl: 75.197960, loss_recon: -0.979317, loss_pred: 0.256835
iteration 72: loss: 18.670296, loss_kl: 75.161621, loss_recon: -0.972455, loss_pred: 0.299789
iteration 73: loss: 18.520054, loss_kl: 74.761345, loss_recon: -0.972367, loss_pred: 0.242289
iteration 74: loss: 18.747713, loss_kl: 74.904373, loss_recon: -0.974750, loss_pred: 0.434456
iteration 75: loss: 18.467089, loss_kl: 74.735794, loss_recon: -0.973664, loss_pred: 0.193941
iteration 76: loss: 18.158577, loss_kl: 73.412994, loss_recon: -0.977063, loss_pred: 0.188226
iteration 77: loss: 17.816692, loss_kl: 71.670189, loss_recon: -0.969936, loss_pred: 0.256884
iteration 78: loss: 20.160566, loss_kl: 81.380882, loss_recon: -0.974523, loss_pred: 0.348386
  6%|█▊                            | 6/100 [36:30<8:48:57, 337.63s/it]iteration 79: loss: 33.287773, loss_kl: 73.057007, loss_recon: -0.974152, loss_pred: 0.147621
iteration 80: loss: 33.804634, loss_kl: 73.570015, loss_recon: -0.970712, loss_pred: 0.442049
iteration 81: loss: 34.095177, loss_kl: 74.139336, loss_recon: -0.970410, loss_pred: 0.482229
iteration 82: loss: 34.243183, loss_kl: 74.863213, loss_recon: -0.967669, loss_pred: 0.314265
iteration 83: loss: 33.845722, loss_kl: 73.678734, loss_recon: -0.968026, loss_pred: 0.437958
iteration 84: loss: 33.177223, loss_kl: 72.232719, loss_recon: -0.966660, loss_pred: 0.407482
iteration 85: loss: 32.847809, loss_kl: 71.422424, loss_recon: -0.964538, loss_pred: 0.436956
iteration 86: loss: 32.821041, loss_kl: 71.425491, loss_recon: -0.965878, loss_pred: 0.407493
iteration 87: loss: 33.805866, loss_kl: 73.155426, loss_recon: -0.961267, loss_pred: 0.635267
iteration 88: loss: 34.187634, loss_kl: 74.983482, loss_recon: -0.965164, loss_pred: 0.208266
iteration 89: loss: 33.944412, loss_kl: 74.123924, loss_recon: -0.961597, loss_pred: 0.347064
iteration 90: loss: 32.951046, loss_kl: 72.146637, loss_recon: -0.962363, loss_pred: 0.223504
iteration 91: loss: 34.296482, loss_kl: 74.938683, loss_recon: -0.965258, loss_pred: 0.336746
  7%|██                            | 7/100 [41:53<8:36:09, 333.00s/it]iteration 92: loss: 50.081409, loss_kl: 72.647369, loss_recon: -0.962692, loss_pred: 0.255590
iteration 93: loss: 49.703575, loss_kl: 72.016266, loss_recon: -0.961031, loss_pred: 0.303900
iteration 94: loss: 50.559975, loss_kl: 72.717209, loss_recon: -0.958124, loss_pred: 0.691751
iteration 95: loss: 52.548817, loss_kl: 76.086189, loss_recon: -0.961289, loss_pred: 0.411424
iteration 96: loss: 51.637489, loss_kl: 74.723206, loss_recon: -0.958519, loss_pred: 0.419621
iteration 97: loss: 49.020668, loss_kl: 70.941429, loss_recon: -0.955157, loss_pred: 0.349808
iteration 98: loss: 51.544750, loss_kl: 74.666977, loss_recon: -0.954905, loss_pred: 0.368311
iteration 99: loss: 47.454247, loss_kl: 68.536713, loss_recon: -0.954688, loss_pred: 0.401284
iteration 100: loss: 48.560810, loss_kl: 70.381187, loss_recon: -0.957450, loss_pred: 0.264480
iteration 101: loss: 49.950687, loss_kl: 72.531906, loss_recon: -0.955428, loss_pred: 0.209791
iteration 102: loss: 48.084053, loss_kl: 69.403519, loss_recon: -0.949684, loss_pred: 0.453075
iteration 103: loss: 46.401333, loss_kl: 67.107567, loss_recon: -0.947992, loss_pred: 0.316322
iteration 104: loss: 48.070911, loss_kl: 69.813950, loss_recon: -0.957101, loss_pred: 0.156459
  8%|██▍                           | 8/100 [47:16<8:25:34, 329.73s/it]iteration 105: loss: 58.493805, loss_kl: 67.767937, loss_recon: -0.950011, loss_pred: 0.421893
iteration 106: loss: 57.119999, loss_kl: 66.227959, loss_recon: -0.946977, loss_pred: 0.349179
iteration 107: loss: 56.857380, loss_kl: 65.747131, loss_recon: -0.945332, loss_pred: 0.493497
iteration 108: loss: 55.636398, loss_kl: 64.087677, loss_recon: -0.953846, loss_pred: 0.662761
iteration 109: loss: 57.116043, loss_kl: 66.199615, loss_recon: -0.951397, loss_pred: 0.364693
iteration 110: loss: 58.969189, loss_kl: 68.310440, loss_recon: -0.950626, loss_pred: 0.439386
iteration 111: loss: 56.437466, loss_kl: 65.070770, loss_recon: -0.944462, loss_pred: 0.644559
iteration 112: loss: 55.919044, loss_kl: 64.437592, loss_recon: -0.947608, loss_pred: 0.656702
iteration 113: loss: 52.959049, loss_kl: 61.144829, loss_recon: -0.945010, loss_pred: 0.474786
iteration 114: loss: 53.256393, loss_kl: 61.350128, loss_recon: -0.944160, loss_pred: 0.599933
iteration 115: loss: 51.044121, loss_kl: 58.800209, loss_recon: -0.945932, loss_pred: 0.535228
iteration 116: loss: 48.406990, loss_kl: 55.816307, loss_recon: -0.943588, loss_pred: 0.415585
iteration 117: loss: 46.968369, loss_kl: 53.663845, loss_recon: -0.943186, loss_pred: 0.791688
  9%|██▋                           | 9/100 [52:38<8:16:42, 327.50s/it]iteration 118: loss: 56.848747, loss_kl: 58.780022, loss_recon: -0.943758, loss_pred: 1.040187
iteration 119: loss: 50.367634, loss_kl: 52.126415, loss_recon: -0.939311, loss_pred: 0.773945
iteration 120: loss: 52.272556, loss_kl: 54.433037, loss_recon: -0.944561, loss_pred: 0.520634
iteration 121: loss: 48.451817, loss_kl: 50.251827, loss_recon: -0.941676, loss_pred: 0.605488
iteration 122: loss: 45.987377, loss_kl: 47.836529, loss_recon: -0.939870, loss_pred: 0.397273
iteration 123: loss: 53.765015, loss_kl: 55.567959, loss_recon: -0.936784, loss_pred: 0.961543
iteration 124: loss: 46.531174, loss_kl: 48.434021, loss_recon: -0.939767, loss_pred: 0.383478
iteration 125: loss: 38.114624, loss_kl: 38.883011, loss_recon: -0.940557, loss_pred: 0.880979
iteration 126: loss: 40.149220, loss_kl: 41.706894, loss_recon: -0.937707, loss_pred: 0.282635
iteration 127: loss: 30.289656, loss_kl: 31.041882, loss_recon: -0.940911, loss_pred: 0.374503
iteration 128: loss: 26.188126, loss_kl: 26.493294, loss_recon: -0.937739, loss_pred: 0.521759
iteration 129: loss: 30.180046, loss_kl: 30.616806, loss_recon: -0.935491, loss_pred: 0.667072
iteration 130: loss: 36.104176, loss_kl: 36.710018, loss_recon: -0.935306, loss_pred: 0.904032
 10%|██▉                          | 10/100 [58:01<8:08:58, 325.98s/it]iteration 131: loss: 34.888546, loss_kl: 33.901054, loss_recon: -0.937931, loss_pred: 0.951230
iteration 132: loss: 27.353214, loss_kl: 26.453979, loss_recon: -0.937479, loss_pred: 0.665353
iteration 133: loss: 21.745216, loss_kl: 20.405350, loss_recon: -0.936578, loss_pred: 0.946010
iteration 134: loss: 30.920902, loss_kl: 29.356812, loss_recon: -0.943760, loss_pred: 1.401135
iteration 135: loss: 21.565426, loss_kl: 20.009274, loss_recon: -0.934798, loss_pred: 1.153540
iteration 136: loss: 26.927021, loss_kl: 25.978277, loss_recon: -0.936833, loss_pred: 0.702856
iteration 137: loss: 14.339398, loss_kl: 13.149300, loss_recon: -0.933237, loss_pred: 0.606594
iteration 138: loss: 17.010227, loss_kl: 15.520473, loss_recon: -0.934119, loss_pred: 0.968433
iteration 139: loss: 13.286211, loss_kl: 11.987455, loss_recon: -0.931128, loss_pred: 0.686458
iteration 140: loss: 16.243729, loss_kl: 14.671397, loss_recon: -0.924444, loss_pred: 1.038104
iteration 141: loss: 11.182613, loss_kl: 9.641598, loss_recon: -0.928581, loss_pred: 0.868873
iteration 142: loss: 10.690309, loss_kl: 9.555983, loss_recon: -0.933737, loss_pred: 0.454750
iteration 143: loss: 9.819208, loss_kl: 8.456140, loss_recon: -0.932208, loss_pred: 0.655769
 11%|██▉                        | 11/100 [1:03:23<8:01:55, 324.89s/it]iteration 144: loss: 11.778999, loss_kl: 10.071882, loss_recon: -0.927847, loss_pred: 0.883552
iteration 145: loss: 9.196560, loss_kl: 7.363536, loss_recon: -0.922562, loss_pred: 0.986702
iteration 146: loss: 8.235807, loss_kl: 6.662158, loss_recon: -0.925763, loss_pred: 0.716864
iteration 147: loss: 7.788254, loss_kl: 6.315560, loss_recon: -0.929155, loss_pred: 0.608929
iteration 148: loss: 8.144667, loss_kl: 6.485851, loss_recon: -0.924355, loss_pred: 0.801612
iteration 149: loss: 8.114625, loss_kl: 6.226511, loss_recon: -0.923139, loss_pred: 1.029444
iteration 150: loss: 7.243133, loss_kl: 5.535024, loss_recon: -0.921374, loss_pred: 0.844042
iteration 151: loss: 7.010373, loss_kl: 5.506710, loss_recon: -0.922471, loss_pred: 0.638207
iteration 152: loss: 7.520987, loss_kl: 5.851188, loss_recon: -0.922940, loss_pred: 0.807441
iteration 153: loss: 6.449367, loss_kl: 4.947234, loss_recon: -0.918654, loss_pred: 0.634701
iteration 154: loss: 7.457614, loss_kl: 6.211816, loss_recon: -0.922231, loss_pred: 0.387882
iteration 155: loss: 6.632429, loss_kl: 5.094987, loss_recon: -0.916837, loss_pred: 0.673357
iteration 156: loss: 5.082324, loss_kl: 3.643365, loss_recon: -0.920569, loss_pred: 0.556113
 12%|███▏                       | 12/100 [1:08:46<7:55:26, 324.16s/it]iteration 157: loss: 7.557239, loss_kl: 6.040765, loss_recon: -0.915514, loss_pred: 0.625062
iteration 158: loss: 6.400731, loss_kl: 4.950876, loss_recon: -0.915657, loss_pred: 0.553951
iteration 159: loss: 6.698651, loss_kl: 5.315034, loss_recon: -0.917496, loss_pred: 0.487327
iteration 160: loss: 6.084438, loss_kl: 4.660638, loss_recon: -0.912302, loss_pred: 0.530093
iteration 161: loss: 6.970016, loss_kl: 5.579022, loss_recon: -0.916434, loss_pred: 0.496819
iteration 162: loss: 5.030227, loss_kl: 3.329371, loss_recon: -0.915861, loss_pred: 0.798279
iteration 163: loss: 5.124011, loss_kl: 3.358602, loss_recon: -0.913934, loss_pred: 0.864876
iteration 164: loss: 5.751409, loss_kl: 4.288177, loss_recon: -0.909605, loss_pred: 0.570735
iteration 165: loss: 5.655888, loss_kl: 4.288239, loss_recon: -0.911406, loss_pred: 0.473352
iteration 166: loss: 4.963439, loss_kl: 3.789896, loss_recon: -0.909079, loss_pred: 0.279585
iteration 167: loss: 4.119706, loss_kl: 2.822811, loss_recon: -0.914654, loss_pred: 0.393503
iteration 168: loss: 4.659067, loss_kl: 3.259694, loss_recon: -0.910384, loss_pred: 0.501994
iteration 169: loss: 3.952050, loss_kl: 2.482662, loss_recon: -0.917617, loss_pred: 0.561676
 13%|███▌                       | 13/100 [1:14:10<7:50:00, 324.14s/it]iteration 170: loss: 4.780294, loss_kl: 3.518574, loss_recon: -0.906777, loss_pred: 0.354942
iteration 171: loss: 4.753564, loss_kl: 3.434087, loss_recon: -0.905336, loss_pred: 0.414142
iteration 172: loss: 4.661407, loss_kl: 3.363533, loss_recon: -0.901724, loss_pred: 0.396150
iteration 173: loss: 5.081424, loss_kl: 3.832799, loss_recon: -0.905874, loss_pred: 0.342751
iteration 174: loss: 4.575214, loss_kl: 3.367793, loss_recon: -0.902013, loss_pred: 0.305409
iteration 175: loss: 3.241007, loss_kl: 2.065798, loss_recon: -0.903755, loss_pred: 0.271454
iteration 176: loss: 4.045892, loss_kl: 2.918276, loss_recon: -0.903038, loss_pred: 0.224577
iteration 177: loss: 3.271184, loss_kl: 2.144597, loss_recon: -0.903198, loss_pred: 0.223388
iteration 178: loss: 3.598619, loss_kl: 2.546519, loss_recon: -0.907088, loss_pred: 0.145012
iteration 179: loss: 3.126803, loss_kl: 1.867289, loss_recon: -0.901333, loss_pred: 0.358181
iteration 180: loss: 3.058800, loss_kl: 2.004314, loss_recon: -0.903654, loss_pred: 0.150832
iteration 181: loss: 3.157552, loss_kl: 2.055416, loss_recon: -0.900367, loss_pred: 0.201768
iteration 182: loss: 2.700274, loss_kl: 1.510389, loss_recon: -0.907510, loss_pred: 0.282375
 14%|███▊                       | 14/100 [1:19:33<7:44:04, 323.77s/it]iteration 183: loss: 2.926024, loss_kl: 1.764560, loss_recon: -0.900903, loss_pred: 0.260562
iteration 184: loss: 3.630604, loss_kl: 2.434528, loss_recon: -0.898196, loss_pred: 0.297880
iteration 185: loss: 3.580873, loss_kl: 2.456998, loss_recon: -0.899052, loss_pred: 0.224824
iteration 186: loss: 4.010960, loss_kl: 2.819573, loss_recon: -0.898667, loss_pred: 0.292719
iteration 187: loss: 2.927329, loss_kl: 1.762213, loss_recon: -0.893486, loss_pred: 0.271629
iteration 188: loss: 2.783697, loss_kl: 1.494851, loss_recon: -0.892478, loss_pred: 0.396368
iteration 189: loss: 4.084005, loss_kl: 2.914281, loss_recon: -0.896640, loss_pred: 0.273083
iteration 190: loss: 3.191075, loss_kl: 1.993110, loss_recon: -0.895605, loss_pred: 0.302361
iteration 191: loss: 3.629380, loss_kl: 2.409873, loss_recon: -0.892589, loss_pred: 0.326918
iteration 192: loss: 3.454261, loss_kl: 2.366881, loss_recon: -0.896066, loss_pred: 0.191314
iteration 193: loss: 2.235338, loss_kl: 1.192260, loss_recon: -0.893663, loss_pred: 0.149415
iteration 194: loss: 2.936095, loss_kl: 1.838710, loss_recon: -0.895129, loss_pred: 0.202256
iteration 195: loss: 3.812504, loss_kl: 2.514030, loss_recon: -0.895452, loss_pred: 0.403022
 15%|████                       | 15/100 [1:24:55<7:37:58, 323.28s/it]iteration 196: loss: 4.785368, loss_kl: 3.497971, loss_recon: -0.893749, loss_pred: 0.393649
iteration 197: loss: 3.558672, loss_kl: 2.429940, loss_recon: -0.892160, loss_pred: 0.236572
iteration 198: loss: 3.841228, loss_kl: 2.706588, loss_recon: -0.890415, loss_pred: 0.244225
iteration 199: loss: 4.311385, loss_kl: 3.281638, loss_recon: -0.888539, loss_pred: 0.141208
iteration 200: loss: 3.525544, loss_kl: 2.337487, loss_recon: -0.890292, loss_pred: 0.297765
iteration 201: loss: 5.316147, loss_kl: 4.223680, loss_recon: -0.885664, loss_pred: 0.206803
iteration 202: loss: 2.486358, loss_kl: 1.428268, loss_recon: -0.885921, loss_pred: 0.172169
iteration 203: loss: 4.187406, loss_kl: 3.136650, loss_recon: -0.889613, loss_pred: 0.161143
iteration 204: loss: 2.870193, loss_kl: 1.751291, loss_recon: -0.886635, loss_pred: 0.232267
iteration 205: loss: 3.023804, loss_kl: 1.952536, loss_recon: -0.889310, loss_pred: 0.181958
iteration 206: loss: 3.494933, loss_kl: 2.463840, loss_recon: -0.887637, loss_pred: 0.143456
iteration 207: loss: 3.467599, loss_kl: 2.400011, loss_recon: -0.882995, loss_pred: 0.184593
iteration 208: loss: 2.343478, loss_kl: 1.265192, loss_recon: -0.888124, loss_pred: 0.190162
 16%|████▎                      | 16/100 [1:30:18<7:32:20, 323.10s/it]iteration 209: loss: 2.994129, loss_kl: 1.853031, loss_recon: -0.887271, loss_pred: 0.253828
iteration 210: loss: 3.168087, loss_kl: 1.907593, loss_recon: -0.881563, loss_pred: 0.378930
iteration 211: loss: 2.711584, loss_kl: 1.527791, loss_recon: -0.877150, loss_pred: 0.306642
iteration 212: loss: 2.599881, loss_kl: 1.494919, loss_recon: -0.883656, loss_pred: 0.221305
iteration 213: loss: 2.600008, loss_kl: 1.549511, loss_recon: -0.878003, loss_pred: 0.172494
iteration 214: loss: 2.770823, loss_kl: 1.636918, loss_recon: -0.880910, loss_pred: 0.252996
iteration 215: loss: 2.471457, loss_kl: 1.449238, loss_recon: -0.881701, loss_pred: 0.140518
iteration 216: loss: 2.315456, loss_kl: 1.338969, loss_recon: -0.879025, loss_pred: 0.097463
iteration 217: loss: 2.950327, loss_kl: 1.868061, loss_recon: -0.879879, loss_pred: 0.202387
iteration 218: loss: 2.714452, loss_kl: 1.559511, loss_recon: -0.876211, loss_pred: 0.278729
iteration 219: loss: 3.221570, loss_kl: 2.192549, loss_recon: -0.877831, loss_pred: 0.151190
iteration 220: loss: 2.294305, loss_kl: 1.275555, loss_recon: -0.881514, loss_pred: 0.137236
iteration 221: loss: 3.188729, loss_kl: 2.045824, loss_recon: -0.879759, loss_pred: 0.263147
 17%|████▌                      | 17/100 [1:35:41<7:27:10, 323.26s/it]iteration 222: loss: 3.263005, loss_kl: 2.190589, loss_recon: -0.873345, loss_pred: 0.199071
iteration 223: loss: 3.013071, loss_kl: 1.961960, loss_recon: -0.872853, loss_pred: 0.178258
iteration 224: loss: 2.520169, loss_kl: 1.527024, loss_recon: -0.875814, loss_pred: 0.117332
iteration 225: loss: 3.144198, loss_kl: 2.147049, loss_recon: -0.876217, loss_pred: 0.120932
iteration 226: loss: 2.523999, loss_kl: 1.513108, loss_recon: -0.875803, loss_pred: 0.135087
iteration 227: loss: 3.375054, loss_kl: 2.331657, loss_recon: -0.871264, loss_pred: 0.172133
iteration 228: loss: 3.112107, loss_kl: 1.936189, loss_recon: -0.866114, loss_pred: 0.309803
iteration 229: loss: 2.850623, loss_kl: 1.829376, loss_recon: -0.870368, loss_pred: 0.150879
iteration 230: loss: 2.630611, loss_kl: 1.541174, loss_recon: -0.868196, loss_pred: 0.221241
iteration 231: loss: 2.981392, loss_kl: 1.810808, loss_recon: -0.874892, loss_pred: 0.295693
iteration 232: loss: 2.302359, loss_kl: 1.304706, loss_recon: -0.870295, loss_pred: 0.127359
iteration 233: loss: 2.341188, loss_kl: 1.226686, loss_recon: -0.866781, loss_pred: 0.247720
iteration 234: loss: 3.559005, loss_kl: 2.557661, loss_recon: -0.872336, loss_pred: 0.129009
 18%|████▊                      | 18/100 [1:41:04<7:21:42, 323.20s/it]iteration 235: loss: 3.338868, loss_kl: 2.233068, loss_recon: -0.864039, loss_pred: 0.241761
iteration 236: loss: 3.130954, loss_kl: 2.131887, loss_recon: -0.873731, loss_pred: 0.125337
iteration 237: loss: 2.495787, loss_kl: 1.471704, loss_recon: -0.867700, loss_pred: 0.156383
iteration 238: loss: 2.948426, loss_kl: 1.897414, loss_recon: -0.866869, loss_pred: 0.184143
iteration 239: loss: 3.334608, loss_kl: 2.300180, loss_recon: -0.861879, loss_pred: 0.172549
iteration 240: loss: 3.263200, loss_kl: 2.213519, loss_recon: -0.859699, loss_pred: 0.189982
iteration 241: loss: 2.658126, loss_kl: 1.656496, loss_recon: -0.866161, loss_pred: 0.135468
iteration 242: loss: 3.691694, loss_kl: 2.628583, loss_recon: -0.862181, loss_pred: 0.200930
iteration 243: loss: 3.271510, loss_kl: 2.296891, loss_recon: -0.861662, loss_pred: 0.112957
iteration 244: loss: 2.176741, loss_kl: 1.183404, loss_recon: -0.862563, loss_pred: 0.130775
iteration 245: loss: 3.376081, loss_kl: 2.402668, loss_recon: -0.859466, loss_pred: 0.113946
iteration 246: loss: 2.292746, loss_kl: 1.317887, loss_recon: -0.863991, loss_pred: 0.110868
iteration 247: loss: 2.929919, loss_kl: 1.785709, loss_recon: -0.868280, loss_pred: 0.275930
 19%|█████▏                     | 19/100 [1:46:29<7:16:57, 323.68s/it]iteration 248: loss: 2.550619, loss_kl: 1.563193, loss_recon: -0.864281, loss_pred: 0.123145
iteration 249: loss: 3.761390, loss_kl: 2.735530, loss_recon: -0.862624, loss_pred: 0.163237
iteration 250: loss: 2.569811, loss_kl: 1.537325, loss_recon: -0.859810, loss_pred: 0.172675
iteration 251: loss: 3.007554, loss_kl: 1.992941, loss_recon: -0.862411, loss_pred: 0.152201
iteration 252: loss: 2.724480, loss_kl: 1.729051, loss_recon: -0.856404, loss_pred: 0.139025
iteration 253: loss: 2.001949, loss_kl: 0.940821, loss_recon: -0.855947, loss_pred: 0.205181
iteration 254: loss: 3.113137, loss_kl: 2.100545, loss_recon: -0.855172, loss_pred: 0.157420
iteration 255: loss: 2.567827, loss_kl: 1.604820, loss_recon: -0.854619, loss_pred: 0.108388
iteration 256: loss: 2.934361, loss_kl: 1.978930, loss_recon: -0.857991, loss_pred: 0.097440
iteration 257: loss: 2.265462, loss_kl: 1.280059, loss_recon: -0.857582, loss_pred: 0.127821
iteration 258: loss: 1.779779, loss_kl: 0.847180, loss_recon: -0.854146, loss_pred: 0.078453
iteration 259: loss: 2.204698, loss_kl: 1.241730, loss_recon: -0.850061, loss_pred: 0.112907
iteration 260: loss: 1.947621, loss_kl: 0.994717, loss_recon: -0.854805, loss_pred: 0.098100
 20%|█████▍                     | 20/100 [1:51:52<7:11:03, 323.29s/it]iteration 261: loss: 2.207154, loss_kl: 1.258286, loss_recon: -0.849109, loss_pred: 0.099760
iteration 262: loss: 2.436571, loss_kl: 1.464943, loss_recon: -0.848068, loss_pred: 0.123559
iteration 263: loss: 2.399569, loss_kl: 1.457130, loss_recon: -0.857600, loss_pred: 0.084838
iteration 264: loss: 2.461338, loss_kl: 1.487545, loss_recon: -0.854506, loss_pred: 0.119287
iteration 265: loss: 1.951078, loss_kl: 0.968052, loss_recon: -0.854737, loss_pred: 0.128289
iteration 266: loss: 2.125466, loss_kl: 1.158625, loss_recon: -0.847941, loss_pred: 0.118900
iteration 267: loss: 2.605411, loss_kl: 1.583802, loss_recon: -0.850311, loss_pred: 0.171297
iteration 268: loss: 2.240874, loss_kl: 1.246775, loss_recon: -0.846153, loss_pred: 0.147946
iteration 269: loss: 2.834325, loss_kl: 1.846172, loss_recon: -0.848752, loss_pred: 0.139401
iteration 270: loss: 3.127895, loss_kl: 2.132192, loss_recon: -0.849421, loss_pred: 0.146282
iteration 271: loss: 1.998058, loss_kl: 1.014506, loss_recon: -0.843028, loss_pred: 0.140524
iteration 272: loss: 3.202667, loss_kl: 2.216717, loss_recon: -0.847052, loss_pred: 0.138898
iteration 273: loss: 2.186038, loss_kl: 1.167338, loss_recon: -0.844129, loss_pred: 0.174571
 21%|█████▋                     | 21/100 [1:57:14<7:05:22, 323.08s/it]iteration 274: loss: 2.144666, loss_kl: 1.199489, loss_recon: -0.843137, loss_pred: 0.102040
iteration 275: loss: 2.734953, loss_kl: 1.797826, loss_recon: -0.847576, loss_pred: 0.089550
iteration 276: loss: 2.459961, loss_kl: 1.491562, loss_recon: -0.842283, loss_pred: 0.126117
iteration 277: loss: 2.360677, loss_kl: 1.371067, loss_recon: -0.844772, loss_pred: 0.144838
iteration 278: loss: 2.311132, loss_kl: 1.369123, loss_recon: -0.843098, loss_pred: 0.098911
iteration 279: loss: 2.489247, loss_kl: 1.512781, loss_recon: -0.844177, loss_pred: 0.132289
iteration 280: loss: 2.263234, loss_kl: 1.277220, loss_recon: -0.839842, loss_pred: 0.146172
iteration 281: loss: 2.690276, loss_kl: 1.738297, loss_recon: -0.841917, loss_pred: 0.110062
iteration 282: loss: 2.472724, loss_kl: 1.460359, loss_recon: -0.840702, loss_pred: 0.171663
iteration 283: loss: 2.328916, loss_kl: 1.400971, loss_recon: -0.840222, loss_pred: 0.087723
iteration 284: loss: 3.175004, loss_kl: 2.217527, loss_recon: -0.833780, loss_pred: 0.123697
iteration 285: loss: 2.044670, loss_kl: 1.113472, loss_recon: -0.844480, loss_pred: 0.086717
iteration 286: loss: 2.809053, loss_kl: 1.768729, loss_recon: -0.838770, loss_pred: 0.201555
 22%|█████▉                     | 22/100 [2:02:36<6:59:41, 322.84s/it]iteration 287: loss: 1.956802, loss_kl: 1.038930, loss_recon: -0.835378, loss_pred: 0.082494
iteration 288: loss: 2.726343, loss_kl: 1.790477, loss_recon: -0.836497, loss_pred: 0.099370
iteration 289: loss: 2.105819, loss_kl: 1.186641, loss_recon: -0.837256, loss_pred: 0.081921
iteration 290: loss: 2.698592, loss_kl: 1.769756, loss_recon: -0.836098, loss_pred: 0.092738
iteration 291: loss: 2.983741, loss_kl: 1.915920, loss_recon: -0.831952, loss_pred: 0.235869
iteration 292: loss: 1.822702, loss_kl: 0.875501, loss_recon: -0.837346, loss_pred: 0.109855
iteration 293: loss: 2.381119, loss_kl: 1.422192, loss_recon: -0.837501, loss_pred: 0.121425
iteration 294: loss: 2.179697, loss_kl: 1.250061, loss_recon: -0.834439, loss_pred: 0.095197
iteration 295: loss: 2.531660, loss_kl: 1.625396, loss_recon: -0.831848, loss_pred: 0.074416
iteration 296: loss: 2.378957, loss_kl: 1.441068, loss_recon: -0.829768, loss_pred: 0.108122
iteration 297: loss: 2.326818, loss_kl: 1.385714, loss_recon: -0.837578, loss_pred: 0.103526
iteration 298: loss: 1.620728, loss_kl: 0.677700, loss_recon: -0.836600, loss_pred: 0.106429
iteration 299: loss: 2.455963, loss_kl: 1.386762, loss_recon: -0.824857, loss_pred: 0.244344
 23%|██████▏                    | 23/100 [2:07:59<6:54:13, 322.78s/it]iteration 300: loss: 2.532821, loss_kl: 1.615423, loss_recon: -0.833624, loss_pred: 0.083774
iteration 301: loss: 2.955654, loss_kl: 1.969297, loss_recon: -0.821461, loss_pred: 0.164895
iteration 302: loss: 2.229230, loss_kl: 1.288761, loss_recon: -0.830782, loss_pred: 0.109687
iteration 303: loss: 2.335659, loss_kl: 1.341252, loss_recon: -0.829754, loss_pred: 0.164653
iteration 304: loss: 1.839839, loss_kl: 0.896667, loss_recon: -0.830198, loss_pred: 0.112975
iteration 305: loss: 2.466409, loss_kl: 1.495392, loss_recon: -0.827391, loss_pred: 0.143626
iteration 306: loss: 1.760232, loss_kl: 0.755862, loss_recon: -0.830806, loss_pred: 0.173564
iteration 307: loss: 2.622022, loss_kl: 1.630834, loss_recon: -0.826184, loss_pred: 0.165004
iteration 308: loss: 1.880583, loss_kl: 0.945233, loss_recon: -0.827423, loss_pred: 0.107927
iteration 309: loss: 1.942042, loss_kl: 0.984829, loss_recon: -0.829719, loss_pred: 0.127494
iteration 310: loss: 2.135566, loss_kl: 1.159382, loss_recon: -0.830760, loss_pred: 0.145424
iteration 311: loss: 2.284079, loss_kl: 1.268508, loss_recon: -0.826456, loss_pred: 0.189116
iteration 312: loss: 3.685210, loss_kl: 2.732364, loss_recon: -0.829260, loss_pred: 0.123586
 24%|██████▍                    | 24/100 [2:13:23<6:49:14, 323.08s/it]iteration 313: loss: 3.266128, loss_kl: 2.318429, loss_recon: -0.820564, loss_pred: 0.127134
iteration 314: loss: 3.077466, loss_kl: 1.997001, loss_recon: -0.827685, loss_pred: 0.252780
iteration 315: loss: 2.698918, loss_kl: 1.776005, loss_recon: -0.822289, loss_pred: 0.100625
iteration 316: loss: 2.361055, loss_kl: 1.427049, loss_recon: -0.824821, loss_pred: 0.109185
iteration 317: loss: 3.246911, loss_kl: 2.084149, loss_recon: -0.821358, loss_pred: 0.341403
iteration 318: loss: 2.593991, loss_kl: 1.634190, loss_recon: -0.827778, loss_pred: 0.132023
iteration 319: loss: 2.381622, loss_kl: 1.446126, loss_recon: -0.818473, loss_pred: 0.117023
iteration 320: loss: 3.004133, loss_kl: 2.046072, loss_recon: -0.818089, loss_pred: 0.139972
iteration 321: loss: 2.463877, loss_kl: 1.515507, loss_recon: -0.821265, loss_pred: 0.127105
iteration 322: loss: 2.209343, loss_kl: 1.263724, loss_recon: -0.814962, loss_pred: 0.130657
iteration 323: loss: 1.764281, loss_kl: 0.848788, loss_recon: -0.820037, loss_pred: 0.095456
iteration 324: loss: 2.477575, loss_kl: 1.477526, loss_recon: -0.816342, loss_pred: 0.183707
iteration 325: loss: 2.512178, loss_kl: 1.464122, loss_recon: -0.822658, loss_pred: 0.225397
 25%|██████▊                    | 25/100 [2:18:46<6:43:40, 322.94s/it]iteration 326: loss: 1.014397, loss_kl: 1.740033, loss_recon: -0.817766, loss_pred: 0.192329
iteration 327: loss: 0.990344, loss_kl: 3.224944, loss_recon: -0.816962, loss_pred: 0.165408
iteration 328: loss: 0.948719, loss_kl: 5.609768, loss_recon: -0.814553, loss_pred: 0.120295
iteration 329: loss: 0.937881, loss_kl: 6.941582, loss_recon: -0.816558, loss_pred: 0.104160
iteration 330: loss: 0.962150, loss_kl: 10.675506, loss_recon: -0.811089, loss_pred: 0.124664
iteration 331: loss: 0.954523, loss_kl: 13.494456, loss_recon: -0.818406, loss_pred: 0.102750
iteration 332: loss: 0.939776, loss_kl: 16.107445, loss_recon: -0.815977, loss_pred: 0.083972
iteration 333: loss: 1.012826, loss_kl: 16.053371, loss_recon: -0.808162, loss_pred: 0.164971
iteration 334: loss: 1.042816, loss_kl: 18.114700, loss_recon: -0.819085, loss_pred: 0.178939
iteration 335: loss: 0.981498, loss_kl: 17.967754, loss_recon: -0.816957, loss_pred: 0.120113
iteration 336: loss: 0.988819, loss_kl: 22.125473, loss_recon: -0.810441, loss_pred: 0.123670
iteration 337: loss: 0.990563, loss_kl: 21.886963, loss_recon: -0.807748, loss_pred: 0.128697
iteration 338: loss: 1.030894, loss_kl: 22.808147, loss_recon: -0.817176, loss_pred: 0.157323
 26%|███████                    | 26/100 [2:24:08<6:38:08, 322.82s/it]iteration 339: loss: 1.103605, loss_kl: 23.846130, loss_recon: -0.805344, loss_pred: 0.144881
iteration 340: loss: 1.071362, loss_kl: 22.939787, loss_recon: -0.818642, loss_pred: 0.105169
iteration 341: loss: 1.077521, loss_kl: 21.280695, loss_recon: -0.811503, loss_pred: 0.129138
iteration 342: loss: 1.034021, loss_kl: 25.519180, loss_recon: -0.809055, loss_pred: 0.060824
iteration 343: loss: 1.026527, loss_kl: 21.786320, loss_recon: -0.807129, loss_pred: 0.079267
iteration 344: loss: 1.103612, loss_kl: 25.695145, loss_recon: -0.800947, loss_pred: 0.137391
iteration 345: loss: 1.049936, loss_kl: 24.850246, loss_recon: -0.806945, loss_pred: 0.083151
iteration 346: loss: 1.052695, loss_kl: 22.329130, loss_recon: -0.814584, loss_pred: 0.094487
iteration 347: loss: 1.070372, loss_kl: 22.654018, loss_recon: -0.805103, loss_pred: 0.119556
iteration 348: loss: 1.033574, loss_kl: 22.355129, loss_recon: -0.808989, loss_pred: 0.080794
iteration 349: loss: 1.050495, loss_kl: 21.460119, loss_recon: -0.804584, loss_pred: 0.107877
iteration 350: loss: 1.038292, loss_kl: 21.293507, loss_recon: -0.795775, loss_pred: 0.105555
iteration 351: loss: 1.202284, loss_kl: 23.468473, loss_recon: -0.809724, loss_pred: 0.241608
 27%|███████▎                   | 27/100 [2:29:31<6:32:44, 322.79s/it]iteration 352: loss: 1.257008, loss_kl: 22.414877, loss_recon: -0.799417, loss_pred: 0.084914
iteration 353: loss: 1.257367, loss_kl: 20.673813, loss_recon: -0.805697, loss_pred: 0.107941
iteration 354: loss: 1.322219, loss_kl: 21.197876, loss_recon: -0.802416, loss_pred: 0.167359
iteration 355: loss: 1.210113, loss_kl: 19.649422, loss_recon: -0.799164, loss_pred: 0.084251
iteration 356: loss: 1.341815, loss_kl: 20.085257, loss_recon: -0.803265, loss_pred: 0.204606
iteration 357: loss: 1.187769, loss_kl: 17.512543, loss_recon: -0.801073, loss_pred: 0.095526
iteration 358: loss: 1.216586, loss_kl: 15.048790, loss_recon: -0.803101, loss_pred: 0.163278
iteration 359: loss: 1.190195, loss_kl: 15.132174, loss_recon: -0.800341, loss_pred: 0.138261
iteration 360: loss: 1.055279, loss_kl: 11.348875, loss_recon: -0.802482, loss_pred: 0.064107
iteration 361: loss: 1.111078, loss_kl: 12.988058, loss_recon: -0.801210, loss_pred: 0.093924
iteration 362: loss: 1.088629, loss_kl: 11.181056, loss_recon: -0.800872, loss_pred: 0.101857
iteration 363: loss: 1.067383, loss_kl: 10.242541, loss_recon: -0.805846, loss_pred: 0.091240
iteration 364: loss: 1.132635, loss_kl: 10.400842, loss_recon: -0.798166, loss_pred: 0.161541
 28%|███████▌                   | 28/100 [2:34:55<6:27:53, 323.24s/it]iteration 365: loss: 1.229762, loss_kl: 7.983925, loss_recon: -0.801455, loss_pred: 0.090668
iteration 366: loss: 1.207172, loss_kl: 7.743137, loss_recon: -0.796516, loss_pred: 0.083200
iteration 367: loss: 1.179608, loss_kl: 7.047450, loss_recon: -0.800697, loss_pred: 0.080876
iteration 368: loss: 1.094245, loss_kl: 5.437006, loss_recon: -0.792107, loss_pred: 0.072209
iteration 369: loss: 1.073177, loss_kl: 4.944132, loss_recon: -0.792442, loss_pred: 0.071648
iteration 370: loss: 1.092444, loss_kl: 3.988788, loss_recon: -0.798443, loss_pred: 0.125316
iteration 371: loss: 1.081835, loss_kl: 4.578535, loss_recon: -0.792994, loss_pred: 0.095216
iteration 372: loss: 1.039811, loss_kl: 3.925619, loss_recon: -0.795188, loss_pred: 0.078609
iteration 373: loss: 0.992878, loss_kl: 2.328629, loss_recon: -0.795358, loss_pred: 0.099043
iteration 374: loss: 0.956282, loss_kl: 2.101655, loss_recon: -0.792083, loss_pred: 0.075321
iteration 375: loss: 0.982722, loss_kl: 2.676733, loss_recon: -0.793686, loss_pred: 0.075838
iteration 376: loss: 0.951579, loss_kl: 1.808339, loss_recon: -0.793802, loss_pred: 0.081303
iteration 377: loss: 0.961433, loss_kl: 2.301396, loss_recon: -0.797524, loss_pred: 0.066583
 29%|███████▊                   | 29/100 [2:40:18<6:22:18, 323.08s/it]iteration 378: loss: 1.106917, loss_kl: 2.152303, loss_recon: -0.791080, loss_pred: 0.093287
iteration 379: loss: 1.037473, loss_kl: 1.476197, loss_recon: -0.788872, loss_pred: 0.095961
iteration 380: loss: 1.024146, loss_kl: 1.682272, loss_recon: -0.784465, loss_pred: 0.065733
iteration 381: loss: 1.100507, loss_kl: 1.833220, loss_recon: -0.794222, loss_pred: 0.116729
iteration 382: loss: 1.057885, loss_kl: 1.413568, loss_recon: -0.789333, loss_pred: 0.122388
iteration 383: loss: 0.952217, loss_kl: 1.039255, loss_recon: -0.795459, loss_pred: 0.049299
iteration 384: loss: 1.149483, loss_kl: 1.319423, loss_recon: -0.782487, loss_pred: 0.230566
iteration 385: loss: 0.953222, loss_kl: 0.921362, loss_recon: -0.788946, loss_pred: 0.069007
iteration 386: loss: 0.941069, loss_kl: 0.866589, loss_recon: -0.785793, loss_pred: 0.065670
iteration 387: loss: 1.034398, loss_kl: 1.192621, loss_recon: -0.787469, loss_pred: 0.123611
iteration 388: loss: 0.934428, loss_kl: 0.751184, loss_recon: -0.786880, loss_pred: 0.069875
iteration 389: loss: 0.943699, loss_kl: 0.921027, loss_recon: -0.787309, loss_pred: 0.061155
iteration 390: loss: 0.991732, loss_kl: 0.492292, loss_recon: -0.785690, loss_pred: 0.155139
 30%|████████                   | 30/100 [2:45:41<6:16:49, 323.00s/it]iteration 391: loss: 1.004519, loss_kl: 0.598924, loss_recon: -0.781087, loss_pred: 0.084796
iteration 392: loss: 1.016595, loss_kl: 0.723541, loss_recon: -0.787081, loss_pred: 0.062033
iteration 393: loss: 1.137555, loss_kl: 1.112900, loss_recon: -0.778111, loss_pred: 0.101835
iteration 394: loss: 1.079462, loss_kl: 0.804964, loss_recon: -0.787298, loss_pred: 0.105835
iteration 395: loss: 1.011384, loss_kl: 0.755901, loss_recon: -0.785525, loss_pred: 0.050886
iteration 396: loss: 1.022341, loss_kl: 0.724241, loss_recon: -0.789361, loss_pred: 0.065337
iteration 397: loss: 1.278070, loss_kl: 1.028651, loss_recon: -0.782526, loss_pred: 0.257437
iteration 398: loss: 1.096565, loss_kl: 1.070275, loss_recon: -0.788841, loss_pred: 0.059982
iteration 399: loss: 1.110429, loss_kl: 0.908121, loss_recon: -0.785808, loss_pred: 0.114413
iteration 400: loss: 1.091240, loss_kl: 0.821854, loss_recon: -0.783753, loss_pred: 0.117248
iteration 401: loss: 0.954106, loss_kl: 0.490806, loss_recon: -0.780404, loss_pred: 0.060093
iteration 402: loss: 1.055771, loss_kl: 0.788626, loss_recon: -0.781409, loss_pred: 0.091814
iteration 403: loss: 1.007413, loss_kl: 0.443594, loss_recon: -0.772857, loss_pred: 0.131875
 31%|████████▎                  | 31/100 [2:51:03<6:11:11, 322.78s/it]iteration 404: loss: 1.121873, loss_kl: 0.530704, loss_recon: -0.789327, loss_pred: 0.098884
iteration 405: loss: 1.083604, loss_kl: 0.493438, loss_recon: -0.776441, loss_pred: 0.089909
iteration 406: loss: 0.843494, loss_kl: -0.126095, loss_recon: -0.785361, loss_pred: 0.113651
iteration 407: loss: 0.988798, loss_kl: 0.361735, loss_recon: -0.777335, loss_pred: 0.052196
iteration 408: loss: 1.410857, loss_kl: 1.142097, loss_recon: -0.773390, loss_pred: 0.134617
iteration 409: loss: 1.395128, loss_kl: 1.213801, loss_recon: -0.782086, loss_pred: 0.078621
iteration 410: loss: 1.462782, loss_kl: 1.392513, loss_recon: -0.778834, loss_pred: 0.070844
iteration 411: loss: 1.389513, loss_kl: 1.264840, loss_recon: -0.771242, loss_pred: 0.061380
iteration 412: loss: 1.189915, loss_kl: 0.787395, loss_recon: -0.776853, loss_pred: 0.066382
iteration 413: loss: 1.287524, loss_kl: 1.022870, loss_recon: -0.773088, loss_pred: 0.064080
iteration 414: loss: 1.086448, loss_kl: 0.576686, loss_recon: -0.776470, loss_pred: 0.056071
iteration 415: loss: 1.209058, loss_kl: 0.837201, loss_recon: -0.777356, loss_pred: 0.063094
iteration 416: loss: 1.289929, loss_kl: 0.968551, loss_recon: -0.782376, loss_pred: 0.081113
 32%|████████▋                  | 32/100 [2:56:26<6:06:00, 322.94s/it]iteration 417: loss: 1.233232, loss_kl: 0.526033, loss_recon: -0.782803, loss_pred: 0.096616
iteration 418: loss: 1.491515, loss_kl: 0.974724, loss_recon: -0.771536, loss_pred: 0.064373
iteration 419: loss: 1.286194, loss_kl: 0.659135, loss_recon: -0.781050, loss_pred: 0.061805
iteration 420: loss: 1.728716, loss_kl: 1.251998, loss_recon: -0.772489, loss_pred: 0.114124
iteration 421: loss: 1.722104, loss_kl: 1.242404, loss_recon: -0.776076, loss_pred: 0.110379
iteration 422: loss: 1.433679, loss_kl: 0.861065, loss_recon: -0.772410, loss_pred: 0.082110
iteration 423: loss: 1.331671, loss_kl: 0.699721, loss_recon: -0.764157, loss_pred: 0.096877
iteration 424: loss: 1.477012, loss_kl: 0.900908, loss_recon: -0.769119, loss_pred: 0.101936
iteration 425: loss: 1.187275, loss_kl: 0.505466, loss_recon: -0.766321, loss_pred: 0.080974
iteration 426: loss: 1.464752, loss_kl: 0.885121, loss_recon: -0.774022, loss_pred: 0.095391
iteration 427: loss: 1.384950, loss_kl: 0.792278, loss_recon: -0.768264, loss_pred: 0.083795
iteration 428: loss: 1.393165, loss_kl: 0.849404, loss_recon: -0.770130, loss_pred: 0.051720
iteration 429: loss: 1.708835, loss_kl: 1.167570, loss_recon: -0.762838, loss_pred: 0.160681
 33%|████████▉                  | 33/100 [3:01:48<6:00:23, 322.74s/it]iteration 430: loss: 1.661935, loss_kl: 0.947073, loss_recon: -0.763651, loss_pred: 0.099993
iteration 431: loss: 1.515209, loss_kl: 0.809666, loss_recon: -0.766011, loss_pred: 0.066726
iteration 432: loss: 1.857991, loss_kl: 1.200698, loss_recon: -0.770755, loss_pred: 0.075164
iteration 433: loss: 1.802356, loss_kl: 1.142532, loss_recon: -0.776825, loss_pred: 0.062485
iteration 434: loss: 1.839321, loss_kl: 1.103739, loss_recon: -0.768451, loss_pred: 0.140524
iteration 435: loss: 2.169400, loss_kl: 1.560966, loss_recon: -0.769709, loss_pred: 0.083946
iteration 436: loss: 1.734988, loss_kl: 1.076743, loss_recon: -0.767183, loss_pred: 0.060214
iteration 437: loss: 1.719578, loss_kl: 1.032424, loss_recon: -0.765129, loss_pred: 0.084214
iteration 438: loss: 1.762303, loss_kl: 1.106527, loss_recon: -0.771675, loss_pred: 0.057932
iteration 439: loss: 1.699731, loss_kl: 0.978076, loss_recon: -0.757149, loss_pred: 0.118157
iteration 440: loss: 2.177417, loss_kl: 1.575689, loss_recon: -0.761271, loss_pred: 0.087990
iteration 441: loss: 1.658502, loss_kl: 0.967314, loss_recon: -0.757510, loss_pred: 0.085638
iteration 442: loss: 1.725503, loss_kl: 0.926709, loss_recon: -0.788066, loss_pred: 0.156309
 34%|█████████▏                 | 34/100 [3:07:11<5:54:58, 322.70s/it]iteration 443: loss: 2.093524, loss_kl: 1.345890, loss_recon: -0.770018, loss_pred: 0.067263
iteration 444: loss: 1.820262, loss_kl: 1.039129, loss_recon: -0.766429, loss_pred: 0.083919
iteration 445: loss: 1.935669, loss_kl: 1.177058, loss_recon: -0.762831, loss_pred: 0.074181
iteration 446: loss: 2.211841, loss_kl: 1.474543, loss_recon: -0.769927, loss_pred: 0.065587
iteration 447: loss: 2.171990, loss_kl: 1.435327, loss_recon: -0.751561, loss_pred: 0.080707
iteration 448: loss: 1.877039, loss_kl: 1.129204, loss_recon: -0.756389, loss_pred: 0.066660
iteration 449: loss: 1.335508, loss_kl: 0.541139, loss_recon: -0.758996, loss_pred: 0.071417
iteration 450: loss: 1.836282, loss_kl: 1.097204, loss_recon: -0.761814, loss_pred: 0.050346
iteration 451: loss: 2.048672, loss_kl: 1.292793, loss_recon: -0.767214, loss_pred: 0.074776
iteration 452: loss: 1.624299, loss_kl: 0.832490, loss_recon: -0.770279, loss_pred: 0.076981
iteration 453: loss: 1.888378, loss_kl: 1.130636, loss_recon: -0.757483, loss_pred: 0.075567
iteration 454: loss: 1.856284, loss_kl: 1.093416, loss_recon: -0.760634, loss_pred: 0.075064
iteration 455: loss: 1.055636, loss_kl: 0.199999, loss_recon: -0.753061, loss_pred: 0.115897
 35%|█████████▍                 | 35/100 [3:12:35<5:50:01, 323.10s/it]iteration 456: loss: 2.014752, loss_kl: 1.210473, loss_recon: -0.756344, loss_pred: 0.080130
iteration 457: loss: 2.273758, loss_kl: 1.460607, loss_recon: -0.760430, loss_pred: 0.091569
iteration 458: loss: 2.468260, loss_kl: 1.692830, loss_recon: -0.760387, loss_pred: 0.060068
iteration 459: loss: 1.840864, loss_kl: 1.013445, loss_recon: -0.759160, loss_pred: 0.095213
iteration 460: loss: 2.490420, loss_kl: 1.638701, loss_recon: -0.752897, loss_pred: 0.142406
iteration 461: loss: 2.021502, loss_kl: 1.241457, loss_recon: -0.754928, loss_pred: 0.058136
iteration 462: loss: 2.050754, loss_kl: 1.231417, loss_recon: -0.753074, loss_pred: 0.099015
iteration 463: loss: 2.186249, loss_kl: 1.386779, loss_recon: -0.755320, loss_pred: 0.081034
iteration 464: loss: 2.226536, loss_kl: 1.451269, loss_recon: -0.751174, loss_pred: 0.062693
iteration 465: loss: 2.030849, loss_kl: 1.230603, loss_recon: -0.757135, loss_pred: 0.075841
iteration 466: loss: 1.887041, loss_kl: 1.109092, loss_recon: -0.757351, loss_pred: 0.050097
iteration 467: loss: 2.456016, loss_kl: 1.655120, loss_recon: -0.752442, loss_pred: 0.092474
iteration 468: loss: 2.146250, loss_kl: 1.360443, loss_recon: -0.748418, loss_pred: 0.073572
 36%|█████████▋                 | 36/100 [3:17:58<5:44:32, 323.01s/it]iteration 469: loss: 2.545353, loss_kl: 1.734221, loss_recon: -0.746867, loss_pred: 0.082221
iteration 470: loss: 2.456688, loss_kl: 1.640337, loss_recon: -0.747630, loss_pred: 0.085705
iteration 471: loss: 2.430221, loss_kl: 1.607457, loss_recon: -0.744455, loss_pred: 0.094952
iteration 472: loss: 1.889655, loss_kl: 1.047060, loss_recon: -0.747630, loss_pred: 0.105806
iteration 473: loss: 2.311049, loss_kl: 1.507644, loss_recon: -0.748729, loss_pred: 0.070287
iteration 474: loss: 2.157888, loss_kl: 1.360587, loss_recon: -0.750113, loss_pred: 0.061274
iteration 475: loss: 1.912713, loss_kl: 1.112130, loss_recon: -0.755373, loss_pred: 0.056725
iteration 476: loss: 2.142576, loss_kl: 1.336609, loss_recon: -0.745137, loss_pred: 0.074669
iteration 477: loss: 2.205827, loss_kl: 1.403054, loss_recon: -0.750407, loss_pred: 0.066893
iteration 478: loss: 1.884760, loss_kl: 1.019781, loss_recon: -0.757774, loss_pred: 0.117763
iteration 479: loss: 1.730401, loss_kl: 0.910650, loss_recon: -0.750493, loss_pred: 0.078687
iteration 480: loss: 1.908412, loss_kl: 1.067428, loss_recon: -0.752914, loss_pred: 0.099121
iteration 481: loss: 2.249805, loss_kl: 1.367237, loss_recon: -0.749591, loss_pred: 0.147133
 37%|█████████▉                 | 37/100 [3:23:22<5:39:25, 323.26s/it]iteration 482: loss: 2.128567, loss_kl: 1.286169, loss_recon: -0.745554, loss_pred: 0.101975
iteration 483: loss: 2.595967, loss_kl: 1.781454, loss_recon: -0.743095, loss_pred: 0.078526
iteration 484: loss: 1.927528, loss_kl: 1.078626, loss_recon: -0.742218, loss_pred: 0.110988
iteration 485: loss: 2.838684, loss_kl: 2.030105, loss_recon: -0.739650, loss_pred: 0.077029
iteration 486: loss: 2.143921, loss_kl: 1.334583, loss_recon: -0.747221, loss_pred: 0.067442
iteration 487: loss: 2.996891, loss_kl: 2.161139, loss_recon: -0.754081, loss_pred: 0.090294
iteration 488: loss: 2.568829, loss_kl: 1.655067, loss_recon: -0.751956, loss_pred: 0.168410
iteration 489: loss: 2.076728, loss_kl: 1.168339, loss_recon: -0.745409, loss_pred: 0.167641
iteration 490: loss: 2.640502, loss_kl: 1.778672, loss_recon: -0.751303, loss_pred: 0.117624
iteration 491: loss: 2.300430, loss_kl: 1.408904, loss_recon: -0.747792, loss_pred: 0.149355
iteration 492: loss: 2.150472, loss_kl: 1.327336, loss_recon: -0.745312, loss_pred: 0.083120
iteration 493: loss: 2.638124, loss_kl: 1.827615, loss_recon: -0.746625, loss_pred: 0.071176
iteration 494: loss: 3.003464, loss_kl: 2.152170, loss_recon: -0.755067, loss_pred: 0.104814
 38%|██████████▎                | 38/100 [3:28:44<5:33:51, 323.09s/it]iteration 495: loss: 2.462832, loss_kl: 1.487177, loss_recon: -0.742375, loss_pred: 0.233280
iteration 496: loss: 3.432100, loss_kl: 2.540871, loss_recon: -0.746176, loss_pred: 0.145052
iteration 497: loss: 2.384478, loss_kl: 1.490104, loss_recon: -0.741145, loss_pred: 0.153229
iteration 498: loss: 2.779630, loss_kl: 1.954123, loss_recon: -0.741952, loss_pred: 0.083555
iteration 499: loss: 2.551380, loss_kl: 1.707332, loss_recon: -0.742948, loss_pred: 0.101100
iteration 500: loss: 2.107651, loss_kl: 1.263263, loss_recon: -0.742017, loss_pred: 0.102371
iteration 501: loss: 2.479279, loss_kl: 1.612263, loss_recon: -0.729486, loss_pred: 0.137529
iteration 502: loss: 2.339547, loss_kl: 1.444113, loss_recon: -0.739619, loss_pred: 0.155815
iteration 503: loss: 2.519131, loss_kl: 1.585082, loss_recon: -0.742989, loss_pred: 0.191060
iteration 504: loss: 1.904373, loss_kl: 1.092223, loss_recon: -0.742693, loss_pred: 0.069456
iteration 505: loss: 2.347031, loss_kl: 1.468842, loss_recon: -0.742920, loss_pred: 0.135270
iteration 506: loss: 3.057982, loss_kl: 2.058150, loss_recon: -0.742711, loss_pred: 0.257121
iteration 507: loss: 2.159467, loss_kl: 1.310641, loss_recon: -0.747512, loss_pred: 0.101315
 39%|██████████▌                | 39/100 [3:34:07<5:28:14, 322.86s/it]iteration 508: loss: 1.853693, loss_kl: 1.034902, loss_recon: -0.730451, loss_pred: 0.088339
iteration 509: loss: 2.917484, loss_kl: 2.003392, loss_recon: -0.738459, loss_pred: 0.175633
iteration 510: loss: 2.546138, loss_kl: 1.604278, loss_recon: -0.738985, loss_pred: 0.202875
iteration 511: loss: 2.226140, loss_kl: 1.359291, loss_recon: -0.750615, loss_pred: 0.116234
iteration 512: loss: 2.633541, loss_kl: 1.615831, loss_recon: -0.734814, loss_pred: 0.282897
iteration 513: loss: 2.564383, loss_kl: 1.325761, loss_recon: -0.734511, loss_pred: 0.504111
iteration 514: loss: 2.293356, loss_kl: 1.408787, loss_recon: -0.730847, loss_pred: 0.153722
iteration 515: loss: 2.329191, loss_kl: 1.475575, loss_recon: -0.744251, loss_pred: 0.109365
iteration 516: loss: 2.489602, loss_kl: 1.586688, loss_recon: -0.741426, loss_pred: 0.161488
iteration 517: loss: 2.426844, loss_kl: 1.583303, loss_recon: -0.731889, loss_pred: 0.111651
iteration 518: loss: 2.249434, loss_kl: 1.446133, loss_recon: -0.736282, loss_pred: 0.067019
iteration 519: loss: 1.768717, loss_kl: 0.942758, loss_recon: -0.733971, loss_pred: 0.091988
iteration 520: loss: 2.177632, loss_kl: 1.307606, loss_recon: -0.727648, loss_pred: 0.142378
 40%|██████████▊                | 40/100 [3:39:29<5:22:44, 322.74s/it]iteration 521: loss: 2.425344, loss_kl: 1.595200, loss_recon: -0.743757, loss_pred: 0.086387
iteration 522: loss: 1.951217, loss_kl: 1.143951, loss_recon: -0.727618, loss_pred: 0.079649
iteration 523: loss: 2.638070, loss_kl: 1.818592, loss_recon: -0.729079, loss_pred: 0.090399
iteration 524: loss: 2.569616, loss_kl: 1.751778, loss_recon: -0.732893, loss_pred: 0.084945
iteration 525: loss: 2.113208, loss_kl: 1.325604, loss_recon: -0.729819, loss_pred: 0.057784
iteration 526: loss: 1.709143, loss_kl: 0.875627, loss_recon: -0.730744, loss_pred: 0.102772
iteration 527: loss: 1.854644, loss_kl: 1.029216, loss_recon: -0.731385, loss_pred: 0.094043
iteration 528: loss: 1.903617, loss_kl: 1.067366, loss_recon: -0.741158, loss_pred: 0.095093
iteration 529: loss: 1.774963, loss_kl: 0.942116, loss_recon: -0.721691, loss_pred: 0.111156
iteration 530: loss: 2.256578, loss_kl: 1.437485, loss_recon: -0.732448, loss_pred: 0.086645
iteration 531: loss: 1.618622, loss_kl: 0.797782, loss_recon: -0.734735, loss_pred: 0.086105
iteration 532: loss: 2.065149, loss_kl: 1.252955, loss_recon: -0.725982, loss_pred: 0.086212
iteration 533: loss: 1.851789, loss_kl: 0.934484, loss_recon: -0.738425, loss_pred: 0.178880
 41%|███████████                | 41/100 [3:44:52<5:17:20, 322.71s/it]iteration 534: loss: 2.139086, loss_kl: 1.318301, loss_recon: -0.727124, loss_pred: 0.093660
iteration 535: loss: 2.271095, loss_kl: 1.407662, loss_recon: -0.731066, loss_pred: 0.132367
iteration 536: loss: 2.282754, loss_kl: 1.438974, loss_recon: -0.725074, loss_pred: 0.118706
iteration 537: loss: 2.158009, loss_kl: 1.361905, loss_recon: -0.727244, loss_pred: 0.068860
iteration 538: loss: 1.966961, loss_kl: 1.164931, loss_recon: -0.723487, loss_pred: 0.078544
iteration 539: loss: 2.023533, loss_kl: 1.226044, loss_recon: -0.729497, loss_pred: 0.067992
iteration 540: loss: 1.567849, loss_kl: 0.755780, loss_recon: -0.727306, loss_pred: 0.084763
iteration 541: loss: 2.245339, loss_kl: 1.427582, loss_recon: -0.731993, loss_pred: 0.085764
iteration 542: loss: 1.786929, loss_kl: 0.935775, loss_recon: -0.729015, loss_pred: 0.122138
iteration 543: loss: 1.730011, loss_kl: 0.921655, loss_recon: -0.726683, loss_pred: 0.081672
iteration 544: loss: 1.559590, loss_kl: 0.723077, loss_recon: -0.727696, loss_pred: 0.108817
iteration 545: loss: 2.337186, loss_kl: 1.491004, loss_recon: -0.728356, loss_pred: 0.117826
iteration 546: loss: 2.166713, loss_kl: 1.370481, loss_recon: -0.729776, loss_pred: 0.066456
 42%|███████████▎               | 42/100 [3:50:15<5:11:56, 322.69s/it]iteration 547: loss: 1.957023, loss_kl: 1.153027, loss_recon: -0.728801, loss_pred: 0.075195
iteration 548: loss: 2.375595, loss_kl: 1.509463, loss_recon: -0.715855, loss_pred: 0.150277
iteration 549: loss: 2.746925, loss_kl: 1.975863, loss_recon: -0.725156, loss_pred: 0.045907
iteration 550: loss: 1.723593, loss_kl: 0.919719, loss_recon: -0.719186, loss_pred: 0.084687
iteration 551: loss: 2.222703, loss_kl: 1.377648, loss_recon: -0.735224, loss_pred: 0.109831
iteration 552: loss: 2.330986, loss_kl: 1.530222, loss_recon: -0.728906, loss_pred: 0.071858
iteration 553: loss: 1.969734, loss_kl: 1.106228, loss_recon: -0.722660, loss_pred: 0.140846
iteration 554: loss: 1.865878, loss_kl: 0.947346, loss_recon: -0.720851, loss_pred: 0.197681
iteration 555: loss: 1.981083, loss_kl: 1.139187, loss_recon: -0.721795, loss_pred: 0.120102
iteration 556: loss: 2.859447, loss_kl: 2.000501, loss_recon: -0.723868, loss_pred: 0.135078
iteration 557: loss: 2.498427, loss_kl: 1.696564, loss_recon: -0.727620, loss_pred: 0.074243
iteration 558: loss: 2.326190, loss_kl: 1.548681, loss_recon: -0.728569, loss_pred: 0.048939
iteration 559: loss: 2.274137, loss_kl: 1.432033, loss_recon: -0.719965, loss_pred: 0.122139
 43%|███████████▌               | 43/100 [3:55:37<5:06:25, 322.56s/it]iteration 560: loss: 2.439411, loss_kl: 1.621843, loss_recon: -0.729145, loss_pred: 0.088422
iteration 561: loss: 2.081070, loss_kl: 1.289611, loss_recon: -0.722625, loss_pred: 0.068834
iteration 562: loss: 1.890561, loss_kl: 1.088586, loss_recon: -0.720534, loss_pred: 0.081440
iteration 563: loss: 1.706635, loss_kl: 0.925230, loss_recon: -0.725899, loss_pred: 0.055506
iteration 564: loss: 1.578133, loss_kl: 0.799568, loss_recon: -0.723813, loss_pred: 0.054751
iteration 565: loss: 1.889264, loss_kl: 1.106580, loss_recon: -0.715898, loss_pred: 0.066786
iteration 566: loss: 2.099521, loss_kl: 1.311543, loss_recon: -0.710819, loss_pred: 0.077159
iteration 567: loss: 1.676060, loss_kl: 0.877946, loss_recon: -0.718817, loss_pred: 0.079296
iteration 568: loss: 1.909035, loss_kl: 1.105003, loss_recon: -0.711674, loss_pred: 0.092359
iteration 569: loss: 2.163067, loss_kl: 1.337088, loss_recon: -0.729179, loss_pred: 0.096800
iteration 570: loss: 2.009202, loss_kl: 1.220160, loss_recon: -0.721813, loss_pred: 0.067229
iteration 571: loss: 2.293829, loss_kl: 1.458950, loss_recon: -0.710599, loss_pred: 0.124280
iteration 572: loss: 2.013978, loss_kl: 1.150401, loss_recon: -0.746949, loss_pred: 0.116629
 44%|███████████▉               | 44/100 [4:00:59<5:01:01, 322.53s/it]iteration 573: loss: 1.733195, loss_kl: 0.922275, loss_recon: -0.716288, loss_pred: 0.094632
iteration 574: loss: 1.938589, loss_kl: 1.138664, loss_recon: -0.713949, loss_pred: 0.085975
iteration 575: loss: 1.761801, loss_kl: 0.975126, loss_recon: -0.713942, loss_pred: 0.072733
iteration 576: loss: 1.837407, loss_kl: 1.070130, loss_recon: -0.715146, loss_pred: 0.052131
iteration 577: loss: 1.657830, loss_kl: 0.863610, loss_recon: -0.708383, loss_pred: 0.085836
iteration 578: loss: 1.489033, loss_kl: 0.710399, loss_recon: -0.714129, loss_pred: 0.064505
iteration 579: loss: 2.045330, loss_kl: 1.280280, loss_recon: -0.708616, loss_pred: 0.056434
iteration 580: loss: 1.600226, loss_kl: 0.811648, loss_recon: -0.714308, loss_pred: 0.074269
iteration 581: loss: 2.005364, loss_kl: 1.225979, loss_recon: -0.721197, loss_pred: 0.058187
iteration 582: loss: 2.224153, loss_kl: 1.456268, loss_recon: -0.717120, loss_pred: 0.050765
iteration 583: loss: 2.402606, loss_kl: 1.632635, loss_recon: -0.717988, loss_pred: 0.051982
iteration 584: loss: 2.510910, loss_kl: 1.740693, loss_recon: -0.716682, loss_pred: 0.053534
iteration 585: loss: 2.307398, loss_kl: 1.523669, loss_recon: -0.736473, loss_pred: 0.047257
 45%|████████████▏              | 45/100 [4:06:22<4:55:38, 322.51s/it]iteration 586: loss: 1.934319, loss_kl: 1.082516, loss_recon: -0.715699, loss_pred: 0.136104
iteration 587: loss: 1.684283, loss_kl: 0.901538, loss_recon: -0.709313, loss_pred: 0.073432
iteration 588: loss: 1.858937, loss_kl: 1.026349, loss_recon: -0.719586, loss_pred: 0.113001
iteration 589: loss: 1.918741, loss_kl: 1.136516, loss_recon: -0.712102, loss_pred: 0.070123
iteration 590: loss: 1.870652, loss_kl: 1.116654, loss_recon: -0.711887, loss_pred: 0.042111
iteration 591: loss: 1.932253, loss_kl: 1.165178, loss_recon: -0.714576, loss_pred: 0.052498
iteration 592: loss: 1.895657, loss_kl: 1.124209, loss_recon: -0.715597, loss_pred: 0.055851
iteration 593: loss: 2.061757, loss_kl: 1.241272, loss_recon: -0.718423, loss_pred: 0.102062
iteration 594: loss: 2.174300, loss_kl: 1.350007, loss_recon: -0.711815, loss_pred: 0.112478
iteration 595: loss: 1.726668, loss_kl: 0.928469, loss_recon: -0.716983, loss_pred: 0.081215
iteration 596: loss: 2.630188, loss_kl: 1.825874, loss_recon: -0.705668, loss_pred: 0.098646
iteration 597: loss: 2.382018, loss_kl: 1.598879, loss_recon: -0.706764, loss_pred: 0.076375
iteration 598: loss: 1.826720, loss_kl: 1.057338, loss_recon: -0.714971, loss_pred: 0.054411
 46%|████████████▍              | 46/100 [4:11:45<4:50:35, 322.88s/it]iteration 599: loss: 2.426769, loss_kl: 1.647291, loss_recon: -0.698135, loss_pred: 0.081343
iteration 600: loss: 2.436815, loss_kl: 1.655503, loss_recon: -0.714575, loss_pred: 0.066736
iteration 601: loss: 2.212712, loss_kl: 1.437165, loss_recon: -0.711042, loss_pred: 0.064505
iteration 602: loss: 2.433599, loss_kl: 1.677882, loss_recon: -0.704085, loss_pred: 0.051632
iteration 603: loss: 2.067757, loss_kl: 1.284590, loss_recon: -0.723448, loss_pred: 0.059719
iteration 604: loss: 2.086651, loss_kl: 1.289747, loss_recon: -0.715206, loss_pred: 0.081698
iteration 605: loss: 1.370881, loss_kl: 0.547853, loss_recon: -0.711938, loss_pred: 0.111091
iteration 606: loss: 2.733031, loss_kl: 1.821551, loss_recon: -0.695694, loss_pred: 0.215785
iteration 607: loss: 2.085147, loss_kl: 1.299034, loss_recon: -0.707942, loss_pred: 0.078171
iteration 608: loss: 1.709492, loss_kl: 0.911142, loss_recon: -0.707709, loss_pred: 0.090640
iteration 609: loss: 2.265293, loss_kl: 1.475055, loss_recon: -0.712217, loss_pred: 0.078021
iteration 610: loss: 2.151170, loss_kl: 1.366008, loss_recon: -0.705182, loss_pred: 0.079980
iteration 611: loss: 1.886553, loss_kl: 1.082453, loss_recon: -0.717080, loss_pred: 0.087020
 47%|████████████▋              | 47/100 [4:17:09<4:45:20, 323.03s/it]iteration 612: loss: 2.042811, loss_kl: 1.280531, loss_recon: -0.699658, loss_pred: 0.062622
iteration 613: loss: 2.234414, loss_kl: 1.460914, loss_recon: -0.710316, loss_pred: 0.063183
iteration 614: loss: 2.398640, loss_kl: 1.626201, loss_recon: -0.708736, loss_pred: 0.063703
iteration 615: loss: 1.989267, loss_kl: 1.202266, loss_recon: -0.702760, loss_pred: 0.084241
iteration 616: loss: 1.477408, loss_kl: 0.715345, loss_recon: -0.711594, loss_pred: 0.050470
iteration 617: loss: 2.208863, loss_kl: 1.424226, loss_recon: -0.702458, loss_pred: 0.082180
iteration 618: loss: 2.954732, loss_kl: 2.162707, loss_recon: -0.697562, loss_pred: 0.094463
iteration 619: loss: 1.636054, loss_kl: 0.824478, loss_recon: -0.710275, loss_pred: 0.101302
iteration 620: loss: 2.549803, loss_kl: 1.793385, loss_recon: -0.699721, loss_pred: 0.056697
iteration 621: loss: 1.962037, loss_kl: 1.175222, loss_recon: -0.707235, loss_pred: 0.079581
iteration 622: loss: 1.879544, loss_kl: 1.089585, loss_recon: -0.697116, loss_pred: 0.092843
iteration 623: loss: 2.007175, loss_kl: 1.236042, loss_recon: -0.707023, loss_pred: 0.064110
iteration 624: loss: 2.308484, loss_kl: 1.531184, loss_recon: -0.704534, loss_pred: 0.072766
 48%|████████████▉              | 48/100 [4:22:32<4:39:55, 322.99s/it]iteration 625: loss: 2.218382, loss_kl: 1.421414, loss_recon: -0.702791, loss_pred: 0.094176
iteration 626: loss: 2.692877, loss_kl: 1.859653, loss_recon: -0.702957, loss_pred: 0.130267
iteration 627: loss: 2.785000, loss_kl: 2.008465, loss_recon: -0.700142, loss_pred: 0.076393
iteration 628: loss: 2.489361, loss_kl: 1.685595, loss_recon: -0.703848, loss_pred: 0.099918
iteration 629: loss: 1.878929, loss_kl: 1.078347, loss_recon: -0.712882, loss_pred: 0.087700
iteration 630: loss: 2.391525, loss_kl: 1.601591, loss_recon: -0.697941, loss_pred: 0.091993
iteration 631: loss: 2.259694, loss_kl: 1.488009, loss_recon: -0.709181, loss_pred: 0.062504
iteration 632: loss: 2.166100, loss_kl: 1.385118, loss_recon: -0.699397, loss_pred: 0.081585
iteration 633: loss: 1.713900, loss_kl: 0.952160, loss_recon: -0.698969, loss_pred: 0.062770
iteration 634: loss: 1.772732, loss_kl: 0.987650, loss_recon: -0.701028, loss_pred: 0.084054
iteration 635: loss: 2.791826, loss_kl: 2.024140, loss_recon: -0.696212, loss_pred: 0.071474
iteration 636: loss: 1.956434, loss_kl: 1.209002, loss_recon: -0.699577, loss_pred: 0.047854
iteration 637: loss: 1.866627, loss_kl: 1.103842, loss_recon: -0.700516, loss_pred: 0.062269
 49%|█████████████▏             | 49/100 [4:27:54<4:34:27, 322.88s/it]iteration 638: loss: 2.105537, loss_kl: 1.340980, loss_recon: -0.693571, loss_pred: 0.070986
iteration 639: loss: 2.357036, loss_kl: 1.590402, loss_recon: -0.703521, loss_pred: 0.063112
iteration 640: loss: 1.966068, loss_kl: 1.220302, loss_recon: -0.695866, loss_pred: 0.049900
iteration 641: loss: 1.534585, loss_kl: 0.779065, loss_recon: -0.705991, loss_pred: 0.049529
iteration 642: loss: 2.125649, loss_kl: 1.347403, loss_recon: -0.695771, loss_pred: 0.082475
iteration 643: loss: 2.002004, loss_kl: 1.239957, loss_recon: -0.692941, loss_pred: 0.069106
iteration 644: loss: 1.652995, loss_kl: 0.906131, loss_recon: -0.682641, loss_pred: 0.064223
iteration 645: loss: 2.179165, loss_kl: 1.390900, loss_recon: -0.702261, loss_pred: 0.086005
iteration 646: loss: 1.719281, loss_kl: 0.967219, loss_recon: -0.703500, loss_pred: 0.048562
iteration 647: loss: 1.646033, loss_kl: 0.826329, loss_recon: -0.695601, loss_pred: 0.124103
iteration 648: loss: 2.109727, loss_kl: 1.349906, loss_recon: -0.695351, loss_pred: 0.064471
iteration 649: loss: 2.071708, loss_kl: 1.270221, loss_recon: -0.708230, loss_pred: 0.093257
iteration 650: loss: 2.394918, loss_kl: 1.541900, loss_recon: -0.708927, loss_pred: 0.144091
 50%|█████████████▌             | 50/100 [4:33:17<4:29:07, 322.94s/it]iteration 651: loss: 0.807367, loss_kl: 0.796413, loss_recon: -0.688731, loss_pred: 0.116667
iteration 652: loss: 0.764628, loss_kl: 1.702844, loss_recon: -0.695829, loss_pred: 0.064588
iteration 653: loss: 0.750093, loss_kl: 2.577303, loss_recon: -0.698418, loss_pred: 0.045302
iteration 654: loss: 0.785002, loss_kl: 3.697509, loss_recon: -0.703158, loss_pred: 0.072702
iteration 655: loss: 0.764205, loss_kl: 4.746399, loss_recon: -0.685895, loss_pred: 0.066574
iteration 656: loss: 0.766849, loss_kl: 4.908209, loss_recon: -0.706290, loss_pred: 0.048422
iteration 657: loss: 0.757333, loss_kl: 6.281248, loss_recon: -0.690293, loss_pred: 0.051509
iteration 658: loss: 0.776533, loss_kl: 5.840017, loss_recon: -0.687829, loss_pred: 0.074264
iteration 659: loss: 0.789848, loss_kl: 7.640742, loss_recon: -0.690085, loss_pred: 0.080870
iteration 660: loss: 0.776764, loss_kl: 8.143978, loss_recon: -0.694041, loss_pred: 0.062586
iteration 661: loss: 0.786453, loss_kl: 8.549529, loss_recon: -0.713887, loss_pred: 0.051426
iteration 662: loss: 0.791437, loss_kl: 9.615343, loss_recon: -0.686006, loss_pred: 0.081656
iteration 663: loss: 0.760963, loss_kl: 7.597827, loss_recon: -0.698078, loss_pred: 0.044098
 51%|█████████████▊             | 51/100 [4:38:40<4:23:36, 322.78s/it]iteration 664: loss: 0.824415, loss_kl: 10.994311, loss_recon: -0.694765, loss_pred: 0.058934
iteration 665: loss: 0.828976, loss_kl: 11.482265, loss_recon: -0.696015, loss_pred: 0.059105
iteration 666: loss: 0.824814, loss_kl: 11.049762, loss_recon: -0.699081, loss_pred: 0.054660
iteration 667: loss: 0.838462, loss_kl: 10.901344, loss_recon: -0.692350, loss_pred: 0.075993
iteration 668: loss: 0.802670, loss_kl: 9.411936, loss_recon: -0.701557, loss_pred: 0.040574
iteration 669: loss: 0.823899, loss_kl: 11.110583, loss_recon: -0.692003, loss_pred: 0.060431
iteration 670: loss: 0.809523, loss_kl: 9.775741, loss_recon: -0.692778, loss_pred: 0.053866
iteration 671: loss: 0.808673, loss_kl: 11.011389, loss_recon: -0.691188, loss_pred: 0.046659
iteration 672: loss: 0.809815, loss_kl: 10.754306, loss_recon: -0.699257, loss_pred: 0.041385
iteration 673: loss: 0.817650, loss_kl: 10.421445, loss_recon: -0.685806, loss_pred: 0.064812
iteration 674: loss: 0.806254, loss_kl: 10.792537, loss_recon: -0.687253, loss_pred: 0.049582
iteration 675: loss: 0.818967, loss_kl: 9.287024, loss_recon: -0.687629, loss_pred: 0.071603
iteration 676: loss: 0.811860, loss_kl: 10.743842, loss_recon: -0.699645, loss_pred: 0.043110
 52%|██████████████             | 52/100 [4:44:03<4:18:16, 322.84s/it]iteration 677: loss: 0.924935, loss_kl: 9.747424, loss_recon: -0.687226, loss_pred: 0.075644
iteration 678: loss: 0.901451, loss_kl: 9.115164, loss_recon: -0.688948, loss_pred: 0.060951
iteration 679: loss: 0.902104, loss_kl: 8.952664, loss_recon: -0.693742, loss_pred: 0.059511
iteration 680: loss: 0.892352, loss_kl: 8.872832, loss_recon: -0.684647, loss_pred: 0.060183
iteration 681: loss: 0.882847, loss_kl: 8.495236, loss_recon: -0.686822, loss_pred: 0.054780
iteration 682: loss: 0.885753, loss_kl: 7.337450, loss_recon: -0.692434, loss_pred: 0.071323
iteration 683: loss: 0.841841, loss_kl: 6.599582, loss_recon: -0.689448, loss_pred: 0.042665
iteration 684: loss: 0.871236, loss_kl: 7.016798, loss_recon: -0.698081, loss_pred: 0.056492
iteration 685: loss: 0.861708, loss_kl: 6.621635, loss_recon: -0.696410, loss_pred: 0.055204
iteration 686: loss: 0.830583, loss_kl: 5.747110, loss_recon: -0.681389, loss_pred: 0.053641
iteration 687: loss: 0.850801, loss_kl: 6.122063, loss_recon: -0.685644, loss_pred: 0.063369
iteration 688: loss: 0.822416, loss_kl: 4.691375, loss_recon: -0.698919, loss_pred: 0.045497
iteration 689: loss: 0.811579, loss_kl: 4.258846, loss_recon: -0.695827, loss_pred: 0.044944
 53%|██████████████▎            | 53/100 [4:49:26<4:13:00, 322.99s/it]iteration 690: loss: 0.913075, loss_kl: 4.535496, loss_recon: -0.680255, loss_pred: 0.041014
iteration 691: loss: 0.894727, loss_kl: 3.860087, loss_recon: -0.690996, loss_pred: 0.040489
iteration 692: loss: 0.843701, loss_kl: 2.869092, loss_recon: -0.689948, loss_pred: 0.032420
iteration 693: loss: 0.871873, loss_kl: 3.178686, loss_recon: -0.685633, loss_pred: 0.051814
iteration 694: loss: 0.858598, loss_kl: 3.128808, loss_recon: -0.679957, loss_pred: 0.046325
iteration 695: loss: 0.859247, loss_kl: 2.876904, loss_recon: -0.695127, loss_pred: 0.042456
iteration 696: loss: 0.827829, loss_kl: 2.458075, loss_recon: -0.687905, loss_pred: 0.035973
iteration 697: loss: 0.815626, loss_kl: 1.988866, loss_recon: -0.685748, loss_pred: 0.045769
iteration 698: loss: 0.804701, loss_kl: 1.944664, loss_recon: -0.686887, loss_pred: 0.035574
iteration 699: loss: 0.783963, loss_kl: 1.502921, loss_recon: -0.679448, loss_pred: 0.040957
iteration 700: loss: 0.780874, loss_kl: 1.108863, loss_recon: -0.679632, loss_pred: 0.054348
iteration 701: loss: 0.787667, loss_kl: 1.459495, loss_recon: -0.686176, loss_pred: 0.039770
iteration 702: loss: 0.810475, loss_kl: 1.284601, loss_recon: -0.698788, loss_pred: 0.057362
 54%|██████████████▌            | 54/100 [4:54:48<4:07:28, 322.79s/it]iteration 703: loss: 0.887136, loss_kl: 1.343597, loss_recon: -0.675670, loss_pred: 0.072538
iteration 704: loss: 0.846103, loss_kl: 1.252819, loss_recon: -0.684548, loss_pred: 0.032014
iteration 705: loss: 0.850552, loss_kl: 1.197813, loss_recon: -0.679665, loss_pred: 0.047033
iteration 706: loss: 0.852613, loss_kl: 1.254286, loss_recon: -0.685726, loss_pred: 0.037193
iteration 707: loss: 0.777973, loss_kl: 0.564908, loss_recon: -0.690309, loss_pred: 0.029252
iteration 708: loss: 0.803866, loss_kl: 0.575222, loss_recon: -0.677478, loss_pred: 0.066910
iteration 709: loss: 0.792462, loss_kl: 0.530224, loss_recon: -0.678618, loss_pred: 0.059018
iteration 710: loss: 0.806354, loss_kl: 0.758841, loss_recon: -0.682587, loss_pred: 0.045302
iteration 711: loss: 0.793646, loss_kl: 0.525932, loss_recon: -0.679879, loss_pred: 0.059385
iteration 712: loss: 0.839139, loss_kl: 1.227216, loss_recon: -0.681386, loss_pred: 0.030858
iteration 713: loss: 0.813200, loss_kl: 0.737964, loss_recon: -0.688420, loss_pred: 0.048474
iteration 714: loss: 0.822582, loss_kl: 0.815612, loss_recon: -0.689799, loss_pred: 0.048449
iteration 715: loss: 0.813131, loss_kl: 0.513928, loss_recon: -0.686140, loss_pred: 0.073851
 55%|██████████████▊            | 55/100 [5:00:12<4:02:12, 322.94s/it]iteration 716: loss: 0.862466, loss_kl: 0.549382, loss_recon: -0.687924, loss_pred: 0.047373
iteration 717: loss: 0.893066, loss_kl: 0.744886, loss_recon: -0.689041, loss_pred: 0.031603
iteration 718: loss: 0.947720, loss_kl: 0.873707, loss_recon: -0.676608, loss_pred: 0.068870
iteration 719: loss: 0.938922, loss_kl: 0.812966, loss_recon: -0.682583, loss_pred: 0.068157
iteration 720: loss: 0.903167, loss_kl: 0.776086, loss_recon: -0.673919, loss_pred: 0.049603
iteration 721: loss: 1.020329, loss_kl: 1.121059, loss_recon: -0.675527, loss_pred: 0.085304
iteration 722: loss: 0.967703, loss_kl: 1.064075, loss_recon: -0.674980, loss_pred: 0.046416
iteration 723: loss: 0.935193, loss_kl: 0.621429, loss_recon: -0.672759, loss_pred: 0.118588
iteration 724: loss: 1.042503, loss_kl: 1.334921, loss_recon: -0.683166, loss_pred: 0.050336
iteration 725: loss: 1.037260, loss_kl: 1.309448, loss_recon: -0.675750, loss_pred: 0.058405
iteration 726: loss: 1.034389, loss_kl: 1.207354, loss_recon: -0.690383, loss_pred: 0.064534
iteration 727: loss: 1.027059, loss_kl: 1.282162, loss_recon: -0.679574, loss_pred: 0.050696
iteration 728: loss: 1.056752, loss_kl: 1.426037, loss_recon: -0.692784, loss_pred: 0.033876
 56%|███████████████            | 56/100 [5:05:36<3:57:06, 323.32s/it]iteration 729: loss: 1.119332, loss_kl: 0.865894, loss_recon: -0.679973, loss_pred: 0.058118
iteration 730: loss: 1.212423, loss_kl: 1.111649, loss_recon: -0.679278, loss_pred: 0.043701
iteration 731: loss: 0.866085, loss_kl: 0.274436, loss_recon: -0.679997, loss_pred: 0.065257
iteration 732: loss: 1.018632, loss_kl: 0.672632, loss_recon: -0.677315, loss_pred: 0.045167
iteration 733: loss: 1.205928, loss_kl: 1.090696, loss_recon: -0.687145, loss_pred: 0.038565
iteration 734: loss: 0.995738, loss_kl: 0.639762, loss_recon: -0.671300, loss_pred: 0.042759
iteration 735: loss: 1.247849, loss_kl: 1.192922, loss_recon: -0.677767, loss_pred: 0.044855
iteration 736: loss: 1.092827, loss_kl: 0.864305, loss_recon: -0.670406, loss_pred: 0.041879
iteration 737: loss: 1.165677, loss_kl: 1.017946, loss_recon: -0.680888, loss_pred: 0.036602
iteration 738: loss: 1.244438, loss_kl: 1.169593, loss_recon: -0.683047, loss_pred: 0.046435
iteration 739: loss: 1.159454, loss_kl: 1.009318, loss_recon: -0.675987, loss_pred: 0.039078
iteration 740: loss: 1.230328, loss_kl: 1.163479, loss_recon: -0.674319, loss_pred: 0.043745
iteration 741: loss: 0.841136, loss_kl: 0.283429, loss_recon: -0.683163, loss_pred: 0.033183
 57%|███████████████▍           | 57/100 [5:10:59<3:51:32, 323.09s/it]iteration 742: loss: 1.302073, loss_kl: 0.801975, loss_recon: -0.672752, loss_pred: 0.089907
iteration 743: loss: 1.007428, loss_kl: 0.421312, loss_recon: -0.673171, loss_pred: 0.050879
iteration 744: loss: 1.394860, loss_kl: 0.981569, loss_recon: -0.681723, loss_pred: 0.052926
iteration 745: loss: 1.360777, loss_kl: 0.924864, loss_recon: -0.675942, loss_pred: 0.062765
iteration 746: loss: 1.145373, loss_kl: 0.645420, loss_recon: -0.661402, loss_pred: 0.049857
iteration 747: loss: 1.282301, loss_kl: 0.842425, loss_recon: -0.680255, loss_pred: 0.035424
iteration 748: loss: 1.333280, loss_kl: 0.892340, loss_recon: -0.675716, loss_pred: 0.057370
iteration 749: loss: 1.298893, loss_kl: 0.833018, loss_recon: -0.678644, loss_pred: 0.059954
iteration 750: loss: 1.765719, loss_kl: 1.548043, loss_recon: -0.686563, loss_pred: 0.037931
iteration 751: loss: 1.313127, loss_kl: 0.892645, loss_recon: -0.675341, loss_pred: 0.037387
iteration 752: loss: 1.186888, loss_kl: 0.683108, loss_recon: -0.678535, loss_pred: 0.048891
iteration 753: loss: 1.166523, loss_kl: 0.621948, loss_recon: -0.669793, loss_pred: 0.078403
iteration 754: loss: 1.210907, loss_kl: 0.648637, loss_recon: -0.694346, loss_pred: 0.080283
 58%|███████████████▋           | 58/100 [5:16:22<3:46:15, 323.22s/it]iteration 755: loss: 1.591503, loss_kl: 1.020972, loss_recon: -0.677948, loss_pred: 0.052973
iteration 756: loss: 1.696214, loss_kl: 1.169548, loss_recon: -0.671263, loss_pred: 0.039134
iteration 757: loss: 1.577944, loss_kl: 1.013453, loss_recon: -0.678739, loss_pred: 0.044961
iteration 758: loss: 1.712884, loss_kl: 1.179691, loss_recon: -0.672651, loss_pred: 0.045867
iteration 759: loss: 1.597413, loss_kl: 1.036240, loss_recon: -0.681372, loss_pred: 0.042590
iteration 760: loss: 1.448667, loss_kl: 0.864031, loss_recon: -0.670093, loss_pred: 0.050279
iteration 761: loss: 1.394733, loss_kl: 0.795509, loss_recon: -0.681392, loss_pred: 0.042803
iteration 762: loss: 1.196166, loss_kl: 0.562703, loss_recon: -0.669890, loss_pred: 0.051971
iteration 763: loss: 1.784936, loss_kl: 1.257889, loss_recon: -0.678692, loss_pred: 0.045964
iteration 764: loss: 1.974944, loss_kl: 1.475297, loss_recon: -0.672154, loss_pred: 0.059256
iteration 765: loss: 1.378058, loss_kl: 0.772965, loss_recon: -0.664807, loss_pred: 0.061716
iteration 766: loss: 1.288827, loss_kl: 0.684776, loss_recon: -0.673000, loss_pred: 0.038626
iteration 767: loss: 1.798686, loss_kl: 1.297287, loss_recon: -0.671521, loss_pred: 0.033676
 59%|███████████████▉           | 59/100 [5:21:44<3:40:38, 322.89s/it]iteration 768: loss: 2.161379, loss_kl: 1.531404, loss_recon: -0.679556, loss_pred: 0.052422
iteration 769: loss: 1.825307, loss_kl: 1.160501, loss_recon: -0.674681, loss_pred: 0.067424
iteration 770: loss: 1.700360, loss_kl: 1.033676, loss_recon: -0.673395, loss_pred: 0.062141
iteration 771: loss: 1.779469, loss_kl: 1.129271, loss_recon: -0.674195, loss_pred: 0.051222
iteration 772: loss: 1.993269, loss_kl: 1.328769, loss_recon: -0.673992, loss_pred: 0.079015
iteration 773: loss: 1.772589, loss_kl: 1.084774, loss_recon: -0.677877, loss_pred: 0.082192
iteration 774: loss: 1.301955, loss_kl: 0.604344, loss_recon: -0.677296, loss_pred: 0.060568
iteration 775: loss: 2.215355, loss_kl: 1.548360, loss_recon: -0.669311, loss_pred: 0.100817
iteration 776: loss: 2.022860, loss_kl: 1.384437, loss_recon: -0.677631, loss_pred: 0.053007
iteration 777: loss: 1.416614, loss_kl: 0.739180, loss_recon: -0.664768, loss_pred: 0.061902
iteration 778: loss: 1.857194, loss_kl: 1.200749, loss_recon: -0.664822, loss_pred: 0.071603
iteration 779: loss: 1.460817, loss_kl: 0.792380, loss_recon: -0.665518, loss_pred: 0.055698
iteration 780: loss: 2.404653, loss_kl: 1.708485, loss_recon: -0.651113, loss_pred: 0.158854
 60%|████████████████▏          | 60/100 [5:27:07<3:35:09, 322.74s/it]iteration 781: loss: 2.126058, loss_kl: 1.387932, loss_recon: -0.673889, loss_pred: 0.101152
iteration 782: loss: 1.992007, loss_kl: 1.286334, loss_recon: -0.676432, loss_pred: 0.063454
iteration 783: loss: 1.747572, loss_kl: 1.028426, loss_recon: -0.663324, loss_pred: 0.083175
iteration 784: loss: 2.034638, loss_kl: 1.353392, loss_recon: -0.661755, loss_pred: 0.055487
iteration 785: loss: 2.166189, loss_kl: 1.474452, loss_recon: -0.671690, loss_pred: 0.059263
iteration 786: loss: 1.484677, loss_kl: 0.762981, loss_recon: -0.671904, loss_pred: 0.070085
iteration 787: loss: 2.037132, loss_kl: 1.342945, loss_recon: -0.672725, loss_pred: 0.057181
iteration 788: loss: 1.817924, loss_kl: 1.132937, loss_recon: -0.665983, loss_pred: 0.049136
iteration 789: loss: 2.103395, loss_kl: 1.417368, loss_recon: -0.667100, loss_pred: 0.056624
iteration 790: loss: 1.458242, loss_kl: 0.728305, loss_recon: -0.671870, loss_pred: 0.077438
iteration 791: loss: 1.452022, loss_kl: 0.716171, loss_recon: -0.676326, loss_pred: 0.078572
iteration 792: loss: 2.015961, loss_kl: 1.309359, loss_recon: -0.664492, loss_pred: 0.076935
iteration 793: loss: 1.831435, loss_kl: 1.089610, loss_recon: -0.672678, loss_pred: 0.098128
 61%|████████████████▍          | 61/100 [5:32:29<3:29:42, 322.64s/it]iteration 794: loss: 2.120905, loss_kl: 1.396777, loss_recon: -0.665324, loss_pred: 0.073265
iteration 795: loss: 2.565444, loss_kl: 1.852856, loss_recon: -0.671744, loss_pred: 0.060028
iteration 796: loss: 2.514861, loss_kl: 1.793912, loss_recon: -0.680513, loss_pred: 0.059010
iteration 797: loss: 1.859004, loss_kl: 1.122216, loss_recon: -0.656155, loss_pred: 0.092253
iteration 798: loss: 2.238433, loss_kl: 1.517097, loss_recon: -0.671817, loss_pred: 0.065227
iteration 799: loss: 1.783563, loss_kl: 1.059075, loss_recon: -0.656811, loss_pred: 0.078642
iteration 800: loss: 1.594432, loss_kl: 0.869022, loss_recon: -0.678222, loss_pred: 0.056186
iteration 801: loss: 1.995225, loss_kl: 1.264226, loss_recon: -0.658522, loss_pred: 0.085566
iteration 802: loss: 2.003453, loss_kl: 1.278258, loss_recon: -0.677298, loss_pred: 0.061132
iteration 803: loss: 2.026016, loss_kl: 1.310390, loss_recon: -0.672829, loss_pred: 0.056365
iteration 804: loss: 2.288121, loss_kl: 1.569962, loss_recon: -0.661916, loss_pred: 0.072497
iteration 805: loss: 1.868940, loss_kl: 1.152490, loss_recon: -0.674455, loss_pred: 0.053928
iteration 806: loss: 1.957689, loss_kl: 1.160454, loss_recon: -0.657070, loss_pred: 0.152180
 62%|████████████████▋          | 62/100 [5:37:52<3:24:20, 322.65s/it]iteration 807: loss: 1.591075, loss_kl: 0.871159, loss_recon: -0.671758, loss_pred: 0.051634
iteration 808: loss: 2.149883, loss_kl: 1.401941, loss_recon: -0.663165, loss_pred: 0.090371
iteration 809: loss: 2.279619, loss_kl: 1.546602, loss_recon: -0.661640, loss_pred: 0.077548
iteration 810: loss: 2.220762, loss_kl: 1.474984, loss_recon: -0.665064, loss_pred: 0.086598
iteration 811: loss: 1.848567, loss_kl: 1.129538, loss_recon: -0.665619, loss_pred: 0.057917
iteration 812: loss: 1.803025, loss_kl: 1.066392, loss_recon: -0.672974, loss_pred: 0.067914
iteration 813: loss: 1.929270, loss_kl: 1.216536, loss_recon: -0.670074, loss_pred: 0.047514
iteration 814: loss: 1.892487, loss_kl: 1.158090, loss_recon: -0.664978, loss_pred: 0.074040
iteration 815: loss: 2.033180, loss_kl: 1.310987, loss_recon: -0.664759, loss_pred: 0.062665
iteration 816: loss: 2.086075, loss_kl: 1.353450, loss_recon: -0.662113, loss_pred: 0.075911
iteration 817: loss: 2.198365, loss_kl: 1.486045, loss_recon: -0.663961, loss_pred: 0.054288
iteration 818: loss: 2.011513, loss_kl: 1.302842, loss_recon: -0.667957, loss_pred: 0.045912
iteration 819: loss: 1.040478, loss_kl: 0.298136, loss_recon: -0.688282, loss_pred: 0.055250
 63%|█████████████████          | 63/100 [5:43:14<3:18:58, 322.67s/it]iteration 820: loss: 1.978152, loss_kl: 1.251302, loss_recon: -0.661382, loss_pred: 0.065467
iteration 821: loss: 2.143479, loss_kl: 1.418084, loss_recon: -0.671218, loss_pred: 0.054178
iteration 822: loss: 2.078611, loss_kl: 1.360101, loss_recon: -0.664885, loss_pred: 0.053624
iteration 823: loss: 1.781614, loss_kl: 1.059826, loss_recon: -0.664760, loss_pred: 0.057028
iteration 824: loss: 1.835630, loss_kl: 1.107092, loss_recon: -0.670237, loss_pred: 0.058301
iteration 825: loss: 1.774510, loss_kl: 1.003960, loss_recon: -0.671502, loss_pred: 0.099048
iteration 826: loss: 1.960497, loss_kl: 1.208188, loss_recon: -0.671113, loss_pred: 0.081196
iteration 827: loss: 1.654243, loss_kl: 0.941624, loss_recon: -0.655451, loss_pred: 0.057169
iteration 828: loss: 2.280466, loss_kl: 1.555667, loss_recon: -0.668605, loss_pred: 0.056194
iteration 829: loss: 2.237267, loss_kl: 1.533926, loss_recon: -0.663604, loss_pred: 0.039737
iteration 830: loss: 1.770803, loss_kl: 1.049867, loss_recon: -0.673711, loss_pred: 0.047225
iteration 831: loss: 1.595883, loss_kl: 0.882025, loss_recon: -0.668746, loss_pred: 0.045112
iteration 832: loss: 1.082610, loss_kl: 0.373086, loss_recon: -0.668192, loss_pred: 0.041332
 64%|█████████████████▎         | 64/100 [5:48:37<3:13:34, 322.62s/it]iteration 833: loss: 1.721248, loss_kl: 1.017582, loss_recon: -0.669442, loss_pred: 0.034224
iteration 834: loss: 2.627138, loss_kl: 1.893423, loss_recon: -0.664184, loss_pred: 0.069530
iteration 835: loss: 1.785203, loss_kl: 1.074374, loss_recon: -0.665300, loss_pred: 0.045530
iteration 836: loss: 1.850283, loss_kl: 1.116688, loss_recon: -0.653698, loss_pred: 0.079897
iteration 837: loss: 1.859906, loss_kl: 1.120677, loss_recon: -0.684324, loss_pred: 0.054905
iteration 838: loss: 2.535886, loss_kl: 1.823043, loss_recon: -0.668669, loss_pred: 0.044174
iteration 839: loss: 2.174736, loss_kl: 1.436733, loss_recon: -0.658029, loss_pred: 0.079974
iteration 840: loss: 1.772558, loss_kl: 1.056773, loss_recon: -0.660914, loss_pred: 0.054871
iteration 841: loss: 2.277840, loss_kl: 1.560617, loss_recon: -0.662035, loss_pred: 0.055188
iteration 842: loss: 2.323241, loss_kl: 1.614709, loss_recon: -0.662749, loss_pred: 0.045784
iteration 843: loss: 2.119921, loss_kl: 1.404751, loss_recon: -0.672459, loss_pred: 0.042712
iteration 844: loss: 1.859745, loss_kl: 1.155797, loss_recon: -0.667999, loss_pred: 0.035948
iteration 845: loss: 1.715318, loss_kl: 1.012115, loss_recon: -0.658425, loss_pred: 0.044778
 65%|█████████████████▌         | 65/100 [5:54:00<3:08:13, 322.66s/it]iteration 846: loss: 2.155473, loss_kl: 1.437089, loss_recon: -0.679613, loss_pred: 0.038771
iteration 847: loss: 2.269600, loss_kl: 1.553900, loss_recon: -0.664813, loss_pred: 0.050887
iteration 848: loss: 1.828683, loss_kl: 1.101730, loss_recon: -0.652200, loss_pred: 0.074753
iteration 849: loss: 1.667609, loss_kl: 0.906807, loss_recon: -0.671684, loss_pred: 0.089117
iteration 850: loss: 1.889004, loss_kl: 1.159775, loss_recon: -0.668136, loss_pred: 0.061093
iteration 851: loss: 2.666342, loss_kl: 1.895739, loss_recon: -0.672440, loss_pred: 0.098163
iteration 852: loss: 2.405750, loss_kl: 1.609227, loss_recon: -0.662180, loss_pred: 0.134343
iteration 853: loss: 2.581254, loss_kl: 1.865698, loss_recon: -0.658398, loss_pred: 0.057158
iteration 854: loss: 1.858219, loss_kl: 0.979038, loss_recon: -0.664785, loss_pred: 0.214396
iteration 855: loss: 1.480483, loss_kl: 0.734331, loss_recon: -0.664370, loss_pred: 0.081782
iteration 856: loss: 2.204731, loss_kl: 1.385218, loss_recon: -0.658952, loss_pred: 0.160561
iteration 857: loss: 1.971567, loss_kl: 1.187963, loss_recon: -0.654788, loss_pred: 0.128815
iteration 858: loss: 1.569224, loss_kl: 0.782797, loss_recon: -0.669107, loss_pred: 0.117321
 66%|█████████████████▊         | 66/100 [5:59:24<3:03:12, 323.30s/it]iteration 859: loss: 2.046036, loss_kl: 1.273140, loss_recon: -0.677589, loss_pred: 0.095306
iteration 860: loss: 2.444446, loss_kl: 1.716787, loss_recon: -0.664156, loss_pred: 0.063502
iteration 861: loss: 2.056622, loss_kl: 1.239823, loss_recon: -0.670763, loss_pred: 0.146036
iteration 862: loss: 2.071421, loss_kl: 1.308513, loss_recon: -0.650573, loss_pred: 0.112335
iteration 863: loss: 2.587816, loss_kl: 1.741235, loss_recon: -0.657659, loss_pred: 0.188923
iteration 864: loss: 2.589696, loss_kl: 1.809382, loss_recon: -0.654835, loss_pred: 0.125479
iteration 865: loss: 2.155593, loss_kl: 1.395383, loss_recon: -0.677748, loss_pred: 0.082462
iteration 866: loss: 2.432981, loss_kl: 1.456476, loss_recon: -0.662366, loss_pred: 0.314139
iteration 867: loss: 1.685008, loss_kl: 0.964510, loss_recon: -0.662725, loss_pred: 0.057772
iteration 868: loss: 2.057716, loss_kl: 1.270265, loss_recon: -0.660228, loss_pred: 0.127223
iteration 869: loss: 1.760171, loss_kl: 1.018497, loss_recon: -0.661326, loss_pred: 0.080348
iteration 870: loss: 1.741461, loss_kl: 1.024695, loss_recon: -0.669282, loss_pred: 0.047484
iteration 871: loss: 1.289832, loss_kl: 0.456072, loss_recon: -0.671786, loss_pred: 0.161974
 67%|██████████████████         | 67/100 [6:04:47<2:57:40, 323.04s/it]iteration 872: loss: 1.736906, loss_kl: 0.932150, loss_recon: -0.665154, loss_pred: 0.139601
iteration 873: loss: 2.135156, loss_kl: 1.420133, loss_recon: -0.659903, loss_pred: 0.055120
iteration 874: loss: 2.844850, loss_kl: 1.962154, loss_recon: -0.677090, loss_pred: 0.205606
iteration 875: loss: 1.914675, loss_kl: 1.185240, loss_recon: -0.670823, loss_pred: 0.058612
iteration 876: loss: 2.504524, loss_kl: 1.723090, loss_recon: -0.650819, loss_pred: 0.130615
iteration 877: loss: 1.890109, loss_kl: 1.145135, loss_recon: -0.664029, loss_pred: 0.080945
iteration 878: loss: 2.255009, loss_kl: 1.555333, loss_recon: -0.657355, loss_pred: 0.042321
iteration 879: loss: 1.487749, loss_kl: 0.775825, loss_recon: -0.653743, loss_pred: 0.058181
iteration 880: loss: 1.457568, loss_kl: 0.754265, loss_recon: -0.662568, loss_pred: 0.040735
iteration 881: loss: 1.688560, loss_kl: 0.974064, loss_recon: -0.665131, loss_pred: 0.049365
iteration 882: loss: 1.998039, loss_kl: 1.267231, loss_recon: -0.656506, loss_pred: 0.074302
iteration 883: loss: 1.569062, loss_kl: 0.860578, loss_recon: -0.658202, loss_pred: 0.050282
iteration 884: loss: 2.296426, loss_kl: 1.561647, loss_recon: -0.657139, loss_pred: 0.077639
 68%|██████████████████▎        | 68/100 [6:10:11<2:52:30, 323.44s/it]iteration 885: loss: 1.394888, loss_kl: 0.685713, loss_recon: -0.662363, loss_pred: 0.046812
iteration 886: loss: 2.088685, loss_kl: 1.297224, loss_recon: -0.662216, loss_pred: 0.129245
iteration 887: loss: 2.018405, loss_kl: 1.253104, loss_recon: -0.658658, loss_pred: 0.106644
iteration 888: loss: 1.857445, loss_kl: 1.154180, loss_recon: -0.656198, loss_pred: 0.047067
iteration 889: loss: 2.038721, loss_kl: 1.242435, loss_recon: -0.658367, loss_pred: 0.137919
iteration 890: loss: 1.883115, loss_kl: 1.171908, loss_recon: -0.661953, loss_pred: 0.049253
iteration 891: loss: 1.793746, loss_kl: 1.046453, loss_recon: -0.662851, loss_pred: 0.084443
iteration 892: loss: 1.711383, loss_kl: 0.928692, loss_recon: -0.642900, loss_pred: 0.139791
iteration 893: loss: 1.703372, loss_kl: 0.938262, loss_recon: -0.669632, loss_pred: 0.095478
iteration 894: loss: 2.367237, loss_kl: 1.621240, loss_recon: -0.659702, loss_pred: 0.086295
iteration 895: loss: 1.823186, loss_kl: 1.093745, loss_recon: -0.673074, loss_pred: 0.056367
iteration 896: loss: 2.006781, loss_kl: 1.217413, loss_recon: -0.666277, loss_pred: 0.123091
iteration 897: loss: 2.697003, loss_kl: 1.932264, loss_recon: -0.656842, loss_pred: 0.107896
 69%|██████████████████▋        | 69/100 [6:15:34<2:46:56, 323.12s/it]iteration 898: loss: 1.738666, loss_kl: 0.990346, loss_recon: -0.672223, loss_pred: 0.076097
iteration 899: loss: 2.147202, loss_kl: 1.400744, loss_recon: -0.649372, loss_pred: 0.097086
iteration 900: loss: 1.786575, loss_kl: 1.085666, loss_recon: -0.659632, loss_pred: 0.041277
iteration 901: loss: 2.009887, loss_kl: 1.252823, loss_recon: -0.664486, loss_pred: 0.092578
iteration 902: loss: 1.829804, loss_kl: 1.029646, loss_recon: -0.661758, loss_pred: 0.138400
iteration 903: loss: 1.453765, loss_kl: 0.714799, loss_recon: -0.653767, loss_pred: 0.085199
iteration 904: loss: 2.243304, loss_kl: 1.468571, loss_recon: -0.648181, loss_pred: 0.126553
iteration 905: loss: 2.076166, loss_kl: 1.309600, loss_recon: -0.667466, loss_pred: 0.099100
iteration 906: loss: 2.114340, loss_kl: 1.347448, loss_recon: -0.648025, loss_pred: 0.118867
iteration 907: loss: 2.423255, loss_kl: 1.656913, loss_recon: -0.659670, loss_pred: 0.106672
iteration 908: loss: 2.008254, loss_kl: 1.305318, loss_recon: -0.656480, loss_pred: 0.046456
iteration 909: loss: 1.890665, loss_kl: 1.130353, loss_recon: -0.656788, loss_pred: 0.103524
iteration 910: loss: 1.436146, loss_kl: 0.740179, loss_recon: -0.656838, loss_pred: 0.039128
 70%|██████████████████▉        | 70/100 [6:20:57<2:41:39, 323.31s/it]iteration 911: loss: 1.777262, loss_kl: 1.069086, loss_recon: -0.659943, loss_pred: 0.048233
iteration 912: loss: 1.987063, loss_kl: 1.286072, loss_recon: -0.653666, loss_pred: 0.047325
iteration 913: loss: 1.565407, loss_kl: 0.863607, loss_recon: -0.652028, loss_pred: 0.049772
iteration 914: loss: 1.759005, loss_kl: 1.047950, loss_recon: -0.659208, loss_pred: 0.051846
iteration 915: loss: 1.694528, loss_kl: 0.982223, loss_recon: -0.664763, loss_pred: 0.047542
iteration 916: loss: 1.912759, loss_kl: 1.211255, loss_recon: -0.657079, loss_pred: 0.044425
iteration 917: loss: 2.142470, loss_kl: 1.403792, loss_recon: -0.662650, loss_pred: 0.076028
iteration 918: loss: 2.779013, loss_kl: 2.087651, loss_recon: -0.655213, loss_pred: 0.036149
iteration 919: loss: 2.094005, loss_kl: 1.366780, loss_recon: -0.660759, loss_pred: 0.066466
iteration 920: loss: 2.046642, loss_kl: 1.302914, loss_recon: -0.652764, loss_pred: 0.090964
iteration 921: loss: 2.023717, loss_kl: 1.313064, loss_recon: -0.660756, loss_pred: 0.049897
iteration 922: loss: 1.749885, loss_kl: 0.985113, loss_recon: -0.653152, loss_pred: 0.111621
iteration 923: loss: 1.510865, loss_kl: 0.799269, loss_recon: -0.647591, loss_pred: 0.064005
 71%|███████████████████▏       | 71/100 [6:26:21<2:36:19, 323.42s/it]iteration 924: loss: 1.850715, loss_kl: 1.137967, loss_recon: -0.650361, loss_pred: 0.062387
iteration 925: loss: 1.720617, loss_kl: 0.997350, loss_recon: -0.650901, loss_pred: 0.072366
iteration 926: loss: 1.558806, loss_kl: 0.866037, loss_recon: -0.650469, loss_pred: 0.042299
iteration 927: loss: 2.614557, loss_kl: 1.896266, loss_recon: -0.679886, loss_pred: 0.038406
iteration 928: loss: 2.151383, loss_kl: 1.450570, loss_recon: -0.650773, loss_pred: 0.050039
iteration 929: loss: 2.164484, loss_kl: 1.469968, loss_recon: -0.655838, loss_pred: 0.038678
iteration 930: loss: 1.691193, loss_kl: 1.001913, loss_recon: -0.636372, loss_pred: 0.052908
iteration 931: loss: 2.216559, loss_kl: 1.485521, loss_recon: -0.661997, loss_pred: 0.069041
iteration 932: loss: 2.277212, loss_kl: 1.583806, loss_recon: -0.660716, loss_pred: 0.032689
iteration 933: loss: 1.800232, loss_kl: 1.079485, loss_recon: -0.640877, loss_pred: 0.079870
iteration 934: loss: 1.570068, loss_kl: 0.866027, loss_recon: -0.661893, loss_pred: 0.042149
iteration 935: loss: 1.581217, loss_kl: 0.872973, loss_recon: -0.668394, loss_pred: 0.039850
iteration 936: loss: 2.170779, loss_kl: 1.452168, loss_recon: -0.661558, loss_pred: 0.057053
 72%|███████████████████▍       | 72/100 [6:31:45<2:30:58, 323.51s/it]iteration 937: loss: 1.886122, loss_kl: 1.185163, loss_recon: -0.648584, loss_pred: 0.052374
iteration 938: loss: 1.417219, loss_kl: 0.719578, loss_recon: -0.661582, loss_pred: 0.036059
iteration 939: loss: 1.987969, loss_kl: 1.273030, loss_recon: -0.656777, loss_pred: 0.058162
iteration 940: loss: 2.040476, loss_kl: 1.340948, loss_recon: -0.649785, loss_pred: 0.049744
iteration 941: loss: 2.465418, loss_kl: 1.750789, loss_recon: -0.661538, loss_pred: 0.053090
iteration 942: loss: 2.094801, loss_kl: 1.397181, loss_recon: -0.655232, loss_pred: 0.042388
iteration 943: loss: 2.138140, loss_kl: 1.443132, loss_recon: -0.662485, loss_pred: 0.032523
iteration 944: loss: 2.325372, loss_kl: 1.621732, loss_recon: -0.639198, loss_pred: 0.064442
iteration 945: loss: 1.924889, loss_kl: 1.249309, loss_recon: -0.636773, loss_pred: 0.038807
iteration 946: loss: 1.822237, loss_kl: 1.107486, loss_recon: -0.654690, loss_pred: 0.060061
iteration 947: loss: 1.860518, loss_kl: 1.142515, loss_recon: -0.663167, loss_pred: 0.054837
iteration 948: loss: 1.889093, loss_kl: 1.185258, loss_recon: -0.645416, loss_pred: 0.058420
iteration 949: loss: 1.323702, loss_kl: 0.566648, loss_recon: -0.701867, loss_pred: 0.055187
 73%|███████████████████▋       | 73/100 [6:37:08<2:25:36, 323.58s/it]iteration 950: loss: 2.355829, loss_kl: 1.655540, loss_recon: -0.664884, loss_pred: 0.035404
iteration 951: loss: 1.797201, loss_kl: 1.108107, loss_recon: -0.651474, loss_pred: 0.037620
iteration 952: loss: 1.583256, loss_kl: 0.877501, loss_recon: -0.652957, loss_pred: 0.052798
iteration 953: loss: 2.473393, loss_kl: 1.769047, loss_recon: -0.658902, loss_pred: 0.045444
iteration 954: loss: 1.553391, loss_kl: 0.862189, loss_recon: -0.644422, loss_pred: 0.046781
iteration 955: loss: 1.783632, loss_kl: 1.096465, loss_recon: -0.659120, loss_pred: 0.028047
iteration 956: loss: 1.779301, loss_kl: 1.084815, loss_recon: -0.661150, loss_pred: 0.033336
iteration 957: loss: 1.919006, loss_kl: 1.211719, loss_recon: -0.653498, loss_pred: 0.053789
iteration 958: loss: 2.132247, loss_kl: 1.435741, loss_recon: -0.647075, loss_pred: 0.049430
iteration 959: loss: 1.414173, loss_kl: 0.688634, loss_recon: -0.641580, loss_pred: 0.083959
iteration 960: loss: 2.111166, loss_kl: 1.396334, loss_recon: -0.656169, loss_pred: 0.058663
iteration 961: loss: 1.483307, loss_kl: 0.770727, loss_recon: -0.659513, loss_pred: 0.053066
iteration 962: loss: 1.616830, loss_kl: 0.907998, loss_recon: -0.658699, loss_pred: 0.050134
 74%|███████████████████▉       | 74/100 [6:42:32<2:20:13, 323.61s/it]iteration 963: loss: 1.832159, loss_kl: 1.131485, loss_recon: -0.656121, loss_pred: 0.044553
iteration 964: loss: 1.930874, loss_kl: 1.207165, loss_recon: -0.649457, loss_pred: 0.074252
iteration 965: loss: 2.316262, loss_kl: 1.609228, loss_recon: -0.658430, loss_pred: 0.048605
iteration 966: loss: 2.311474, loss_kl: 1.622829, loss_recon: -0.647246, loss_pred: 0.041398
iteration 967: loss: 1.915060, loss_kl: 1.221553, loss_recon: -0.653286, loss_pred: 0.040221
iteration 968: loss: 2.146099, loss_kl: 1.434627, loss_recon: -0.655598, loss_pred: 0.055874
iteration 969: loss: 1.859423, loss_kl: 1.147149, loss_recon: -0.661220, loss_pred: 0.051054
iteration 970: loss: 1.756970, loss_kl: 1.028092, loss_recon: -0.664170, loss_pred: 0.064709
iteration 971: loss: 2.191255, loss_kl: 1.484249, loss_recon: -0.652070, loss_pred: 0.054937
iteration 972: loss: 1.857190, loss_kl: 1.152725, loss_recon: -0.653443, loss_pred: 0.051021
iteration 973: loss: 1.811618, loss_kl: 1.096912, loss_recon: -0.655497, loss_pred: 0.059209
iteration 974: loss: 1.609245, loss_kl: 0.919992, loss_recon: -0.643179, loss_pred: 0.046074
iteration 975: loss: 1.882376, loss_kl: 1.192831, loss_recon: -0.629087, loss_pred: 0.060458
 75%|████████████████████▎      | 75/100 [6:47:56<2:14:51, 323.68s/it]iteration 976: loss: 0.679203, loss_kl: 1.036878, loss_recon: -0.637406, loss_pred: 0.039233
iteration 977: loss: 0.704169, loss_kl: 1.256264, loss_recon: -0.650798, loss_pred: 0.050265
iteration 978: loss: 0.725670, loss_kl: 1.869460, loss_recon: -0.654062, loss_pred: 0.066986
iteration 979: loss: 0.699993, loss_kl: 2.956666, loss_recon: -0.654284, loss_pred: 0.038398
iteration 980: loss: 0.715604, loss_kl: 3.861401, loss_recon: -0.666648, loss_pred: 0.039407
iteration 981: loss: 0.708444, loss_kl: 4.958023, loss_recon: -0.648325, loss_pred: 0.047860
iteration 982: loss: 0.721561, loss_kl: 3.865609, loss_recon: -0.665078, loss_pred: 0.046925
iteration 983: loss: 0.720324, loss_kl: 5.406752, loss_recon: -0.661862, loss_pred: 0.045093
iteration 984: loss: 0.714806, loss_kl: 6.673177, loss_recon: -0.651835, loss_pred: 0.046471
iteration 985: loss: 0.692699, loss_kl: 5.650857, loss_recon: -0.643640, loss_pred: 0.035087
iteration 986: loss: 0.695012, loss_kl: 8.345748, loss_recon: -0.637782, loss_pred: 0.036594
iteration 987: loss: 0.714287, loss_kl: 7.380464, loss_recon: -0.661130, loss_pred: 0.034908
iteration 988: loss: 0.706131, loss_kl: 7.383126, loss_recon: -0.660151, loss_pred: 0.027724
 76%|████████████████████▌      | 76/100 [6:53:18<2:09:18, 323.27s/it]iteration 989: loss: 0.740350, loss_kl: 7.997739, loss_recon: -0.646339, loss_pred: 0.042569
iteration 990: loss: 0.748323, loss_kl: 7.902445, loss_recon: -0.662248, loss_pred: 0.035245
iteration 991: loss: 0.735078, loss_kl: 8.888525, loss_recon: -0.641368, loss_pred: 0.036538
iteration 992: loss: 0.747581, loss_kl: 8.206954, loss_recon: -0.658471, loss_pred: 0.036323
iteration 993: loss: 0.754882, loss_kl: 9.416924, loss_recon: -0.651031, loss_pred: 0.043281
iteration 994: loss: 0.756537, loss_kl: 10.105950, loss_recon: -0.657340, loss_pred: 0.034195
iteration 995: loss: 0.741733, loss_kl: 8.541510, loss_recon: -0.651079, loss_pred: 0.035714
iteration 996: loss: 0.735318, loss_kl: 8.064918, loss_recon: -0.645352, loss_pred: 0.038092
iteration 997: loss: 0.749358, loss_kl: 8.620579, loss_recon: -0.661447, loss_pred: 0.032463
iteration 998: loss: 0.759211, loss_kl: 8.764688, loss_recon: -0.659771, loss_pred: 0.043064
iteration 999: loss: 0.733036, loss_kl: 8.046983, loss_recon: -0.642150, loss_pred: 0.039127
iteration 1000: loss: 0.727768, loss_kl: 7.028763, loss_recon: -0.642184, loss_pred: 0.040373
iteration 1001: loss: 0.761194, loss_kl: 8.594872, loss_recon: -0.656576, loss_pred: 0.049335
 77%|████████████████████▊      | 77/100 [6:58:41<2:03:48, 323.00s/it]iteration 1002: loss: 0.838888, loss_kl: 8.798733, loss_recon: -0.648328, loss_pred: 0.044269
iteration 1003: loss: 0.823994, loss_kl: 8.399817, loss_recon: -0.649784, loss_pred: 0.034552
iteration 1004: loss: 0.814446, loss_kl: 7.665183, loss_recon: -0.649132, loss_pred: 0.037869
iteration 1005: loss: 0.836924, loss_kl: 8.162836, loss_recon: -0.650354, loss_pred: 0.050851
iteration 1006: loss: 0.833263, loss_kl: 8.775276, loss_recon: -0.641084, loss_pred: 0.046278
iteration 1007: loss: 0.816976, loss_kl: 7.085977, loss_recon: -0.665873, loss_pred: 0.033289
iteration 1008: loss: 0.780834, loss_kl: 6.372856, loss_recon: -0.648570, loss_pred: 0.026307
iteration 1009: loss: 0.819918, loss_kl: 5.790924, loss_recon: -0.660252, loss_pred: 0.063383
iteration 1010: loss: 0.748072, loss_kl: 4.532005, loss_recon: -0.639736, loss_pred: 0.032985
iteration 1011: loss: 0.782673, loss_kl: 5.527299, loss_recon: -0.647454, loss_pred: 0.043321
iteration 1012: loss: 0.757158, loss_kl: 4.816614, loss_recon: -0.641744, loss_pred: 0.035332
iteration 1013: loss: 0.780321, loss_kl: 4.668873, loss_recon: -0.656646, loss_pred: 0.046049
iteration 1014: loss: 0.737814, loss_kl: 4.117247, loss_recon: -0.638608, loss_pred: 0.030751
 78%|█████████████████████      | 78/100 [7:04:04<1:58:27, 323.05s/it]iteration 1015: loss: 0.894514, loss_kl: 4.007075, loss_recon: -0.640581, loss_pred: 0.084475
iteration 1016: loss: 0.822108, loss_kl: 3.280735, loss_recon: -0.641015, loss_pred: 0.042352
iteration 1017: loss: 0.815940, loss_kl: 2.807083, loss_recon: -0.661000, loss_pred: 0.036229
iteration 1018: loss: 0.756059, loss_kl: 1.934898, loss_recon: -0.644492, loss_pred: 0.029740
iteration 1019: loss: 0.790820, loss_kl: 2.447175, loss_recon: -0.649714, loss_pred: 0.037615
iteration 1020: loss: 0.772302, loss_kl: 2.108093, loss_recon: -0.654269, loss_pred: 0.028882
iteration 1021: loss: 0.748311, loss_kl: 1.732792, loss_recon: -0.645870, loss_pred: 0.029162
iteration 1022: loss: 0.733603, loss_kl: 0.933982, loss_recon: -0.659208, loss_pred: 0.034898
iteration 1023: loss: 0.727304, loss_kl: 1.379008, loss_recon: -0.636930, loss_pred: 0.032057
iteration 1024: loss: 0.736339, loss_kl: 1.307688, loss_recon: -0.652288, loss_pred: 0.028749
iteration 1025: loss: 0.718961, loss_kl: 1.071877, loss_recon: -0.642524, loss_pred: 0.031108
iteration 1026: loss: 0.713212, loss_kl: 0.896511, loss_recon: -0.652912, loss_pred: 0.022386
iteration 1027: loss: 0.729806, loss_kl: 0.877439, loss_recon: -0.667683, loss_pred: 0.025017
 79%|█████████████████████▎     | 79/100 [7:09:26<1:53:01, 322.91s/it]iteration 1028: loss: 0.745215, loss_kl: 0.763831, loss_recon: -0.643441, loss_pred: 0.022794
iteration 1029: loss: 0.782050, loss_kl: 1.064636, loss_recon: -0.644798, loss_pred: 0.027168
iteration 1030: loss: 0.728875, loss_kl: 0.630547, loss_recon: -0.636458, loss_pred: 0.027218
iteration 1031: loss: 0.749069, loss_kl: 0.598100, loss_recon: -0.660201, loss_pred: 0.027024
iteration 1032: loss: 0.735703, loss_kl: 0.581791, loss_recon: -0.649013, loss_pred: 0.026533
iteration 1033: loss: 0.758518, loss_kl: 0.685414, loss_recon: -0.659697, loss_pred: 0.027949
iteration 1034: loss: 0.806845, loss_kl: 1.191453, loss_recon: -0.652932, loss_pred: 0.030716
iteration 1035: loss: 0.734126, loss_kl: 0.621293, loss_recon: -0.646444, loss_pred: 0.023440
iteration 1036: loss: 0.779277, loss_kl: 0.907640, loss_recon: -0.652966, loss_pred: 0.032460
iteration 1037: loss: 0.758588, loss_kl: 0.587242, loss_recon: -0.643802, loss_pred: 0.054066
iteration 1038: loss: 0.758340, loss_kl: 0.809138, loss_recon: -0.647112, loss_pred: 0.027563
iteration 1039: loss: 0.741187, loss_kl: 0.659972, loss_recon: -0.640799, loss_pred: 0.032146
iteration 1040: loss: 0.803763, loss_kl: 0.984734, loss_recon: -0.657472, loss_pred: 0.044469
 80%|█████████████████████▌     | 80/100 [7:14:49<1:47:33, 322.69s/it]iteration 1041: loss: 0.937820, loss_kl: 1.109361, loss_recon: -0.656651, loss_pred: 0.024380
iteration 1042: loss: 0.821812, loss_kl: 0.631395, loss_recon: -0.653265, loss_pred: 0.022394
iteration 1043: loss: 0.874137, loss_kl: 0.879780, loss_recon: -0.648437, loss_pred: 0.022053
iteration 1044: loss: 0.879712, loss_kl: 0.869413, loss_recon: -0.651471, loss_pred: 0.026994
iteration 1045: loss: 0.912755, loss_kl: 1.037682, loss_recon: -0.641682, loss_pred: 0.030875
iteration 1046: loss: 0.862609, loss_kl: 0.860311, loss_recon: -0.632251, loss_pred: 0.031217
iteration 1047: loss: 0.913855, loss_kl: 1.077972, loss_recon: -0.645709, loss_pred: 0.018622
iteration 1048: loss: 0.899959, loss_kl: 0.973515, loss_recon: -0.640940, loss_pred: 0.033674
iteration 1049: loss: 0.888753, loss_kl: 0.877606, loss_recon: -0.654307, loss_pred: 0.031303
iteration 1050: loss: 0.919594, loss_kl: 1.052410, loss_recon: -0.636835, loss_pred: 0.039152
iteration 1051: loss: 0.884308, loss_kl: 0.906631, loss_recon: -0.649406, loss_pred: 0.025039
iteration 1052: loss: 0.905449, loss_kl: 0.954982, loss_recon: -0.661020, loss_pred: 0.023374
iteration 1053: loss: 0.911023, loss_kl: 0.931316, loss_recon: -0.672037, loss_pred: 0.023409
 81%|█████████████████████▊     | 81/100 [7:20:13<1:42:19, 323.13s/it]iteration 1054: loss: 1.059363, loss_kl: 0.892118, loss_recon: -0.639844, loss_pred: 0.026732
iteration 1055: loss: 1.208364, loss_kl: 1.195100, loss_recon: -0.636006, loss_pred: 0.046172
iteration 1056: loss: 1.078534, loss_kl: 0.895678, loss_recon: -0.644038, loss_pred: 0.040142
iteration 1057: loss: 1.120880, loss_kl: 1.019878, loss_recon: -0.644743, loss_pred: 0.027099
iteration 1058: loss: 1.324457, loss_kl: 1.422277, loss_recon: -0.636160, loss_pred: 0.062088
iteration 1059: loss: 1.298274, loss_kl: 1.383502, loss_recon: -0.667006, loss_pred: 0.022131
iteration 1060: loss: 1.206198, loss_kl: 1.126642, loss_recon: -0.667121, loss_pred: 0.043032
iteration 1061: loss: 1.173383, loss_kl: 1.141813, loss_recon: -0.642855, loss_pred: 0.027804
iteration 1062: loss: 1.182799, loss_kl: 1.132902, loss_recon: -0.655782, loss_pred: 0.028216
iteration 1063: loss: 1.204685, loss_kl: 1.151981, loss_recon: -0.644101, loss_pred: 0.053383
iteration 1064: loss: 1.105946, loss_kl: 0.930725, loss_recon: -0.661506, loss_pred: 0.034654
iteration 1065: loss: 1.198855, loss_kl: 1.144203, loss_recon: -0.647133, loss_pred: 0.047945
iteration 1066: loss: 1.105125, loss_kl: 0.969245, loss_recon: -0.641900, loss_pred: 0.036480
 82%|██████████████████████▏    | 82/100 [7:25:38<1:37:06, 323.67s/it]iteration 1067: loss: 1.247395, loss_kl: 0.852855, loss_recon: -0.642424, loss_pred: 0.031336
iteration 1068: loss: 1.364669, loss_kl: 1.042065, loss_recon: -0.633109, loss_pred: 0.030660
iteration 1069: loss: 1.178689, loss_kl: 0.724601, loss_recon: -0.653680, loss_pred: 0.037638
iteration 1070: loss: 1.150023, loss_kl: 0.683103, loss_recon: -0.657274, loss_pred: 0.033289
iteration 1071: loss: 1.293770, loss_kl: 0.925874, loss_recon: -0.629861, loss_pred: 0.041160
iteration 1072: loss: 1.330287, loss_kl: 0.981706, loss_recon: -0.647243, loss_pred: 0.022742
iteration 1073: loss: 1.230245, loss_kl: 0.815960, loss_recon: -0.653800, loss_pred: 0.027624
iteration 1074: loss: 1.344878, loss_kl: 0.995201, loss_recon: -0.649673, loss_pred: 0.025826
iteration 1075: loss: 1.273357, loss_kl: 0.861248, loss_recon: -0.663811, loss_pred: 0.030265
iteration 1076: loss: 1.289255, loss_kl: 0.913047, loss_recon: -0.652996, loss_pred: 0.022138
iteration 1077: loss: 1.186505, loss_kl: 0.763923, loss_recon: -0.643367, loss_pred: 0.029317
iteration 1078: loss: 1.344409, loss_kl: 0.975730, loss_recon: -0.656623, loss_pred: 0.031503
iteration 1079: loss: 1.522218, loss_kl: 1.254288, loss_recon: -0.633633, loss_pred: 0.044942
 83%|██████████████████████▍    | 83/100 [7:31:01<1:31:38, 323.42s/it]iteration 1080: loss: 1.562807, loss_kl: 1.039975, loss_recon: -0.648474, loss_pred: 0.037732
iteration 1081: loss: 0.970885, loss_kl: 0.337894, loss_recon: -0.659526, loss_pred: 0.026547
iteration 1082: loss: 1.348760, loss_kl: 0.798672, loss_recon: -0.646196, loss_pred: 0.029360
iteration 1083: loss: 1.405987, loss_kl: 0.874034, loss_recon: -0.637978, loss_pred: 0.031282
iteration 1084: loss: 1.713085, loss_kl: 1.248679, loss_recon: -0.635770, loss_pred: 0.024797
iteration 1085: loss: 1.638594, loss_kl: 1.135713, loss_recon: -0.646626, loss_pred: 0.034670
iteration 1086: loss: 1.478611, loss_kl: 0.955238, loss_recon: -0.640205, loss_pred: 0.033231
iteration 1087: loss: 1.718334, loss_kl: 1.220434, loss_recon: -0.652995, loss_pred: 0.036631
iteration 1088: loss: 1.505786, loss_kl: 0.980746, loss_recon: -0.646441, loss_pred: 0.032670
iteration 1089: loss: 1.234739, loss_kl: 0.643123, loss_recon: -0.656450, loss_pred: 0.036199
iteration 1090: loss: 1.397383, loss_kl: 0.853887, loss_recon: -0.652270, loss_pred: 0.025367
iteration 1091: loss: 1.674465, loss_kl: 1.187343, loss_recon: -0.651950, loss_pred: 0.021699
iteration 1092: loss: 1.452700, loss_kl: 0.927339, loss_recon: -0.643949, loss_pred: 0.027094
 84%|██████████████████████▋    | 84/100 [7:36:24<1:26:16, 323.54s/it]iteration 1093: loss: 1.838123, loss_kl: 1.235785, loss_recon: -0.627653, loss_pred: 0.056998
iteration 1094: loss: 1.830977, loss_kl: 1.221697, loss_recon: -0.656877, loss_pred: 0.033779
iteration 1095: loss: 2.383918, loss_kl: 1.813657, loss_recon: -0.653511, loss_pred: 0.037555
iteration 1096: loss: 1.707898, loss_kl: 1.115897, loss_recon: -0.636146, loss_pred: 0.030182
iteration 1097: loss: 1.308622, loss_kl: 0.645347, loss_recon: -0.672938, loss_pred: 0.033323
iteration 1098: loss: 1.304387, loss_kl: 0.657989, loss_recon: -0.652430, loss_pred: 0.037795
iteration 1099: loss: 1.664308, loss_kl: 1.063716, loss_recon: -0.644943, loss_pred: 0.026501
iteration 1100: loss: 1.758809, loss_kl: 1.128594, loss_recon: -0.671837, loss_pred: 0.033552
iteration 1101: loss: 1.603675, loss_kl: 0.990955, loss_recon: -0.637330, loss_pred: 0.041395
iteration 1102: loss: 2.020062, loss_kl: 1.433463, loss_recon: -0.648422, loss_pred: 0.033659
iteration 1103: loss: 1.793476, loss_kl: 1.201549, loss_recon: -0.633198, loss_pred: 0.038762
iteration 1104: loss: 1.741668, loss_kl: 1.127240, loss_recon: -0.640798, loss_pred: 0.048713
iteration 1105: loss: 1.490877, loss_kl: 0.865667, loss_recon: -0.627157, loss_pred: 0.055712
 85%|██████████████████████▉    | 85/100 [7:41:48<1:20:55, 323.68s/it]iteration 1106: loss: 2.305570, loss_kl: 1.651071, loss_recon: -0.658282, loss_pred: 0.040130
iteration 1107: loss: 2.100879, loss_kl: 1.409706, loss_recon: -0.630816, loss_pred: 0.097851
iteration 1108: loss: 1.733398, loss_kl: 1.073464, loss_recon: -0.654179, loss_pred: 0.034306
iteration 1109: loss: 2.286992, loss_kl: 1.623884, loss_recon: -0.653164, loss_pred: 0.053135
iteration 1110: loss: 1.785244, loss_kl: 1.127773, loss_recon: -0.645580, loss_pred: 0.041886
iteration 1111: loss: 1.815893, loss_kl: 1.159499, loss_recon: -0.641473, loss_pred: 0.045760
iteration 1112: loss: 1.777050, loss_kl: 1.121529, loss_recon: -0.645283, loss_pred: 0.040067
iteration 1113: loss: 2.150403, loss_kl: 1.468202, loss_recon: -0.652934, loss_pred: 0.068317
iteration 1114: loss: 2.208516, loss_kl: 1.547108, loss_recon: -0.638221, loss_pred: 0.064335
iteration 1115: loss: 2.308674, loss_kl: 1.667227, loss_recon: -0.647318, loss_pred: 0.038472
iteration 1116: loss: 1.566280, loss_kl: 0.883231, loss_recon: -0.654971, loss_pred: 0.051570
iteration 1117: loss: 1.790715, loss_kl: 1.137839, loss_recon: -0.650297, loss_pred: 0.032842
iteration 1118: loss: 1.818313, loss_kl: 1.152787, loss_recon: -0.653368, loss_pred: 0.042818
 86%|███████████████████████▏   | 86/100 [7:47:12<1:15:32, 323.75s/it]iteration 1119: loss: 2.189873, loss_kl: 1.531713, loss_recon: -0.652646, loss_pred: 0.021373
iteration 1120: loss: 2.143678, loss_kl: 1.470644, loss_recon: -0.646326, loss_pred: 0.041935
iteration 1121: loss: 1.661130, loss_kl: 0.984922, loss_recon: -0.646273, loss_pred: 0.040132
iteration 1122: loss: 1.727177, loss_kl: 1.053354, loss_recon: -0.640585, loss_pred: 0.044144
iteration 1123: loss: 1.681331, loss_kl: 1.019950, loss_recon: -0.634138, loss_pred: 0.037804
iteration 1124: loss: 1.486116, loss_kl: 0.808751, loss_recon: -0.650121, loss_pred: 0.035618
iteration 1125: loss: 2.051919, loss_kl: 1.382405, loss_recon: -0.655199, loss_pred: 0.028628
iteration 1126: loss: 1.324999, loss_kl: 0.652694, loss_recon: -0.642291, loss_pred: 0.036772
iteration 1127: loss: 1.563279, loss_kl: 0.885923, loss_recon: -0.652509, loss_pred: 0.034020
iteration 1128: loss: 2.144964, loss_kl: 1.469323, loss_recon: -0.643557, loss_pred: 0.047296
iteration 1129: loss: 1.569960, loss_kl: 0.888942, loss_recon: -0.653496, loss_pred: 0.036726
iteration 1130: loss: 1.398767, loss_kl: 0.732033, loss_recon: -0.634264, loss_pred: 0.040049
iteration 1131: loss: 2.159895, loss_kl: 1.483192, loss_recon: -0.659772, loss_pred: 0.032287
 87%|███████████████████████▍   | 87/100 [7:52:35<1:10:06, 323.58s/it]iteration 1132: loss: 1.485236, loss_kl: 0.808264, loss_recon: -0.650705, loss_pred: 0.029492
iteration 1133: loss: 2.007641, loss_kl: 1.348606, loss_recon: -0.636093, loss_pred: 0.028322
iteration 1134: loss: 1.577294, loss_kl: 0.920148, loss_recon: -0.632185, loss_pred: 0.028632
iteration 1135: loss: 1.872395, loss_kl: 1.215675, loss_recon: -0.637824, loss_pred: 0.023746
iteration 1136: loss: 1.377210, loss_kl: 0.695952, loss_recon: -0.656046, loss_pred: 0.027988
iteration 1137: loss: 2.040124, loss_kl: 1.365666, loss_recon: -0.645154, loss_pred: 0.034752
iteration 1138: loss: 2.289253, loss_kl: 1.619774, loss_recon: -0.654516, loss_pred: 0.021426
iteration 1139: loss: 1.597103, loss_kl: 0.913656, loss_recon: -0.651846, loss_pred: 0.035246
iteration 1140: loss: 1.942032, loss_kl: 1.271574, loss_recon: -0.644719, loss_pred: 0.030811
iteration 1141: loss: 1.358153, loss_kl: 0.681106, loss_recon: -0.643369, loss_pred: 0.036396
iteration 1142: loss: 1.785262, loss_kl: 1.119750, loss_recon: -0.641950, loss_pred: 0.028029
iteration 1143: loss: 1.703042, loss_kl: 1.023956, loss_recon: -0.652540, loss_pred: 0.030632
iteration 1144: loss: 2.203537, loss_kl: 1.519817, loss_recon: -0.640191, loss_pred: 0.049593
 88%|███████████████████████▊   | 88/100 [7:57:59<1:04:43, 323.59s/it]iteration 1145: loss: 2.460686, loss_kl: 1.724226, loss_recon: -0.655257, loss_pred: 0.081204
iteration 1146: loss: 2.074843, loss_kl: 1.378246, loss_recon: -0.660166, loss_pred: 0.036431
iteration 1147: loss: 1.856668, loss_kl: 1.147096, loss_recon: -0.656832, loss_pred: 0.052739
iteration 1148: loss: 2.352352, loss_kl: 1.668742, loss_recon: -0.629907, loss_pred: 0.053704
iteration 1149: loss: 1.690472, loss_kl: 0.964238, loss_recon: -0.641657, loss_pred: 0.084577
iteration 1150: loss: 1.502817, loss_kl: 0.828743, loss_recon: -0.645870, loss_pred: 0.028203
iteration 1151: loss: 1.733599, loss_kl: 1.057449, loss_recon: -0.649249, loss_pred: 0.026901
iteration 1152: loss: 1.844000, loss_kl: 1.185215, loss_recon: -0.629130, loss_pred: 0.029654
iteration 1153: loss: 2.094331, loss_kl: 1.418862, loss_recon: -0.648984, loss_pred: 0.026485
iteration 1154: loss: 1.227414, loss_kl: 0.546017, loss_recon: -0.647759, loss_pred: 0.033637
iteration 1155: loss: 1.937370, loss_kl: 1.257931, loss_recon: -0.642402, loss_pred: 0.037038
iteration 1156: loss: 1.658777, loss_kl: 0.983264, loss_recon: -0.641561, loss_pred: 0.033952
iteration 1157: loss: 1.834050, loss_kl: 1.153066, loss_recon: -0.644084, loss_pred: 0.036900
 89%|█████████████████████████▊   | 89/100 [8:03:22<59:16, 323.30s/it]iteration 1158: loss: 1.973301, loss_kl: 1.269564, loss_recon: -0.662108, loss_pred: 0.041629
iteration 1159: loss: 2.068615, loss_kl: 1.392284, loss_recon: -0.632174, loss_pred: 0.044158
iteration 1160: loss: 1.372796, loss_kl: 0.680232, loss_recon: -0.651386, loss_pred: 0.041177
iteration 1161: loss: 2.069542, loss_kl: 1.384313, loss_recon: -0.645042, loss_pred: 0.040188
iteration 1162: loss: 2.174135, loss_kl: 1.481255, loss_recon: -0.645650, loss_pred: 0.047230
iteration 1163: loss: 1.699260, loss_kl: 0.984956, loss_recon: -0.645514, loss_pred: 0.068790
iteration 1164: loss: 1.790022, loss_kl: 1.117926, loss_recon: -0.637748, loss_pred: 0.034348
iteration 1165: loss: 1.413252, loss_kl: 0.684663, loss_recon: -0.656531, loss_pred: 0.072057
iteration 1166: loss: 2.434099, loss_kl: 1.754191, loss_recon: -0.646480, loss_pred: 0.033428
iteration 1167: loss: 1.917866, loss_kl: 1.222076, loss_recon: -0.629887, loss_pred: 0.065904
iteration 1168: loss: 1.578549, loss_kl: 0.900344, loss_recon: -0.652031, loss_pred: 0.026175
iteration 1169: loss: 1.677966, loss_kl: 0.995605, loss_recon: -0.642007, loss_pred: 0.040355
iteration 1170: loss: 1.641953, loss_kl: 0.966701, loss_recon: -0.650702, loss_pred: 0.024549
 90%|██████████████████████████   | 90/100 [8:08:45<53:54, 323.44s/it]iteration 1171: loss: 1.703179, loss_kl: 1.025444, loss_recon: -0.656962, loss_pred: 0.020772
iteration 1172: loss: 2.245461, loss_kl: 1.537535, loss_recon: -0.637757, loss_pred: 0.070168
iteration 1173: loss: 2.061308, loss_kl: 1.397325, loss_recon: -0.634241, loss_pred: 0.029741
iteration 1174: loss: 2.066617, loss_kl: 1.339604, loss_recon: -0.654165, loss_pred: 0.072847
iteration 1175: loss: 1.893816, loss_kl: 1.220055, loss_recon: -0.642812, loss_pred: 0.030949
iteration 1176: loss: 2.064322, loss_kl: 1.369768, loss_recon: -0.656661, loss_pred: 0.037893
iteration 1177: loss: 2.270771, loss_kl: 1.603540, loss_recon: -0.633132, loss_pred: 0.034099
iteration 1178: loss: 1.684912, loss_kl: 1.015648, loss_recon: -0.646970, loss_pred: 0.022294
iteration 1179: loss: 1.848371, loss_kl: 1.144305, loss_recon: -0.662287, loss_pred: 0.041778
iteration 1180: loss: 1.557854, loss_kl: 0.875936, loss_recon: -0.628771, loss_pred: 0.053147
iteration 1181: loss: 1.630635, loss_kl: 0.955734, loss_recon: -0.640960, loss_pred: 0.033940
iteration 1182: loss: 1.933430, loss_kl: 1.236704, loss_recon: -0.633717, loss_pred: 0.063010
iteration 1183: loss: 2.314598, loss_kl: 1.593040, loss_recon: -0.669124, loss_pred: 0.052434
 91%|██████████████████████████▍  | 91/100 [8:14:10<48:34, 323.86s/it]iteration 1184: loss: 1.986821, loss_kl: 1.270667, loss_recon: -0.646316, loss_pred: 0.069838
iteration 1185: loss: 1.454016, loss_kl: 0.766364, loss_recon: -0.641742, loss_pred: 0.045910
iteration 1186: loss: 2.000008, loss_kl: 1.324940, loss_recon: -0.637602, loss_pred: 0.037466
iteration 1187: loss: 2.571240, loss_kl: 1.883443, loss_recon: -0.634905, loss_pred: 0.052892
iteration 1188: loss: 1.635196, loss_kl: 0.948253, loss_recon: -0.652700, loss_pred: 0.034244
iteration 1189: loss: 1.787335, loss_kl: 1.039390, loss_recon: -0.646057, loss_pred: 0.101888
iteration 1190: loss: 2.184051, loss_kl: 1.495415, loss_recon: -0.649386, loss_pred: 0.039250
iteration 1191: loss: 1.388160, loss_kl: 0.682052, loss_recon: -0.636861, loss_pred: 0.069246
iteration 1192: loss: 1.602203, loss_kl: 0.925687, loss_recon: -0.656513, loss_pred: 0.020002
iteration 1193: loss: 1.707857, loss_kl: 0.976431, loss_recon: -0.655533, loss_pred: 0.075893
iteration 1194: loss: 1.944339, loss_kl: 1.247596, loss_recon: -0.646433, loss_pred: 0.050310
iteration 1195: loss: 1.465757, loss_kl: 0.743972, loss_recon: -0.651684, loss_pred: 0.070101
iteration 1196: loss: 0.987665, loss_kl: 0.247443, loss_recon: -0.653811, loss_pred: 0.086411
 92%|██████████████████████████▋  | 92/100 [8:19:33<43:07, 323.50s/it]iteration 1197: loss: 2.133822, loss_kl: 1.432675, loss_recon: -0.644297, loss_pred: 0.056850
iteration 1198: loss: 1.526977, loss_kl: 0.839153, loss_recon: -0.652449, loss_pred: 0.035375
iteration 1199: loss: 2.058481, loss_kl: 1.381463, loss_recon: -0.640653, loss_pred: 0.036366
iteration 1200: loss: 1.961539, loss_kl: 1.261418, loss_recon: -0.650791, loss_pred: 0.049330
iteration 1201: loss: 1.535540, loss_kl: 0.844228, loss_recon: -0.654705, loss_pred: 0.036607
iteration 1202: loss: 1.635059, loss_kl: 0.933232, loss_recon: -0.646734, loss_pred: 0.055093
iteration 1203: loss: 1.469504, loss_kl: 0.805031, loss_recon: -0.643208, loss_pred: 0.021265
iteration 1204: loss: 1.398861, loss_kl: 0.703873, loss_recon: -0.658639, loss_pred: 0.036349
iteration 1205: loss: 1.804139, loss_kl: 1.126945, loss_recon: -0.633838, loss_pred: 0.043356
iteration 1206: loss: 1.478812, loss_kl: 0.808806, loss_recon: -0.637292, loss_pred: 0.032714
iteration 1207: loss: 1.304107, loss_kl: 0.625024, loss_recon: -0.641023, loss_pred: 0.038060
iteration 1208: loss: 1.481646, loss_kl: 0.791918, loss_recon: -0.665205, loss_pred: 0.024523
iteration 1209: loss: 1.295171, loss_kl: 0.576012, loss_recon: -0.634997, loss_pred: 0.084162
 93%|██████████████████████████▉  | 93/100 [8:24:57<37:45, 323.58s/it]iteration 1210: loss: 2.093662, loss_kl: 1.434391, loss_recon: -0.631318, loss_pred: 0.027952
iteration 1211: loss: 2.118696, loss_kl: 1.421747, loss_recon: -0.653112, loss_pred: 0.043836
iteration 1212: loss: 1.786313, loss_kl: 1.121220, loss_recon: -0.631324, loss_pred: 0.033769
iteration 1213: loss: 2.168564, loss_kl: 1.463323, loss_recon: -0.645883, loss_pred: 0.059357
iteration 1214: loss: 1.950374, loss_kl: 1.236681, loss_recon: -0.636608, loss_pred: 0.077084
iteration 1215: loss: 1.643367, loss_kl: 0.963577, loss_recon: -0.646995, loss_pred: 0.032795
iteration 1216: loss: 1.950439, loss_kl: 1.258977, loss_recon: -0.659145, loss_pred: 0.032317
iteration 1217: loss: 2.109310, loss_kl: 1.436675, loss_recon: -0.640228, loss_pred: 0.032407
iteration 1218: loss: 1.450641, loss_kl: 0.737806, loss_recon: -0.654072, loss_pred: 0.058763
iteration 1219: loss: 1.901816, loss_kl: 1.221932, loss_recon: -0.645405, loss_pred: 0.034480
iteration 1220: loss: 1.981357, loss_kl: 1.287744, loss_recon: -0.639309, loss_pred: 0.054304
iteration 1221: loss: 1.608629, loss_kl: 0.927180, loss_recon: -0.650840, loss_pred: 0.030610
iteration 1222: loss: 1.355550, loss_kl: 0.618840, loss_recon: -0.660881, loss_pred: 0.075828
 94%|███████████████████████████▎ | 94/100 [8:30:19<32:19, 323.19s/it]iteration 1223: loss: 1.908700, loss_kl: 1.199658, loss_recon: -0.651173, loss_pred: 0.057869
iteration 1224: loss: 1.210297, loss_kl: 0.495710, loss_recon: -0.642270, loss_pred: 0.072317
iteration 1225: loss: 1.855575, loss_kl: 1.163727, loss_recon: -0.644443, loss_pred: 0.047404
iteration 1226: loss: 1.954420, loss_kl: 1.223334, loss_recon: -0.657573, loss_pred: 0.073512
iteration 1227: loss: 1.509089, loss_kl: 0.823282, loss_recon: -0.639068, loss_pred: 0.046739
iteration 1228: loss: 1.801532, loss_kl: 1.118116, loss_recon: -0.627739, loss_pred: 0.055677
iteration 1229: loss: 1.660616, loss_kl: 0.976399, loss_recon: -0.643764, loss_pred: 0.040453
iteration 1230: loss: 1.897597, loss_kl: 1.220414, loss_recon: -0.642699, loss_pred: 0.034484
iteration 1231: loss: 1.733877, loss_kl: 1.028008, loss_recon: -0.656831, loss_pred: 0.049038
iteration 1232: loss: 1.954958, loss_kl: 1.244306, loss_recon: -0.647405, loss_pred: 0.063247
iteration 1233: loss: 2.031314, loss_kl: 1.333481, loss_recon: -0.640983, loss_pred: 0.056851
iteration 1234: loss: 1.578582, loss_kl: 0.903076, loss_recon: -0.640505, loss_pred: 0.035001
iteration 1235: loss: 1.581756, loss_kl: 0.917783, loss_recon: -0.631757, loss_pred: 0.032217
 95%|███████████████████████████▌ | 95/100 [8:35:43<26:56, 323.29s/it]iteration 1236: loss: 1.865515, loss_kl: 1.191324, loss_recon: -0.646937, loss_pred: 0.027254
iteration 1237: loss: 1.988448, loss_kl: 1.308564, loss_recon: -0.631486, loss_pred: 0.048398
iteration 1238: loss: 1.749825, loss_kl: 1.064836, loss_recon: -0.641676, loss_pred: 0.043313
iteration 1239: loss: 2.192399, loss_kl: 1.512744, loss_recon: -0.651585, loss_pred: 0.028071
iteration 1240: loss: 1.914992, loss_kl: 1.255158, loss_recon: -0.632491, loss_pred: 0.027343
iteration 1241: loss: 1.627690, loss_kl: 0.938916, loss_recon: -0.646311, loss_pred: 0.042464
iteration 1242: loss: 1.857494, loss_kl: 1.148918, loss_recon: -0.640787, loss_pred: 0.067789
iteration 1243: loss: 1.958917, loss_kl: 1.264123, loss_recon: -0.638637, loss_pred: 0.056157
iteration 1244: loss: 1.902879, loss_kl: 1.208012, loss_recon: -0.654611, loss_pred: 0.040257
iteration 1245: loss: 2.048912, loss_kl: 1.337218, loss_recon: -0.652653, loss_pred: 0.059041
iteration 1246: loss: 1.665861, loss_kl: 0.979809, loss_recon: -0.657571, loss_pred: 0.028482
iteration 1247: loss: 1.397951, loss_kl: 0.715938, loss_recon: -0.645078, loss_pred: 0.036935
iteration 1248: loss: 1.800824, loss_kl: 1.110053, loss_recon: -0.665217, loss_pred: 0.025554
 96%|███████████████████████████▊ | 96/100 [8:41:06<21:33, 323.46s/it]iteration 1249: loss: 1.825069, loss_kl: 1.136475, loss_recon: -0.644331, loss_pred: 0.044263
iteration 1250: loss: 1.857477, loss_kl: 1.176460, loss_recon: -0.646519, loss_pred: 0.034498
iteration 1251: loss: 2.128017, loss_kl: 1.455230, loss_recon: -0.640777, loss_pred: 0.032010
iteration 1252: loss: 1.815029, loss_kl: 1.120650, loss_recon: -0.653401, loss_pred: 0.040978
iteration 1253: loss: 2.061286, loss_kl: 1.358255, loss_recon: -0.662362, loss_pred: 0.040669
iteration 1254: loss: 1.875482, loss_kl: 1.200754, loss_recon: -0.636832, loss_pred: 0.037896
iteration 1255: loss: 2.081151, loss_kl: 1.409775, loss_recon: -0.642823, loss_pred: 0.028553
iteration 1256: loss: 1.751285, loss_kl: 1.067657, loss_recon: -0.650335, loss_pred: 0.033293
iteration 1257: loss: 1.628072, loss_kl: 0.950601, loss_recon: -0.631765, loss_pred: 0.045707
iteration 1258: loss: 1.649045, loss_kl: 0.954939, loss_recon: -0.653184, loss_pred: 0.040922
iteration 1259: loss: 1.770513, loss_kl: 1.104217, loss_recon: -0.629379, loss_pred: 0.036917
iteration 1260: loss: 1.844959, loss_kl: 1.164016, loss_recon: -0.641137, loss_pred: 0.039806
iteration 1261: loss: 1.284410, loss_kl: 0.610996, loss_recon: -0.644132, loss_pred: 0.029283
 97%|████████████████████████████▏| 97/100 [8:46:29<16:09, 323.06s/it]iteration 1262: loss: 2.060302, loss_kl: 1.378006, loss_recon: -0.650983, loss_pred: 0.031312
iteration 1263: loss: 1.566624, loss_kl: 0.890430, loss_recon: -0.631315, loss_pred: 0.044878
iteration 1264: loss: 2.282055, loss_kl: 1.604142, loss_recon: -0.628860, loss_pred: 0.049053
iteration 1265: loss: 2.207326, loss_kl: 1.519601, loss_recon: -0.648634, loss_pred: 0.039091
iteration 1266: loss: 2.161144, loss_kl: 1.471647, loss_recon: -0.653231, loss_pred: 0.036266
iteration 1267: loss: 1.548940, loss_kl: 0.866773, loss_recon: -0.643359, loss_pred: 0.038808
iteration 1268: loss: 1.292157, loss_kl: 0.615503, loss_recon: -0.646195, loss_pred: 0.030460
iteration 1269: loss: 1.256304, loss_kl: 0.602870, loss_recon: -0.633113, loss_pred: 0.020321
iteration 1270: loss: 1.781376, loss_kl: 1.099522, loss_recon: -0.655459, loss_pred: 0.026395
iteration 1271: loss: 1.610210, loss_kl: 0.938220, loss_recon: -0.640919, loss_pred: 0.031070
iteration 1272: loss: 1.754978, loss_kl: 1.088959, loss_recon: -0.646948, loss_pred: 0.019070
iteration 1273: loss: 1.698838, loss_kl: 1.037536, loss_recon: -0.638880, loss_pred: 0.022422
iteration 1274: loss: 1.929290, loss_kl: 1.278371, loss_recon: -0.628132, loss_pred: 0.022787
 98%|████████████████████████████▍| 98/100 [8:51:53<10:46, 323.39s/it]iteration 1275: loss: 1.520799, loss_kl: 0.854276, loss_recon: -0.638739, loss_pred: 0.027785
iteration 1276: loss: 2.089647, loss_kl: 1.418964, loss_recon: -0.641244, loss_pred: 0.029439
iteration 1277: loss: 2.027119, loss_kl: 1.334353, loss_recon: -0.659593, loss_pred: 0.033173
iteration 1278: loss: 1.674951, loss_kl: 1.006782, loss_recon: -0.643847, loss_pred: 0.024323
iteration 1279: loss: 1.909595, loss_kl: 1.242575, loss_recon: -0.637428, loss_pred: 0.029593
iteration 1280: loss: 1.737669, loss_kl: 1.067504, loss_recon: -0.642144, loss_pred: 0.028021
iteration 1281: loss: 1.469040, loss_kl: 0.796382, loss_recon: -0.644389, loss_pred: 0.028269
iteration 1282: loss: 1.703103, loss_kl: 1.045635, loss_recon: -0.632072, loss_pred: 0.025396
iteration 1283: loss: 1.946059, loss_kl: 1.282615, loss_recon: -0.631704, loss_pred: 0.031740
iteration 1284: loss: 1.908789, loss_kl: 1.226736, loss_recon: -0.651253, loss_pred: 0.030800
iteration 1285: loss: 1.831132, loss_kl: 1.159158, loss_recon: -0.649636, loss_pred: 0.022338
iteration 1286: loss: 1.473105, loss_kl: 0.809987, loss_recon: -0.636427, loss_pred: 0.026691
iteration 1287: loss: 1.311667, loss_kl: 0.616913, loss_recon: -0.659208, loss_pred: 0.035546
 99%|████████████████████████████▋| 99/100 [8:57:15<05:23, 323.01s/it]iteration 1288: loss: 1.982846, loss_kl: 1.303962, loss_recon: -0.644136, loss_pred: 0.034748
iteration 1289: loss: 2.136304, loss_kl: 1.467273, loss_recon: -0.643030, loss_pred: 0.026001
iteration 1290: loss: 2.893507, loss_kl: 2.221912, loss_recon: -0.644077, loss_pred: 0.027517
iteration 1291: loss: 2.036755, loss_kl: 1.354081, loss_recon: -0.640728, loss_pred: 0.041946
iteration 1292: loss: 1.734504, loss_kl: 1.067062, loss_recon: -0.638784, loss_pred: 0.028658
iteration 1293: loss: 1.613791, loss_kl: 0.898088, loss_recon: -0.665735, loss_pred: 0.049968
iteration 1294: loss: 2.276703, loss_kl: 1.599038, loss_recon: -0.639765, loss_pred: 0.037900
iteration 1295: loss: 1.898359, loss_kl: 1.187426, loss_recon: -0.636215, loss_pred: 0.074719
iteration 1296: loss: 1.787695, loss_kl: 1.093476, loss_recon: -0.650866, loss_pred: 0.043353
iteration 1297: loss: 2.111696, loss_kl: 1.397025, loss_recon: -0.624341, loss_pred: 0.090330
iteration 1298: loss: 1.704084, loss_kl: 1.033841, loss_recon: -0.638396, loss_pred: 0.031847
iteration 1299: loss: 2.046600, loss_kl: 1.279574, loss_recon: -0.646357, loss_pred: 0.120669
iteration 1300: loss: 1.301927, loss_kl: 0.608881, loss_recon: -0.641595, loss_pred: 0.051451
save model to ../model/TV_Design[160, 160, 160]/TV_pretrain_Conv-ViT-Gen-B_16_skip4_vitpatch[8, 8, 8]_28k_epo100_bs24_lr0.01_[160, 160, 160]_s1234/epoch_99.pth
save model to ../model/TV_Design[160, 160, 160]/TV_pretrain_Conv-ViT-Gen-B_16_skip4_vitpatch[8, 8, 8]_28k_epo100_bs24_lr0.01_[160, 160, 160]_s1234/epoch_99.pth
 99%|████████████████████████████▋| 99/100 [9:02:40<05:28, 328.89s/it]
