/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Degradation', img_size=[64, 64, 64], vit_patches_size=[8, 8, 8], vit_name='ViT-B_16', pretrained_net_path=False, is_encoder_pretrained=True, deterministic=1, max_epochs=5, batch_size=64, base_lr=0.01, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/deg', list_dir='./lists/lists_Degradation', exp='TVD_Degradation[64, 64, 64]', distributed=False)
113 iterations per epoch. 565 max iterations 
  0%|                                           | 0/5 [00:00<?, ?it/s]iteration      1: loss: 0.546775, loss_ce: 0.693139, loss_dice: 0.400410
iteration      2: loss: 0.506519, loss_ce: 0.621303, loss_dice: 0.391735
iteration      3: loss: 0.451227, loss_ce: 0.537130, loss_dice: 0.365325
iteration      4: loss: 0.429937, loss_ce: 0.492923, loss_dice: 0.366950
iteration      5: loss: 0.422416, loss_ce: 0.464815, loss_dice: 0.380018
iteration      6: loss: 0.428659, loss_ce: 0.465528, loss_dice: 0.391790
iteration      7: loss: 0.427005, loss_ce: 0.466750, loss_dice: 0.387260
iteration      8: loss: 0.407236, loss_ce: 0.446856, loss_dice: 0.367617
iteration      9: loss: 0.394966, loss_ce: 0.441437, loss_dice: 0.348495
iteration     10: loss: 0.388224, loss_ce: 0.440676, loss_dice: 0.335773
iteration     11: loss: 0.388130, loss_ce: 0.445244, loss_dice: 0.331015
iteration     12: loss: 0.395340, loss_ce: 0.463271, loss_dice: 0.327410
iteration     13: loss: 0.385854, loss_ce: 0.441761, loss_dice: 0.329947
iteration     14: loss: 0.402872, loss_ce: 0.475488, loss_dice: 0.330256
iteration     15: loss: 0.390869, loss_ce: 0.450354, loss_dice: 0.331385
iteration     16: loss: 0.381061, loss_ce: 0.430448, loss_dice: 0.331673
iteration     17: loss: 0.384799, loss_ce: 0.439321, loss_dice: 0.330277
iteration     18: loss: 0.377105, loss_ce: 0.427097, loss_dice: 0.327112
iteration     19: loss: 0.380877, loss_ce: 0.438123, loss_dice: 0.323630
iteration     20: loss: 0.379505, loss_ce: 0.437977, loss_dice: 0.321032
iteration     21: loss: 0.381476, loss_ce: 0.440056, loss_dice: 0.322896
iteration     22: loss: 0.373071, loss_ce: 0.425029, loss_dice: 0.321114
iteration     23: loss: 0.373906, loss_ce: 0.427502, loss_dice: 0.320310
iteration     24: loss: 0.377053, loss_ce: 0.432050, loss_dice: 0.322055
iteration     25: loss: 0.374525, loss_ce: 0.426378, loss_dice: 0.322672
iteration     26: loss: 0.370784, loss_ce: 0.423348, loss_dice: 0.318220
iteration     27: loss: 0.389176, loss_ce: 0.454924, loss_dice: 0.323428
iteration     28: loss: 0.371852, loss_ce: 0.424171, loss_dice: 0.319533
iteration     29: loss: 0.369485, loss_ce: 0.423092, loss_dice: 0.315878
iteration     30: loss: 0.368229, loss_ce: 0.419261, loss_dice: 0.317197
iteration     31: loss: 0.362225, loss_ce: 0.412373, loss_dice: 0.312077
iteration     32: loss: 0.363757, loss_ce: 0.413660, loss_dice: 0.313854
iteration     33: loss: 0.356637, loss_ce: 0.400841, loss_dice: 0.312434
iteration     34: loss: 0.364616, loss_ce: 0.414167, loss_dice: 0.315065
iteration     35: loss: 0.364329, loss_ce: 0.414680, loss_dice: 0.313979
iteration     36: loss: 0.363378, loss_ce: 0.411476, loss_dice: 0.315280
iteration     37: loss: 0.360885, loss_ce: 0.409271, loss_dice: 0.312499
iteration     38: loss: 0.366874, loss_ce: 0.420585, loss_dice: 0.313163
iteration     39: loss: 0.364879, loss_ce: 0.416897, loss_dice: 0.312861
iteration     40: loss: 0.370006, loss_ce: 0.427534, loss_dice: 0.312478
iteration     41: loss: 0.361236, loss_ce: 0.412326, loss_dice: 0.310146
iteration     42: loss: 0.369988, loss_ce: 0.428402, loss_dice: 0.311573
iteration     43: loss: 0.355281, loss_ce: 0.400948, loss_dice: 0.309615
iteration     44: loss: 0.357361, loss_ce: 0.402914, loss_dice: 0.311808
iteration     45: loss: 0.364071, loss_ce: 0.416497, loss_dice: 0.311646
iteration     46: loss: 0.357729, loss_ce: 0.404701, loss_dice: 0.310757
iteration     47: loss: 0.358670, loss_ce: 0.404833, loss_dice: 0.312508
iteration     48: loss: 0.344311, loss_ce: 0.380483, loss_dice: 0.308140
iteration     49: loss: 0.366870, loss_ce: 0.423555, loss_dice: 0.310186
iteration     50: loss: 0.363880, loss_ce: 0.415175, loss_dice: 0.312584
iteration     51: loss: 0.355587, loss_ce: 0.403452, loss_dice: 0.307722
iteration     52: loss: 0.365793, loss_ce: 0.421205, loss_dice: 0.310380
iteration     53: loss: 0.358607, loss_ce: 0.407821, loss_dice: 0.309392
iteration     54: loss: 0.361965, loss_ce: 0.414108, loss_dice: 0.309822
iteration     55: loss: 0.366956, loss_ce: 0.423036, loss_dice: 0.310875
iteration     56: loss: 0.360399, loss_ce: 0.410784, loss_dice: 0.310014
iteration     57: loss: 0.359851, loss_ce: 0.411605, loss_dice: 0.308097
iteration     58: loss: 0.362973, loss_ce: 0.416812, loss_dice: 0.309134
iteration     59: loss: 0.358961, loss_ce: 0.410732, loss_dice: 0.307189
iteration     60: loss: 0.356179, loss_ce: 0.405558, loss_dice: 0.306801
iteration     61: loss: 0.369604, loss_ce: 0.429856, loss_dice: 0.309352
iteration     62: loss: 0.356268, loss_ce: 0.402843, loss_dice: 0.309693
iteration     63: loss: 0.360847, loss_ce: 0.411604, loss_dice: 0.310091
iteration     64: loss: 0.353931, loss_ce: 0.402069, loss_dice: 0.305794
iteration     65: loss: 0.368644, loss_ce: 0.428249, loss_dice: 0.309040
iteration     66: loss: 0.369968, loss_ce: 0.429493, loss_dice: 0.310444
iteration     67: loss: 0.357632, loss_ce: 0.406927, loss_dice: 0.308338
iteration     68: loss: 0.364140, loss_ce: 0.416939, loss_dice: 0.311341
iteration     69: loss: 0.361207, loss_ce: 0.413786, loss_dice: 0.308628
iteration     70: loss: 0.355067, loss_ce: 0.402043, loss_dice: 0.308091
iteration     71: loss: 0.362625, loss_ce: 0.413483, loss_dice: 0.311766
iteration     72: loss: 0.361103, loss_ce: 0.409172, loss_dice: 0.313035
iteration     73: loss: 0.344645, loss_ce: 0.383983, loss_dice: 0.305308
iteration     74: loss: 0.369200, loss_ce: 0.426384, loss_dice: 0.312017
iteration     75: loss: 0.368156, loss_ce: 0.425892, loss_dice: 0.310419
iteration     76: loss: 0.364214, loss_ce: 0.418631, loss_dice: 0.309797
iteration     77: loss: 0.361915, loss_ce: 0.414322, loss_dice: 0.309508
iteration     78: loss: 0.353448, loss_ce: 0.403197, loss_dice: 0.303699
iteration     79: loss: 0.354061, loss_ce: 0.405085, loss_dice: 0.303037
iteration     80: loss: 0.348608, loss_ce: 0.395249, loss_dice: 0.301968
iteration     81: loss: 0.350991, loss_ce: 0.398821, loss_dice: 0.303161
iteration     82: loss: 0.350132, loss_ce: 0.397694, loss_dice: 0.302569
iteration     83: loss: 0.358051, loss_ce: 0.410037, loss_dice: 0.306066
iteration     84: loss: 0.361422, loss_ce: 0.415327, loss_dice: 0.307517
iteration     85: loss: 0.349966, loss_ce: 0.396191, loss_dice: 0.303740
iteration     86: loss: 0.365159, loss_ce: 0.419626, loss_dice: 0.310693
iteration     87: loss: 0.361928, loss_ce: 0.415863, loss_dice: 0.307992
iteration     88: loss: 0.345473, loss_ce: 0.387344, loss_dice: 0.303603
iteration     89: loss: 0.362900, loss_ce: 0.415754, loss_dice: 0.310047
iteration     90: loss: 0.354130, loss_ce: 0.399895, loss_dice: 0.308364
iteration     91: loss: 0.358102, loss_ce: 0.407570, loss_dice: 0.308634
iteration     92: loss: 0.371171, loss_ce: 0.432770, loss_dice: 0.309573
iteration     93: loss: 0.363949, loss_ce: 0.419620, loss_dice: 0.308279
iteration     94: loss: 0.359857, loss_ce: 0.412935, loss_dice: 0.306779
iteration     95: loss: 0.350339, loss_ce: 0.396346, loss_dice: 0.304332
iteration     96: loss: 0.368305, loss_ce: 0.428616, loss_dice: 0.307995
iteration     97: loss: 0.356245, loss_ce: 0.406444, loss_dice: 0.306046
iteration     98: loss: 0.359540, loss_ce: 0.412397, loss_dice: 0.306683
iteration     99: loss: 0.364070, loss_ce: 0.419518, loss_dice: 0.308621
iteration    100: loss: 0.346844, loss_ce: 0.388911, loss_dice: 0.304776
iteration    101: loss: 0.351680, loss_ce: 0.398561, loss_dice: 0.304799
iteration    102: loss: 0.348158, loss_ce: 0.393316, loss_dice: 0.303001
iteration    103: loss: 0.362305, loss_ce: 0.416928, loss_dice: 0.307682
iteration    104: loss: 0.350161, loss_ce: 0.395946, loss_dice: 0.304377
iteration    105: loss: 0.362049, loss_ce: 0.417500, loss_dice: 0.306598
iteration    106: loss: 0.347442, loss_ce: 0.389988, loss_dice: 0.304897
iteration    107: loss: 0.359461, loss_ce: 0.410629, loss_dice: 0.308293
iteration    108: loss: 0.352088, loss_ce: 0.399413, loss_dice: 0.304764
iteration    109: loss: 0.360226, loss_ce: 0.411581, loss_dice: 0.308871
iteration    110: loss: 0.355804, loss_ce: 0.403996, loss_dice: 0.307611
iteration    111: loss: 0.356613, loss_ce: 0.405722, loss_dice: 0.307503
iteration    112: loss: 0.366184, loss_ce: 0.424385, loss_dice: 0.307983
iteration    113: loss: 0.360887, loss_ce: 0.418414, loss_dice: 0.303360
 20%|█████▊                       | 1/5 [1:04:36<4:18:26, 3876.64s/it]iteration    114: loss: 0.350119, loss_ce: 0.394153, loss_dice: 0.306084
iteration    115: loss: 0.360073, loss_ce: 0.414801, loss_dice: 0.305345
iteration    116: loss: 0.357203, loss_ce: 0.409348, loss_dice: 0.305058
iteration    117: loss: 0.360384, loss_ce: 0.416039, loss_dice: 0.304729
iteration    118: loss: 0.344439, loss_ce: 0.386541, loss_dice: 0.302337
iteration    119: loss: 0.356419, loss_ce: 0.408180, loss_dice: 0.304658
iteration    120: loss: 0.360135, loss_ce: 0.412584, loss_dice: 0.307687
iteration    121: loss: 0.364609, loss_ce: 0.421547, loss_dice: 0.307671
iteration    122: loss: 0.356846, loss_ce: 0.407627, loss_dice: 0.306064
iteration    123: loss: 0.350039, loss_ce: 0.392618, loss_dice: 0.307461
iteration    124: loss: 0.362359, loss_ce: 0.415013, loss_dice: 0.309705
iteration    125: loss: 0.360505, loss_ce: 0.414634, loss_dice: 0.306375
iteration    126: loss: 0.359740, loss_ce: 0.409313, loss_dice: 0.310167
iteration    127: loss: 0.365392, loss_ce: 0.422889, loss_dice: 0.307895
iteration    128: loss: 0.358190, loss_ce: 0.409499, loss_dice: 0.306880
iteration    129: loss: 0.356147, loss_ce: 0.406012, loss_dice: 0.306281
iteration    130: loss: 0.358977, loss_ce: 0.412477, loss_dice: 0.305477
iteration    131: loss: 0.355883, loss_ce: 0.404250, loss_dice: 0.307516
iteration    132: loss: 0.350395, loss_ce: 0.395432, loss_dice: 0.305359
iteration    133: loss: 0.369129, loss_ce: 0.428102, loss_dice: 0.310157
iteration    134: loss: 0.363345, loss_ce: 0.417767, loss_dice: 0.308924
iteration    135: loss: 0.359156, loss_ce: 0.412173, loss_dice: 0.306140
iteration    136: loss: 0.357254, loss_ce: 0.405430, loss_dice: 0.309078
iteration    137: loss: 0.364693, loss_ce: 0.423549, loss_dice: 0.305838
iteration    138: loss: 0.359724, loss_ce: 0.415209, loss_dice: 0.304238
iteration    139: loss: 0.354413, loss_ce: 0.402655, loss_dice: 0.306171
iteration    140: loss: 0.361185, loss_ce: 0.415515, loss_dice: 0.306854
iteration    141: loss: 0.357682, loss_ce: 0.408390, loss_dice: 0.306974
iteration    142: loss: 0.357122, loss_ce: 0.405476, loss_dice: 0.308768
iteration    143: loss: 0.360327, loss_ce: 0.412473, loss_dice: 0.308180
iteration    144: loss: 0.349057, loss_ce: 0.391592, loss_dice: 0.306521
iteration    145: loss: 0.362789, loss_ce: 0.420345, loss_dice: 0.305232
iteration    146: loss: 0.360089, loss_ce: 0.414890, loss_dice: 0.305288
iteration    147: loss: 0.354503, loss_ce: 0.404352, loss_dice: 0.304654
iteration    148: loss: 0.356680, loss_ce: 0.409052, loss_dice: 0.304308
iteration    149: loss: 0.356777, loss_ce: 0.406433, loss_dice: 0.307120
iteration    150: loss: 0.348063, loss_ce: 0.391572, loss_dice: 0.304555
iteration    151: loss: 0.361011, loss_ce: 0.419230, loss_dice: 0.302792
iteration    152: loss: 0.362928, loss_ce: 0.415808, loss_dice: 0.310048
iteration    153: loss: 0.352512, loss_ce: 0.399176, loss_dice: 0.305849
iteration    154: loss: 0.358787, loss_ce: 0.410071, loss_dice: 0.307504
iteration    155: loss: 0.341851, loss_ce: 0.378439, loss_dice: 0.305262
iteration    156: loss: 0.360281, loss_ce: 0.412998, loss_dice: 0.307565
iteration    157: loss: 0.357291, loss_ce: 0.409841, loss_dice: 0.304741
iteration    158: loss: 0.351251, loss_ce: 0.397461, loss_dice: 0.305042
iteration    159: loss: 0.353666, loss_ce: 0.403722, loss_dice: 0.303610
iteration    160: loss: 0.357122, loss_ce: 0.403823, loss_dice: 0.310421
iteration    161: loss: 0.353001, loss_ce: 0.400194, loss_dice: 0.305808
iteration    162: loss: 0.354647, loss_ce: 0.405593, loss_dice: 0.303701
iteration    163: loss: 0.351234, loss_ce: 0.395545, loss_dice: 0.306924
iteration    164: loss: 0.360940, loss_ce: 0.413384, loss_dice: 0.308497
iteration    165: loss: 0.356737, loss_ce: 0.406936, loss_dice: 0.306539
iteration    166: loss: 0.357447, loss_ce: 0.407489, loss_dice: 0.307405
iteration    167: loss: 0.359378, loss_ce: 0.413441, loss_dice: 0.305314
iteration    168: loss: 0.353791, loss_ce: 0.402086, loss_dice: 0.305496
iteration    169: loss: 0.339686, loss_ce: 0.376499, loss_dice: 0.302873
iteration    170: loss: 0.355613, loss_ce: 0.404039, loss_dice: 0.307186
iteration    171: loss: 0.360176, loss_ce: 0.411791, loss_dice: 0.308562
iteration    172: loss: 0.357694, loss_ce: 0.409932, loss_dice: 0.305456
iteration    173: loss: 0.349257, loss_ce: 0.394435, loss_dice: 0.304079
iteration    174: loss: 0.355259, loss_ce: 0.404438, loss_dice: 0.306079
iteration    175: loss: 0.361087, loss_ce: 0.417086, loss_dice: 0.305089
iteration    176: loss: 0.360586, loss_ce: 0.414134, loss_dice: 0.307038
iteration    177: loss: 0.356871, loss_ce: 0.409920, loss_dice: 0.303822
iteration    178: loss: 0.370558, loss_ce: 0.435172, loss_dice: 0.305944
iteration    179: loss: 0.348464, loss_ce: 0.390918, loss_dice: 0.306011
iteration    180: loss: 0.353160, loss_ce: 0.402047, loss_dice: 0.304272
iteration    181: loss: 0.353619, loss_ce: 0.403444, loss_dice: 0.303794
iteration    182: loss: 0.353615, loss_ce: 0.402777, loss_dice: 0.304452
iteration    183: loss: 0.363186, loss_ce: 0.420089, loss_dice: 0.306284
iteration    184: loss: 0.352927, loss_ce: 0.402883, loss_dice: 0.302972
iteration    185: loss: 0.354265, loss_ce: 0.403561, loss_dice: 0.304970
iteration    186: loss: 0.354896, loss_ce: 0.405879, loss_dice: 0.303912
iteration    187: loss: 0.357445, loss_ce: 0.410022, loss_dice: 0.304867
iteration    188: loss: 0.350798, loss_ce: 0.395469, loss_dice: 0.306128
iteration    189: loss: 0.352780, loss_ce: 0.398896, loss_dice: 0.306665
iteration    190: loss: 0.367068, loss_ce: 0.427294, loss_dice: 0.306842
iteration    191: loss: 0.355257, loss_ce: 0.404624, loss_dice: 0.305891
iteration    192: loss: 0.348290, loss_ce: 0.394309, loss_dice: 0.302272
iteration    193: loss: 0.359286, loss_ce: 0.415659, loss_dice: 0.302913
iteration    194: loss: 0.365266, loss_ce: 0.421491, loss_dice: 0.309041
iteration    195: loss: 0.368011, loss_ce: 0.428710, loss_dice: 0.307311
iteration    196: loss: 0.354469, loss_ce: 0.404969, loss_dice: 0.303969
iteration    197: loss: 0.354175, loss_ce: 0.404707, loss_dice: 0.303643
iteration    198: loss: 0.353518, loss_ce: 0.401359, loss_dice: 0.305677
iteration    199: loss: 0.359074, loss_ce: 0.410044, loss_dice: 0.308103
iteration    200: loss: 0.351929, loss_ce: 0.398344, loss_dice: 0.305515
iteration    201: loss: 0.348776, loss_ce: 0.393261, loss_dice: 0.304290
iteration    202: loss: 0.345982, loss_ce: 0.385724, loss_dice: 0.306240
iteration    203: loss: 0.347445, loss_ce: 0.390040, loss_dice: 0.304851
iteration    204: loss: 0.355619, loss_ce: 0.404096, loss_dice: 0.307142
iteration    205: loss: 0.359381, loss_ce: 0.411231, loss_dice: 0.307531
iteration    206: loss: 0.345274, loss_ce: 0.386986, loss_dice: 0.303562
iteration    207: loss: 0.367368, loss_ce: 0.426726, loss_dice: 0.308010
iteration    208: loss: 0.351311, loss_ce: 0.398785, loss_dice: 0.303837
iteration    209: loss: 0.357050, loss_ce: 0.408996, loss_dice: 0.305104
iteration    210: loss: 0.362500, loss_ce: 0.418366, loss_dice: 0.306635
iteration    211: loss: 0.371451, loss_ce: 0.435259, loss_dice: 0.307643
iteration    212: loss: 0.349692, loss_ce: 0.397741, loss_dice: 0.301643
iteration    213: loss: 0.354842, loss_ce: 0.405744, loss_dice: 0.303939
iteration    214: loss: 0.361402, loss_ce: 0.416285, loss_dice: 0.306520
iteration    215: loss: 0.364649, loss_ce: 0.422687, loss_dice: 0.306610
iteration    216: loss: 0.362890, loss_ce: 0.418811, loss_dice: 0.306969
iteration    217: loss: 0.360802, loss_ce: 0.416512, loss_dice: 0.305093
iteration    218: loss: 0.351193, loss_ce: 0.397452, loss_dice: 0.304933
iteration    219: loss: 0.352477, loss_ce: 0.399788, loss_dice: 0.305166
iteration    220: loss: 0.352374, loss_ce: 0.399790, loss_dice: 0.304958
iteration    221: loss: 0.349454, loss_ce: 0.391928, loss_dice: 0.306980
iteration    222: loss: 0.361922, loss_ce: 0.416082, loss_dice: 0.307762
iteration    223: loss: 0.363843, loss_ce: 0.418416, loss_dice: 0.309270
iteration    224: loss: 0.345390, loss_ce: 0.387950, loss_dice: 0.302829
iteration    225: loss: 0.349696, loss_ce: 0.392295, loss_dice: 0.307098
iteration    226: loss: 0.348538, loss_ce: 0.395467, loss_dice: 0.301609
 40%|███████████▌                 | 2/5 [2:08:49<3:13:07, 3862.65s/it]iteration    227: loss: 0.366200, loss_ce: 0.424385, loss_dice: 0.308015
iteration    228: loss: 0.358467, loss_ce: 0.410749, loss_dice: 0.306185
iteration    229: loss: 0.357492, loss_ce: 0.409327, loss_dice: 0.305658
iteration    230: loss: 0.357904, loss_ce: 0.407154, loss_dice: 0.308654
iteration    231: loss: 0.363121, loss_ce: 0.420210, loss_dice: 0.306031
iteration    232: loss: 0.360728, loss_ce: 0.416209, loss_dice: 0.305246
iteration    233: loss: 0.350996, loss_ce: 0.396546, loss_dice: 0.305445
iteration    234: loss: 0.358506, loss_ce: 0.411631, loss_dice: 0.305381
iteration    235: loss: 0.347054, loss_ce: 0.390864, loss_dice: 0.303244
iteration    236: loss: 0.359816, loss_ce: 0.412920, loss_dice: 0.306712
iteration    237: loss: 0.353665, loss_ce: 0.401541, loss_dice: 0.305788
iteration    238: loss: 0.357925, loss_ce: 0.408515, loss_dice: 0.307335
iteration    239: loss: 0.362572, loss_ce: 0.419806, loss_dice: 0.305338
iteration    240: loss: 0.361495, loss_ce: 0.413942, loss_dice: 0.309048
iteration    241: loss: 0.367091, loss_ce: 0.423486, loss_dice: 0.310696
iteration    242: loss: 0.359368, loss_ce: 0.415206, loss_dice: 0.303530
iteration    243: loss: 0.361065, loss_ce: 0.416595, loss_dice: 0.305535
iteration    244: loss: 0.347187, loss_ce: 0.391670, loss_dice: 0.302703
iteration    245: loss: 0.348989, loss_ce: 0.396561, loss_dice: 0.301416
iteration    246: loss: 0.355364, loss_ce: 0.406082, loss_dice: 0.304647
iteration    247: loss: 0.355681, loss_ce: 0.408387, loss_dice: 0.302975
iteration    248: loss: 0.352405, loss_ce: 0.401597, loss_dice: 0.303213
iteration    249: loss: 0.353760, loss_ce: 0.404947, loss_dice: 0.302573
iteration    250: loss: 0.353949, loss_ce: 0.402878, loss_dice: 0.305020
iteration    251: loss: 0.358484, loss_ce: 0.411014, loss_dice: 0.305954
iteration    252: loss: 0.358354, loss_ce: 0.411660, loss_dice: 0.305048
iteration    253: loss: 0.356399, loss_ce: 0.408406, loss_dice: 0.304393
iteration    254: loss: 0.357603, loss_ce: 0.413431, loss_dice: 0.301774
iteration    255: loss: 0.354967, loss_ce: 0.407016, loss_dice: 0.302918
iteration    256: loss: 0.361569, loss_ce: 0.415999, loss_dice: 0.307138
iteration    257: loss: 0.343598, loss_ce: 0.385364, loss_dice: 0.301833
iteration    258: loss: 0.374008, loss_ce: 0.441401, loss_dice: 0.306615
iteration    259: loss: 0.341598, loss_ce: 0.381543, loss_dice: 0.301653
iteration    260: loss: 0.346838, loss_ce: 0.391159, loss_dice: 0.302516
iteration    261: loss: 0.356125, loss_ce: 0.407961, loss_dice: 0.304288
iteration    262: loss: 0.346001, loss_ce: 0.387855, loss_dice: 0.304147
iteration    263: loss: 0.346337, loss_ce: 0.389086, loss_dice: 0.303589
iteration    264: loss: 0.353518, loss_ce: 0.403545, loss_dice: 0.303492
iteration    265: loss: 0.351856, loss_ce: 0.400393, loss_dice: 0.303319
iteration    266: loss: 0.367207, loss_ce: 0.427319, loss_dice: 0.307096
iteration    267: loss: 0.361732, loss_ce: 0.416647, loss_dice: 0.306817
iteration    268: loss: 0.340797, loss_ce: 0.378165, loss_dice: 0.303429
iteration    269: loss: 0.355225, loss_ce: 0.406079, loss_dice: 0.304371
iteration    270: loss: 0.353930, loss_ce: 0.407521, loss_dice: 0.300339
iteration    271: loss: 0.356104, loss_ce: 0.407808, loss_dice: 0.304401
iteration    272: loss: 0.357303, loss_ce: 0.412169, loss_dice: 0.302438
iteration    273: loss: 0.358179, loss_ce: 0.412017, loss_dice: 0.304341
iteration    274: loss: 0.348962, loss_ce: 0.397715, loss_dice: 0.300210
iteration    275: loss: 0.363083, loss_ce: 0.420060, loss_dice: 0.306106
iteration    276: loss: 0.350846, loss_ce: 0.396741, loss_dice: 0.304952
iteration    277: loss: 0.364200, loss_ce: 0.419976, loss_dice: 0.308424
iteration    278: loss: 0.358792, loss_ce: 0.412313, loss_dice: 0.305272
iteration    279: loss: 0.360157, loss_ce: 0.412857, loss_dice: 0.307457
iteration    280: loss: 0.353274, loss_ce: 0.401353, loss_dice: 0.305196
iteration    281: loss: 0.356101, loss_ce: 0.406058, loss_dice: 0.306144
iteration    282: loss: 0.360897, loss_ce: 0.414915, loss_dice: 0.306879
iteration    283: loss: 0.358226, loss_ce: 0.411280, loss_dice: 0.305171
iteration    284: loss: 0.354942, loss_ce: 0.406154, loss_dice: 0.303730
iteration    285: loss: 0.352905, loss_ce: 0.403045, loss_dice: 0.302764
iteration    286: loss: 0.360590, loss_ce: 0.416146, loss_dice: 0.305033
iteration    287: loss: 0.358539, loss_ce: 0.411566, loss_dice: 0.305513
iteration    288: loss: 0.349675, loss_ce: 0.398176, loss_dice: 0.301173
iteration    289: loss: 0.355807, loss_ce: 0.407825, loss_dice: 0.303790
iteration    290: loss: 0.351223, loss_ce: 0.396946, loss_dice: 0.305500
iteration    291: loss: 0.348286, loss_ce: 0.391546, loss_dice: 0.305027
iteration    292: loss: 0.344146, loss_ce: 0.387568, loss_dice: 0.300725
iteration    293: loss: 0.350259, loss_ce: 0.395887, loss_dice: 0.304632
iteration    294: loss: 0.353990, loss_ce: 0.404292, loss_dice: 0.303687
iteration    295: loss: 0.354778, loss_ce: 0.404968, loss_dice: 0.304588
iteration    296: loss: 0.355746, loss_ce: 0.407527, loss_dice: 0.303965
iteration    297: loss: 0.346560, loss_ce: 0.391585, loss_dice: 0.301535
iteration    298: loss: 0.352686, loss_ce: 0.402300, loss_dice: 0.303073
iteration    299: loss: 0.340968, loss_ce: 0.380996, loss_dice: 0.300940
iteration    300: loss: 0.350774, loss_ce: 0.399774, loss_dice: 0.301773
iteration    301: loss: 0.355232, loss_ce: 0.408046, loss_dice: 0.302419
iteration    302: loss: 0.364659, loss_ce: 0.422847, loss_dice: 0.306471
iteration    303: loss: 0.361129, loss_ce: 0.415333, loss_dice: 0.306925
iteration    304: loss: 0.358262, loss_ce: 0.414597, loss_dice: 0.301927
iteration    305: loss: 0.351005, loss_ce: 0.399188, loss_dice: 0.302823
iteration    306: loss: 0.359595, loss_ce: 0.413926, loss_dice: 0.305265
iteration    307: loss: 0.355175, loss_ce: 0.403354, loss_dice: 0.306996
iteration    308: loss: 0.353065, loss_ce: 0.400037, loss_dice: 0.306093
iteration    309: loss: 0.353063, loss_ce: 0.401464, loss_dice: 0.304662
iteration    310: loss: 0.360647, loss_ce: 0.414843, loss_dice: 0.306452
iteration    311: loss: 0.353628, loss_ce: 0.404068, loss_dice: 0.303187
iteration    312: loss: 0.357307, loss_ce: 0.408604, loss_dice: 0.306010
iteration    313: loss: 0.352884, loss_ce: 0.401020, loss_dice: 0.304748
iteration    314: loss: 0.349212, loss_ce: 0.395468, loss_dice: 0.302956
iteration    315: loss: 0.355270, loss_ce: 0.405782, loss_dice: 0.304757
iteration    316: loss: 0.345550, loss_ce: 0.387809, loss_dice: 0.303291
iteration    317: loss: 0.353875, loss_ce: 0.401561, loss_dice: 0.306190
iteration    318: loss: 0.360393, loss_ce: 0.414833, loss_dice: 0.305954
iteration    319: loss: 0.362696, loss_ce: 0.418478, loss_dice: 0.306913
iteration    320: loss: 0.353551, loss_ce: 0.399426, loss_dice: 0.307676
iteration    321: loss: 0.366198, loss_ce: 0.426632, loss_dice: 0.305764
iteration    322: loss: 0.363010, loss_ce: 0.420585, loss_dice: 0.305436
iteration    323: loss: 0.359325, loss_ce: 0.412125, loss_dice: 0.306525
iteration    324: loss: 0.351859, loss_ce: 0.398073, loss_dice: 0.305645
iteration    325: loss: 0.351531, loss_ce: 0.396909, loss_dice: 0.306153
iteration    326: loss: 0.346967, loss_ce: 0.389746, loss_dice: 0.304188
iteration    327: loss: 0.359562, loss_ce: 0.414090, loss_dice: 0.305033
iteration    328: loss: 0.350181, loss_ce: 0.396775, loss_dice: 0.303586
iteration    329: loss: 0.344986, loss_ce: 0.384781, loss_dice: 0.305192
iteration    330: loss: 0.352901, loss_ce: 0.401236, loss_dice: 0.304565
iteration    331: loss: 0.357944, loss_ce: 0.413173, loss_dice: 0.302716
iteration    332: loss: 0.353932, loss_ce: 0.400491, loss_dice: 0.307372
iteration    333: loss: 0.348766, loss_ce: 0.393657, loss_dice: 0.303876
iteration    334: loss: 0.352035, loss_ce: 0.403102, loss_dice: 0.300969
iteration    335: loss: 0.348199, loss_ce: 0.394826, loss_dice: 0.301573
iteration    336: loss: 0.354041, loss_ce: 0.402945, loss_dice: 0.305137
iteration    337: loss: 0.354717, loss_ce: 0.407917, loss_dice: 0.301518
iteration    338: loss: 0.357050, loss_ce: 0.411375, loss_dice: 0.302725
iteration    339: loss: 0.349537, loss_ce: 0.398221, loss_dice: 0.300853
 60%|█████████████████▍           | 3/5 [3:12:35<2:08:11, 3845.91s/it]iteration    340: loss: 0.353210, loss_ce: 0.402595, loss_dice: 0.303824
iteration    341: loss: 0.353546, loss_ce: 0.406180, loss_dice: 0.300912
iteration    342: loss: 0.357378, loss_ce: 0.410107, loss_dice: 0.304649
iteration    343: loss: 0.358945, loss_ce: 0.413861, loss_dice: 0.304029
iteration    344: loss: 0.356082, loss_ce: 0.408788, loss_dice: 0.303376
iteration    345: loss: 0.347744, loss_ce: 0.391903, loss_dice: 0.303585
iteration    346: loss: 0.361849, loss_ce: 0.418955, loss_dice: 0.304743
iteration    347: loss: 0.348724, loss_ce: 0.394942, loss_dice: 0.302506
iteration    348: loss: 0.350060, loss_ce: 0.396756, loss_dice: 0.303363
iteration    349: loss: 0.345963, loss_ce: 0.391693, loss_dice: 0.300233
iteration    350: loss: 0.350626, loss_ce: 0.399004, loss_dice: 0.302248
iteration    351: loss: 0.356552, loss_ce: 0.408181, loss_dice: 0.304922
iteration    352: loss: 0.353820, loss_ce: 0.405081, loss_dice: 0.302560
iteration    353: loss: 0.353072, loss_ce: 0.403910, loss_dice: 0.302234
iteration    354: loss: 0.362426, loss_ce: 0.420509, loss_dice: 0.304344
iteration    355: loss: 0.361054, loss_ce: 0.417184, loss_dice: 0.304924
iteration    356: loss: 0.363397, loss_ce: 0.420407, loss_dice: 0.306388
iteration    357: loss: 0.369718, loss_ce: 0.432113, loss_dice: 0.307323
iteration    358: loss: 0.345693, loss_ce: 0.389045, loss_dice: 0.302342
iteration    359: loss: 0.360763, loss_ce: 0.416105, loss_dice: 0.305421
iteration    360: loss: 0.351231, loss_ce: 0.396529, loss_dice: 0.305933
iteration    361: loss: 0.357626, loss_ce: 0.410808, loss_dice: 0.304444
iteration    362: loss: 0.364294, loss_ce: 0.421270, loss_dice: 0.307317
iteration    363: loss: 0.350236, loss_ce: 0.396520, loss_dice: 0.303951
iteration    364: loss: 0.353593, loss_ce: 0.405624, loss_dice: 0.301562
iteration    365: loss: 0.352777, loss_ce: 0.402907, loss_dice: 0.302647
iteration    366: loss: 0.353014, loss_ce: 0.403129, loss_dice: 0.302899
iteration    367: loss: 0.353208, loss_ce: 0.400287, loss_dice: 0.306128
iteration    368: loss: 0.338950, loss_ce: 0.375829, loss_dice: 0.302071
iteration    369: loss: 0.341449, loss_ce: 0.381308, loss_dice: 0.301590
iteration    370: loss: 0.360946, loss_ce: 0.416563, loss_dice: 0.305328
iteration    371: loss: 0.357932, loss_ce: 0.410769, loss_dice: 0.305095
iteration    372: loss: 0.358863, loss_ce: 0.414888, loss_dice: 0.302837
iteration    373: loss: 0.363471, loss_ce: 0.421633, loss_dice: 0.305308
iteration    374: loss: 0.344149, loss_ce: 0.384106, loss_dice: 0.304192
iteration    375: loss: 0.366797, loss_ce: 0.427321, loss_dice: 0.306272
iteration    376: loss: 0.345072, loss_ce: 0.389712, loss_dice: 0.300433
iteration    377: loss: 0.341871, loss_ce: 0.379600, loss_dice: 0.304141
iteration    378: loss: 0.349881, loss_ce: 0.398216, loss_dice: 0.301546
iteration    379: loss: 0.356411, loss_ce: 0.409386, loss_dice: 0.303436
iteration    380: loss: 0.350601, loss_ce: 0.398436, loss_dice: 0.302766
iteration    381: loss: 0.361996, loss_ce: 0.418939, loss_dice: 0.305053
iteration    382: loss: 0.361806, loss_ce: 0.415535, loss_dice: 0.308077
iteration    383: loss: 0.358735, loss_ce: 0.414110, loss_dice: 0.303361
iteration    384: loss: 0.354918, loss_ce: 0.406409, loss_dice: 0.303427
iteration    385: loss: 0.359640, loss_ce: 0.414384, loss_dice: 0.304895
iteration    386: loss: 0.358536, loss_ce: 0.412678, loss_dice: 0.304394
iteration    387: loss: 0.354208, loss_ce: 0.405543, loss_dice: 0.302873
iteration    388: loss: 0.354739, loss_ce: 0.405039, loss_dice: 0.304440
iteration    389: loss: 0.361655, loss_ce: 0.416286, loss_dice: 0.307025
iteration    390: loss: 0.368065, loss_ce: 0.428697, loss_dice: 0.307433
iteration    391: loss: 0.353713, loss_ce: 0.405481, loss_dice: 0.301945
iteration    392: loss: 0.360876, loss_ce: 0.417305, loss_dice: 0.304448
iteration    393: loss: 0.351462, loss_ce: 0.400027, loss_dice: 0.302898
iteration    394: loss: 0.358145, loss_ce: 0.411573, loss_dice: 0.304716
iteration    395: loss: 0.355028, loss_ce: 0.405559, loss_dice: 0.304497
iteration    396: loss: 0.352331, loss_ce: 0.401959, loss_dice: 0.302704
iteration    397: loss: 0.350369, loss_ce: 0.398571, loss_dice: 0.302167
iteration    398: loss: 0.359472, loss_ce: 0.413512, loss_dice: 0.305432
iteration    399: loss: 0.358032, loss_ce: 0.407633, loss_dice: 0.308431
iteration    400: loss: 0.350406, loss_ce: 0.398171, loss_dice: 0.302642
iteration    401: loss: 0.357427, loss_ce: 0.408376, loss_dice: 0.306478
iteration    402: loss: 0.349527, loss_ce: 0.395930, loss_dice: 0.303124
iteration    403: loss: 0.349184, loss_ce: 0.394945, loss_dice: 0.303423
iteration    404: loss: 0.352891, loss_ce: 0.400450, loss_dice: 0.305331
iteration    405: loss: 0.360471, loss_ce: 0.415733, loss_dice: 0.305210
iteration    406: loss: 0.350454, loss_ce: 0.396216, loss_dice: 0.304692
iteration    407: loss: 0.357604, loss_ce: 0.411230, loss_dice: 0.303979
iteration    408: loss: 0.351398, loss_ce: 0.399569, loss_dice: 0.303227
iteration    409: loss: 0.361424, loss_ce: 0.416459, loss_dice: 0.306389
iteration    410: loss: 0.345926, loss_ce: 0.390841, loss_dice: 0.301011
iteration    411: loss: 0.351563, loss_ce: 0.402162, loss_dice: 0.300963
iteration    412: loss: 0.340605, loss_ce: 0.381232, loss_dice: 0.299979
iteration    413: loss: 0.349213, loss_ce: 0.394944, loss_dice: 0.303481
iteration    414: loss: 0.347294, loss_ce: 0.391698, loss_dice: 0.302890
iteration    415: loss: 0.354112, loss_ce: 0.405078, loss_dice: 0.303146
iteration    416: loss: 0.359168, loss_ce: 0.415047, loss_dice: 0.303290
iteration    417: loss: 0.354847, loss_ce: 0.406856, loss_dice: 0.302837
iteration    418: loss: 0.351501, loss_ce: 0.399887, loss_dice: 0.303116
iteration    419: loss: 0.350478, loss_ce: 0.399568, loss_dice: 0.301389
iteration    420: loss: 0.352261, loss_ce: 0.401728, loss_dice: 0.302795
iteration    421: loss: 0.359873, loss_ce: 0.415692, loss_dice: 0.304055
iteration    422: loss: 0.351538, loss_ce: 0.399513, loss_dice: 0.303562
iteration    423: loss: 0.353003, loss_ce: 0.399640, loss_dice: 0.306366
iteration    424: loss: 0.350728, loss_ce: 0.397377, loss_dice: 0.304078
iteration    425: loss: 0.359739, loss_ce: 0.415012, loss_dice: 0.304467
iteration    426: loss: 0.354407, loss_ce: 0.404170, loss_dice: 0.304644
iteration    427: loss: 0.351243, loss_ce: 0.398603, loss_dice: 0.303884
iteration    428: loss: 0.350731, loss_ce: 0.395736, loss_dice: 0.305726
iteration    429: loss: 0.351096, loss_ce: 0.400145, loss_dice: 0.302047
iteration    430: loss: 0.356087, loss_ce: 0.405494, loss_dice: 0.306680
iteration    431: loss: 0.352660, loss_ce: 0.403071, loss_dice: 0.302249
iteration    432: loss: 0.350434, loss_ce: 0.397449, loss_dice: 0.303420
iteration    433: loss: 0.360890, loss_ce: 0.415703, loss_dice: 0.306077
iteration    434: loss: 0.363443, loss_ce: 0.421403, loss_dice: 0.305483
iteration    435: loss: 0.360332, loss_ce: 0.416293, loss_dice: 0.304372
iteration    436: loss: 0.354422, loss_ce: 0.405993, loss_dice: 0.302850
iteration    437: loss: 0.353179, loss_ce: 0.404752, loss_dice: 0.301606
iteration    438: loss: 0.351962, loss_ce: 0.401342, loss_dice: 0.302582
iteration    439: loss: 0.355421, loss_ce: 0.406305, loss_dice: 0.304537
iteration    440: loss: 0.349279, loss_ce: 0.394844, loss_dice: 0.303714
iteration    441: loss: 0.337201, loss_ce: 0.372805, loss_dice: 0.301596
iteration    442: loss: 0.352986, loss_ce: 0.405892, loss_dice: 0.300081
iteration    443: loss: 0.356876, loss_ce: 0.410323, loss_dice: 0.303429
iteration    444: loss: 0.362013, loss_ce: 0.416593, loss_dice: 0.307432
iteration    445: loss: 0.355250, loss_ce: 0.406611, loss_dice: 0.303890
iteration    446: loss: 0.343786, loss_ce: 0.386104, loss_dice: 0.301468
iteration    447: loss: 0.357349, loss_ce: 0.410950, loss_dice: 0.303748
iteration    448: loss: 0.353289, loss_ce: 0.402230, loss_dice: 0.304348
iteration    449: loss: 0.357028, loss_ce: 0.406293, loss_dice: 0.307763
iteration    450: loss: 0.348739, loss_ce: 0.393650, loss_dice: 0.303827
iteration    451: loss: 0.349982, loss_ce: 0.396927, loss_dice: 0.303037
iteration    452: loss: 0.368021, loss_ce: 0.427746, loss_dice: 0.308296
 60%|█████████████████▍           | 3/5 [4:16:51<2:51:14, 5137.28s/it]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/train.py", line 195, in <module>
    trainer[dataset_name](args, model, snapshot_path)
  File "/home/mhashemi/TransVNet/trainer.py", line 189, in trainer_deg
    if (epoch_num + 1) % save_interval == 0:
ZeroDivisionError: integer division or modulo by zero
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 382, in <module>
    net.load_state_dict(torch.load(snapshot)) # Loading the parameters from the training results
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/serialization.py", line 791, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/serialization.py", line 271, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/serialization.py", line 252, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '../model/TVD_Degradation[64, 64, 64]/TVD_encoderpretrained_ViT-B_16_vitpatch[8, 8, 8]_epo5_bs64_lr0.01_seed1234/epoch_4.pth'
