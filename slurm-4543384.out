/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=True, deterministic=1, max_epochs=200, batch_size=64, base_lr=0.003, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
5 iterations per epoch. 1000 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 2.359440, loss_kl: 16.567310, loss_recon: 0.693149, loss_pred: 1.649723
iteration 2: loss: 3.185211, loss_kl: 44.154236, loss_recon: 0.693297, loss_pred: 2.447760
iteration 3: loss: 21.446199, loss_kl: 1633.962769, loss_recon: 0.687679, loss_pred: 19.124557
iteration 4: loss: 3.573411, loss_kl: 211.972015, loss_recon: 0.689296, loss_pred: 2.672142
iteration 5: loss: 6.212995, loss_kl: 216.780899, loss_recon: 0.690990, loss_pred: 5.305224
  0%|▏                              | 1/200 [01:24<4:41:45, 84.95s/it]iteration 6: loss: 7.952552, loss_kl: 259.827698, loss_recon: 0.690375, loss_pred: 7.002349
iteration 7: loss: 3.588779, loss_kl: 147.135986, loss_recon: 0.689228, loss_pred: 2.752416
iteration 8: loss: 2.250375, loss_kl: 115.476990, loss_recon: 0.685996, loss_pred: 1.448902
iteration 9: loss: 4.261625, loss_kl: 158.910004, loss_recon: 0.692864, loss_pred: 3.409852
iteration 10: loss: 4.104172, loss_kl: 155.891647, loss_recon: 0.686723, loss_pred: 3.261557
  1%|▎                              | 2/200 [02:19<3:41:38, 67.16s/it]iteration 11: loss: 2.485697, loss_kl: 129.748535, loss_recon: 0.679663, loss_pred: 1.676285
iteration 12: loss: 1.713385, loss_kl: 138.992249, loss_recon: 0.680804, loss_pred: 0.893589
iteration 13: loss: 2.259375, loss_kl: 157.607300, loss_recon: 0.670877, loss_pred: 1.430891
iteration 14: loss: 2.612331, loss_kl: 160.334732, loss_recon: 0.658017, loss_pred: 1.793979
iteration 15: loss: 2.384549, loss_kl: 153.500763, loss_recon: 0.650676, loss_pred: 1.580372
  2%|▍                              | 3/200 [03:14<3:21:56, 61.51s/it]iteration 16: loss: 1.832955, loss_kl: 149.156982, loss_recon: 0.632353, loss_pred: 1.051444
iteration 17: loss: 1.521546, loss_kl: 137.707031, loss_recon: 0.656565, loss_pred: 0.727275
iteration 18: loss: 1.372073, loss_kl: 127.553253, loss_recon: 0.649072, loss_pred: 0.595449
iteration 19: loss: 1.397938, loss_kl: 122.206375, loss_recon: 0.647267, loss_pred: 0.628464
iteration 20: loss: 1.446707, loss_kl: 120.603378, loss_recon: 0.644552, loss_pred: 0.681551
  2%|▌                              | 4/200 [04:09<3:12:17, 58.86s/it]iteration 21: loss: 1.630842, loss_kl: 121.669060, loss_recon: 0.643591, loss_pred: 0.865582
iteration 22: loss: 1.374779, loss_kl: 117.124588, loss_recon: 0.642883, loss_pred: 0.614772
iteration 23: loss: 1.203586, loss_kl: 110.930679, loss_recon: 0.649816, loss_pred: 0.442839
iteration 24: loss: 1.190544, loss_kl: 106.689102, loss_recon: 0.642908, loss_pred: 0.440947
iteration 25: loss: 1.182160, loss_kl: 103.989868, loss_recon: 0.646951, loss_pred: 0.431218
  2%|▊                              | 5/200 [05:03<3:06:22, 57.35s/it]iteration 26: loss: 1.207069, loss_kl: 101.182304, loss_recon: 0.646638, loss_pred: 0.459248
iteration 27: loss: 1.164693, loss_kl: 94.502533, loss_recon: 0.637466, loss_pred: 0.432725
iteration 28: loss: 1.198481, loss_kl: 83.660896, loss_recon: 0.646834, loss_pred: 0.467986
iteration 29: loss: 1.119533, loss_kl: 79.837730, loss_recon: 0.640740, loss_pred: 0.398955
iteration 30: loss: 1.038973, loss_kl: 78.110161, loss_recon: 0.632101, loss_pred: 0.328761
  3%|▉                              | 6/200 [05:58<3:02:40, 56.50s/it]iteration 31: loss: 0.926105, loss_kl: 75.715561, loss_recon: 0.631695, loss_pred: 0.218695
iteration 32: loss: 0.982760, loss_kl: 73.887688, loss_recon: 0.648950, loss_pred: 0.259923
iteration 33: loss: 1.033128, loss_kl: 71.813202, loss_recon: 0.637561, loss_pred: 0.323754
iteration 34: loss: 0.991010, loss_kl: 65.147011, loss_recon: 0.643054, loss_pred: 0.282808
iteration 35: loss: 0.905208, loss_kl: 62.833359, loss_recon: 0.637251, loss_pred: 0.205124
  4%|█                              | 7/200 [06:53<2:59:47, 55.89s/it]iteration 36: loss: 0.860656, loss_kl: 64.055252, loss_recon: 0.636808, loss_pred: 0.159793
iteration 37: loss: 0.889265, loss_kl: 58.343700, loss_recon: 0.638676, loss_pred: 0.192246
iteration 38: loss: 0.925513, loss_kl: 52.043949, loss_recon: 0.635374, loss_pred: 0.238095
iteration 39: loss: 0.898216, loss_kl: 47.790714, loss_recon: 0.628420, loss_pred: 0.222005
iteration 40: loss: 0.831790, loss_kl: 45.317616, loss_recon: 0.637444, loss_pred: 0.149028
  4%|█▏                             | 8/200 [07:48<2:58:17, 55.72s/it]iteration 41: loss: 0.829522, loss_kl: 45.251350, loss_recon: 0.633081, loss_pred: 0.151189
iteration 42: loss: 0.822411, loss_kl: 43.130402, loss_recon: 0.638107, loss_pred: 0.141174
iteration 43: loss: 0.848102, loss_kl: 40.280552, loss_recon: 0.633051, loss_pred: 0.174771
iteration 44: loss: 0.816377, loss_kl: 36.049076, loss_recon: 0.625893, loss_pred: 0.154435
iteration 45: loss: 0.799653, loss_kl: 34.876106, loss_recon: 0.624238, loss_pred: 0.140539
  4%|█▍                             | 9/200 [08:43<2:56:49, 55.55s/it]iteration 46: loss: 0.783238, loss_kl: 33.580894, loss_recon: 0.627834, loss_pred: 0.121823
iteration 47: loss: 0.786196, loss_kl: 30.479403, loss_recon: 0.632346, loss_pred: 0.123370
iteration 48: loss: 0.788158, loss_kl: 26.129631, loss_recon: 0.637005, loss_pred: 0.125023
iteration 49: loss: 0.774969, loss_kl: 23.049126, loss_recon: 0.629014, loss_pred: 0.122906
iteration 50: loss: 0.782430, loss_kl: 21.034512, loss_recon: 0.622153, loss_pred: 0.139242
  5%|█▌                            | 10/200 [09:38<2:55:26, 55.40s/it]iteration 51: loss: 0.747170, loss_kl: 21.073355, loss_recon: 0.622449, loss_pred: 0.103647
iteration 52: loss: 0.759248, loss_kl: 21.886040, loss_recon: 0.624240, loss_pred: 0.113121
iteration 53: loss: 0.735693, loss_kl: 19.551254, loss_recon: 0.623188, loss_pred: 0.092954
iteration 54: loss: 0.752555, loss_kl: 18.186550, loss_recon: 0.626759, loss_pred: 0.107609
iteration 55: loss: 0.739910, loss_kl: 16.326643, loss_recon: 0.631256, loss_pred: 0.092327
  6%|█▋                            | 11/200 [10:33<2:53:41, 55.14s/it]iteration 56: loss: 0.788614, loss_kl: 15.507445, loss_recon: 0.632164, loss_pred: 0.140942
iteration 57: loss: 0.716053, loss_kl: 16.787327, loss_recon: 0.618872, loss_pred: 0.080394
iteration 58: loss: 0.719793, loss_kl: 13.653325, loss_recon: 0.625501, loss_pred: 0.080639
iteration 59: loss: 0.732189, loss_kl: 13.511473, loss_recon: 0.622767, loss_pred: 0.095911
iteration 60: loss: 0.731129, loss_kl: 12.519482, loss_recon: 0.629366, loss_pred: 0.089244
  6%|█▊                            | 12/200 [11:28<2:52:41, 55.11s/it]iteration 61: loss: 0.725002, loss_kl: 12.566615, loss_recon: 0.613420, loss_pred: 0.099016
iteration 62: loss: 0.723743, loss_kl: 12.051739, loss_recon: 0.622962, loss_pred: 0.088730
iteration 63: loss: 0.723707, loss_kl: 11.047396, loss_recon: 0.625176, loss_pred: 0.087484
iteration 64: loss: 0.724581, loss_kl: 10.485779, loss_recon: 0.627029, loss_pred: 0.087066
iteration 65: loss: 0.745536, loss_kl: 10.413471, loss_recon: 0.623891, loss_pred: 0.111231
  6%|█▉                            | 13/200 [12:23<2:51:29, 55.03s/it]iteration 66: loss: 0.691884, loss_kl: 8.759873, loss_recon: 0.612695, loss_pred: 0.070429
iteration 67: loss: 0.723101, loss_kl: 8.846881, loss_recon: 0.625242, loss_pred: 0.089013
iteration 68: loss: 0.713477, loss_kl: 8.218622, loss_recon: 0.619207, loss_pred: 0.086051
iteration 69: loss: 0.741591, loss_kl: 8.207161, loss_recon: 0.633495, loss_pred: 0.099889
iteration 70: loss: 0.705951, loss_kl: 7.518001, loss_recon: 0.620157, loss_pred: 0.078276
  7%|██                            | 14/200 [13:18<2:50:30, 55.00s/it]iteration 71: loss: 0.734486, loss_kl: 7.543424, loss_recon: 0.620174, loss_pred: 0.106769
iteration 72: loss: 0.712907, loss_kl: 6.479889, loss_recon: 0.619619, loss_pred: 0.086808
iteration 73: loss: 0.715744, loss_kl: 7.030847, loss_recon: 0.618871, loss_pred: 0.089841
iteration 74: loss: 0.722704, loss_kl: 7.174734, loss_recon: 0.623807, loss_pred: 0.091722
iteration 75: loss: 0.733914, loss_kl: 6.568211, loss_recon: 0.624000, loss_pred: 0.103346
  8%|██▎                           | 15/200 [14:13<2:49:25, 54.95s/it]iteration 76: loss: 0.685436, loss_kl: 6.315241, loss_recon: 0.617818, loss_pred: 0.061302
iteration 77: loss: 0.706091, loss_kl: 6.809753, loss_recon: 0.614775, loss_pred: 0.084506
iteration 78: loss: 0.695997, loss_kl: 5.206022, loss_recon: 0.612260, loss_pred: 0.078531
iteration 79: loss: 0.741549, loss_kl: 5.367681, loss_recon: 0.639087, loss_pred: 0.097094
iteration 80: loss: 0.699101, loss_kl: 6.304534, loss_recon: 0.613618, loss_pred: 0.079178
  8%|██▍                           | 16/200 [15:07<2:48:15, 54.87s/it]iteration 81: loss: 0.707847, loss_kl: 5.233580, loss_recon: 0.626106, loss_pred: 0.076507
iteration 82: loss: 0.690324, loss_kl: 5.144482, loss_recon: 0.616791, loss_pred: 0.068388
iteration 83: loss: 0.735923, loss_kl: 4.669192, loss_recon: 0.620370, loss_pred: 0.110884
iteration 84: loss: 0.738183, loss_kl: 5.033665, loss_recon: 0.609242, loss_pred: 0.123907
iteration 85: loss: 0.714201, loss_kl: 5.237416, loss_recon: 0.623493, loss_pred: 0.085471
  8%|██▌                           | 17/200 [16:02<2:47:19, 54.86s/it]iteration 86: loss: 0.695265, loss_kl: 5.302629, loss_recon: 0.614101, loss_pred: 0.075861
iteration 87: loss: 0.738238, loss_kl: 5.041758, loss_recon: 0.627902, loss_pred: 0.105294
iteration 88: loss: 0.690799, loss_kl: 4.522265, loss_recon: 0.618729, loss_pred: 0.067547
iteration 89: loss: 0.712957, loss_kl: 5.242084, loss_recon: 0.616422, loss_pred: 0.091292
iteration 90: loss: 0.699498, loss_kl: 5.641821, loss_recon: 0.616784, loss_pred: 0.077072
  9%|██▋                           | 18/200 [16:57<2:46:20, 54.84s/it]iteration 91: loss: 0.681468, loss_kl: 4.898332, loss_recon: 0.617611, loss_pred: 0.058959
iteration 92: loss: 0.691970, loss_kl: 5.240913, loss_recon: 0.612971, loss_pred: 0.073758
iteration 93: loss: 0.675090, loss_kl: 5.654896, loss_recon: 0.611148, loss_pred: 0.058288
iteration 94: loss: 0.721195, loss_kl: 5.630226, loss_recon: 0.625530, loss_pred: 0.090035
iteration 95: loss: 0.674725, loss_kl: 5.670209, loss_recon: 0.619207, loss_pred: 0.049848
 10%|██▊                           | 19/200 [17:52<2:45:24, 54.83s/it]iteration 96: loss: 0.700244, loss_kl: 5.649948, loss_recon: 0.617471, loss_pred: 0.077122
iteration 97: loss: 0.738874, loss_kl: 5.394213, loss_recon: 0.626437, loss_pred: 0.107043
iteration 98: loss: 0.694142, loss_kl: 4.480789, loss_recon: 0.608076, loss_pred: 0.081585
iteration 99: loss: 0.710648, loss_kl: 4.985802, loss_recon: 0.620310, loss_pred: 0.085352
iteration 100: loss: 0.698297, loss_kl: 4.593523, loss_recon: 0.616620, loss_pred: 0.077083
 10%|███                           | 20/200 [18:47<2:44:47, 54.93s/it]iteration 101: loss: 0.678151, loss_kl: 5.011928, loss_recon: 0.610608, loss_pred: 0.062531
iteration 102: loss: 0.693342, loss_kl: 5.423254, loss_recon: 0.609983, loss_pred: 0.077936
iteration 103: loss: 0.697164, loss_kl: 5.261274, loss_recon: 0.623678, loss_pred: 0.068225
iteration 104: loss: 0.687115, loss_kl: 5.651699, loss_recon: 0.619876, loss_pred: 0.061587
iteration 105: loss: 0.685007, loss_kl: 4.743021, loss_recon: 0.609905, loss_pred: 0.070359
 10%|███▏                          | 21/200 [19:45<2:46:33, 55.83s/it]iteration 106: loss: 0.692603, loss_kl: 5.222977, loss_recon: 0.617452, loss_pred: 0.069928
iteration 107: loss: 0.678355, loss_kl: 4.679162, loss_recon: 0.613240, loss_pred: 0.060436
iteration 108: loss: 0.676110, loss_kl: 3.981038, loss_recon: 0.614712, loss_pred: 0.057417
iteration 109: loss: 0.687263, loss_kl: 4.523274, loss_recon: 0.619362, loss_pred: 0.063378
iteration 110: loss: 0.678102, loss_kl: 4.821995, loss_recon: 0.615551, loss_pred: 0.057729
 11%|███▎                          | 22/200 [20:40<2:44:53, 55.58s/it]iteration 111: loss: 0.667946, loss_kl: 4.280868, loss_recon: 0.619694, loss_pred: 0.043971
iteration 112: loss: 0.686742, loss_kl: 4.408376, loss_recon: 0.606621, loss_pred: 0.075713
iteration 113: loss: 0.705009, loss_kl: 5.034830, loss_recon: 0.611841, loss_pred: 0.088133
iteration 114: loss: 0.684807, loss_kl: 4.585959, loss_recon: 0.623876, loss_pred: 0.056345
iteration 115: loss: 0.714244, loss_kl: 4.639506, loss_recon: 0.614396, loss_pred: 0.095209
 12%|███▍                          | 23/200 [21:35<2:43:09, 55.31s/it]iteration 116: loss: 0.681463, loss_kl: 4.796740, loss_recon: 0.610481, loss_pred: 0.066184
iteration 117: loss: 0.707472, loss_kl: 5.317259, loss_recon: 0.612134, loss_pred: 0.090021
iteration 118: loss: 0.674949, loss_kl: 4.545405, loss_recon: 0.610635, loss_pred: 0.059768
iteration 119: loss: 0.709175, loss_kl: 4.661142, loss_recon: 0.622224, loss_pred: 0.082289
iteration 120: loss: 0.670755, loss_kl: 3.869421, loss_recon: 0.613941, loss_pred: 0.052945
 12%|███▌                          | 24/200 [22:30<2:42:02, 55.24s/it]iteration 121: loss: 0.718918, loss_kl: 4.744605, loss_recon: 0.615599, loss_pred: 0.098574
iteration 122: loss: 0.697369, loss_kl: 4.107138, loss_recon: 0.626533, loss_pred: 0.066729
iteration 123: loss: 0.681203, loss_kl: 4.259866, loss_recon: 0.611716, loss_pred: 0.065228
iteration 124: loss: 0.672226, loss_kl: 4.374384, loss_recon: 0.607559, loss_pred: 0.060292
iteration 125: loss: 0.669009, loss_kl: 4.740645, loss_recon: 0.613949, loss_pred: 0.050319
 12%|███▊                          | 25/200 [23:25<2:40:50, 55.15s/it]iteration 126: loss: 0.674733, loss_kl: 4.977650, loss_recon: 0.602831, loss_pred: 0.066924
iteration 127: loss: 0.703029, loss_kl: 4.523493, loss_recon: 0.625565, loss_pred: 0.072940
iteration 128: loss: 0.670614, loss_kl: 4.686111, loss_recon: 0.614508, loss_pred: 0.051420
iteration 129: loss: 0.668894, loss_kl: 4.626873, loss_recon: 0.614183, loss_pred: 0.050085
iteration 130: loss: 0.723903, loss_kl: 4.426534, loss_recon: 0.618675, loss_pred: 0.100802
 13%|███▉                          | 26/200 [24:19<2:39:24, 54.97s/it]iteration 131: loss: 0.684642, loss_kl: 4.408635, loss_recon: 0.615200, loss_pred: 0.065034
iteration 132: loss: 0.702672, loss_kl: 4.865569, loss_recon: 0.612042, loss_pred: 0.085764
iteration 133: loss: 0.666501, loss_kl: 4.438080, loss_recon: 0.614029, loss_pred: 0.048034
iteration 134: loss: 0.724680, loss_kl: 3.756461, loss_recon: 0.605014, loss_pred: 0.115910
iteration 135: loss: 0.684713, loss_kl: 5.460250, loss_recon: 0.626538, loss_pred: 0.052715
 14%|████                          | 27/200 [25:14<2:38:31, 54.98s/it]iteration 136: loss: 0.682119, loss_kl: 5.482912, loss_recon: 0.612826, loss_pred: 0.063810
iteration 137: loss: 0.681561, loss_kl: 6.052731, loss_recon: 0.613486, loss_pred: 0.062022
iteration 138: loss: 0.690401, loss_kl: 4.830542, loss_recon: 0.614993, loss_pred: 0.070578
iteration 139: loss: 0.683524, loss_kl: 5.198162, loss_recon: 0.603805, loss_pred: 0.074522
iteration 140: loss: 0.731971, loss_kl: 4.605520, loss_recon: 0.625082, loss_pred: 0.102283
 14%|████▏                         | 28/200 [26:09<2:37:31, 54.95s/it]iteration 141: loss: 0.665242, loss_kl: 4.416611, loss_recon: 0.612420, loss_pred: 0.048406
iteration 142: loss: 0.663274, loss_kl: 5.274752, loss_recon: 0.611373, loss_pred: 0.046626
iteration 143: loss: 0.681217, loss_kl: 4.406853, loss_recon: 0.618679, loss_pred: 0.058131
iteration 144: loss: 0.688432, loss_kl: 4.372646, loss_recon: 0.606532, loss_pred: 0.077528
iteration 145: loss: 0.721904, loss_kl: 5.880429, loss_recon: 0.621015, loss_pred: 0.095008
 14%|████▎                         | 29/200 [27:04<2:36:32, 54.92s/it]iteration 146: loss: 0.666486, loss_kl: 4.351733, loss_recon: 0.611581, loss_pred: 0.050553
iteration 147: loss: 0.685181, loss_kl: 4.915468, loss_recon: 0.609436, loss_pred: 0.070830
iteration 148: loss: 0.666114, loss_kl: 4.141715, loss_recon: 0.615055, loss_pred: 0.046917
iteration 149: loss: 0.668105, loss_kl: 4.468282, loss_recon: 0.615300, loss_pred: 0.048337
iteration 150: loss: 0.684756, loss_kl: 4.388747, loss_recon: 0.616402, loss_pred: 0.063965
 15%|████▌                         | 30/200 [27:59<2:35:41, 54.95s/it]iteration 151: loss: 0.686912, loss_kl: 4.188487, loss_recon: 0.618768, loss_pred: 0.063956
iteration 152: loss: 0.685958, loss_kl: 5.140994, loss_recon: 0.613096, loss_pred: 0.067721
iteration 153: loss: 0.660800, loss_kl: 5.015102, loss_recon: 0.610001, loss_pred: 0.045784
iteration 154: loss: 0.681800, loss_kl: 4.539691, loss_recon: 0.612441, loss_pred: 0.064819
iteration 155: loss: 0.654899, loss_kl: 4.448779, loss_recon: 0.610049, loss_pred: 0.040401
 16%|████▋                         | 31/200 [28:53<2:33:44, 54.59s/it]iteration 156: loss: 0.673654, loss_kl: 5.468568, loss_recon: 0.623220, loss_pred: 0.044965
iteration 157: loss: 0.687017, loss_kl: 4.141356, loss_recon: 0.619716, loss_pred: 0.063159
iteration 158: loss: 0.685508, loss_kl: 5.833362, loss_recon: 0.608274, loss_pred: 0.071400
iteration 159: loss: 0.782396, loss_kl: 5.199862, loss_recon: 0.611763, loss_pred: 0.165432
iteration 160: loss: 0.680621, loss_kl: 5.518843, loss_recon: 0.616958, loss_pred: 0.058144
 16%|████▊                         | 32/200 [29:48<2:33:16, 54.74s/it]iteration 161: loss: 0.822878, loss_kl: 5.328367, loss_recon: 0.614231, loss_pred: 0.203318
iteration 162: loss: 0.666394, loss_kl: 5.939326, loss_recon: 0.603002, loss_pred: 0.057453
iteration 163: loss: 0.803880, loss_kl: 5.673387, loss_recon: 0.615478, loss_pred: 0.182728
iteration 164: loss: 0.712833, loss_kl: 5.694787, loss_recon: 0.622897, loss_pred: 0.084241
iteration 165: loss: 0.794005, loss_kl: 5.048913, loss_recon: 0.615602, loss_pred: 0.173354
 16%|████▉                         | 33/200 [30:41<2:31:17, 54.36s/it]iteration 166: loss: 0.723093, loss_kl: 4.226518, loss_recon: 0.608340, loss_pred: 0.110527
iteration 167: loss: 0.735395, loss_kl: 4.705167, loss_recon: 0.612342, loss_pred: 0.118348
iteration 168: loss: 0.721492, loss_kl: 5.517938, loss_recon: 0.619331, loss_pred: 0.096643
iteration 169: loss: 0.674723, loss_kl: 4.487109, loss_recon: 0.616526, loss_pred: 0.053710
iteration 170: loss: 0.707597, loss_kl: 4.295905, loss_recon: 0.613644, loss_pred: 0.089657
 17%|█████                         | 34/200 [31:35<2:30:08, 54.27s/it]iteration 171: loss: 0.672745, loss_kl: 4.926884, loss_recon: 0.612239, loss_pred: 0.055579
iteration 172: loss: 0.679846, loss_kl: 4.588570, loss_recon: 0.615144, loss_pred: 0.060114
iteration 173: loss: 0.691438, loss_kl: 4.277142, loss_recon: 0.613826, loss_pred: 0.073335
iteration 174: loss: 0.682449, loss_kl: 4.705746, loss_recon: 0.621009, loss_pred: 0.056735
iteration 175: loss: 0.726777, loss_kl: 4.405799, loss_recon: 0.604105, loss_pred: 0.118266
 18%|█████▎                        | 35/200 [32:30<2:29:29, 54.36s/it]iteration 176: loss: 0.662082, loss_kl: 4.641253, loss_recon: 0.603335, loss_pred: 0.054106
iteration 177: loss: 0.707292, loss_kl: 4.667094, loss_recon: 0.616605, loss_pred: 0.086020
iteration 178: loss: 0.672223, loss_kl: 4.472654, loss_recon: 0.616845, loss_pred: 0.050905
iteration 179: loss: 0.679752, loss_kl: 4.633712, loss_recon: 0.609661, loss_pred: 0.065457
iteration 180: loss: 0.707880, loss_kl: 4.411550, loss_recon: 0.626666, loss_pred: 0.076803
 18%|█████▍                        | 36/200 [33:27<2:30:37, 55.11s/it]iteration 181: loss: 0.662302, loss_kl: 4.452255, loss_recon: 0.614762, loss_pred: 0.043087
iteration 182: loss: 0.687337, loss_kl: 4.203447, loss_recon: 0.618276, loss_pred: 0.064858
iteration 183: loss: 0.661505, loss_kl: 4.541471, loss_recon: 0.610089, loss_pred: 0.046874
iteration 184: loss: 0.669731, loss_kl: 3.506094, loss_recon: 0.614709, loss_pred: 0.051516
iteration 185: loss: 0.669423, loss_kl: 4.300081, loss_recon: 0.619644, loss_pred: 0.045480
 18%|█████▌                        | 37/200 [34:21<2:29:11, 54.92s/it]iteration 186: loss: 0.674666, loss_kl: 4.167338, loss_recon: 0.614052, loss_pred: 0.056447
iteration 187: loss: 0.674491, loss_kl: 4.460324, loss_recon: 0.617438, loss_pred: 0.052592
iteration 188: loss: 0.659393, loss_kl: 4.079794, loss_recon: 0.615177, loss_pred: 0.040136
iteration 189: loss: 0.659641, loss_kl: 3.737802, loss_recon: 0.612512, loss_pred: 0.043391
iteration 190: loss: 0.676642, loss_kl: 4.675515, loss_recon: 0.612290, loss_pred: 0.059676
 19%|█████▋                        | 38/200 [35:16<2:28:08, 54.86s/it]iteration 191: loss: 0.680038, loss_kl: 4.107752, loss_recon: 0.616174, loss_pred: 0.059756
iteration 192: loss: 0.654586, loss_kl: 4.673884, loss_recon: 0.606304, loss_pred: 0.043608
iteration 193: loss: 0.674129, loss_kl: 4.460784, loss_recon: 0.619280, loss_pred: 0.050388
iteration 194: loss: 0.665867, loss_kl: 5.296921, loss_recon: 0.616208, loss_pred: 0.044362
iteration 195: loss: 0.721300, loss_kl: 4.850949, loss_recon: 0.616598, loss_pred: 0.099851
 20%|█████▊                        | 39/200 [36:11<2:27:04, 54.81s/it]slurmstepd: error: *** JOB 4543384 ON nova21-gpu-6 CANCELLED AT 2023-06-24T20:21:14 ***
