/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.001, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
10 iterations per epoch. 2000 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 73.342346, loss_kl: 19.409136, loss_recon: 0.693146, loss_pred: 3.833698
iteration 2: loss: 72.324059, loss_kl: 102.218948, loss_recon: 0.691890, loss_pred: 2.112856
iteration 3: loss: 72.650925, loss_kl: 82.944855, loss_recon: 0.687486, loss_pred: 3.072834
iteration 4: loss: 72.576340, loss_kl: 69.832726, loss_recon: 0.687262, loss_pred: 3.151816
iteration 5: loss: 71.885727, loss_kl: 67.427330, loss_recon: 0.683496, loss_pred: 2.861882
iteration 6: loss: 72.364098, loss_kl: 75.180847, loss_recon: 0.690523, loss_pred: 2.559963
iteration 7: loss: 71.111504, loss_kl: 68.541557, loss_recon: 0.686763, loss_pred: 1.749804
iteration 8: loss: 71.817612, loss_kl: 50.452690, loss_recon: 0.693178, loss_pred: 1.995308
iteration 9: loss: 70.686485, loss_kl: 41.735931, loss_recon: 0.681826, loss_pred: 2.086561
iteration 10: loss: 69.670525, loss_kl: 59.652466, loss_recon: 0.668796, loss_pred: 2.194391
  0%|▏                              | 1/200 [01:05<3:36:43, 65.34s/it]iteration 11: loss: 67.446571, loss_kl: 75.286560, loss_recon: 0.653115, loss_pred: 1.382214
iteration 12: loss: 66.929863, loss_kl: 76.273361, loss_recon: 0.651527, loss_pred: 1.014475
iteration 13: loss: 66.733688, loss_kl: 64.996887, loss_recon: 0.647386, loss_pred: 1.345082
iteration 14: loss: 65.069084, loss_kl: 40.099274, loss_recon: 0.630770, loss_pred: 1.591108
iteration 15: loss: 64.303360, loss_kl: 42.699165, loss_recon: 0.628161, loss_pred: 1.060234
iteration 16: loss: 63.716743, loss_kl: 51.247955, loss_recon: 0.623603, loss_pred: 0.844011
iteration 17: loss: 63.368523, loss_kl: 52.569405, loss_recon: 0.618117, loss_pred: 1.031101
iteration 18: loss: 64.297333, loss_kl: 43.848930, loss_recon: 0.629913, loss_pred: 0.867568
iteration 19: loss: 64.394112, loss_kl: 34.489296, loss_recon: 0.632815, loss_pred: 0.767763
iteration 20: loss: 62.471207, loss_kl: 28.101742, loss_recon: 0.607387, loss_pred: 1.451538
  1%|▎                              | 2/200 [01:54<3:03:47, 55.69s/it]iteration 21: loss: 62.852516, loss_kl: 37.489609, loss_recon: 0.618448, loss_pred: 0.632840
iteration 22: loss: 64.192375, loss_kl: 42.204098, loss_recon: 0.630747, loss_pred: 0.695616
iteration 23: loss: 61.513538, loss_kl: 40.491096, loss_recon: 0.606098, loss_pred: 0.498803
iteration 24: loss: 62.899769, loss_kl: 36.533882, loss_recon: 0.619167, loss_pred: 0.617752
iteration 25: loss: 62.885593, loss_kl: 33.848991, loss_recon: 0.620316, loss_pred: 0.515485
iteration 26: loss: 62.554642, loss_kl: 33.207577, loss_recon: 0.615395, loss_pred: 0.683044
iteration 27: loss: 63.952637, loss_kl: 33.247730, loss_recon: 0.631957, loss_pred: 0.424483
iteration 28: loss: 62.711182, loss_kl: 38.164272, loss_recon: 0.619479, loss_pred: 0.381675
iteration 29: loss: 60.921516, loss_kl: 39.452518, loss_recon: 0.601316, loss_pred: 0.395419
iteration 30: loss: 62.302647, loss_kl: 40.018608, loss_recon: 0.613720, loss_pred: 0.530449
  2%|▍                              | 3/200 [02:43<2:52:38, 52.58s/it]iteration 31: loss: 62.606110, loss_kl: 39.199875, loss_recon: 0.618167, loss_pred: 0.397375
iteration 32: loss: 62.431503, loss_kl: 36.685822, loss_recon: 0.618214, loss_pred: 0.243219
iteration 33: loss: 61.898193, loss_kl: 37.115211, loss_recon: 0.612624, loss_pred: 0.264602
iteration 34: loss: 61.126457, loss_kl: 32.631882, loss_recon: 0.605245, loss_pred: 0.275685
iteration 35: loss: 62.444813, loss_kl: 38.208748, loss_recon: 0.618741, loss_pred: 0.188601
iteration 36: loss: 62.130878, loss_kl: 41.178238, loss_recon: 0.615128, loss_pred: 0.206280
iteration 37: loss: 62.302540, loss_kl: 34.453281, loss_recon: 0.617241, loss_pred: 0.233905
iteration 38: loss: 61.272018, loss_kl: 32.409225, loss_recon: 0.605912, loss_pred: 0.356765
iteration 39: loss: 62.494362, loss_kl: 30.180157, loss_recon: 0.620538, loss_pred: 0.138714
iteration 40: loss: 64.889252, loss_kl: 25.364904, loss_recon: 0.645019, loss_pred: 0.133711
  2%|▌                              | 4/200 [03:33<2:48:22, 51.54s/it]iteration 41: loss: 61.917171, loss_kl: 30.360344, loss_recon: 0.613939, loss_pred: 0.219640
iteration 42: loss: 60.211098, loss_kl: 35.551567, loss_recon: 0.596591, loss_pred: 0.196442
iteration 43: loss: 62.512581, loss_kl: 36.832382, loss_recon: 0.618191, loss_pred: 0.325203
iteration 44: loss: 62.246281, loss_kl: 34.150021, loss_recon: 0.616942, loss_pred: 0.210613
iteration 45: loss: 62.601055, loss_kl: 31.632710, loss_recon: 0.620156, loss_pred: 0.269177
iteration 46: loss: 62.272728, loss_kl: 30.210697, loss_recon: 0.616325, loss_pred: 0.338070
iteration 47: loss: 62.196503, loss_kl: 29.359049, loss_recon: 0.617074, loss_pred: 0.195547
iteration 48: loss: 61.970238, loss_kl: 31.823477, loss_recon: 0.613446, loss_pred: 0.307361
iteration 49: loss: 62.006611, loss_kl: 32.694992, loss_recon: 0.612471, loss_pred: 0.432544
iteration 50: loss: 63.356407, loss_kl: 28.242516, loss_recon: 0.628705, loss_pred: 0.203528
  2%|▊                              | 5/200 [04:22<2:45:19, 50.87s/it]iteration 51: loss: 62.050014, loss_kl: 28.483204, loss_recon: 0.614369, loss_pred: 0.328277
iteration 52: loss: 61.170765, loss_kl: 27.518738, loss_recon: 0.606141, loss_pred: 0.281489
iteration 53: loss: 60.386742, loss_kl: 31.059982, loss_recon: 0.597379, loss_pred: 0.338258
iteration 54: loss: 62.989582, loss_kl: 29.546909, loss_recon: 0.623639, loss_pred: 0.330256
iteration 55: loss: 61.765610, loss_kl: 27.648964, loss_recon: 0.611967, loss_pred: 0.292413
iteration 56: loss: 61.803478, loss_kl: 30.007412, loss_recon: 0.612785, loss_pred: 0.224940
iteration 57: loss: 61.882736, loss_kl: 26.147072, loss_recon: 0.614438, loss_pred: 0.177482
iteration 58: loss: 62.401955, loss_kl: 25.624588, loss_recon: 0.618983, loss_pred: 0.247373
iteration 59: loss: 61.772739, loss_kl: 25.882683, loss_recon: 0.612414, loss_pred: 0.272495
iteration 60: loss: 61.191704, loss_kl: 27.593782, loss_recon: 0.605492, loss_pred: 0.366598
  3%|▉                              | 6/200 [05:13<2:43:46, 50.65s/it]iteration 61: loss: 61.363747, loss_kl: 28.441128, loss_recon: 0.607424, loss_pred: 0.336892
iteration 62: loss: 62.798721, loss_kl: 30.235374, loss_recon: 0.618593, loss_pred: 0.637107
iteration 63: loss: 62.195057, loss_kl: 27.582336, loss_recon: 0.617270, loss_pred: 0.192254
iteration 64: loss: 61.266342, loss_kl: 26.226688, loss_recon: 0.607700, loss_pred: 0.234099
iteration 65: loss: 61.640793, loss_kl: 23.474741, loss_recon: 0.608290, loss_pred: 0.577017
iteration 66: loss: 61.839573, loss_kl: 24.555017, loss_recon: 0.613996, loss_pred: 0.194403
iteration 67: loss: 61.044044, loss_kl: 25.433678, loss_recon: 0.606506, loss_pred: 0.139104
iteration 68: loss: 61.809185, loss_kl: 25.577648, loss_recon: 0.613468, loss_pred: 0.206657
iteration 69: loss: 61.414249, loss_kl: 29.038675, loss_recon: 0.608485, loss_pred: 0.275395
iteration 70: loss: 63.495312, loss_kl: 28.518145, loss_recon: 0.629122, loss_pred: 0.297975
  4%|█                              | 7/200 [06:03<2:43:01, 50.68s/it]iteration 71: loss: 61.875755, loss_kl: 26.345839, loss_recon: 0.614046, loss_pred: 0.207741
iteration 72: loss: 61.328354, loss_kl: 23.123581, loss_recon: 0.607808, loss_pred: 0.316326
iteration 73: loss: 61.964485, loss_kl: 23.620476, loss_recon: 0.613248, loss_pred: 0.403491
iteration 74: loss: 61.754990, loss_kl: 23.612524, loss_recon: 0.612658, loss_pred: 0.253056
iteration 75: loss: 60.832603, loss_kl: 27.491142, loss_recon: 0.604229, loss_pred: 0.134831
iteration 76: loss: 62.350792, loss_kl: 27.448189, loss_recon: 0.618094, loss_pred: 0.266866
iteration 77: loss: 61.064293, loss_kl: 26.025711, loss_recon: 0.605860, loss_pred: 0.218015
iteration 78: loss: 62.952229, loss_kl: 27.192303, loss_recon: 0.624574, loss_pred: 0.222885
iteration 79: loss: 62.185226, loss_kl: 26.429913, loss_recon: 0.616376, loss_pred: 0.283325
iteration 80: loss: 63.244328, loss_kl: 23.064318, loss_recon: 0.626461, loss_pred: 0.367599
  4%|█▏                             | 8/200 [06:53<2:41:19, 50.42s/it]iteration 81: loss: 63.568188, loss_kl: 24.320015, loss_recon: 0.630601, loss_pred: 0.264887
iteration 82: loss: 61.831764, loss_kl: 23.968546, loss_recon: 0.611650, loss_pred: 0.427061
iteration 83: loss: 62.388271, loss_kl: 25.748371, loss_recon: 0.618895, loss_pred: 0.241308
iteration 84: loss: 60.895515, loss_kl: 25.163090, loss_recon: 0.604107, loss_pred: 0.233204
iteration 85: loss: 61.262337, loss_kl: 28.215687, loss_recon: 0.608190, loss_pred: 0.161186
iteration 86: loss: 62.110279, loss_kl: 25.693968, loss_recon: 0.616321, loss_pred: 0.221225
iteration 87: loss: 62.182690, loss_kl: 25.522039, loss_recon: 0.617552, loss_pred: 0.172323
iteration 88: loss: 61.494324, loss_kl: 23.187214, loss_recon: 0.611089, loss_pred: 0.153523
iteration 89: loss: 61.645580, loss_kl: 22.638508, loss_recon: 0.609944, loss_pred: 0.424840
iteration 90: loss: 60.759304, loss_kl: 27.585932, loss_recon: 0.601419, loss_pred: 0.341501
  4%|█▍                             | 9/200 [07:43<2:39:38, 50.15s/it]iteration 91: loss: 61.915157, loss_kl: 25.790154, loss_recon: 0.613523, loss_pred: 0.304950
iteration 92: loss: 62.618572, loss_kl: 28.024736, loss_recon: 0.619372, loss_pred: 0.401098
iteration 93: loss: 61.841946, loss_kl: 23.213543, loss_recon: 0.613275, loss_pred: 0.282281
iteration 94: loss: 62.165768, loss_kl: 22.094856, loss_recon: 0.617477, loss_pred: 0.197144
iteration 95: loss: 62.315819, loss_kl: 21.993168, loss_recon: 0.618680, loss_pred: 0.227853
iteration 96: loss: 61.466324, loss_kl: 17.796242, loss_recon: 0.611047, loss_pred: 0.183672
iteration 97: loss: 60.921124, loss_kl: 21.592262, loss_recon: 0.603098, loss_pred: 0.395444
iteration 98: loss: 61.006512, loss_kl: 24.476368, loss_recon: 0.605180, loss_pred: 0.243747
iteration 99: loss: 61.485317, loss_kl: 27.416418, loss_recon: 0.609618, loss_pred: 0.249379
iteration 100: loss: 61.486996, loss_kl: 26.368612, loss_recon: 0.607961, loss_pred: 0.427184
  5%|█▌                            | 10/200 [08:32<2:38:22, 50.01s/it]iteration 101: loss: 61.119831, loss_kl: 25.435957, loss_recon: 0.605348, loss_pred: 0.330699
iteration 102: loss: 62.539787, loss_kl: 22.188639, loss_recon: 0.620439, loss_pred: 0.274034
iteration 103: loss: 60.594440, loss_kl: 22.695261, loss_recon: 0.601447, loss_pred: 0.222760
iteration 104: loss: 61.403290, loss_kl: 21.926168, loss_recon: 0.608216, loss_pred: 0.362434
iteration 105: loss: 62.253635, loss_kl: 20.402618, loss_recon: 0.617919, loss_pred: 0.257713
iteration 106: loss: 62.326653, loss_kl: 22.836632, loss_recon: 0.617956, loss_pred: 0.302704
iteration 107: loss: 62.012161, loss_kl: 24.989101, loss_recon: 0.616441, loss_pred: 0.118204
iteration 108: loss: 61.775822, loss_kl: 26.038750, loss_recon: 0.613738, loss_pred: 0.141652
iteration 109: loss: 62.335541, loss_kl: 25.025318, loss_recon: 0.617626, loss_pred: 0.322722
iteration 110: loss: 62.344402, loss_kl: 23.486553, loss_recon: 0.618875, loss_pred: 0.222062
  6%|█▋                            | 11/200 [09:21<2:36:35, 49.71s/it]iteration 111: loss: 61.895485, loss_kl: 22.488754, loss_recon: 0.615055, loss_pred: 0.165066
iteration 112: loss: 60.972946, loss_kl: 20.026428, loss_recon: 0.606052, loss_pred: 0.167505
iteration 113: loss: 61.597805, loss_kl: 21.092346, loss_recon: 0.611960, loss_pred: 0.190854
iteration 114: loss: 61.770023, loss_kl: 21.493088, loss_recon: 0.613028, loss_pred: 0.252305
iteration 115: loss: 62.409737, loss_kl: 21.007820, loss_recon: 0.620698, loss_pred: 0.129860
iteration 116: loss: 61.081211, loss_kl: 24.067163, loss_recon: 0.605902, loss_pred: 0.250373
iteration 117: loss: 62.724064, loss_kl: 21.253342, loss_recon: 0.623474, loss_pred: 0.164130
iteration 118: loss: 60.761055, loss_kl: 24.106403, loss_recon: 0.603754, loss_pred: 0.144635
iteration 119: loss: 61.034489, loss_kl: 20.897896, loss_recon: 0.606806, loss_pred: 0.144940
iteration 120: loss: 61.818073, loss_kl: 20.487202, loss_recon: 0.607108, loss_pred: 0.902388
  6%|█▊                            | 12/200 [10:12<2:36:08, 49.83s/it]iteration 121: loss: 61.564072, loss_kl: 21.709415, loss_recon: 0.611676, loss_pred: 0.179406
iteration 122: loss: 61.605568, loss_kl: 21.699036, loss_recon: 0.612209, loss_pred: 0.167665
iteration 123: loss: 61.689083, loss_kl: 21.479317, loss_recon: 0.613541, loss_pred: 0.120206
iteration 124: loss: 61.246773, loss_kl: 21.981060, loss_recon: 0.608498, loss_pred: 0.177130
iteration 125: loss: 60.152393, loss_kl: 19.372326, loss_recon: 0.595711, loss_pred: 0.387561
iteration 126: loss: 61.827366, loss_kl: 18.205494, loss_recon: 0.614758, loss_pred: 0.169524
iteration 127: loss: 61.539066, loss_kl: 20.474247, loss_recon: 0.611921, loss_pred: 0.142212
iteration 128: loss: 63.225304, loss_kl: 20.361746, loss_recon: 0.628401, loss_pred: 0.181607
iteration 129: loss: 60.652679, loss_kl: 21.144083, loss_recon: 0.603212, loss_pred: 0.119992
iteration 130: loss: 61.735935, loss_kl: 19.384466, loss_recon: 0.614593, loss_pred: 0.082751
  6%|█▉                            | 13/200 [11:01<2:35:22, 49.86s/it]iteration 131: loss: 62.830517, loss_kl: 18.599211, loss_recon: 0.617939, loss_pred: 0.114132
iteration 132: loss: 62.941784, loss_kl: 21.021883, loss_recon: 0.617592, loss_pred: 0.139911
iteration 133: loss: 61.455021, loss_kl: 17.028439, loss_recon: 0.603848, loss_pred: 0.225607
iteration 134: loss: 61.857239, loss_kl: 19.867752, loss_recon: 0.607332, loss_pred: 0.138620
iteration 135: loss: 63.357670, loss_kl: 18.302618, loss_recon: 0.622728, loss_pred: 0.177103
iteration 136: loss: 62.199688, loss_kl: 17.519863, loss_recon: 0.611150, loss_pred: 0.215659
iteration 137: loss: 62.349548, loss_kl: 17.638287, loss_recon: 0.611642, loss_pred: 0.310511
iteration 138: loss: 62.242592, loss_kl: 17.335197, loss_recon: 0.611690, loss_pred: 0.213811
iteration 139: loss: 61.931725, loss_kl: 17.611271, loss_recon: 0.607773, loss_pred: 0.280947
iteration 140: loss: 62.618141, loss_kl: 15.594691, loss_recon: 0.611042, loss_pred: 0.740421
  7%|██                            | 14/200 [11:53<2:36:07, 50.36s/it]iteration 141: loss: 63.711983, loss_kl: 19.648088, loss_recon: 0.617746, loss_pred: 0.184726
iteration 142: loss: 62.907379, loss_kl: 19.654596, loss_recon: 0.609652, loss_pred: 0.188947
iteration 143: loss: 63.496368, loss_kl: 17.121811, loss_recon: 0.618067, loss_pred: 0.162414
iteration 144: loss: 64.416214, loss_kl: 14.905769, loss_recon: 0.629078, loss_pred: 0.178818
iteration 145: loss: 61.979717, loss_kl: 14.203856, loss_recon: 0.599775, loss_pred: 0.735278
iteration 146: loss: 62.245358, loss_kl: 16.747036, loss_recon: 0.603572, loss_pred: 0.394301
iteration 147: loss: 61.863560, loss_kl: 16.353987, loss_recon: 0.601828, loss_pred: 0.222031
iteration 148: loss: 61.695225, loss_kl: 16.982048, loss_recon: 0.599637, loss_pred: 0.216760
iteration 149: loss: 63.923592, loss_kl: 17.271317, loss_recon: 0.621104, loss_pred: 0.272617
iteration 150: loss: 65.304260, loss_kl: 13.599248, loss_recon: 0.637442, loss_pred: 0.347051
  8%|██▎                           | 15/200 [12:42<2:34:09, 50.00s/it]iteration 151: loss: 63.985012, loss_kl: 13.761978, loss_recon: 0.619739, loss_pred: 0.238600
iteration 152: loss: 63.183147, loss_kl: 13.581473, loss_recon: 0.610552, loss_pred: 0.378649
iteration 153: loss: 63.560383, loss_kl: 13.025408, loss_recon: 0.614559, loss_pred: 0.426761
iteration 154: loss: 64.164711, loss_kl: 14.039501, loss_recon: 0.621295, loss_pred: 0.226877
iteration 155: loss: 62.597095, loss_kl: 14.560604, loss_recon: 0.605901, loss_pred: 0.131601
iteration 156: loss: 61.748749, loss_kl: 13.713820, loss_recon: 0.597385, loss_pred: 0.243958
iteration 157: loss: 63.257565, loss_kl: 13.698017, loss_recon: 0.612334, loss_pred: 0.259906
iteration 158: loss: 63.295696, loss_kl: 11.467305, loss_recon: 0.615442, loss_pred: 0.274523
iteration 159: loss: 62.285408, loss_kl: 11.999857, loss_recon: 0.600893, loss_pred: 0.650543
iteration 160: loss: 66.370232, loss_kl: 15.415083, loss_recon: 0.637787, loss_pred: 0.606051
  8%|██▍                           | 16/200 [13:32<2:33:02, 49.91s/it]iteration 161: loss: 64.191521, loss_kl: 12.608430, loss_recon: 0.618410, loss_pred: 0.227231
iteration 162: loss: 63.124802, loss_kl: 12.108989, loss_recon: 0.606868, loss_pred: 0.398880
iteration 163: loss: 64.213928, loss_kl: 12.382229, loss_recon: 0.619283, loss_pred: 0.200425
iteration 164: loss: 63.166973, loss_kl: 13.477914, loss_recon: 0.606181, loss_pred: 0.279231
iteration 165: loss: 63.994846, loss_kl: 13.083738, loss_recon: 0.616012, loss_pred: 0.190348
iteration 166: loss: 63.084465, loss_kl: 9.884971, loss_recon: 0.612108, loss_pred: 0.209053
iteration 167: loss: 63.682564, loss_kl: 11.431870, loss_recon: 0.614857, loss_pred: 0.271744
iteration 168: loss: 63.743225, loss_kl: 12.903335, loss_recon: 0.613943, loss_pred: 0.176031
iteration 169: loss: 62.088692, loss_kl: 10.248332, loss_recon: 0.601502, loss_pred: 0.212693
iteration 170: loss: 63.754681, loss_kl: 11.037458, loss_recon: 0.617745, loss_pred: 0.121495
  8%|██▌                           | 17/200 [14:21<2:31:56, 49.82s/it]iteration 171: loss: 62.705585, loss_kl: 11.118256, loss_recon: 0.601440, loss_pred: 0.248972
iteration 172: loss: 64.412521, loss_kl: 10.547975, loss_recon: 0.619629, loss_pred: 0.255613
iteration 173: loss: 62.962261, loss_kl: 10.650262, loss_recon: 0.604057, loss_pred: 0.341264
iteration 174: loss: 64.177223, loss_kl: 10.452990, loss_recon: 0.616865, loss_pred: 0.316513
iteration 175: loss: 63.564911, loss_kl: 9.119408, loss_recon: 0.614950, loss_pred: 0.173089
iteration 176: loss: 64.888748, loss_kl: 9.634027, loss_recon: 0.626610, loss_pred: 0.223900
iteration 177: loss: 63.092808, loss_kl: 10.807538, loss_recon: 0.604840, loss_pred: 0.360794
iteration 178: loss: 62.186768, loss_kl: 9.322554, loss_recon: 0.599093, loss_pred: 0.338419
iteration 179: loss: 63.762650, loss_kl: 10.547425, loss_recon: 0.613388, loss_pred: 0.229987
iteration 180: loss: 65.655037, loss_kl: 8.356702, loss_recon: 0.635657, loss_pred: 0.351128
  9%|██▋                           | 18/200 [15:10<2:30:24, 49.59s/it]iteration 181: loss: 63.871098, loss_kl: 7.885872, loss_recon: 0.616898, loss_pred: 0.228754
iteration 182: loss: 63.310684, loss_kl: 8.904826, loss_recon: 0.607572, loss_pred: 0.348683
iteration 183: loss: 64.169830, loss_kl: 9.479639, loss_recon: 0.616326, loss_pred: 0.190101
iteration 184: loss: 63.863594, loss_kl: 8.750189, loss_recon: 0.615780, loss_pred: 0.119069
iteration 185: loss: 63.223782, loss_kl: 7.800148, loss_recon: 0.610807, loss_pred: 0.211717
iteration 186: loss: 64.295746, loss_kl: 8.106703, loss_recon: 0.621142, loss_pred: 0.174284
iteration 187: loss: 62.506607, loss_kl: 9.644598, loss_recon: 0.599285, loss_pred: 0.190108
iteration 188: loss: 62.289707, loss_kl: 8.106457, loss_recon: 0.601243, loss_pred: 0.158265
iteration 189: loss: 63.909710, loss_kl: 8.002692, loss_recon: 0.617496, loss_pred: 0.178676
iteration 190: loss: 61.845055, loss_kl: 5.577605, loss_recon: 0.601940, loss_pred: 0.270003
 10%|██▊                           | 19/200 [16:00<2:29:42, 49.63s/it]iteration 191: loss: 63.933670, loss_kl: 8.416540, loss_recon: 0.613427, loss_pred: 0.173707
iteration 192: loss: 63.807457, loss_kl: 8.683414, loss_recon: 0.610837, loss_pred: 0.229840
iteration 193: loss: 62.638252, loss_kl: 6.561456, loss_recon: 0.605601, loss_pred: 0.193690
iteration 194: loss: 64.824257, loss_kl: 7.909874, loss_recon: 0.623662, loss_pred: 0.186316
iteration 195: loss: 63.147121, loss_kl: 7.782167, loss_recon: 0.606757, loss_pred: 0.236388
iteration 196: loss: 63.685966, loss_kl: 7.710139, loss_recon: 0.612575, loss_pred: 0.214122
iteration 197: loss: 63.655285, loss_kl: 9.229366, loss_recon: 0.607205, loss_pred: 0.284147
iteration 198: loss: 62.629677, loss_kl: 6.783210, loss_recon: 0.604154, loss_pred: 0.266125
iteration 199: loss: 63.047337, loss_kl: 8.262566, loss_recon: 0.602676, loss_pred: 0.406697
iteration 200: loss: 63.977119, loss_kl: 8.744553, loss_recon: 0.612398, loss_pred: 0.225912
 10%|███                           | 20/200 [16:51<2:29:30, 49.84s/it]iteration 201: loss: 63.355129, loss_kl: 7.507990, loss_recon: 0.605988, loss_pred: 0.302735
iteration 202: loss: 63.854359, loss_kl: 6.936770, loss_recon: 0.613818, loss_pred: 0.205624
iteration 203: loss: 62.536156, loss_kl: 5.343219, loss_recon: 0.604238, loss_pred: 0.366175
iteration 204: loss: 62.781773, loss_kl: 6.101035, loss_recon: 0.604821, loss_pred: 0.305872
iteration 205: loss: 63.853302, loss_kl: 8.222901, loss_recon: 0.609908, loss_pred: 0.175290
iteration 206: loss: 63.847263, loss_kl: 6.868738, loss_recon: 0.613840, loss_pred: 0.218535
iteration 207: loss: 63.095860, loss_kl: 6.210958, loss_recon: 0.609343, loss_pred: 0.131842
iteration 208: loss: 64.210396, loss_kl: 6.823612, loss_recon: 0.617519, loss_pred: 0.228577
iteration 209: loss: 62.852303, loss_kl: 5.795614, loss_recon: 0.606407, loss_pred: 0.317570
iteration 210: loss: 62.454521, loss_kl: 5.876039, loss_recon: 0.603924, loss_pred: 0.141841
 10%|███▏                          | 21/200 [17:40<2:28:01, 49.62s/it]iteration 211: loss: 63.741653, loss_kl: 5.965829, loss_recon: 0.613186, loss_pred: 0.237196
iteration 212: loss: 64.285774, loss_kl: 6.837706, loss_recon: 0.615882, loss_pred: 0.192217
iteration 213: loss: 62.305599, loss_kl: 6.001312, loss_recon: 0.596109, loss_pred: 0.495865
iteration 214: loss: 63.664364, loss_kl: 6.520406, loss_recon: 0.610037, loss_pred: 0.271584
iteration 215: loss: 63.650234, loss_kl: 5.862720, loss_recon: 0.613358, loss_pred: 0.166293
iteration 216: loss: 62.806976, loss_kl: 6.264321, loss_recon: 0.602483, loss_pred: 0.263399
iteration 217: loss: 63.143211, loss_kl: 5.210329, loss_recon: 0.610260, loss_pred: 0.208114
iteration 218: loss: 64.276718, loss_kl: 6.145057, loss_recon: 0.618190, loss_pred: 0.206150
iteration 219: loss: 62.972942, loss_kl: 5.845824, loss_recon: 0.605973, loss_pred: 0.233689
iteration 220: loss: 66.645851, loss_kl: 5.060789, loss_recon: 0.645942, loss_pred: 0.197378
 11%|███▎                          | 22/200 [18:29<2:27:23, 49.68s/it]iteration 221: loss: 63.260921, loss_kl: 6.595842, loss_recon: 0.604219, loss_pred: 0.161065
iteration 222: loss: 64.899689, loss_kl: 5.112106, loss_recon: 0.626788, loss_pred: 0.145357
iteration 223: loss: 62.409149, loss_kl: 5.079654, loss_recon: 0.600806, loss_pred: 0.266207
iteration 224: loss: 64.266533, loss_kl: 5.358666, loss_recon: 0.619183, loss_pred: 0.172627
iteration 225: loss: 64.179764, loss_kl: 5.485821, loss_recon: 0.616948, loss_pred: 0.257754
iteration 226: loss: 62.846607, loss_kl: 4.762468, loss_recon: 0.606010, loss_pred: 0.312045
iteration 227: loss: 62.919186, loss_kl: 5.619564, loss_recon: 0.603653, loss_pred: 0.272317
iteration 228: loss: 62.984570, loss_kl: 5.085998, loss_recon: 0.606803, loss_pred: 0.239312
iteration 229: loss: 63.414730, loss_kl: 4.330985, loss_recon: 0.614832, loss_pred: 0.173164
iteration 230: loss: 65.062500, loss_kl: 4.790370, loss_recon: 0.627720, loss_pred: 0.345589
 12%|███▍                          | 23/200 [19:19<2:26:50, 49.78s/it]iteration 231: loss: 62.620026, loss_kl: 4.526765, loss_recon: 0.604114, loss_pred: 0.191505
iteration 232: loss: 63.248512, loss_kl: 4.563527, loss_recon: 0.609297, loss_pred: 0.285334
iteration 233: loss: 64.624390, loss_kl: 5.017391, loss_recon: 0.622445, loss_pred: 0.144178
iteration 234: loss: 63.317989, loss_kl: 5.491319, loss_recon: 0.606566, loss_pred: 0.214503
iteration 235: loss: 63.303959, loss_kl: 4.767925, loss_recon: 0.609216, loss_pred: 0.257749
iteration 236: loss: 63.540924, loss_kl: 4.819678, loss_recon: 0.611879, loss_pred: 0.205424
iteration 237: loss: 63.612251, loss_kl: 4.445561, loss_recon: 0.613908, loss_pred: 0.240550
iteration 238: loss: 63.664623, loss_kl: 4.592567, loss_recon: 0.613899, loss_pred: 0.228316
iteration 239: loss: 62.661751, loss_kl: 3.893613, loss_recon: 0.607792, loss_pred: 0.147591
iteration 240: loss: 62.999317, loss_kl: 5.281091, loss_recon: 0.603533, loss_pred: 0.292750
 12%|███▌                          | 24/200 [20:08<2:25:19, 49.54s/it]iteration 241: loss: 62.664425, loss_kl: 4.334970, loss_recon: 0.602604, loss_pred: 0.300693
iteration 242: loss: 64.090004, loss_kl: 3.180370, loss_recon: 0.623773, loss_pred: 0.169617
iteration 243: loss: 63.013943, loss_kl: 3.307810, loss_recon: 0.609202, loss_pred: 0.488816
iteration 244: loss: 63.575188, loss_kl: 5.715048, loss_recon: 0.606595, loss_pred: 0.142796
iteration 245: loss: 62.255070, loss_kl: 4.836239, loss_recon: 0.596324, loss_pred: 0.276163
iteration 246: loss: 64.048988, loss_kl: 4.050252, loss_recon: 0.618460, loss_pred: 0.237759
iteration 247: loss: 63.037910, loss_kl: 4.184830, loss_recon: 0.607543, loss_pred: 0.253141
iteration 248: loss: 63.307961, loss_kl: 4.344001, loss_recon: 0.607584, loss_pred: 0.441834
iteration 249: loss: 63.167400, loss_kl: 2.925133, loss_recon: 0.616007, loss_pred: 0.147390
iteration 250: loss: 62.463535, loss_kl: 3.394306, loss_recon: 0.606393, loss_pred: 0.177310
 12%|███▊                          | 25/200 [20:58<2:24:34, 49.57s/it]iteration 251: loss: 62.101669, loss_kl: 4.118578, loss_recon: 0.596202, loss_pred: 0.320020
iteration 252: loss: 62.877266, loss_kl: 2.772719, loss_recon: 0.612549, loss_pred: 0.167282
iteration 253: loss: 63.210224, loss_kl: 3.852222, loss_recon: 0.609003, loss_pred: 0.288306
iteration 254: loss: 63.924782, loss_kl: 4.121352, loss_recon: 0.615339, loss_pred: 0.227958
iteration 255: loss: 62.513592, loss_kl: 3.495757, loss_recon: 0.603640, loss_pred: 0.314970
iteration 256: loss: 65.016571, loss_kl: 3.942873, loss_recon: 0.624187, loss_pred: 0.528647
iteration 257: loss: 62.653370, loss_kl: 3.380743, loss_recon: 0.606411, loss_pred: 0.238047
iteration 258: loss: 63.631531, loss_kl: 3.320451, loss_recon: 0.614993, loss_pred: 0.389690
iteration 259: loss: 62.782120, loss_kl: 3.465855, loss_recon: 0.605826, loss_pred: 0.380594
iteration 260: loss: 63.627033, loss_kl: 1.969483, loss_recon: 0.623278, loss_pred: 0.265629
 13%|███▉                          | 26/200 [21:48<2:24:15, 49.74s/it]iteration 261: loss: 62.716824, loss_kl: 3.531029, loss_recon: 0.604060, loss_pred: 0.317936
iteration 262: loss: 62.977863, loss_kl: 3.087867, loss_recon: 0.610382, loss_pred: 0.196827
iteration 263: loss: 62.917992, loss_kl: 3.400329, loss_recon: 0.606363, loss_pred: 0.362516
iteration 264: loss: 63.607929, loss_kl: 3.195117, loss_recon: 0.614716, loss_pred: 0.332994
iteration 265: loss: 61.682270, loss_kl: 1.966622, loss_recon: 0.603287, loss_pred: 0.243622
iteration 266: loss: 62.979267, loss_kl: 3.156313, loss_recon: 0.607986, loss_pred: 0.399212
iteration 267: loss: 62.360786, loss_kl: 3.630187, loss_recon: 0.600386, loss_pred: 0.273282
iteration 268: loss: 63.324993, loss_kl: 3.369080, loss_recon: 0.612485, loss_pred: 0.174972
iteration 269: loss: 64.768745, loss_kl: 3.495780, loss_recon: 0.626520, loss_pred: 0.143739
iteration 270: loss: 64.406891, loss_kl: 2.659656, loss_recon: 0.627332, loss_pred: 0.172586
 14%|████                          | 27/200 [22:37<2:22:44, 49.51s/it]iteration 271: loss: 62.311062, loss_kl: 2.038580, loss_recon: 0.608738, loss_pred: 0.205943
iteration 272: loss: 63.067150, loss_kl: 3.651607, loss_recon: 0.606054, loss_pred: 0.256141
iteration 273: loss: 63.241539, loss_kl: 3.210589, loss_recon: 0.610187, loss_pred: 0.283651
iteration 274: loss: 64.331718, loss_kl: 3.088539, loss_recon: 0.623232, loss_pred: 0.143020
iteration 275: loss: 63.305943, loss_kl: 3.347329, loss_recon: 0.609930, loss_pred: 0.291194
iteration 276: loss: 62.971867, loss_kl: 3.105052, loss_recon: 0.608605, loss_pred: 0.235928
iteration 277: loss: 63.591835, loss_kl: 2.324556, loss_recon: 0.620027, loss_pred: 0.185071
iteration 278: loss: 62.756683, loss_kl: 2.943629, loss_recon: 0.606609, loss_pred: 0.317808
iteration 279: loss: 62.336918, loss_kl: 2.941429, loss_recon: 0.601933, loss_pred: 0.367030
iteration 280: loss: 62.963001, loss_kl: 2.148107, loss_recon: 0.615005, loss_pred: 0.165044
 14%|████▏                         | 28/200 [23:26<2:21:27, 49.34s/it]iteration 281: loss: 63.325550, loss_kl: 2.968146, loss_recon: 0.612519, loss_pred: 0.163307
iteration 282: loss: 62.665920, loss_kl: 3.074783, loss_recon: 0.605400, loss_pred: 0.146990
iteration 283: loss: 63.280186, loss_kl: 2.917109, loss_recon: 0.612488, loss_pred: 0.153964
iteration 284: loss: 62.446308, loss_kl: 3.100486, loss_recon: 0.603403, loss_pred: 0.110533
iteration 285: loss: 60.837948, loss_kl: 2.341326, loss_recon: 0.591824, loss_pred: 0.148711
iteration 286: loss: 63.237026, loss_kl: 2.799769, loss_recon: 0.612956, loss_pred: 0.139498
iteration 287: loss: 63.941284, loss_kl: 2.098197, loss_recon: 0.624219, loss_pred: 0.168969
iteration 288: loss: 63.270370, loss_kl: 2.548769, loss_recon: 0.613990, loss_pred: 0.231018
iteration 289: loss: 62.612354, loss_kl: 2.541391, loss_recon: 0.607760, loss_pred: 0.200708
iteration 290: loss: 63.662975, loss_kl: 2.348626, loss_recon: 0.619560, loss_pred: 0.195359
 14%|████▎                         | 29/200 [24:16<2:20:53, 49.44s/it]iteration 291: loss: 63.012074, loss_kl: 2.656253, loss_recon: 0.610105, loss_pred: 0.186836
iteration 292: loss: 63.181416, loss_kl: 2.643031, loss_recon: 0.612216, loss_pred: 0.154055
iteration 293: loss: 63.054070, loss_kl: 2.724179, loss_recon: 0.610691, loss_pred: 0.123812
iteration 294: loss: 62.665718, loss_kl: 2.629561, loss_recon: 0.606350, loss_pred: 0.234185
iteration 295: loss: 61.144642, loss_kl: 2.281917, loss_recon: 0.593376, loss_pred: 0.248067
iteration 296: loss: 63.144955, loss_kl: 2.280825, loss_recon: 0.613632, loss_pred: 0.223478
iteration 297: loss: 63.811317, loss_kl: 2.313889, loss_recon: 0.620271, loss_pred: 0.203352
iteration 298: loss: 63.408321, loss_kl: 2.284468, loss_recon: 0.616790, loss_pred: 0.168567
iteration 299: loss: 62.231667, loss_kl: 2.283692, loss_recon: 0.603420, loss_pred: 0.329405
iteration 300: loss: 65.166740, loss_kl: 2.266897, loss_recon: 0.634077, loss_pred: 0.210272
 15%|████▌                         | 30/200 [25:05<2:20:07, 49.46s/it]iteration 301: loss: 63.284363, loss_kl: 1.897786, loss_recon: 0.614468, loss_pred: 0.465863
iteration 302: loss: 63.149960, loss_kl: 2.399976, loss_recon: 0.612918, loss_pred: 0.123488
iteration 303: loss: 62.899014, loss_kl: 2.366585, loss_recon: 0.609547, loss_pred: 0.233768
iteration 304: loss: 63.453690, loss_kl: 1.943849, loss_recon: 0.618293, loss_pred: 0.219328
iteration 305: loss: 62.860806, loss_kl: 2.732958, loss_recon: 0.604435, loss_pred: 0.441915
iteration 306: loss: 62.554005, loss_kl: 1.879666, loss_recon: 0.609157, loss_pred: 0.279698
iteration 307: loss: 61.526329, loss_kl: 1.326273, loss_recon: 0.601949, loss_pred: 0.372833
iteration 308: loss: 63.828800, loss_kl: 1.860515, loss_recon: 0.622836, loss_pred: 0.200440
iteration 309: loss: 62.426727, loss_kl: 2.096624, loss_recon: 0.605336, loss_pred: 0.377677
iteration 310: loss: 61.256542, loss_kl: 2.039235, loss_recon: 0.587353, loss_pred: 1.047294
 16%|████▋                         | 31/200 [25:55<2:19:38, 49.57s/it]iteration 311: loss: 64.439201, loss_kl: 2.240612, loss_recon: 0.620106, loss_pred: 0.720400
iteration 312: loss: 62.931122, loss_kl: 1.778220, loss_recon: 0.608632, loss_pred: 0.712198
iteration 313: loss: 62.720978, loss_kl: 1.895108, loss_recon: 0.609965, loss_pred: 0.279691
iteration 314: loss: 63.703064, loss_kl: 2.104084, loss_recon: 0.617111, loss_pred: 0.387855
iteration 315: loss: 63.200569, loss_kl: 2.101248, loss_recon: 0.610774, loss_pred: 0.521143
iteration 316: loss: 62.697384, loss_kl: 1.453908, loss_recon: 0.614039, loss_pred: 0.185003
iteration 317: loss: 62.777298, loss_kl: 2.501915, loss_recon: 0.605408, loss_pred: 0.329088
iteration 318: loss: 62.118912, loss_kl: 1.977813, loss_recon: 0.602175, loss_pred: 0.393550
iteration 319: loss: 62.483368, loss_kl: 1.589170, loss_recon: 0.609975, loss_pred: 0.274332
iteration 320: loss: 61.697609, loss_kl: 1.399320, loss_recon: 0.603472, loss_pred: 0.283615
 16%|████▊                         | 32/200 [26:44<2:18:19, 49.40s/it]iteration 321: loss: 63.156536, loss_kl: 1.966870, loss_recon: 0.613688, loss_pred: 0.210324
iteration 322: loss: 62.680729, loss_kl: 1.617181, loss_recon: 0.612561, loss_pred: 0.127628
iteration 323: loss: 60.840508, loss_kl: 1.030236, loss_recon: 0.598220, loss_pred: 0.192279
iteration 324: loss: 63.159199, loss_kl: 1.679085, loss_recon: 0.615907, loss_pred: 0.221921
iteration 325: loss: 63.043957, loss_kl: 1.835247, loss_recon: 0.612954, loss_pred: 0.276644
iteration 326: loss: 62.563286, loss_kl: 1.343943, loss_recon: 0.613016, loss_pred: 0.183865
iteration 327: loss: 62.473549, loss_kl: 1.281070, loss_recon: 0.612948, loss_pred: 0.151380
iteration 328: loss: 61.642868, loss_kl: 1.310620, loss_recon: 0.604338, loss_pred: 0.157962
iteration 329: loss: 63.221737, loss_kl: 0.870947, loss_recon: 0.623994, loss_pred: 0.123789
iteration 330: loss: 62.184132, loss_kl: 1.473433, loss_recon: 0.602851, loss_pred: 0.717345
 16%|████▉                         | 33/200 [27:34<2:17:54, 49.55s/it]iteration 331: loss: 61.512035, loss_kl: 1.116508, loss_recon: 0.604683, loss_pred: 0.104091
iteration 332: loss: 62.570625, loss_kl: 1.437043, loss_recon: 0.611573, loss_pred: 0.203867
iteration 333: loss: 62.259258, loss_kl: 1.345957, loss_recon: 0.608377, loss_pred: 0.288784
iteration 334: loss: 61.465775, loss_kl: 0.998461, loss_recon: 0.603621, loss_pred: 0.263384
iteration 335: loss: 61.401268, loss_kl: 1.270655, loss_recon: 0.601387, loss_pred: 0.193155
iteration 336: loss: 63.013523, loss_kl: 2.231847, loss_recon: 0.609803, loss_pred: 0.154933
iteration 337: loss: 62.855637, loss_kl: 1.693429, loss_recon: 0.612926, loss_pred: 0.137817
iteration 338: loss: 63.019608, loss_kl: 1.238495, loss_recon: 0.618334, loss_pred: 0.143862
iteration 339: loss: 62.960403, loss_kl: 1.292826, loss_recon: 0.617426, loss_pred: 0.129714
iteration 340: loss: 63.711517, loss_kl: 0.661226, loss_recon: 0.627341, loss_pred: 0.420882
 17%|█████                         | 34/200 [28:24<2:17:02, 49.53s/it]iteration 341: loss: 61.575954, loss_kl: 1.640685, loss_recon: 0.599389, loss_pred: 0.191245
iteration 342: loss: 62.851334, loss_kl: 1.552536, loss_recon: 0.613116, loss_pred: 0.171623
iteration 343: loss: 63.168655, loss_kl: 2.138344, loss_recon: 0.611204, loss_pred: 0.163924
iteration 344: loss: 62.448898, loss_kl: 1.309112, loss_recon: 0.611386, loss_pred: 0.156718
iteration 345: loss: 62.888412, loss_kl: 1.387020, loss_recon: 0.615660, loss_pred: 0.100170
iteration 346: loss: 62.555767, loss_kl: 1.063139, loss_recon: 0.614974, loss_pred: 0.121516
iteration 347: loss: 61.126614, loss_kl: 0.941350, loss_recon: 0.601709, loss_pred: 0.126201
iteration 348: loss: 61.370743, loss_kl: 1.228767, loss_recon: 0.601054, loss_pred: 0.182553
iteration 349: loss: 63.026608, loss_kl: 1.606169, loss_recon: 0.615072, loss_pred: 0.104050
iteration 350: loss: 63.425415, loss_kl: 1.012903, loss_recon: 0.623484, loss_pred: 0.184394
 18%|█████▎                        | 35/200 [29:13<2:15:48, 49.38s/it]iteration 351: loss: 62.412483, loss_kl: 2.127538, loss_recon: 0.602930, loss_pred: 0.160472
iteration 352: loss: 61.660320, loss_kl: 1.144083, loss_recon: 0.604640, loss_pred: 0.142827
iteration 353: loss: 62.483994, loss_kl: 1.487055, loss_recon: 0.608345, loss_pred: 0.280236
iteration 354: loss: 61.989269, loss_kl: 0.702944, loss_recon: 0.610353, loss_pred: 0.306738
iteration 355: loss: 63.231613, loss_kl: 1.338685, loss_recon: 0.617912, loss_pred: 0.207764
iteration 356: loss: 61.825935, loss_kl: 1.177398, loss_recon: 0.605501, loss_pred: 0.191722
iteration 357: loss: 63.726234, loss_kl: 1.347716, loss_recon: 0.623621, loss_pred: 0.123158
iteration 358: loss: 62.239483, loss_kl: 1.237110, loss_recon: 0.609865, loss_pred: 0.113878
iteration 359: loss: 62.334583, loss_kl: 0.880366, loss_recon: 0.613837, loss_pred: 0.140217
iteration 360: loss: 60.854568, loss_kl: 1.723724, loss_recon: 0.589117, loss_pred: 0.355640
 18%|█████▍                        | 36/200 [30:02<2:15:08, 49.44s/it]iteration 361: loss: 61.673283, loss_kl: 0.985124, loss_recon: 0.603429, loss_pred: 0.384223
iteration 362: loss: 62.418819, loss_kl: 1.278511, loss_recon: 0.608822, loss_pred: 0.308749
iteration 363: loss: 63.606651, loss_kl: 1.063905, loss_recon: 0.623957, loss_pred: 0.189186
iteration 364: loss: 61.711971, loss_kl: 0.893954, loss_recon: 0.603984, loss_pred: 0.454994
iteration 365: loss: 62.696499, loss_kl: 1.056339, loss_recon: 0.615359, loss_pred: 0.146086
iteration 366: loss: 62.427742, loss_kl: 0.840871, loss_recon: 0.613002, loss_pred: 0.319928
iteration 367: loss: 61.899250, loss_kl: 1.150151, loss_recon: 0.604876, loss_pred: 0.307086
iteration 368: loss: 61.660198, loss_kl: 1.032423, loss_recon: 0.603445, loss_pred: 0.324200
iteration 369: loss: 63.177773, loss_kl: 1.051540, loss_recon: 0.619355, loss_pred: 0.232323
iteration 370: loss: 60.383987, loss_kl: 0.768269, loss_recon: 0.593791, loss_pred: 0.266990
 18%|█████▌                        | 37/200 [30:52<2:14:35, 49.54s/it]iteration 371: loss: 61.357788, loss_kl: 0.601473, loss_recon: 0.603323, loss_pred: 0.423997
iteration 372: loss: 62.639236, loss_kl: 0.722206, loss_recon: 0.616281, loss_pred: 0.288946
iteration 373: loss: 62.688145, loss_kl: 1.064267, loss_recon: 0.614170, loss_pred: 0.206904
iteration 374: loss: 61.367741, loss_kl: 0.893418, loss_recon: 0.600559, loss_pred: 0.418433
iteration 375: loss: 62.720222, loss_kl: 0.690673, loss_recon: 0.616965, loss_pred: 0.333071
iteration 376: loss: 62.152370, loss_kl: 1.242223, loss_recon: 0.606539, loss_pred: 0.256227
iteration 377: loss: 62.696655, loss_kl: 1.572856, loss_recon: 0.607872, loss_pred: 0.336590
iteration 378: loss: 62.343227, loss_kl: 0.952244, loss_recon: 0.612455, loss_pred: 0.145459
iteration 379: loss: 62.369717, loss_kl: 0.779798, loss_recon: 0.614619, loss_pred: 0.128055
iteration 380: loss: 65.851097, loss_kl: 0.871576, loss_recon: 0.643524, loss_pred: 0.627089
 19%|█████▋                        | 38/200 [31:42<2:13:56, 49.61s/it]iteration 381: loss: 60.837440, loss_kl: 1.163069, loss_recon: 0.593437, loss_pred: 0.330698
iteration 382: loss: 62.648174, loss_kl: 1.062893, loss_recon: 0.614132, loss_pred: 0.172079
iteration 383: loss: 63.108768, loss_kl: 1.196501, loss_recon: 0.613204, loss_pred: 0.591886
iteration 384: loss: 62.127136, loss_kl: 0.983148, loss_recon: 0.607531, loss_pred: 0.390869
iteration 385: loss: 62.563580, loss_kl: 1.012303, loss_recon: 0.612882, loss_pred: 0.263029
iteration 386: loss: 62.114384, loss_kl: 0.843455, loss_recon: 0.606038, loss_pred: 0.667140
iteration 387: loss: 62.679943, loss_kl: 0.635483, loss_recon: 0.617111, loss_pred: 0.333400
iteration 388: loss: 61.765911, loss_kl: 1.014005, loss_recon: 0.604344, loss_pred: 0.317542
iteration 389: loss: 63.081985, loss_kl: 0.863547, loss_recon: 0.620989, loss_pred: 0.119570
iteration 390: loss: 63.599648, loss_kl: 0.696311, loss_recon: 0.624432, loss_pred: 0.460154
 20%|█████▊                        | 39/200 [32:31<2:13:14, 49.65s/it]iteration 391: loss: 61.584953, loss_kl: 0.759222, loss_recon: 0.605245, loss_pred: 0.301241
iteration 392: loss: 60.668968, loss_kl: 0.833002, loss_recon: 0.596310, loss_pred: 0.204993
iteration 393: loss: 62.273930, loss_kl: 0.701258, loss_recon: 0.612845, loss_pred: 0.288195
iteration 394: loss: 61.225319, loss_kl: 0.595090, loss_recon: 0.603401, loss_pred: 0.290111
iteration 395: loss: 62.795372, loss_kl: 0.812599, loss_recon: 0.618703, loss_pred: 0.112465
iteration 396: loss: 62.436691, loss_kl: 0.856298, loss_recon: 0.613728, loss_pred: 0.207595
iteration 397: loss: 62.140232, loss_kl: 0.599346, loss_recon: 0.612506, loss_pred: 0.290328
iteration 398: loss: 62.173027, loss_kl: 0.924568, loss_recon: 0.609715, loss_pred: 0.276944
iteration 399: loss: 61.159439, loss_kl: 0.777703, loss_recon: 0.601049, loss_pred: 0.276855
iteration 400: loss: 63.978718, loss_kl: 0.730332, loss_recon: 0.630827, loss_pred: 0.165650
 20%|██████                        | 40/200 [33:21<2:12:29, 49.68s/it]iteration 401: loss: 62.034054, loss_kl: 0.570176, loss_recon: 0.612848, loss_pred: 0.179074
iteration 402: loss: 61.776897, loss_kl: 0.671009, loss_recon: 0.610159, loss_pred: 0.089975
iteration 403: loss: 62.346722, loss_kl: 0.955021, loss_recon: 0.612455, loss_pred: 0.146212
iteration 404: loss: 61.201778, loss_kl: 0.728628, loss_recon: 0.603110, loss_pred: 0.162098
iteration 405: loss: 62.269188, loss_kl: 0.685002, loss_recon: 0.613935, loss_pred: 0.190727
iteration 406: loss: 61.894184, loss_kl: 0.727976, loss_recon: 0.610214, loss_pred: 0.144824
iteration 407: loss: 61.484825, loss_kl: 0.643446, loss_recon: 0.607218, loss_pred: 0.119590
iteration 408: loss: 61.986088, loss_kl: 0.476055, loss_recon: 0.613241, loss_pred: 0.185890
iteration 409: loss: 61.223244, loss_kl: 0.917296, loss_recon: 0.601349, loss_pred: 0.171036
iteration 410: loss: 62.452957, loss_kl: 0.592498, loss_recon: 0.617510, loss_pred: 0.109472
 20%|██████▏                       | 41/200 [34:11<2:12:04, 49.84s/it]iteration 411: loss: 60.671661, loss_kl: 0.946533, loss_recon: 0.595778, loss_pred: 0.147280
iteration 412: loss: 61.916012, loss_kl: 0.534046, loss_recon: 0.611611, loss_pred: 0.220852
iteration 413: loss: 61.531849, loss_kl: 0.436916, loss_recon: 0.608605, loss_pred: 0.234414
iteration 414: loss: 62.344227, loss_kl: 0.449191, loss_recon: 0.616962, loss_pred: 0.198822
iteration 415: loss: 61.926292, loss_kl: 0.489566, loss_recon: 0.612371, loss_pred: 0.199587
iteration 416: loss: 61.355576, loss_kl: 0.676567, loss_recon: 0.604682, loss_pred: 0.210793
iteration 417: loss: 61.479214, loss_kl: 0.546531, loss_recon: 0.607588, loss_pred: 0.173847
iteration 418: loss: 62.232098, loss_kl: 0.652203, loss_recon: 0.613992, loss_pred: 0.180661
iteration 419: loss: 62.015583, loss_kl: 0.456876, loss_recon: 0.613277, loss_pred: 0.231038
iteration 420: loss: 62.636433, loss_kl: 0.668567, loss_recon: 0.616331, loss_pred: 0.334724
 21%|██████▎                       | 42/200 [35:02<2:11:27, 49.92s/it]iteration 421: loss: 62.198700, loss_kl: 0.657634, loss_recon: 0.614185, loss_pred: 0.122552
iteration 422: loss: 61.756706, loss_kl: 0.498977, loss_recon: 0.610477, loss_pred: 0.210018
iteration 423: loss: 62.352325, loss_kl: 0.643947, loss_recon: 0.615853, loss_pred: 0.123040
iteration 424: loss: 61.073116, loss_kl: 0.637714, loss_recon: 0.602809, loss_pred: 0.154513
iteration 425: loss: 61.212025, loss_kl: 0.426818, loss_recon: 0.605957, loss_pred: 0.189491
iteration 426: loss: 61.639511, loss_kl: 0.621299, loss_recon: 0.607489, loss_pred: 0.269301
iteration 427: loss: 62.297134, loss_kl: 0.672886, loss_recon: 0.614245, loss_pred: 0.199781
iteration 428: loss: 61.985550, loss_kl: 0.869186, loss_recon: 0.608150, loss_pred: 0.301399
iteration 429: loss: 60.883732, loss_kl: 0.726485, loss_recon: 0.599256, loss_pred: 0.231605
iteration 430: loss: 63.667839, loss_kl: 0.240565, loss_recon: 0.633232, loss_pred: 0.104030
 22%|██████▍                       | 43/200 [35:52<2:11:06, 50.10s/it]iteration 431: loss: 62.583817, loss_kl: 0.596913, loss_recon: 0.618794, loss_pred: 0.107501
iteration 432: loss: 61.607903, loss_kl: 0.456457, loss_recon: 0.610382, loss_pred: 0.113207
iteration 433: loss: 61.713032, loss_kl: 0.413912, loss_recon: 0.611721, loss_pred: 0.127005
iteration 434: loss: 60.978233, loss_kl: 0.507834, loss_recon: 0.603523, loss_pred: 0.118062
iteration 435: loss: 61.276199, loss_kl: 0.558108, loss_recon: 0.605689, loss_pred: 0.149164
iteration 436: loss: 60.056232, loss_kl: 0.306266, loss_recon: 0.596302, loss_pred: 0.119770
iteration 437: loss: 62.848030, loss_kl: 0.893223, loss_recon: 0.618106, loss_pred: 0.144251
iteration 438: loss: 61.904991, loss_kl: 0.812070, loss_recon: 0.609503, loss_pred: 0.142610
iteration 439: loss: 61.455799, loss_kl: 0.716688, loss_recon: 0.606005, loss_pred: 0.138594
iteration 440: loss: 61.079102, loss_kl: 0.626153, loss_recon: 0.602278, loss_pred: 0.225127
 22%|██████▌                       | 44/200 [36:42<2:10:19, 50.13s/it]iteration 441: loss: 61.026546, loss_kl: 0.447114, loss_recon: 0.603812, loss_pred: 0.198230
iteration 442: loss: 61.930344, loss_kl: 0.626777, loss_recon: 0.611904, loss_pred: 0.113186
iteration 443: loss: 60.908916, loss_kl: 0.604985, loss_recon: 0.601458, loss_pred: 0.158085
iteration 444: loss: 61.442707, loss_kl: 0.485758, loss_recon: 0.607401, loss_pred: 0.216875
iteration 445: loss: 61.528622, loss_kl: 0.680615, loss_recon: 0.606888, loss_pred: 0.159160
iteration 446: loss: 62.597176, loss_kl: 0.500129, loss_recon: 0.619653, loss_pred: 0.131784
iteration 447: loss: 61.761456, loss_kl: 0.305150, loss_recon: 0.613433, loss_pred: 0.112999
iteration 448: loss: 61.160816, loss_kl: 0.305645, loss_recon: 0.606328, loss_pred: 0.222327
iteration 449: loss: 61.036758, loss_kl: 0.402185, loss_recon: 0.604855, loss_pred: 0.149032
iteration 450: loss: 64.134872, loss_kl: 0.785202, loss_recon: 0.631540, loss_pred: 0.195652
 22%|██████▊                       | 45/200 [37:32<2:09:25, 50.10s/it]iteration 451: loss: 60.364273, loss_kl: 0.628299, loss_recon: 0.595544, loss_pred: 0.181546
iteration 452: loss: 60.224186, loss_kl: 0.638711, loss_recon: 0.594105, loss_pred: 0.174927
iteration 453: loss: 61.746178, loss_kl: 0.553241, loss_recon: 0.610706, loss_pred: 0.122337
iteration 454: loss: 62.945835, loss_kl: 0.440587, loss_recon: 0.623847, loss_pred: 0.120507
iteration 455: loss: 61.154484, loss_kl: 0.369954, loss_recon: 0.606426, loss_pred: 0.141901
iteration 456: loss: 61.781479, loss_kl: 0.459819, loss_recon: 0.611924, loss_pred: 0.129222
iteration 457: loss: 61.589443, loss_kl: 0.430916, loss_recon: 0.610089, loss_pred: 0.149628
iteration 458: loss: 63.032722, loss_kl: 0.494527, loss_recon: 0.623586, loss_pred: 0.179551
iteration 459: loss: 61.921059, loss_kl: 0.385581, loss_recon: 0.613847, loss_pred: 0.150797
iteration 460: loss: 61.767273, loss_kl: 0.141241, loss_recon: 0.614561, loss_pred: 0.169899
 23%|██████▉                       | 46/200 [38:22<2:08:20, 50.00s/it]iteration 461: loss: 62.517162, loss_kl: 0.512626, loss_recon: 0.619067, loss_pred: 0.097810
iteration 462: loss: 63.102341, loss_kl: 0.749091, loss_recon: 0.622672, loss_pred: 0.086051
iteration 463: loss: 61.168919, loss_kl: 0.384243, loss_recon: 0.606676, loss_pred: 0.117047
iteration 464: loss: 62.294018, loss_kl: 0.454962, loss_recon: 0.617245, loss_pred: 0.114511
iteration 465: loss: 60.558296, loss_kl: 0.575906, loss_recon: 0.597991, loss_pred: 0.183291
iteration 466: loss: 62.013638, loss_kl: 0.672306, loss_recon: 0.612046, loss_pred: 0.136748
iteration 467: loss: 60.987476, loss_kl: 0.605562, loss_recon: 0.601892, loss_pred: 0.192697
iteration 468: loss: 61.784874, loss_kl: 0.529901, loss_recon: 0.610311, loss_pred: 0.223859
iteration 469: loss: 61.316704, loss_kl: 0.658970, loss_recon: 0.605148, loss_pred: 0.142894
iteration 470: loss: 61.739372, loss_kl: 0.311151, loss_recon: 0.613231, loss_pred: 0.105171
 24%|███████                       | 47/200 [39:12<2:07:12, 49.89s/it]iteration 471: loss: 62.744484, loss_kl: 0.594156, loss_recon: 0.620296, loss_pred: 0.120753
iteration 472: loss: 61.597206, loss_kl: 0.197056, loss_recon: 0.612914, loss_pred: 0.108741
iteration 473: loss: 62.162273, loss_kl: 0.573651, loss_recon: 0.614757, loss_pred: 0.112900
iteration 474: loss: 60.186142, loss_kl: 0.733965, loss_recon: 0.592430, loss_pred: 0.209192
iteration 475: loss: 61.130230, loss_kl: 0.393699, loss_recon: 0.605350, loss_pred: 0.201557
iteration 476: loss: 61.519009, loss_kl: 0.313346, loss_recon: 0.609142, loss_pred: 0.291440
iteration 477: loss: 61.026043, loss_kl: -0.048872, loss_recon: 0.609239, loss_pred: 0.150987
iteration 478: loss: 61.573544, loss_kl: 0.288492, loss_recon: 0.611454, loss_pred: 0.139602
iteration 479: loss: 61.652130, loss_kl: 0.334613, loss_recon: 0.610691, loss_pred: 0.248391
iteration 480: loss: 62.442787, loss_kl: 1.080884, loss_recon: 0.610825, loss_pred: 0.279427
 24%|███████▏                      | 48/200 [40:01<2:06:17, 49.85s/it]iteration 481: loss: 61.429977, loss_kl: 0.249973, loss_recon: 0.610304, loss_pred: 0.149595
iteration 482: loss: 61.209892, loss_kl: 0.402213, loss_recon: 0.606153, loss_pred: 0.192370
iteration 483: loss: 61.622952, loss_kl: 0.466919, loss_recon: 0.609631, loss_pred: 0.192927
iteration 484: loss: 61.365101, loss_kl: 0.296591, loss_recon: 0.609430, loss_pred: 0.125458
iteration 485: loss: 60.873871, loss_kl: 0.229445, loss_recon: 0.603771, loss_pred: 0.267283
iteration 486: loss: 62.414040, loss_kl: 0.366110, loss_recon: 0.619458, loss_pred: 0.102150
iteration 487: loss: 61.954113, loss_kl: 0.255993, loss_recon: 0.615898, loss_pred: 0.108296
iteration 488: loss: 60.928585, loss_kl: 0.303158, loss_recon: 0.604671, loss_pred: 0.158315
iteration 489: loss: 61.211479, loss_kl: 0.540077, loss_recon: 0.605570, loss_pred: 0.114397
iteration 490: loss: 61.565926, loss_kl: 0.337364, loss_recon: 0.610788, loss_pred: 0.149736
 24%|███████▎                      | 49/200 [40:51<2:05:26, 49.84s/it]iteration 491: loss: 61.015434, loss_kl: 0.244775, loss_recon: 0.606311, loss_pred: 0.139597
iteration 492: loss: 62.958897, loss_kl: 0.206630, loss_recon: 0.626445, loss_pred: 0.107754
iteration 493: loss: 61.550323, loss_kl: 0.326546, loss_recon: 0.611287, loss_pred: 0.095046
iteration 494: loss: 61.488712, loss_kl: 0.338296, loss_recon: 0.610377, loss_pred: 0.112735
iteration 495: loss: 60.682175, loss_kl: 0.400636, loss_recon: 0.601375, loss_pred: 0.144049
iteration 496: loss: 60.868618, loss_kl: 0.534064, loss_recon: 0.602442, loss_pred: 0.090351
iteration 497: loss: 63.201241, loss_kl: 0.219971, loss_recon: 0.628932, loss_pred: 0.088110
iteration 498: loss: 59.840519, loss_kl: 0.221929, loss_recon: 0.595306, loss_pred: 0.088029
iteration 499: loss: 61.049934, loss_kl: 0.432885, loss_recon: 0.605026, loss_pred: 0.114402
iteration 500: loss: 60.369843, loss_kl: 0.353786, loss_recon: 0.598984, loss_pred: 0.117662
 25%|███████▌                      | 50/200 [41:41<2:04:40, 49.87s/it]iteration 501: loss: 61.874805, loss_kl: 0.347132, loss_recon: 0.617921, loss_pred: 0.079196
iteration 502: loss: 60.966030, loss_kl: 0.303409, loss_recon: 0.608391, loss_pred: 0.123927
iteration 503: loss: 59.384956, loss_kl: 0.367541, loss_recon: 0.592351, loss_pred: 0.146132
iteration 504: loss: 61.941383, loss_kl: 0.323259, loss_recon: 0.618216, loss_pred: 0.116535
iteration 505: loss: 61.447201, loss_kl: 0.418028, loss_recon: 0.613309, loss_pred: 0.112161
iteration 506: loss: 59.836250, loss_kl: 0.496823, loss_recon: 0.597326, loss_pred: 0.098647
iteration 507: loss: 61.616581, loss_kl: 0.464918, loss_recon: 0.615107, loss_pred: 0.101228
iteration 508: loss: 60.783401, loss_kl: 0.571251, loss_recon: 0.605807, loss_pred: 0.197038
iteration 509: loss: 60.736504, loss_kl: 0.372092, loss_recon: 0.605675, loss_pred: 0.165300
iteration 510: loss: 62.109600, loss_kl: 0.877610, loss_recon: 0.619500, loss_pred: 0.150783
 26%|███████▋                      | 51/200 [42:30<2:03:17, 49.65s/it]iteration 511: loss: 59.517536, loss_kl: 0.458491, loss_recon: 0.592528, loss_pred: 0.260104
iteration 512: loss: 62.105343, loss_kl: 0.759920, loss_recon: 0.619690, loss_pred: 0.128706
iteration 513: loss: 60.847313, loss_kl: 0.498054, loss_recon: 0.606794, loss_pred: 0.162967
iteration 514: loss: 61.048126, loss_kl: 0.518871, loss_recon: 0.608761, loss_pred: 0.166789
iteration 515: loss: 60.663868, loss_kl: 0.328258, loss_recon: 0.605681, loss_pred: 0.092484
iteration 516: loss: 61.906593, loss_kl: 0.658002, loss_recon: 0.618125, loss_pred: 0.087467
iteration 517: loss: 60.496593, loss_kl: 0.763563, loss_recon: 0.603597, loss_pred: 0.129281
iteration 518: loss: 61.667881, loss_kl: 0.501177, loss_recon: 0.615416, loss_pred: 0.121318
iteration 519: loss: 60.591778, loss_kl: 0.950112, loss_recon: 0.604864, loss_pred: 0.095853
iteration 520: loss: 62.989468, loss_kl: 0.809744, loss_recon: 0.628745, loss_pred: 0.106842
 26%|███████▊                      | 52/200 [43:20<2:02:42, 49.74s/it]iteration 521: loss: 60.874149, loss_kl: 0.551644, loss_recon: 0.607451, loss_pred: 0.123577
iteration 522: loss: 62.225796, loss_kl: 0.465844, loss_recon: 0.620805, loss_pred: 0.140661
iteration 523: loss: 60.833591, loss_kl: 0.440141, loss_recon: 0.606399, loss_pred: 0.189340
iteration 524: loss: 60.397366, loss_kl: 0.423990, loss_recon: 0.602585, loss_pred: 0.134652
iteration 525: loss: 61.026512, loss_kl: 0.306586, loss_recon: 0.609399, loss_pred: 0.083560
iteration 526: loss: 61.069031, loss_kl: 0.284986, loss_recon: 0.609168, loss_pred: 0.149355
iteration 527: loss: 60.784050, loss_kl: 0.557625, loss_recon: 0.606864, loss_pred: 0.092096
iteration 528: loss: 61.604507, loss_kl: 0.467979, loss_recon: 0.615034, loss_pred: 0.096406
iteration 529: loss: 60.449299, loss_kl: 0.408411, loss_recon: 0.603151, loss_pred: 0.130090
iteration 530: loss: 59.564716, loss_kl: -0.144064, loss_recon: 0.593944, loss_pred: 0.171758
 26%|███████▉                      | 53/200 [44:10<2:02:06, 49.84s/it]iteration 531: loss: 61.228233, loss_kl: 0.280521, loss_recon: 0.611240, loss_pred: 0.101466
iteration 532: loss: 61.314262, loss_kl: 0.259369, loss_recon: 0.612287, loss_pred: 0.082976
iteration 533: loss: 61.030331, loss_kl: 0.481847, loss_recon: 0.609155, loss_pred: 0.109993
iteration 534: loss: 61.485912, loss_kl: 0.520666, loss_recon: 0.614145, loss_pred: 0.066207
iteration 535: loss: 60.608551, loss_kl: 0.758290, loss_recon: 0.605072, loss_pred: 0.093795
iteration 536: loss: 60.750404, loss_kl: 0.189833, loss_recon: 0.606640, loss_pred: 0.084488
iteration 537: loss: 60.399147, loss_kl: 0.305542, loss_recon: 0.602691, loss_pred: 0.127011
iteration 538: loss: 59.982349, loss_kl: 0.417938, loss_recon: 0.598583, loss_pred: 0.119838
iteration 539: loss: 62.156258, loss_kl: 0.307809, loss_recon: 0.620656, loss_pred: 0.087586
iteration 540: loss: 60.124294, loss_kl: 0.706111, loss_recon: 0.599983, loss_pred: 0.118964
 27%|████████                      | 54/200 [45:02<2:02:21, 50.28s/it]iteration 541: loss: 60.636826, loss_kl: 0.379232, loss_recon: 0.605191, loss_pred: 0.113898
iteration 542: loss: 62.093491, loss_kl: 0.598017, loss_recon: 0.619723, loss_pred: 0.115240
iteration 543: loss: 60.196552, loss_kl: 0.464056, loss_recon: 0.600684, loss_pred: 0.123538
iteration 544: loss: 60.414070, loss_kl: 0.334820, loss_recon: 0.602702, loss_pred: 0.140519
iteration 545: loss: 60.677761, loss_kl: 0.636882, loss_recon: 0.605506, loss_pred: 0.120817
iteration 546: loss: 61.685593, loss_kl: 0.487204, loss_recon: 0.614910, loss_pred: 0.189739
iteration 547: loss: 61.917149, loss_kl: 0.469530, loss_recon: 0.617258, loss_pred: 0.186639
iteration 548: loss: 61.183498, loss_kl: 0.385882, loss_recon: 0.611033, loss_pred: 0.076356
iteration 549: loss: 60.590736, loss_kl: 0.484914, loss_recon: 0.604027, loss_pred: 0.183157
iteration 550: loss: 60.323860, loss_kl: -0.117070, loss_recon: 0.600588, loss_pred: 0.266246
 28%|████████▎                     | 55/200 [45:52<2:01:12, 50.16s/it]iteration 551: loss: 62.013973, loss_kl: 0.281588, loss_recon: 0.619337, loss_pred: 0.077441
iteration 552: loss: 60.458572, loss_kl: 0.354453, loss_recon: 0.603137, loss_pred: 0.141285
iteration 553: loss: 60.717613, loss_kl: 0.418429, loss_recon: 0.605113, loss_pred: 0.202117
iteration 554: loss: 61.491581, loss_kl: 0.459034, loss_recon: 0.613697, loss_pred: 0.117266
iteration 555: loss: 60.891880, loss_kl: 0.254722, loss_recon: 0.607933, loss_pred: 0.096079
iteration 556: loss: 60.115524, loss_kl: 0.543783, loss_recon: 0.598717, loss_pred: 0.238402
iteration 557: loss: 60.154953, loss_kl: 0.324370, loss_recon: 0.600309, loss_pred: 0.120763
iteration 558: loss: 61.084381, loss_kl: 0.353519, loss_recon: 0.610226, loss_pred: 0.058223
iteration 559: loss: 61.313797, loss_kl: 0.419596, loss_recon: 0.612203, loss_pred: 0.089303
iteration 560: loss: 60.942402, loss_kl: 0.162694, loss_recon: 0.608352, loss_pred: 0.105614
 28%|████████▍                     | 56/200 [46:41<1:59:48, 49.92s/it]iteration 561: loss: 62.430115, loss_kl: 0.600156, loss_recon: 0.623135, loss_pred: 0.110617
iteration 562: loss: 59.697083, loss_kl: 0.403178, loss_recon: 0.595755, loss_pred: 0.117588
iteration 563: loss: 61.440121, loss_kl: 0.218486, loss_recon: 0.613497, loss_pred: 0.088183
iteration 564: loss: 60.649681, loss_kl: 0.379792, loss_recon: 0.605131, loss_pred: 0.132757
iteration 565: loss: 60.418182, loss_kl: 0.544077, loss_recon: 0.603136, loss_pred: 0.099130
iteration 566: loss: 61.395443, loss_kl: 0.394177, loss_recon: 0.613007, loss_pred: 0.090779
iteration 567: loss: 60.827099, loss_kl: 0.389951, loss_recon: 0.607181, loss_pred: 0.105099
iteration 568: loss: 61.243156, loss_kl: 0.613064, loss_recon: 0.611539, loss_pred: 0.083130
iteration 569: loss: 60.099640, loss_kl: 0.387137, loss_recon: 0.600143, loss_pred: 0.081417
iteration 570: loss: 62.709503, loss_kl: 0.685154, loss_recon: 0.625135, loss_pred: 0.189144
 28%|████████▌                     | 57/200 [47:31<1:59:03, 49.96s/it]iteration 571: loss: 61.132671, loss_kl: 0.815445, loss_recon: 0.610028, loss_pred: 0.121671
iteration 572: loss: 59.684967, loss_kl: 0.238760, loss_recon: 0.595272, loss_pred: 0.155353
iteration 573: loss: 61.454014, loss_kl: 0.504176, loss_recon: 0.613742, loss_pred: 0.074784
iteration 574: loss: 59.704254, loss_kl: 0.347116, loss_recon: 0.596147, loss_pred: 0.086038
iteration 575: loss: 61.930828, loss_kl: 0.495159, loss_recon: 0.618674, loss_pred: 0.058467
iteration 576: loss: 61.807461, loss_kl: 0.183773, loss_recon: 0.617330, loss_pred: 0.072670
iteration 577: loss: 60.887856, loss_kl: 0.629295, loss_recon: 0.607976, loss_pred: 0.083935
iteration 578: loss: 61.985977, loss_kl: 0.384472, loss_recon: 0.619412, loss_pred: 0.040950
iteration 579: loss: 60.848900, loss_kl: 0.421886, loss_recon: 0.607531, loss_pred: 0.091546
iteration 580: loss: 60.926849, loss_kl: 0.416442, loss_recon: 0.608246, loss_pred: 0.098038
 29%|████████▋                     | 58/200 [48:21<1:58:33, 50.10s/it]iteration 581: loss: 60.683582, loss_kl: 0.388564, loss_recon: 0.605912, loss_pred: 0.088468
iteration 582: loss: 60.288395, loss_kl: 0.528844, loss_recon: 0.601887, loss_pred: 0.094441
iteration 583: loss: 60.533623, loss_kl: 0.477638, loss_recon: 0.604411, loss_pred: 0.087784
iteration 584: loss: 61.413792, loss_kl: 0.702207, loss_recon: 0.613271, loss_pred: 0.079704
iteration 585: loss: 60.689304, loss_kl: 0.226969, loss_recon: 0.605744, loss_pred: 0.112632
iteration 586: loss: 60.700447, loss_kl: 0.812981, loss_recon: 0.605617, loss_pred: 0.130642
iteration 587: loss: 60.053825, loss_kl: 0.486753, loss_recon: 0.599604, loss_pred: 0.088573
iteration 588: loss: 61.297520, loss_kl: 0.309210, loss_recon: 0.612011, loss_pred: 0.093319
iteration 589: loss: 62.401783, loss_kl: 0.373921, loss_recon: 0.622673, loss_pred: 0.130705
iteration 590: loss: 60.551395, loss_kl: 0.417771, loss_recon: 0.604885, loss_pred: 0.058675
 30%|████████▊                     | 59/200 [49:11<1:57:27, 49.99s/it]iteration 591: loss: 59.852280, loss_kl: 0.517480, loss_recon: 0.597217, loss_pred: 0.125365
iteration 592: loss: 61.155624, loss_kl: 0.147780, loss_recon: 0.610464, loss_pred: 0.107792
iteration 593: loss: 61.767265, loss_kl: 0.513943, loss_recon: 0.616926, loss_pred: 0.069518
iteration 594: loss: 61.420631, loss_kl: 0.610412, loss_recon: 0.613234, loss_pred: 0.091140
iteration 595: loss: 60.638741, loss_kl: 0.579139, loss_recon: 0.605359, loss_pred: 0.097066
iteration 596: loss: 61.865025, loss_kl: 0.434124, loss_recon: 0.617626, loss_pred: 0.098111
iteration 597: loss: 60.423222, loss_kl: 0.439316, loss_recon: 0.603381, loss_pred: 0.080752
iteration 598: loss: 60.089592, loss_kl: 0.408306, loss_recon: 0.599538, loss_pred: 0.131682
iteration 599: loss: 61.391502, loss_kl: 0.368910, loss_recon: 0.612738, loss_pred: 0.114029
iteration 600: loss: 61.933895, loss_kl: 0.582838, loss_recon: 0.618676, loss_pred: 0.060510
 30%|█████████                     | 60/200 [50:01<1:56:19, 49.86s/it]iteration 601: loss: 60.123577, loss_kl: 0.765725, loss_recon: 0.600078, loss_pred: 0.108076
iteration 602: loss: 61.150059, loss_kl: 0.591766, loss_recon: 0.610668, loss_pred: 0.077366
iteration 603: loss: 60.498466, loss_kl: 0.486966, loss_recon: 0.603953, loss_pred: 0.098321
iteration 604: loss: 61.305172, loss_kl: 0.150459, loss_recon: 0.612161, loss_pred: 0.087582
iteration 605: loss: 61.612999, loss_kl: 0.290643, loss_recon: 0.614997, loss_pred: 0.110420
iteration 606: loss: 60.590542, loss_kl: 0.490374, loss_recon: 0.604696, loss_pred: 0.116043
iteration 607: loss: 60.713959, loss_kl: 0.325784, loss_recon: 0.606272, loss_pred: 0.083497
iteration 608: loss: 61.177052, loss_kl: 0.492340, loss_recon: 0.610830, loss_pred: 0.089078
iteration 609: loss: 60.310333, loss_kl: 0.554533, loss_recon: 0.602252, loss_pred: 0.079537
iteration 610: loss: 62.993370, loss_kl: 0.154764, loss_recon: 0.628676, loss_pred: 0.124255
 30%|█████████▏                    | 61/200 [50:50<1:55:25, 49.82s/it]iteration 611: loss: 61.760487, loss_kl: 0.564990, loss_recon: 0.616464, loss_pred: 0.108435
iteration 612: loss: 60.328617, loss_kl: 0.421257, loss_recon: 0.601685, loss_pred: 0.155889
iteration 613: loss: 60.927704, loss_kl: 0.572065, loss_recon: 0.608000, loss_pred: 0.122011
iteration 614: loss: 60.602409, loss_kl: 0.467194, loss_recon: 0.605310, loss_pred: 0.066711
iteration 615: loss: 60.406116, loss_kl: 0.648382, loss_recon: 0.602765, loss_pred: 0.123166
iteration 616: loss: 60.826847, loss_kl: 0.892868, loss_recon: 0.606694, loss_pred: 0.148555
iteration 617: loss: 61.362274, loss_kl: 0.545731, loss_recon: 0.612711, loss_pred: 0.085752
iteration 618: loss: 60.845184, loss_kl: 0.359684, loss_recon: 0.607577, loss_pred: 0.083854
iteration 619: loss: 61.197025, loss_kl: 0.261271, loss_recon: 0.610906, loss_pred: 0.103796
iteration 620: loss: 60.858757, loss_kl: 0.588945, loss_recon: 0.607342, loss_pred: 0.118627
 31%|█████████▎                    | 62/200 [51:40<1:54:16, 49.68s/it]iteration 621: loss: 61.794289, loss_kl: 0.536249, loss_recon: 0.617252, loss_pred: 0.063701
iteration 622: loss: 60.522568, loss_kl: 0.646736, loss_recon: 0.604233, loss_pred: 0.092812
iteration 623: loss: 59.873997, loss_kl: 0.601385, loss_recon: 0.597455, loss_pred: 0.122462
iteration 624: loss: 62.000057, loss_kl: 0.571231, loss_recon: 0.618827, loss_pred: 0.111620
iteration 625: loss: 61.624310, loss_kl: 0.651060, loss_recon: 0.615586, loss_pred: 0.059157
iteration 626: loss: 59.871548, loss_kl: 0.455782, loss_recon: 0.597348, loss_pred: 0.132180
iteration 627: loss: 61.137474, loss_kl: 0.513984, loss_recon: 0.610269, loss_pred: 0.105486
iteration 628: loss: 59.966064, loss_kl: 0.500502, loss_recon: 0.598472, loss_pred: 0.113903
iteration 629: loss: 61.518089, loss_kl: 0.327332, loss_recon: 0.614116, loss_pred: 0.103192
iteration 630: loss: 62.008324, loss_kl: 0.364613, loss_recon: 0.619106, loss_pred: 0.094069
 32%|█████████▍                    | 63/200 [52:30<1:53:55, 49.89s/it]iteration 631: loss: 61.174381, loss_kl: 0.682298, loss_recon: 0.610443, loss_pred: 0.096262
iteration 632: loss: 62.053970, loss_kl: 0.234196, loss_recon: 0.619167, loss_pred: 0.125632
iteration 633: loss: 60.396175, loss_kl: 0.501678, loss_recon: 0.602704, loss_pred: 0.100888
iteration 634: loss: 60.294598, loss_kl: 0.532136, loss_recon: 0.601666, loss_pred: 0.101600
iteration 635: loss: 61.314621, loss_kl: 0.747206, loss_recon: 0.611961, loss_pred: 0.081455
iteration 636: loss: 61.025494, loss_kl: 0.481609, loss_recon: 0.609020, loss_pred: 0.099616
iteration 637: loss: 61.287239, loss_kl: 0.404004, loss_recon: 0.611424, loss_pred: 0.124844
iteration 638: loss: 60.270256, loss_kl: 0.354434, loss_recon: 0.601781, loss_pred: 0.074598
iteration 639: loss: 60.046043, loss_kl: 0.838544, loss_recon: 0.598738, loss_pred: 0.130697
iteration 640: loss: 63.567642, loss_kl: 0.698316, loss_recon: 0.634764, loss_pred: 0.056613
 32%|█████████▌                    | 64/200 [53:19<1:52:36, 49.68s/it]iteration 641: loss: 60.700874, loss_kl: 0.719530, loss_recon: 0.605690, loss_pred: 0.067728
iteration 642: loss: 60.928940, loss_kl: 0.781065, loss_recon: 0.607471, loss_pred: 0.112164
iteration 643: loss: 60.879627, loss_kl: 0.131672, loss_recon: 0.607948, loss_pred: 0.073095
iteration 644: loss: 59.856194, loss_kl: 0.547075, loss_recon: 0.597175, loss_pred: 0.089858
iteration 645: loss: 61.623528, loss_kl: 0.226456, loss_recon: 0.614955, loss_pred: 0.107855
iteration 646: loss: 61.125057, loss_kl: 0.602987, loss_recon: 0.610035, loss_pred: 0.067807
iteration 647: loss: 60.248558, loss_kl: 0.460603, loss_recon: 0.601246, loss_pred: 0.082854
iteration 648: loss: 61.447750, loss_kl: 0.385365, loss_recon: 0.613342, loss_pred: 0.079170
iteration 649: loss: 61.511803, loss_kl: 0.431858, loss_recon: 0.614188, loss_pred: 0.054470
iteration 650: loss: 61.779488, loss_kl: 0.441416, loss_recon: 0.614339, loss_pred: 0.306245
 32%|█████████▊                    | 65/200 [54:09<1:51:47, 49.68s/it]iteration 651: loss: 62.133415, loss_kl: 0.528715, loss_recon: 0.619828, loss_pred: 0.082486
iteration 652: loss: 61.465790, loss_kl: 0.574610, loss_recon: 0.613129, loss_pred: 0.078926
iteration 653: loss: 60.304775, loss_kl: 0.523120, loss_recon: 0.601454, loss_pred: 0.092024
iteration 654: loss: 60.970318, loss_kl: 0.514754, loss_recon: 0.608261, loss_pred: 0.077894
iteration 655: loss: 61.853645, loss_kl: 0.632037, loss_recon: 0.616996, loss_pred: 0.072650
iteration 656: loss: 59.942669, loss_kl: 0.333990, loss_recon: 0.598010, loss_pred: 0.098697
iteration 657: loss: 59.975048, loss_kl: 0.516543, loss_recon: 0.598050, loss_pred: 0.103555
iteration 658: loss: 60.544762, loss_kl: 0.511429, loss_recon: 0.604044, loss_pred: 0.074452
iteration 659: loss: 62.001595, loss_kl: 0.256217, loss_recon: 0.618509, loss_pred: 0.117646
iteration 660: loss: 59.634178, loss_kl: 0.905327, loss_recon: 0.593255, loss_pred: 0.192045
 33%|█████████▉                    | 66/200 [54:59<1:50:53, 49.66s/it]iteration 661: loss: 62.224873, loss_kl: 0.336895, loss_recon: 0.621063, loss_pred: 0.061824
iteration 662: loss: 59.938820, loss_kl: 0.396320, loss_recon: 0.597840, loss_pred: 0.088088
iteration 663: loss: 59.206482, loss_kl: 0.332261, loss_recon: 0.590195, loss_pred: 0.131012
iteration 664: loss: 62.368496, loss_kl: 0.432894, loss_recon: 0.621863, loss_pred: 0.109291
iteration 665: loss: 59.915356, loss_kl: 0.426086, loss_recon: 0.597635, loss_pred: 0.080109
iteration 666: loss: 61.426498, loss_kl: 0.247633, loss_recon: 0.613172, loss_pred: 0.067621
iteration 667: loss: 60.253632, loss_kl: 0.417616, loss_recon: 0.601003, loss_pred: 0.082975
iteration 668: loss: 61.768623, loss_kl: 0.365692, loss_recon: 0.616264, loss_pred: 0.080648
iteration 669: loss: 61.409901, loss_kl: 0.099361, loss_recon: 0.613021, loss_pred: 0.091101
iteration 670: loss: 60.852760, loss_kl: 0.491355, loss_recon: 0.606543, loss_pred: 0.115681
 34%|██████████                    | 67/200 [55:48<1:49:57, 49.61s/it]iteration 671: loss: 60.192127, loss_kl: 0.315439, loss_recon: 0.600499, loss_pred: 0.076577
iteration 672: loss: 61.931705, loss_kl: 0.272541, loss_recon: 0.618250, loss_pred: 0.050039
iteration 673: loss: 61.341339, loss_kl: 0.571172, loss_recon: 0.611583, loss_pred: 0.064238
iteration 674: loss: 61.153248, loss_kl: 0.307448, loss_recon: 0.610392, loss_pred: 0.050065
iteration 675: loss: 61.249054, loss_kl: 0.334760, loss_recon: 0.610813, loss_pred: 0.098142
iteration 676: loss: 60.584499, loss_kl: 0.475239, loss_recon: 0.603646, loss_pred: 0.121022
iteration 677: loss: 60.608688, loss_kl: 0.448346, loss_recon: 0.603991, loss_pred: 0.116294
iteration 678: loss: 60.128883, loss_kl: 0.268282, loss_recon: 0.599721, loss_pred: 0.100974
iteration 679: loss: 60.685604, loss_kl: 0.237562, loss_recon: 0.605280, loss_pred: 0.108177
iteration 680: loss: 62.242146, loss_kl: 0.786433, loss_recon: 0.619881, loss_pred: 0.090436
 34%|██████████▏                   | 68/200 [56:38<1:49:09, 49.62s/it]iteration 681: loss: 61.416271, loss_kl: 0.236280, loss_recon: 0.612382, loss_pred: 0.119542
iteration 682: loss: 62.579075, loss_kl: 0.327148, loss_recon: 0.624113, loss_pred: 0.086746
iteration 683: loss: 61.530357, loss_kl: 0.306489, loss_recon: 0.613762, loss_pred: 0.078236
iteration 684: loss: 60.707207, loss_kl: 0.296364, loss_recon: 0.605308, loss_pred: 0.103077
iteration 685: loss: 60.161549, loss_kl: 0.333092, loss_recon: 0.599426, loss_pred: 0.136457
iteration 686: loss: 60.869358, loss_kl: 0.268154, loss_recon: 0.606888, loss_pred: 0.114188
iteration 687: loss: 59.727459, loss_kl: 0.376337, loss_recon: 0.595085, loss_pred: 0.125756
iteration 688: loss: 58.597904, loss_kl: 0.338948, loss_recon: 0.584048, loss_pred: 0.109163
iteration 689: loss: 63.783993, loss_kl: 0.315474, loss_recon: 0.636522, loss_pred: 0.053672
iteration 690: loss: 60.446846, loss_kl: 0.563013, loss_recon: 0.601280, loss_pred: 0.179469
 34%|██████████▎                   | 69/200 [57:28<1:48:41, 49.78s/it]iteration 691: loss: 62.480980, loss_kl: 0.453212, loss_recon: 0.622473, loss_pred: 0.103520
iteration 692: loss: 61.073963, loss_kl: 0.325344, loss_recon: 0.609093, loss_pred: 0.071182
iteration 693: loss: 61.256252, loss_kl: 0.284018, loss_recon: 0.610966, loss_pred: 0.078079
iteration 694: loss: 59.906261, loss_kl: 0.304538, loss_recon: 0.597447, loss_pred: 0.074120
iteration 695: loss: 60.322044, loss_kl: 0.406018, loss_recon: 0.601406, loss_pred: 0.064835
iteration 696: loss: 61.708794, loss_kl: 0.555460, loss_recon: 0.614808, loss_pred: 0.068471
iteration 697: loss: 61.917286, loss_kl: 0.161763, loss_recon: 0.617783, loss_pred: 0.092514
iteration 698: loss: 59.380833, loss_kl: 0.365095, loss_recon: 0.591934, loss_pred: 0.082571
iteration 699: loss: 62.125011, loss_kl: 0.034863, loss_recon: 0.620222, loss_pred: 0.092771
iteration 700: loss: 59.116413, loss_kl: 0.047210, loss_recon: 0.589710, loss_pred: 0.131864
 35%|██████████▌                   | 70/200 [58:18<1:48:09, 49.92s/it]iteration 701: loss: 59.794918, loss_kl: 0.351742, loss_recon: 0.595205, loss_pred: 0.159439
iteration 702: loss: 60.119972, loss_kl: 0.292663, loss_recon: 0.599312, loss_pred: 0.093159
iteration 703: loss: 62.110428, loss_kl: 0.319044, loss_recon: 0.618665, loss_pred: 0.139657
iteration 704: loss: 61.453796, loss_kl: 0.284754, loss_recon: 0.611982, loss_pred: 0.162571
iteration 705: loss: 61.102829, loss_kl: 0.265582, loss_recon: 0.609173, loss_pred: 0.098690
iteration 706: loss: 61.581806, loss_kl: 0.348051, loss_recon: 0.613549, loss_pred: 0.113180
iteration 707: loss: 61.225212, loss_kl: 0.335566, loss_recon: 0.610134, loss_pred: 0.102109
iteration 708: loss: 61.690182, loss_kl: 0.174334, loss_recon: 0.615675, loss_pred: 0.065701
iteration 709: loss: 59.771904, loss_kl: 0.244718, loss_recon: 0.595842, loss_pred: 0.107740
iteration 710: loss: 63.282372, loss_kl: 0.556308, loss_recon: 0.630161, loss_pred: 0.084520
 36%|██████████▋                   | 71/200 [59:07<1:46:56, 49.74s/it]iteration 711: loss: 60.587868, loss_kl: 0.198808, loss_recon: 0.604437, loss_pred: 0.071360
iteration 712: loss: 61.840649, loss_kl: 0.174640, loss_recon: 0.616518, loss_pred: 0.124815
iteration 713: loss: 61.823723, loss_kl: 0.268928, loss_recon: 0.616339, loss_pred: 0.091337
iteration 714: loss: 61.464985, loss_kl: 0.322905, loss_recon: 0.612390, loss_pred: 0.107694
iteration 715: loss: 60.856106, loss_kl: 0.321405, loss_recon: 0.606068, loss_pred: 0.131566
iteration 716: loss: 60.414055, loss_kl: 0.284978, loss_recon: 0.601700, loss_pred: 0.139638
iteration 717: loss: 61.003681, loss_kl: 0.289947, loss_recon: 0.608321, loss_pred: 0.065377
iteration 718: loss: 60.037468, loss_kl: 0.146048, loss_recon: 0.598456, loss_pred: 0.138359
iteration 719: loss: 61.101707, loss_kl: 0.308459, loss_recon: 0.608552, loss_pred: 0.133471
iteration 720: loss: 61.235863, loss_kl: 0.139894, loss_recon: 0.610774, loss_pred: 0.107190
 36%|██████████▊                   | 72/200 [59:57<1:46:06, 49.74s/it]iteration 721: loss: 61.019455, loss_kl: 0.296865, loss_recon: 0.607874, loss_pred: 0.111526
iteration 722: loss: 60.143414, loss_kl: 0.147554, loss_recon: 0.599336, loss_pred: 0.149929
iteration 723: loss: 60.284424, loss_kl: 0.237290, loss_recon: 0.600369, loss_pred: 0.151161
iteration 724: loss: 61.801605, loss_kl: 0.221005, loss_recon: 0.616100, loss_pred: 0.101846
iteration 725: loss: 61.265675, loss_kl: 0.421291, loss_recon: 0.610252, loss_pred: 0.069422
iteration 726: loss: 62.065681, loss_kl: 0.344220, loss_recon: 0.618638, loss_pred: 0.062084
iteration 727: loss: 60.364601, loss_kl: 0.366279, loss_recon: 0.601282, loss_pred: 0.087669
iteration 728: loss: 60.211670, loss_kl: 0.347427, loss_recon: 0.599890, loss_pred: 0.081565
iteration 729: loss: 61.645424, loss_kl: 0.212401, loss_recon: 0.614574, loss_pred: 0.101789
iteration 730: loss: 61.189342, loss_kl: 0.038965, loss_recon: 0.610656, loss_pred: 0.107946
 36%|██████████▏                 | 73/200 [1:00:47<1:45:04, 49.64s/it]iteration 731: loss: 61.495590, loss_kl: 0.247135, loss_recon: 0.612958, loss_pred: 0.089631
iteration 732: loss: 61.626427, loss_kl: 0.371656, loss_recon: 0.613724, loss_pred: 0.088387
iteration 733: loss: 61.909554, loss_kl: 0.214012, loss_recon: 0.617212, loss_pred: 0.093038
iteration 734: loss: 61.233841, loss_kl: 0.137711, loss_recon: 0.610467, loss_pred: 0.125750
iteration 735: loss: 60.547626, loss_kl: 0.414269, loss_recon: 0.602848, loss_pred: 0.078189
iteration 736: loss: 60.537453, loss_kl: 0.363540, loss_recon: 0.602745, loss_pred: 0.100964
iteration 737: loss: 60.713734, loss_kl: 0.274940, loss_recon: 0.605050, loss_pred: 0.086232
iteration 738: loss: 60.160435, loss_kl: 0.288994, loss_recon: 0.599542, loss_pred: 0.077474
iteration 739: loss: 60.223957, loss_kl: 0.124159, loss_recon: 0.600606, loss_pred: 0.108056
iteration 740: loss: 63.113033, loss_kl: -0.042954, loss_recon: 0.630172, loss_pred: 0.115021
 37%|██████████▎                 | 74/200 [1:01:36<1:44:22, 49.71s/it]iteration 741: loss: 60.557327, loss_kl: 0.091820, loss_recon: 0.603792, loss_pred: 0.133536
iteration 742: loss: 59.302223, loss_kl: 0.160643, loss_recon: 0.590957, loss_pred: 0.128585
iteration 743: loss: 61.739315, loss_kl: 0.279303, loss_recon: 0.615138, loss_pred: 0.090035
iteration 744: loss: 60.661404, loss_kl: 0.261520, loss_recon: 0.604432, loss_pred: 0.091276
iteration 745: loss: 61.969269, loss_kl: 0.287769, loss_recon: 0.617511, loss_pred: 0.078511
iteration 746: loss: 60.691013, loss_kl: 0.304952, loss_recon: 0.604414, loss_pred: 0.101674
iteration 747: loss: 60.856602, loss_kl: 0.229232, loss_recon: 0.606755, loss_pred: 0.069829
iteration 748: loss: 61.242050, loss_kl: 0.233651, loss_recon: 0.610601, loss_pred: 0.068562
iteration 749: loss: 60.903511, loss_kl: 0.108372, loss_recon: 0.607902, loss_pred: 0.060740
iteration 750: loss: 63.318237, loss_kl: 0.362099, loss_recon: 0.630350, loss_pred: 0.107517
 38%|██████████▌                 | 75/200 [1:02:27<1:43:51, 49.85s/it]iteration 751: loss: 60.910980, loss_kl: 0.286490, loss_recon: 0.606645, loss_pred: 0.096176
iteration 752: loss: 61.540180, loss_kl: 0.454537, loss_recon: 0.612291, loss_pred: 0.072590
iteration 753: loss: 61.837509, loss_kl: 0.220280, loss_recon: 0.616221, loss_pred: 0.099768
iteration 754: loss: 60.452847, loss_kl: 0.040773, loss_recon: 0.603525, loss_pred: 0.079000
iteration 755: loss: 61.219215, loss_kl: 0.283496, loss_recon: 0.609893, loss_pred: 0.081113
iteration 756: loss: 59.905384, loss_kl: 0.545428, loss_recon: 0.595194, loss_pred: 0.099729
iteration 757: loss: 60.826557, loss_kl: 0.180017, loss_recon: 0.606540, loss_pred: 0.078082
iteration 758: loss: 61.871620, loss_kl: 0.212816, loss_recon: 0.616638, loss_pred: 0.096122
iteration 759: loss: 60.171562, loss_kl: 0.319734, loss_recon: 0.598996, loss_pred: 0.104142
iteration 760: loss: 62.312576, loss_kl: 0.303175, loss_recon: 0.620353, loss_pred: 0.118124
 38%|██████████▋                 | 76/200 [1:03:18<1:43:38, 50.15s/it]iteration 761: loss: 60.743156, loss_kl: 0.286575, loss_recon: 0.604515, loss_pred: 0.129886
iteration 762: loss: 61.132816, loss_kl: 0.436587, loss_recon: 0.607772, loss_pred: 0.109229
iteration 763: loss: 61.518425, loss_kl: 0.192751, loss_recon: 0.613381, loss_pred: 0.071547
iteration 764: loss: 60.955894, loss_kl: 0.076882, loss_recon: 0.608216, loss_pred: 0.090924
iteration 765: loss: 62.052425, loss_kl: 0.270854, loss_recon: 0.618183, loss_pred: 0.081236
iteration 766: loss: 60.516590, loss_kl: 0.111767, loss_recon: 0.603642, loss_pred: 0.089352
iteration 767: loss: 59.899200, loss_kl: 0.236590, loss_recon: 0.596716, loss_pred: 0.094097
iteration 768: loss: 60.847717, loss_kl: 0.419463, loss_recon: 0.604957, loss_pred: 0.115299
iteration 769: loss: 61.496929, loss_kl: 0.266607, loss_recon: 0.612743, loss_pred: 0.072107
iteration 770: loss: 60.255581, loss_kl: 0.154929, loss_recon: 0.600787, loss_pred: 0.089477
 38%|██████████▊                 | 77/200 [1:04:07<1:42:33, 50.03s/it]iteration 771: loss: 60.630726, loss_kl: 0.245208, loss_recon: 0.603568, loss_pred: 0.125854
iteration 772: loss: 60.362114, loss_kl: 0.120147, loss_recon: 0.601929, loss_pred: 0.096631
iteration 773: loss: 61.018749, loss_kl: 0.311516, loss_recon: 0.606847, loss_pred: 0.145890
iteration 774: loss: 61.208458, loss_kl: 0.098943, loss_recon: 0.610193, loss_pred: 0.129404
iteration 775: loss: 61.377838, loss_kl: 0.279792, loss_recon: 0.610632, loss_pred: 0.145640
iteration 776: loss: 61.609386, loss_kl: 0.373805, loss_recon: 0.612207, loss_pred: 0.162943
iteration 777: loss: 61.421959, loss_kl: 0.149442, loss_recon: 0.612245, loss_pred: 0.107162
iteration 778: loss: 60.400070, loss_kl: 0.303210, loss_recon: 0.600598, loss_pred: 0.157086
iteration 779: loss: 61.410843, loss_kl: 0.229487, loss_recon: 0.611114, loss_pred: 0.160847
iteration 780: loss: 61.533028, loss_kl: -0.104879, loss_recon: 0.614973, loss_pred: 0.099111
 39%|██████████▉                 | 78/200 [1:04:56<1:41:07, 49.73s/it]iteration 781: loss: 60.251087, loss_kl: 0.397827, loss_recon: 0.598688, loss_pred: 0.126242
iteration 782: loss: 60.589607, loss_kl: 0.316326, loss_recon: 0.602205, loss_pred: 0.165559
iteration 783: loss: 60.713493, loss_kl: 0.157649, loss_recon: 0.605357, loss_pred: 0.076336
iteration 784: loss: 60.682907, loss_kl: 0.236064, loss_recon: 0.604532, loss_pred: 0.077733
iteration 785: loss: 62.143967, loss_kl: 0.301806, loss_recon: 0.618485, loss_pred: 0.101259
iteration 786: loss: 60.575169, loss_kl: 0.119887, loss_recon: 0.604226, loss_pred: 0.075460
iteration 787: loss: 61.438770, loss_kl: 0.228674, loss_recon: 0.611946, loss_pred: 0.097027
iteration 788: loss: 61.140720, loss_kl: 0.156257, loss_recon: 0.609696, loss_pred: 0.070521
iteration 789: loss: 61.031872, loss_kl: 0.167920, loss_recon: 0.608205, loss_pred: 0.103346
iteration 790: loss: 63.842957, loss_kl: 0.255727, loss_recon: 0.636313, loss_pred: 0.047110
 40%|███████████                 | 79/200 [1:05:46<1:40:14, 49.71s/it]iteration 791: loss: 61.519630, loss_kl: 0.006967, loss_recon: 0.613517, loss_pred: 0.163213
iteration 792: loss: 60.049660, loss_kl: 0.143472, loss_recon: 0.598002, loss_pred: 0.151396
iteration 793: loss: 61.176479, loss_kl: 0.245173, loss_recon: 0.608792, loss_pred: 0.129759
iteration 794: loss: 60.524200, loss_kl: 0.347286, loss_recon: 0.601851, loss_pred: 0.101819
iteration 795: loss: 60.450176, loss_kl: 0.275589, loss_recon: 0.601570, loss_pred: 0.104873
iteration 796: loss: 61.566433, loss_kl: 0.271119, loss_recon: 0.613099, loss_pred: 0.071260
iteration 797: loss: 61.965111, loss_kl: 0.273425, loss_recon: 0.616850, loss_pred: 0.093271
iteration 798: loss: 61.253899, loss_kl: 0.264062, loss_recon: 0.609656, loss_pred: 0.107892
iteration 799: loss: 61.445576, loss_kl: 0.281973, loss_recon: 0.611441, loss_pred: 0.108859
iteration 800: loss: 61.233841, loss_kl: 0.138255, loss_recon: 0.609836, loss_pred: 0.155789
 40%|███████████▏                | 80/200 [1:06:36<1:39:43, 49.86s/it]iteration 801: loss: 59.766541, loss_kl: 0.236661, loss_recon: 0.594679, loss_pred: 0.127532
iteration 802: loss: 61.230751, loss_kl: 0.117178, loss_recon: 0.609669, loss_pred: 0.179201
iteration 803: loss: 61.453209, loss_kl: 0.302592, loss_recon: 0.611540, loss_pred: 0.080452
iteration 804: loss: 60.509544, loss_kl: 0.034644, loss_recon: 0.603816, loss_pred: 0.102934
iteration 805: loss: 60.717312, loss_kl: 0.155281, loss_recon: 0.605144, loss_pred: 0.090657
iteration 806: loss: 61.947582, loss_kl: 0.208522, loss_recon: 0.617142, loss_pred: 0.082636
iteration 807: loss: 60.907665, loss_kl: 0.416527, loss_recon: 0.605263, loss_pred: 0.080257
iteration 808: loss: 60.665592, loss_kl: 0.112327, loss_recon: 0.604754, loss_pred: 0.109030
iteration 809: loss: 61.885487, loss_kl: 0.156731, loss_recon: 0.616830, loss_pred: 0.089196
iteration 810: loss: 62.825661, loss_kl: -0.058880, loss_recon: 0.626536, loss_pred: 0.214598
 40%|███████████▎                | 81/200 [1:07:26<1:38:59, 49.91s/it]iteration 811: loss: 60.673363, loss_kl: 0.196904, loss_recon: 0.604171, loss_pred: 0.106122
iteration 812: loss: 61.249374, loss_kl: 0.263485, loss_recon: 0.608946, loss_pred: 0.153896
iteration 813: loss: 61.189171, loss_kl: 0.207823, loss_recon: 0.608927, loss_pred: 0.138020
iteration 814: loss: 61.359886, loss_kl: 0.405495, loss_recon: 0.609893, loss_pred: 0.061438
iteration 815: loss: 61.109749, loss_kl: 0.274995, loss_recon: 0.608079, loss_pred: 0.092208
iteration 816: loss: 61.094685, loss_kl: 0.330411, loss_recon: 0.607583, loss_pred: 0.084464
iteration 817: loss: 61.546398, loss_kl: 0.353195, loss_recon: 0.611936, loss_pred: 0.083475
iteration 818: loss: 61.677895, loss_kl: 0.146387, loss_recon: 0.614997, loss_pred: 0.066619
iteration 819: loss: 61.176186, loss_kl: 0.227023, loss_recon: 0.608984, loss_pred: 0.104708
iteration 820: loss: 58.864456, loss_kl: 0.233157, loss_recon: 0.585205, loss_pred: 0.166236
 41%|███████████▍                | 82/200 [1:08:16<1:38:12, 49.94s/it]iteration 821: loss: 61.274067, loss_kl: 0.172411, loss_recon: 0.610510, loss_pred: 0.084829
iteration 822: loss: 60.958275, loss_kl: 0.098089, loss_recon: 0.607733, loss_pred: 0.106338
iteration 823: loss: 61.151432, loss_kl: 0.032705, loss_recon: 0.610010, loss_pred: 0.124197
iteration 824: loss: 60.748268, loss_kl: 0.369530, loss_recon: 0.603277, loss_pred: 0.124236
iteration 825: loss: 61.097332, loss_kl: 0.174580, loss_recon: 0.608439, loss_pred: 0.113421
iteration 826: loss: 59.222267, loss_kl: 0.306402, loss_recon: 0.588572, loss_pred: 0.119303
iteration 827: loss: 62.071529, loss_kl: 0.609186, loss_recon: 0.614491, loss_pred: 0.133876
iteration 828: loss: 61.431976, loss_kl: 0.125427, loss_recon: 0.612010, loss_pred: 0.130334
iteration 829: loss: 61.318810, loss_kl: 0.217834, loss_recon: 0.610310, loss_pred: 0.113149
iteration 830: loss: 60.676964, loss_kl: 0.437251, loss_recon: 0.602072, loss_pred: 0.119091
 42%|███████████▌                | 83/200 [1:09:05<1:36:50, 49.66s/it]iteration 831: loss: 61.089554, loss_kl: 0.251537, loss_recon: 0.607529, loss_pred: 0.124927
iteration 832: loss: 60.447186, loss_kl: 0.253768, loss_recon: 0.600537, loss_pred: 0.179948
iteration 833: loss: 60.741882, loss_kl: 0.160540, loss_recon: 0.604452, loss_pred: 0.161614
iteration 834: loss: 61.358093, loss_kl: 0.163158, loss_recon: 0.611448, loss_pred: 0.075937
iteration 835: loss: 59.843616, loss_kl: -0.065372, loss_recon: 0.597091, loss_pred: 0.189517
iteration 836: loss: 61.486942, loss_kl: 0.094354, loss_recon: 0.612831, loss_pred: 0.124412
iteration 837: loss: 60.256245, loss_kl: 0.321579, loss_recon: 0.598571, loss_pred: 0.128510
iteration 838: loss: 62.445141, loss_kl: 0.135144, loss_recon: 0.622364, loss_pred: 0.095009
iteration 839: loss: 61.308083, loss_kl: 0.056861, loss_recon: 0.611509, loss_pred: 0.109303
iteration 840: loss: 61.400963, loss_kl: 0.277441, loss_recon: 0.610412, loss_pred: 0.126281
 42%|███████████▊                | 84/200 [1:09:55<1:35:51, 49.58s/it]iteration 841: loss: 60.868519, loss_kl: 0.126469, loss_recon: 0.605672, loss_pred: 0.189855
iteration 842: loss: 61.174412, loss_kl: 0.102538, loss_recon: 0.609317, loss_pred: 0.152339
iteration 843: loss: 60.709072, loss_kl: 0.151211, loss_recon: 0.604597, loss_pred: 0.116136
iteration 844: loss: 60.808720, loss_kl: 0.061256, loss_recon: 0.606544, loss_pred: 0.100364
iteration 845: loss: 61.672554, loss_kl: 0.157940, loss_recon: 0.614271, loss_pred: 0.106317
iteration 846: loss: 60.713268, loss_kl: 0.323347, loss_recon: 0.603076, loss_pred: 0.120783
iteration 847: loss: 60.987930, loss_kl: 0.095882, loss_recon: 0.607656, loss_pred: 0.137811
iteration 848: loss: 61.728569, loss_kl: 0.303593, loss_recon: 0.613023, loss_pred: 0.158783
iteration 849: loss: 60.406776, loss_kl: 0.175262, loss_recon: 0.600939, loss_pred: 0.158447
iteration 850: loss: 63.656654, loss_kl: -0.017164, loss_recon: 0.635551, loss_pred: 0.116667
 42%|███████████▉                | 85/200 [1:10:44<1:34:59, 49.56s/it]iteration 851: loss: 61.939281, loss_kl: 0.227111, loss_recon: 0.616474, loss_pred: 0.082755
iteration 852: loss: 61.255489, loss_kl: 0.325748, loss_recon: 0.608658, loss_pred: 0.089765
iteration 853: loss: 61.550419, loss_kl: 0.263304, loss_recon: 0.612307, loss_pred: 0.077256
iteration 854: loss: 59.373024, loss_kl: 0.146591, loss_recon: 0.591412, loss_pred: 0.096875
iteration 855: loss: 59.982204, loss_kl: 0.078258, loss_recon: 0.597983, loss_pred: 0.111886
iteration 856: loss: 62.128151, loss_kl: 0.091241, loss_recon: 0.619581, loss_pred: 0.085992
iteration 857: loss: 61.751469, loss_kl: 0.224682, loss_recon: 0.614322, loss_pred: 0.112402
iteration 858: loss: 59.584942, loss_kl: 0.168998, loss_recon: 0.592934, loss_pred: 0.135929
iteration 859: loss: 61.020908, loss_kl: 0.167894, loss_recon: 0.607754, loss_pred: 0.090942
iteration 860: loss: 62.597122, loss_kl: -0.037804, loss_recon: 0.625440, loss_pred: 0.087955
 43%|████████████                | 86/200 [1:11:33<1:34:01, 49.49s/it]iteration 861: loss: 61.598190, loss_kl: 0.126847, loss_recon: 0.613431, loss_pred: 0.133258
iteration 862: loss: 61.480145, loss_kl: 0.100972, loss_recon: 0.612830, loss_pred: 0.100155
iteration 863: loss: 61.262131, loss_kl: 0.205844, loss_recon: 0.609701, loss_pred: 0.094331
iteration 864: loss: 61.225307, loss_kl: 0.205891, loss_recon: 0.609508, loss_pred: 0.076723
iteration 865: loss: 61.355087, loss_kl: 0.268588, loss_recon: 0.609808, loss_pred: 0.116384
iteration 866: loss: 61.056889, loss_kl: 0.170834, loss_recon: 0.608092, loss_pred: 0.083607
iteration 867: loss: 61.136913, loss_kl: 0.206153, loss_recon: 0.608616, loss_pred: 0.077329
iteration 868: loss: 59.883778, loss_kl: 0.216827, loss_recon: 0.595916, loss_pred: 0.083952
iteration 869: loss: 59.782555, loss_kl: 0.202257, loss_recon: 0.594600, loss_pred: 0.128257
iteration 870: loss: 63.307037, loss_kl: 0.089236, loss_recon: 0.630919, loss_pred: 0.129404
 44%|████████████▏               | 87/200 [1:12:24<1:33:36, 49.71s/it]iteration 871: loss: 60.775169, loss_kl: 0.181642, loss_recon: 0.605129, loss_pred: 0.080641
iteration 872: loss: 61.381256, loss_kl: 0.125609, loss_recon: 0.611520, loss_pred: 0.103673
iteration 873: loss: 61.392811, loss_kl: 0.217717, loss_recon: 0.610780, loss_pred: 0.097124
iteration 874: loss: 59.161293, loss_kl: 0.215129, loss_recon: 0.587684, loss_pred: 0.177730
iteration 875: loss: 60.086941, loss_kl: 0.212527, loss_recon: 0.597239, loss_pred: 0.150462
iteration 876: loss: 61.859726, loss_kl: 0.294697, loss_recon: 0.614827, loss_pred: 0.082338
iteration 877: loss: 60.779137, loss_kl: 0.184222, loss_recon: 0.605143, loss_pred: 0.080647
iteration 878: loss: 62.923805, loss_kl: 0.024142, loss_recon: 0.628291, loss_pred: 0.070555
iteration 879: loss: 61.774220, loss_kl: 0.190499, loss_recon: 0.614695, loss_pred: 0.114261
iteration 880: loss: 59.775181, loss_kl: 0.303181, loss_recon: 0.592738, loss_pred: 0.198204
 44%|████████████▎               | 88/200 [1:13:13<1:32:47, 49.71s/it]iteration 881: loss: 60.884701, loss_kl: 0.152512, loss_recon: 0.606066, loss_pred: 0.125568
iteration 882: loss: 60.550156, loss_kl: 0.198959, loss_recon: 0.601628, loss_pred: 0.188438
iteration 883: loss: 60.483429, loss_kl: 0.133630, loss_recon: 0.602191, loss_pred: 0.130695
iteration 884: loss: 60.626781, loss_kl: 0.094676, loss_recon: 0.603930, loss_pred: 0.139086
iteration 885: loss: 61.499073, loss_kl: 0.100944, loss_recon: 0.612038, loss_pred: 0.194301
iteration 886: loss: 61.069187, loss_kl: 0.150766, loss_recon: 0.607848, loss_pred: 0.133617
iteration 887: loss: 62.039307, loss_kl: 0.104448, loss_recon: 0.618291, loss_pred: 0.105793
iteration 888: loss: 61.082523, loss_kl: 0.111255, loss_recon: 0.608869, loss_pred: 0.084398
iteration 889: loss: 61.306938, loss_kl: 0.134613, loss_recon: 0.610982, loss_pred: 0.074173
iteration 890: loss: 60.567429, loss_kl: 0.123375, loss_recon: 0.603363, loss_pred: 0.107718
 44%|████████████▍               | 89/200 [1:14:03<1:32:04, 49.77s/it]iteration 891: loss: 60.825050, loss_kl: 0.172556, loss_recon: 0.605554, loss_pred: 0.097137
iteration 892: loss: 60.239075, loss_kl: 0.206145, loss_recon: 0.599149, loss_pred: 0.118021
iteration 893: loss: 61.008923, loss_kl: 0.009204, loss_recon: 0.608824, loss_pred: 0.117298
iteration 894: loss: 61.024982, loss_kl: 0.119118, loss_recon: 0.607876, loss_pred: 0.118230
iteration 895: loss: 60.851982, loss_kl: 0.126595, loss_recon: 0.606050, loss_pred: 0.120426
iteration 896: loss: 61.927139, loss_kl: 0.254069, loss_recon: 0.615377, loss_pred: 0.135339
iteration 897: loss: 60.558960, loss_kl: 0.116005, loss_recon: 0.603395, loss_pred: 0.103427
iteration 898: loss: 60.585667, loss_kl: 0.055104, loss_recon: 0.604337, loss_pred: 0.096900
iteration 899: loss: 61.409428, loss_kl: 0.240394, loss_recon: 0.610626, loss_pred: 0.106410
iteration 900: loss: 63.586433, loss_kl: 0.301547, loss_recon: 0.631924, loss_pred: 0.092525
 45%|████████████▌               | 90/200 [1:14:53<1:31:15, 49.78s/it]iteration 901: loss: 60.497391, loss_kl: 0.154659, loss_recon: 0.602743, loss_pred: 0.068446
iteration 902: loss: 60.607059, loss_kl: 0.156219, loss_recon: 0.603361, loss_pred: 0.114718
iteration 903: loss: 61.922874, loss_kl: 0.173537, loss_recon: 0.616487, loss_pred: 0.100680
iteration 904: loss: 60.626724, loss_kl: 0.318959, loss_recon: 0.601766, loss_pred: 0.131138
iteration 905: loss: 60.918175, loss_kl: 0.150227, loss_recon: 0.606472, loss_pred: 0.120792
iteration 906: loss: 60.916206, loss_kl: 0.185794, loss_recon: 0.606094, loss_pred: 0.121011
iteration 907: loss: 60.708015, loss_kl: 0.190849, loss_recon: 0.604412, loss_pred: 0.075938
iteration 908: loss: 61.882820, loss_kl: 0.082724, loss_recon: 0.616778, loss_pred: 0.122247
iteration 909: loss: 60.703049, loss_kl: 0.170277, loss_recon: 0.604091, loss_pred: 0.123702
iteration 910: loss: 64.495926, loss_kl: 0.461483, loss_recon: 0.639752, loss_pred: 0.059209
 46%|████████████▋               | 91/200 [1:15:43<1:30:23, 49.76s/it]iteration 911: loss: 60.586800, loss_kl: 0.286420, loss_recon: 0.601823, loss_pred: 0.118120
iteration 912: loss: 62.145451, loss_kl: 0.188437, loss_recon: 0.617915, loss_pred: 0.165530
iteration 913: loss: 60.132214, loss_kl: 0.029961, loss_recon: 0.599426, loss_pred: 0.159647
iteration 914: loss: 61.207088, loss_kl: 0.033954, loss_recon: 0.610884, loss_pred: 0.084719
iteration 915: loss: 61.495029, loss_kl: 0.275287, loss_recon: 0.611302, loss_pred: 0.089587
iteration 916: loss: 60.338078, loss_kl: 0.255825, loss_recon: 0.599356, loss_pred: 0.146664
iteration 917: loss: 60.227432, loss_kl: 0.251493, loss_recon: 0.598022, loss_pred: 0.173754
iteration 918: loss: 62.173534, loss_kl: 0.257003, loss_recon: 0.618031, loss_pred: 0.113413
iteration 919: loss: 61.383808, loss_kl: 0.067630, loss_recon: 0.612244, loss_pred: 0.091750
iteration 920: loss: 63.306602, loss_kl: 0.073704, loss_recon: 0.630612, loss_pred: 0.171669
 46%|████████████▉               | 92/200 [1:16:33<1:29:42, 49.84s/it]iteration 921: loss: 60.353329, loss_kl: 0.055871, loss_recon: 0.601119, loss_pred: 0.185596
iteration 922: loss: 60.696884, loss_kl: 0.152111, loss_recon: 0.603876, loss_pred: 0.157141
iteration 923: loss: 61.177814, loss_kl: 0.141441, loss_recon: 0.609531, loss_pred: 0.083252
iteration 924: loss: 62.206390, loss_kl: 0.118822, loss_recon: 0.620104, loss_pred: 0.077136
iteration 925: loss: 61.178860, loss_kl: 0.205346, loss_recon: 0.608527, loss_pred: 0.120790
iteration 926: loss: 61.079395, loss_kl: 0.235432, loss_recon: 0.607637, loss_pred: 0.080244
iteration 927: loss: 61.386955, loss_kl: 0.393089, loss_recon: 0.608203, loss_pred: 0.173585
iteration 928: loss: 62.254669, loss_kl: 0.094446, loss_recon: 0.619997, loss_pred: 0.160487
iteration 929: loss: 59.628761, loss_kl: 0.090128, loss_recon: 0.594005, loss_pred: 0.138094
iteration 930: loss: 63.930500, loss_kl: 0.076621, loss_recon: 0.637025, loss_pred: 0.151399
 46%|█████████████               | 93/200 [1:17:22<1:28:39, 49.71s/it]iteration 931: loss: 61.555916, loss_kl: 0.155352, loss_recon: 0.612613, loss_pred: 0.139292
iteration 932: loss: 61.221416, loss_kl: 0.159246, loss_recon: 0.609202, loss_pred: 0.142009
iteration 933: loss: 61.301262, loss_kl: 0.194872, loss_recon: 0.609677, loss_pred: 0.138725
iteration 934: loss: 62.434505, loss_kl: 0.303123, loss_recon: 0.620137, loss_pred: 0.117654
iteration 935: loss: 61.633327, loss_kl: 0.206669, loss_recon: 0.613351, loss_pred: 0.091512
iteration 936: loss: 61.201103, loss_kl: 0.107856, loss_recon: 0.610228, loss_pred: 0.070413
iteration 937: loss: 59.963047, loss_kl: -0.006390, loss_recon: 0.598386, loss_pred: 0.130807
iteration 938: loss: 60.069103, loss_kl: -0.078266, loss_recon: 0.599976, loss_pred: 0.149756
iteration 939: loss: 61.130135, loss_kl: 0.105391, loss_recon: 0.608615, loss_pred: 0.163206
iteration 940: loss: 62.279335, loss_kl: 0.315268, loss_recon: 0.617653, loss_pred: 0.198798
 47%|█████████████▏              | 94/200 [1:18:12<1:27:58, 49.80s/it]iteration 941: loss: 61.038525, loss_kl: 0.056939, loss_recon: 0.607413, loss_pred: 0.240311
iteration 942: loss: 60.799065, loss_kl: 0.097536, loss_recon: 0.604840, loss_pred: 0.217562
iteration 943: loss: 61.703960, loss_kl: 0.123618, loss_recon: 0.614806, loss_pred: 0.099776
iteration 944: loss: 62.328117, loss_kl: 0.102406, loss_recon: 0.620490, loss_pred: 0.176694
iteration 945: loss: 60.267910, loss_kl: 0.168407, loss_recon: 0.599574, loss_pred: 0.142142
iteration 946: loss: 60.179321, loss_kl: 0.115408, loss_recon: 0.598975, loss_pred: 0.166457
iteration 947: loss: 60.353531, loss_kl: -0.009548, loss_recon: 0.602620, loss_pred: 0.101107
iteration 948: loss: 61.787155, loss_kl: 0.107333, loss_recon: 0.615933, loss_pred: 0.086533
iteration 949: loss: 61.093681, loss_kl: 0.084022, loss_recon: 0.608280, loss_pred: 0.181621
iteration 950: loss: 61.344593, loss_kl: 0.171913, loss_recon: 0.609563, loss_pred: 0.216413
 48%|█████████████▎              | 95/200 [1:19:02<1:27:05, 49.76s/it]iteration 951: loss: 61.039120, loss_kl: 0.185983, loss_recon: 0.607472, loss_pred: 0.105929
iteration 952: loss: 61.004745, loss_kl: 0.206702, loss_recon: 0.606461, loss_pred: 0.151976
iteration 953: loss: 59.831001, loss_kl: 0.246693, loss_recon: 0.594425, loss_pred: 0.141767
iteration 954: loss: 60.455639, loss_kl: 0.118634, loss_recon: 0.602401, loss_pred: 0.096948
iteration 955: loss: 60.320415, loss_kl: 0.066558, loss_recon: 0.601494, loss_pred: 0.104463
iteration 956: loss: 63.417366, loss_kl: 0.155622, loss_recon: 0.631526, loss_pred: 0.109194
iteration 957: loss: 61.244213, loss_kl: 0.125994, loss_recon: 0.609790, loss_pred: 0.139200
iteration 958: loss: 62.173588, loss_kl: 0.461441, loss_recon: 0.615867, loss_pred: 0.125460
iteration 959: loss: 61.163601, loss_kl: 0.326410, loss_recon: 0.607102, loss_pred: 0.127042
iteration 960: loss: 58.942539, loss_kl: 0.482202, loss_recon: 0.583224, loss_pred: 0.137987
 48%|█████████████▍              | 96/200 [1:19:52<1:26:39, 50.00s/it]iteration 961: loss: 60.548973, loss_kl: 0.044949, loss_recon: 0.604100, loss_pred: 0.094054
iteration 962: loss: 61.139381, loss_kl: 0.140650, loss_recon: 0.609253, loss_pred: 0.073443
iteration 963: loss: 61.372005, loss_kl: 0.038541, loss_recon: 0.612086, loss_pred: 0.124828
iteration 964: loss: 60.789886, loss_kl: 0.224902, loss_recon: 0.604386, loss_pred: 0.126370
iteration 965: loss: 60.799034, loss_kl: 0.169116, loss_recon: 0.605180, loss_pred: 0.111951
iteration 966: loss: 60.895763, loss_kl: 0.140752, loss_recon: 0.606497, loss_pred: 0.105269
iteration 967: loss: 61.254684, loss_kl: 0.211604, loss_recon: 0.609410, loss_pred: 0.102128
iteration 968: loss: 62.068710, loss_kl: 0.256899, loss_recon: 0.616703, loss_pred: 0.141494
iteration 969: loss: 60.080059, loss_kl: 0.155876, loss_recon: 0.597787, loss_pred: 0.145449
iteration 970: loss: 63.287655, loss_kl: 0.024001, loss_recon: 0.630679, loss_pred: 0.195722
 48%|█████████████▌              | 97/200 [1:20:42<1:25:25, 49.76s/it]iteration 971: loss: 61.674332, loss_kl: 0.171477, loss_recon: 0.614189, loss_pred: 0.083907
iteration 972: loss: 61.369572, loss_kl: 0.061831, loss_recon: 0.612074, loss_pred: 0.100367
iteration 973: loss: 60.733948, loss_kl: 0.138837, loss_recon: 0.604740, loss_pred: 0.121104
iteration 974: loss: 61.605148, loss_kl: 0.322835, loss_recon: 0.611770, loss_pred: 0.105272
iteration 975: loss: 59.556839, loss_kl: 0.088099, loss_recon: 0.593798, loss_pred: 0.088895
iteration 976: loss: 61.029991, loss_kl: 0.181488, loss_recon: 0.607265, loss_pred: 0.121968
iteration 977: loss: 60.742954, loss_kl: 0.142074, loss_recon: 0.605100, loss_pred: 0.090903
iteration 978: loss: 61.092419, loss_kl: 0.104619, loss_recon: 0.608512, loss_pred: 0.136639
iteration 979: loss: 61.338894, loss_kl: 0.134650, loss_recon: 0.610644, loss_pred: 0.139863
iteration 980: loss: 61.883858, loss_kl: -0.047978, loss_recon: 0.618443, loss_pred: 0.087550
 49%|█████████████▋              | 98/200 [1:21:31<1:24:30, 49.71s/it]iteration 981: loss: 61.951370, loss_kl: 0.136327, loss_recon: 0.617454, loss_pred: 0.069657
iteration 982: loss: 61.759789, loss_kl: 0.109865, loss_recon: 0.615163, loss_pred: 0.133621
iteration 983: loss: 61.196316, loss_kl: -0.052095, loss_recon: 0.611176, loss_pred: 0.130794
iteration 984: loss: 60.004902, loss_kl: 0.134115, loss_recon: 0.597558, loss_pred: 0.114981
iteration 985: loss: 61.548351, loss_kl: 0.152557, loss_recon: 0.613027, loss_pred: 0.093111
iteration 986: loss: 60.676350, loss_kl: 0.138661, loss_recon: 0.604087, loss_pred: 0.128948
iteration 987: loss: 59.716328, loss_kl: 0.002460, loss_recon: 0.595859, loss_pred: 0.128001
iteration 988: loss: 60.775982, loss_kl: 0.124934, loss_recon: 0.605331, loss_pred: 0.117906
iteration 989: loss: 60.449947, loss_kl: 0.102830, loss_recon: 0.602432, loss_pred: 0.103915
iteration 990: loss: 62.505047, loss_kl: 0.042699, loss_recon: 0.623804, loss_pred: 0.081992
 50%|█████████████▊              | 99/200 [1:22:21<1:23:50, 49.81s/it]iteration 991: loss: 60.171101, loss_kl: 0.045487, loss_recon: 0.599862, loss_pred: 0.139445
iteration 992: loss: 61.636009, loss_kl: 0.132934, loss_recon: 0.614124, loss_pred: 0.090668
iteration 993: loss: 61.261860, loss_kl: 0.103900, loss_recon: 0.610322, loss_pred: 0.125752
iteration 994: loss: 60.958050, loss_kl: 0.345097, loss_recon: 0.605041, loss_pred: 0.108889
iteration 995: loss: 61.998631, loss_kl: 0.157582, loss_recon: 0.616901, loss_pred: 0.150944
iteration 996: loss: 60.529602, loss_kl: 0.064436, loss_recon: 0.603037, loss_pred: 0.161434
iteration 997: loss: 61.857094, loss_kl: 0.296108, loss_recon: 0.614677, loss_pred: 0.093329
iteration 998: loss: 60.303520, loss_kl: 0.067135, loss_recon: 0.600571, loss_pred: 0.179255
iteration 999: loss: 61.236668, loss_kl: 0.336367, loss_recon: 0.607316, loss_pred: 0.168699
iteration 1000: loss: 60.071136, loss_kl: -0.025388, loss_recon: 0.600263, loss_pred: 0.070211
 50%|█████████████▌             | 100/200 [1:23:12<1:23:15, 49.95s/it]iteration 1001: loss: 60.504032, loss_kl: 0.176753, loss_recon: 0.603532, loss_pred: 0.149095
iteration 1002: loss: 60.908089, loss_kl: 0.232160, loss_recon: 0.608322, loss_pred: 0.073539
iteration 1003: loss: 61.630871, loss_kl: 0.062990, loss_recon: 0.615541, loss_pred: 0.076185
iteration 1004: loss: 61.117397, loss_kl: 0.167792, loss_recon: 0.609833, loss_pred: 0.132424
iteration 1005: loss: 60.876144, loss_kl: 0.008175, loss_recon: 0.607778, loss_pred: 0.098263
iteration 1006: loss: 60.844929, loss_kl: 0.214772, loss_recon: 0.607333, loss_pred: 0.109439
iteration 1007: loss: 61.524422, loss_kl: 0.220232, loss_recon: 0.614463, loss_pred: 0.075915
iteration 1008: loss: 60.561359, loss_kl: 0.150071, loss_recon: 0.604569, loss_pred: 0.102923
iteration 1009: loss: 60.372887, loss_kl: 0.277173, loss_recon: 0.602204, loss_pred: 0.149754
iteration 1010: loss: 63.613632, loss_kl: 0.329316, loss_recon: 0.634547, loss_pred: 0.155618
 50%|█████████████▋             | 101/200 [1:24:02<1:22:32, 50.02s/it]iteration 1011: loss: 60.980438, loss_kl: 0.265585, loss_recon: 0.607385, loss_pred: 0.239264
iteration 1012: loss: 60.798611, loss_kl: 0.158280, loss_recon: 0.606100, loss_pred: 0.187010
iteration 1013: loss: 61.252472, loss_kl: 0.194055, loss_recon: 0.611057, loss_pred: 0.144835
iteration 1014: loss: 59.897789, loss_kl: 0.235242, loss_recon: 0.594261, loss_pred: 0.469288
iteration 1015: loss: 62.110832, loss_kl: 0.245577, loss_recon: 0.620065, loss_pred: 0.101921
iteration 1016: loss: 61.698753, loss_kl: -0.001181, loss_recon: 0.616071, loss_pred: 0.091679
iteration 1017: loss: 59.985596, loss_kl: 0.292868, loss_recon: 0.598350, loss_pred: 0.147643
iteration 1018: loss: 60.571808, loss_kl: 0.147143, loss_recon: 0.604874, loss_pred: 0.082907
iteration 1019: loss: 61.723911, loss_kl: 0.194930, loss_recon: 0.616526, loss_pred: 0.069314
iteration 1020: loss: 62.120735, loss_kl: 0.412006, loss_recon: 0.620061, loss_pred: 0.110472
 51%|█████████████▊             | 102/200 [1:24:52<1:21:35, 49.95s/it]iteration 1021: loss: 60.869801, loss_kl: 0.382383, loss_recon: 0.607689, loss_pred: 0.097035
iteration 1022: loss: 60.660080, loss_kl: 0.340558, loss_recon: 0.606060, loss_pred: 0.050690
iteration 1023: loss: 60.962219, loss_kl: 0.254336, loss_recon: 0.607842, loss_pred: 0.175507
iteration 1024: loss: 61.260075, loss_kl: 0.197359, loss_recon: 0.611354, loss_pred: 0.122706
iteration 1025: loss: 61.922394, loss_kl: 0.084478, loss_recon: 0.618453, loss_pred: 0.076256
iteration 1026: loss: 61.026730, loss_kl: 0.254402, loss_recon: 0.609014, loss_pred: 0.122755
iteration 1027: loss: 60.315254, loss_kl: 0.179003, loss_recon: 0.601243, loss_pred: 0.189129
iteration 1028: loss: 61.153622, loss_kl: 0.171090, loss_recon: 0.610743, loss_pred: 0.077589
iteration 1029: loss: 59.581398, loss_kl: 0.141100, loss_recon: 0.594425, loss_pred: 0.137465
iteration 1030: loss: 61.950539, loss_kl: 0.290282, loss_recon: 0.618237, loss_pred: 0.123887
 52%|█████████████▉             | 103/200 [1:25:42<1:20:47, 49.98s/it]iteration 1031: loss: 60.822544, loss_kl: 0.244451, loss_recon: 0.607166, loss_pred: 0.103499
iteration 1032: loss: 61.149162, loss_kl: 0.178218, loss_recon: 0.610747, loss_pred: 0.072701
iteration 1033: loss: 60.783134, loss_kl: 0.217282, loss_recon: 0.606617, loss_pred: 0.119262
iteration 1034: loss: 61.433121, loss_kl: 0.313004, loss_recon: 0.613243, loss_pred: 0.105721
iteration 1035: loss: 59.807602, loss_kl: 0.135885, loss_recon: 0.596600, loss_pred: 0.146244
iteration 1036: loss: 60.767670, loss_kl: 0.173833, loss_recon: 0.606831, loss_pred: 0.082783
iteration 1037: loss: 60.915829, loss_kl: 0.260752, loss_recon: 0.608315, loss_pred: 0.081737
iteration 1038: loss: 61.722973, loss_kl: 0.140183, loss_recon: 0.616263, loss_pred: 0.095267
iteration 1039: loss: 60.693111, loss_kl: 0.028574, loss_recon: 0.605765, loss_pred: 0.116334
iteration 1040: loss: 60.772724, loss_kl: 0.290981, loss_recon: 0.605829, loss_pred: 0.186878
 52%|██████████████             | 104/200 [1:26:31<1:19:52, 49.93s/it]iteration 1041: loss: 61.579063, loss_kl: 0.314469, loss_recon: 0.614852, loss_pred: 0.090744
iteration 1042: loss: 60.130192, loss_kl: 0.327059, loss_recon: 0.600063, loss_pred: 0.120580
iteration 1043: loss: 61.435390, loss_kl: 0.041280, loss_recon: 0.613164, loss_pred: 0.118617
iteration 1044: loss: 60.578510, loss_kl: 0.260351, loss_recon: 0.605183, loss_pred: 0.057624
iteration 1045: loss: 61.363987, loss_kl: 0.239022, loss_recon: 0.612938, loss_pred: 0.067792
iteration 1046: loss: 60.514423, loss_kl: 0.203723, loss_recon: 0.604151, loss_pred: 0.097257
iteration 1047: loss: 59.577816, loss_kl: 0.162797, loss_recon: 0.595065, loss_pred: 0.069666
iteration 1048: loss: 60.348045, loss_kl: 0.293432, loss_recon: 0.602182, loss_pred: 0.126919
iteration 1049: loss: 63.170750, loss_kl: 0.058305, loss_recon: 0.631021, loss_pred: 0.068094
iteration 1050: loss: 60.836460, loss_kl: 0.127589, loss_recon: 0.605726, loss_pred: 0.262544
 52%|██████████████▏            | 105/200 [1:27:21<1:18:57, 49.87s/it]iteration 1051: loss: 60.779678, loss_kl: 0.375046, loss_recon: 0.606994, loss_pred: 0.076488
iteration 1052: loss: 59.894547, loss_kl: 0.360745, loss_recon: 0.597551, loss_pred: 0.135870
iteration 1053: loss: 60.364403, loss_kl: 0.110160, loss_recon: 0.602523, loss_pred: 0.110993
iteration 1054: loss: 61.311283, loss_kl: 0.293052, loss_recon: 0.612227, loss_pred: 0.085608
iteration 1055: loss: 61.118790, loss_kl: 0.274654, loss_recon: 0.610599, loss_pred: 0.056100
iteration 1056: loss: 61.155102, loss_kl: 0.154471, loss_recon: 0.610372, loss_pred: 0.116311
iteration 1057: loss: 59.966827, loss_kl: 0.341640, loss_recon: 0.598378, loss_pred: 0.125573
iteration 1058: loss: 60.486927, loss_kl: 0.121096, loss_recon: 0.603964, loss_pred: 0.089297
iteration 1059: loss: 62.139370, loss_kl: 0.175140, loss_recon: 0.620766, loss_pred: 0.061020
iteration 1060: loss: 63.391125, loss_kl: 0.120830, loss_recon: 0.633062, loss_pred: 0.083764
 53%|██████████████▎            | 106/200 [1:28:11<1:17:58, 49.77s/it]iteration 1061: loss: 61.191074, loss_kl: 0.230606, loss_recon: 0.611220, loss_pred: 0.066752
iteration 1062: loss: 62.930195, loss_kl: 0.225560, loss_recon: 0.628637, loss_pred: 0.064271
iteration 1063: loss: 60.880592, loss_kl: 0.327436, loss_recon: 0.607620, loss_pred: 0.115356
iteration 1064: loss: 60.431236, loss_kl: 0.367242, loss_recon: 0.603066, loss_pred: 0.120981
iteration 1065: loss: 59.601601, loss_kl: 0.152381, loss_recon: 0.594764, loss_pred: 0.123722
iteration 1066: loss: 61.316856, loss_kl: 0.076150, loss_recon: 0.612289, loss_pred: 0.087191
iteration 1067: loss: 60.742786, loss_kl: 0.151951, loss_recon: 0.605960, loss_pred: 0.145235
iteration 1068: loss: 60.173012, loss_kl: 0.463936, loss_recon: 0.600434, loss_pred: 0.124962
iteration 1069: loss: 61.441154, loss_kl: 0.227591, loss_recon: 0.613647, loss_pred: 0.074133
iteration 1070: loss: 61.653084, loss_kl: 0.377723, loss_recon: 0.614744, loss_pred: 0.174869
 54%|██████████████▍            | 107/200 [1:29:00<1:17:00, 49.68s/it]iteration 1071: loss: 60.760265, loss_kl: 0.327801, loss_recon: 0.606259, loss_pred: 0.131054
iteration 1072: loss: 62.200069, loss_kl: 0.223814, loss_recon: 0.620885, loss_pred: 0.109341
iteration 1073: loss: 61.161175, loss_kl: 0.329093, loss_recon: 0.610058, loss_pred: 0.152045
iteration 1074: loss: 60.729965, loss_kl: 0.337452, loss_recon: 0.606231, loss_pred: 0.103536
iteration 1075: loss: 60.770641, loss_kl: 0.335922, loss_recon: 0.606679, loss_pred: 0.099405
iteration 1076: loss: 60.083374, loss_kl: 0.195267, loss_recon: 0.600029, loss_pred: 0.078486
iteration 1077: loss: 62.252850, loss_kl: 0.058197, loss_recon: 0.621675, loss_pred: 0.084729
iteration 1078: loss: 59.754803, loss_kl: 0.209794, loss_recon: 0.596794, loss_pred: 0.073271
iteration 1079: loss: 60.802345, loss_kl: 0.171291, loss_recon: 0.607234, loss_pred: 0.077267
iteration 1080: loss: 59.223045, loss_kl: 0.460482, loss_recon: 0.591339, loss_pred: 0.084531
 54%|██████████████▌            | 108/200 [1:29:49<1:15:55, 49.52s/it]iteration 1081: loss: 61.077637, loss_kl: 0.209376, loss_recon: 0.610124, loss_pred: 0.063093
iteration 1082: loss: 60.163803, loss_kl: 0.336900, loss_recon: 0.600928, loss_pred: 0.067668
iteration 1083: loss: 61.723770, loss_kl: 0.246473, loss_recon: 0.616587, loss_pred: 0.062647
iteration 1084: loss: 60.592777, loss_kl: 0.126746, loss_recon: 0.604991, loss_pred: 0.092365
iteration 1085: loss: 60.418877, loss_kl: 0.223483, loss_recon: 0.603276, loss_pred: 0.089078
iteration 1086: loss: 61.093853, loss_kl: 0.119435, loss_recon: 0.610211, loss_pred: 0.071595
iteration 1087: loss: 59.874668, loss_kl: 0.245065, loss_recon: 0.597968, loss_pred: 0.075452
iteration 1088: loss: 61.606365, loss_kl: 0.463838, loss_recon: 0.615133, loss_pred: 0.088460
iteration 1089: loss: 60.533669, loss_kl: 0.074542, loss_recon: 0.604802, loss_pred: 0.052693
iteration 1090: loss: 61.999062, loss_kl: -0.058445, loss_recon: 0.619401, loss_pred: 0.059571
 55%|██████████████▋            | 109/200 [1:30:39<1:14:59, 49.44s/it]iteration 1091: loss: 61.301933, loss_kl: 0.222611, loss_recon: 0.612336, loss_pred: 0.066149
iteration 1092: loss: 60.843159, loss_kl: 0.331072, loss_recon: 0.607576, loss_pred: 0.082262
iteration 1093: loss: 60.178600, loss_kl: 0.404334, loss_recon: 0.600805, loss_pred: 0.094059
iteration 1094: loss: 61.215130, loss_kl: 0.336972, loss_recon: 0.610999, loss_pred: 0.111837
iteration 1095: loss: 61.500076, loss_kl: 0.160663, loss_recon: 0.613968, loss_pred: 0.101628
iteration 1096: loss: 60.228058, loss_kl: 0.111182, loss_recon: 0.601541, loss_pred: 0.072832
iteration 1097: loss: 62.352409, loss_kl: 0.127279, loss_recon: 0.622620, loss_pred: 0.089127
iteration 1098: loss: 60.055660, loss_kl: 0.283080, loss_recon: 0.599437, loss_pred: 0.109160
iteration 1099: loss: 59.390835, loss_kl: 0.257498, loss_recon: 0.592795, loss_pred: 0.108715
iteration 1100: loss: 61.702831, loss_kl: 0.215152, loss_recon: 0.615865, loss_pred: 0.114152
 55%|██████████████▊            | 110/200 [1:31:30<1:15:07, 50.09s/it]iteration 1101: loss: 60.557976, loss_kl: 0.088905, loss_recon: 0.604749, loss_pred: 0.082151
iteration 1102: loss: 60.510979, loss_kl: 0.522630, loss_recon: 0.604135, loss_pred: 0.092304
iteration 1103: loss: 60.135544, loss_kl: 0.063635, loss_recon: 0.600537, loss_pred: 0.081241
iteration 1104: loss: 60.869286, loss_kl: 0.270188, loss_recon: 0.607661, loss_pred: 0.100534
iteration 1105: loss: 59.926857, loss_kl: 0.254222, loss_recon: 0.598345, loss_pred: 0.089790
iteration 1106: loss: 60.270794, loss_kl: 0.178797, loss_recon: 0.602050, loss_pred: 0.064045
iteration 1107: loss: 61.390331, loss_kl: 0.211215, loss_recon: 0.613168, loss_pred: 0.071400
iteration 1108: loss: 61.432735, loss_kl: 0.141202, loss_recon: 0.613475, loss_pred: 0.083817
iteration 1109: loss: 62.437725, loss_kl: 0.135090, loss_recon: 0.623247, loss_pred: 0.111706
iteration 1110: loss: 61.030846, loss_kl: 0.240984, loss_recon: 0.609674, loss_pred: 0.061031
 56%|██████████████▉            | 111/200 [1:32:20<1:14:20, 50.12s/it]iteration 1111: loss: 60.383389, loss_kl: 0.218301, loss_recon: 0.603173, loss_pred: 0.063868
iteration 1112: loss: 61.883373, loss_kl: 0.247734, loss_recon: 0.617537, loss_pred: 0.127171
iteration 1113: loss: 61.381897, loss_kl: 0.203925, loss_recon: 0.612810, loss_pred: 0.098883
iteration 1114: loss: 60.300819, loss_kl: 0.230652, loss_recon: 0.602337, loss_pred: 0.064761
iteration 1115: loss: 60.814110, loss_kl: 0.237703, loss_recon: 0.607649, loss_pred: 0.046819
iteration 1116: loss: 61.693939, loss_kl: 0.132242, loss_recon: 0.616370, loss_pred: 0.055662
iteration 1117: loss: 60.744476, loss_kl: 0.250094, loss_recon: 0.606423, loss_pred: 0.099678
iteration 1118: loss: 60.368767, loss_kl: 0.249363, loss_recon: 0.602938, loss_pred: 0.072450
iteration 1119: loss: 60.072300, loss_kl: 0.051679, loss_recon: 0.600080, loss_pred: 0.063734
iteration 1120: loss: 60.286861, loss_kl: 0.626332, loss_recon: 0.602279, loss_pred: 0.052684
 56%|███████████████            | 112/200 [1:33:09<1:13:02, 49.80s/it]iteration 1121: loss: 60.556843, loss_kl: 0.135021, loss_recon: 0.604747, loss_pred: 0.080841
iteration 1122: loss: 60.891815, loss_kl: 0.191514, loss_recon: 0.608199, loss_pred: 0.070049
iteration 1123: loss: 60.861874, loss_kl: 0.232687, loss_recon: 0.607795, loss_pred: 0.080077
iteration 1124: loss: 61.818462, loss_kl: 0.279120, loss_recon: 0.617459, loss_pred: 0.069802
iteration 1125: loss: 60.387161, loss_kl: 0.289428, loss_recon: 0.602991, loss_pred: 0.085198
iteration 1126: loss: 61.662560, loss_kl: 0.119081, loss_recon: 0.615238, loss_pred: 0.137594
iteration 1127: loss: 60.608032, loss_kl: 0.251997, loss_recon: 0.604790, loss_pred: 0.126504
iteration 1128: loss: 59.966343, loss_kl: 0.051554, loss_recon: 0.598801, loss_pred: 0.085770
iteration 1129: loss: 60.279907, loss_kl: 0.397671, loss_recon: 0.601862, loss_pred: 0.089781
iteration 1130: loss: 61.236298, loss_kl: 0.054985, loss_recon: 0.611789, loss_pred: 0.056862
 56%|███████████████▎           | 113/200 [1:33:59<1:12:17, 49.86s/it]iteration 1131: loss: 60.821442, loss_kl: 0.281977, loss_recon: 0.607302, loss_pred: 0.077209
iteration 1132: loss: 62.417866, loss_kl: 0.277351, loss_recon: 0.622974, loss_pred: 0.106702
iteration 1133: loss: 61.353809, loss_kl: 0.330784, loss_recon: 0.612723, loss_pred: 0.065089
iteration 1134: loss: 61.536049, loss_kl: 0.148028, loss_recon: 0.614469, loss_pred: 0.081838
iteration 1135: loss: 59.670776, loss_kl: 0.288127, loss_recon: 0.595512, loss_pred: 0.105292
iteration 1136: loss: 60.364029, loss_kl: 0.289996, loss_recon: 0.602473, loss_pred: 0.102383
iteration 1137: loss: 58.938728, loss_kl: 0.159725, loss_recon: 0.588481, loss_pred: 0.082724
iteration 1138: loss: 60.103230, loss_kl: 0.252214, loss_recon: 0.600068, loss_pred: 0.083954
iteration 1139: loss: 61.709530, loss_kl: 0.481390, loss_recon: 0.615772, loss_pred: 0.108470
iteration 1140: loss: 63.091766, loss_kl: 0.145329, loss_recon: 0.629937, loss_pred: 0.090890
 57%|███████████████▍           | 114/200 [1:34:49<1:11:21, 49.78s/it]iteration 1141: loss: 60.107113, loss_kl: 0.395806, loss_recon: 0.599950, loss_pred: 0.076775
iteration 1142: loss: 60.702946, loss_kl: 0.229017, loss_recon: 0.605897, loss_pred: 0.092769
iteration 1143: loss: 60.945915, loss_kl: 0.161217, loss_recon: 0.608308, loss_pred: 0.100743
iteration 1144: loss: 61.737118, loss_kl: 0.300796, loss_recon: 0.616657, loss_pred: 0.044583
iteration 1145: loss: 60.721607, loss_kl: 0.117280, loss_recon: 0.605680, loss_pred: 0.143142
iteration 1146: loss: 61.017513, loss_kl: 0.099357, loss_recon: 0.609400, loss_pred: 0.068618
iteration 1147: loss: 60.215473, loss_kl: 0.134058, loss_recon: 0.601389, loss_pred: 0.064641
iteration 1148: loss: 61.665333, loss_kl: 0.144640, loss_recon: 0.615733, loss_pred: 0.079104
iteration 1149: loss: 60.698238, loss_kl: 0.170675, loss_recon: 0.605975, loss_pred: 0.085534
iteration 1150: loss: 62.067886, loss_kl: 0.007405, loss_recon: 0.619639, loss_pred: 0.103308
 57%|███████████████▌           | 115/200 [1:35:38<1:10:11, 49.55s/it]iteration 1151: loss: 60.840858, loss_kl: 0.035258, loss_recon: 0.607055, loss_pred: 0.130858
iteration 1152: loss: 61.681374, loss_kl: 0.224407, loss_recon: 0.615772, loss_pred: 0.075243
iteration 1153: loss: 60.771675, loss_kl: 0.073898, loss_recon: 0.606871, loss_pred: 0.075040
iteration 1154: loss: 61.893318, loss_kl: 0.283651, loss_recon: 0.617118, loss_pred: 0.145004
iteration 1155: loss: 61.215820, loss_kl: 0.118239, loss_recon: 0.611162, loss_pred: 0.084403
iteration 1156: loss: 60.167915, loss_kl: 0.207452, loss_recon: 0.600452, loss_pred: 0.096028
iteration 1157: loss: 61.682026, loss_kl: 0.117593, loss_recon: 0.614468, loss_pred: 0.220076
iteration 1158: loss: 61.358639, loss_kl: 0.089603, loss_recon: 0.612330, loss_pred: 0.114064
iteration 1159: loss: 59.716400, loss_kl: 0.037040, loss_recon: 0.596413, loss_pred: 0.070291
iteration 1160: loss: 60.327740, loss_kl: 0.132818, loss_recon: 0.598511, loss_pred: 0.459496
 58%|███████████████▋           | 116/200 [1:36:28<1:09:26, 49.60s/it]iteration 1161: loss: 59.449814, loss_kl: 0.176496, loss_recon: 0.593271, loss_pred: 0.092990
iteration 1162: loss: 61.564117, loss_kl: 0.147161, loss_recon: 0.614713, loss_pred: 0.067993
iteration 1163: loss: 62.407211, loss_kl: 0.090089, loss_recon: 0.623022, loss_pred: 0.089822
iteration 1164: loss: 61.756630, loss_kl: 0.235278, loss_recon: 0.616600, loss_pred: 0.057048
iteration 1165: loss: 60.469894, loss_kl: 0.118924, loss_recon: 0.603567, loss_pred: 0.093167
iteration 1166: loss: 58.845570, loss_kl: 0.083808, loss_recon: 0.587454, loss_pred: 0.086082
iteration 1167: loss: 61.362282, loss_kl: 0.104116, loss_recon: 0.612612, loss_pred: 0.083581
iteration 1168: loss: 60.630280, loss_kl: 0.096050, loss_recon: 0.605256, loss_pred: 0.088551
iteration 1169: loss: 61.231983, loss_kl: 0.109535, loss_recon: 0.611467, loss_pred: 0.066826
iteration 1170: loss: 61.667294, loss_kl: 0.264851, loss_recon: 0.615500, loss_pred: 0.072695
 58%|███████████████▊           | 117/200 [1:37:17<1:08:38, 49.62s/it]iteration 1171: loss: 59.824913, loss_kl: 0.209221, loss_recon: 0.596561, loss_pred: 0.125297
iteration 1172: loss: 61.641926, loss_kl: 0.078543, loss_recon: 0.614951, loss_pred: 0.130485
iteration 1173: loss: 60.263760, loss_kl: 0.174868, loss_recon: 0.600779, loss_pred: 0.149494
iteration 1174: loss: 61.062515, loss_kl: 0.147392, loss_recon: 0.608664, loss_pred: 0.165426
iteration 1175: loss: 60.804550, loss_kl: 0.106453, loss_recon: 0.606473, loss_pred: 0.135133
iteration 1176: loss: 62.274117, loss_kl: 0.374972, loss_recon: 0.620028, loss_pred: 0.193331
iteration 1177: loss: 61.343025, loss_kl: 0.232984, loss_recon: 0.611578, loss_pred: 0.136809
iteration 1178: loss: 61.163094, loss_kl: 0.066483, loss_recon: 0.610873, loss_pred: 0.061967
iteration 1179: loss: 61.048462, loss_kl: 0.185369, loss_recon: 0.608983, loss_pred: 0.111560
iteration 1180: loss: 57.937283, loss_kl: -0.074285, loss_recon: 0.578125, loss_pred: 0.140255
 59%|███████████████▉           | 118/200 [1:38:07<1:07:49, 49.63s/it]iteration 1181: loss: 61.779541, loss_kl: 0.075188, loss_recon: 0.616353, loss_pred: 0.125629
iteration 1182: loss: 60.655869, loss_kl: 0.113185, loss_recon: 0.605503, loss_pred: 0.077531
iteration 1183: loss: 60.589790, loss_kl: 0.060507, loss_recon: 0.604584, loss_pred: 0.116444
iteration 1184: loss: 60.123051, loss_kl: 0.116646, loss_recon: 0.599812, loss_pred: 0.113005
iteration 1185: loss: 60.789112, loss_kl: 0.217255, loss_recon: 0.606273, loss_pred: 0.107990
iteration 1186: loss: 61.742699, loss_kl: 0.105880, loss_recon: 0.616245, loss_pred: 0.092029
iteration 1187: loss: 60.592953, loss_kl: 0.142593, loss_recon: 0.604330, loss_pred: 0.124633
iteration 1188: loss: 60.469563, loss_kl: 0.043272, loss_recon: 0.603788, loss_pred: 0.080082
iteration 1189: loss: 60.731052, loss_kl: 0.150502, loss_recon: 0.605849, loss_pred: 0.108893
iteration 1190: loss: 63.423382, loss_kl: 0.269286, loss_recon: 0.632415, loss_pred: 0.115176
 60%|████████████████           | 119/200 [1:38:57<1:07:02, 49.66s/it]iteration 1191: loss: 60.946968, loss_kl: 0.225510, loss_recon: 0.607901, loss_pred: 0.092110
iteration 1192: loss: 61.424553, loss_kl: 0.192455, loss_recon: 0.613029, loss_pred: 0.066401
iteration 1193: loss: 60.486130, loss_kl: 0.143785, loss_recon: 0.603201, loss_pred: 0.124721
iteration 1194: loss: 60.394600, loss_kl: 0.182021, loss_recon: 0.602416, loss_pred: 0.100753
iteration 1195: loss: 62.234486, loss_kl: 0.206015, loss_recon: 0.621047, loss_pred: 0.070570
iteration 1196: loss: 61.420010, loss_kl: 0.071686, loss_recon: 0.612887, loss_pred: 0.110768
iteration 1197: loss: 60.256351, loss_kl: 0.099872, loss_recon: 0.601010, loss_pred: 0.126714
iteration 1198: loss: 61.453583, loss_kl: 0.078599, loss_recon: 0.613461, loss_pred: 0.084891
iteration 1199: loss: 60.017075, loss_kl: 0.218885, loss_recon: 0.598809, loss_pred: 0.073290
iteration 1200: loss: 60.093426, loss_kl: 0.022567, loss_recon: 0.598927, loss_pred: 0.194257
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_119.pth
 60%|████████████████▏          | 120/200 [1:39:46<1:06:07, 49.59s/it]iteration 1201: loss: 61.847679, loss_kl: 0.173927, loss_recon: 0.617152, loss_pred: 0.075601
iteration 1202: loss: 59.972458, loss_kl: 0.165874, loss_recon: 0.598296, loss_pred: 0.088699
iteration 1203: loss: 61.098972, loss_kl: 0.122077, loss_recon: 0.609744, loss_pred: 0.084651
iteration 1204: loss: 59.856045, loss_kl: 0.117328, loss_recon: 0.596789, loss_pred: 0.138758
iteration 1205: loss: 61.187309, loss_kl: 0.095215, loss_recon: 0.610463, loss_pred: 0.109886
iteration 1206: loss: 60.642998, loss_kl: 0.107858, loss_recon: 0.605313, loss_pred: 0.076454
iteration 1207: loss: 60.367050, loss_kl: 0.139688, loss_recon: 0.602000, loss_pred: 0.121364
iteration 1208: loss: 61.750206, loss_kl: 0.062693, loss_recon: 0.616349, loss_pred: 0.094785
iteration 1209: loss: 60.778530, loss_kl: 0.010014, loss_recon: 0.606607, loss_pred: 0.114600
iteration 1210: loss: 61.561428, loss_kl: 0.015333, loss_recon: 0.615037, loss_pred: 0.052695
 60%|████████████████▎          | 121/200 [1:40:36<1:05:18, 49.60s/it]iteration 1211: loss: 59.634041, loss_kl: -0.012994, loss_recon: 0.595650, loss_pred: 0.073821
iteration 1212: loss: 59.496120, loss_kl: 0.099451, loss_recon: 0.593284, loss_pred: 0.131326
iteration 1213: loss: 61.818314, loss_kl: 0.191952, loss_recon: 0.616505, loss_pred: 0.097469
iteration 1214: loss: 59.824203, loss_kl: 0.252194, loss_recon: 0.596532, loss_pred: 0.078583
iteration 1215: loss: 61.253155, loss_kl: 0.174822, loss_recon: 0.610930, loss_pred: 0.096119
iteration 1216: loss: 61.190964, loss_kl: 0.039577, loss_recon: 0.611155, loss_pred: 0.061008
iteration 1217: loss: 59.585609, loss_kl: 0.186372, loss_recon: 0.594215, loss_pred: 0.095791
iteration 1218: loss: 62.229099, loss_kl: 0.037172, loss_recon: 0.621468, loss_pred: 0.068721
iteration 1219: loss: 61.707417, loss_kl: 0.072203, loss_recon: 0.616098, loss_pred: 0.071204
iteration 1220: loss: 63.819962, loss_kl: 0.029685, loss_recon: 0.637600, loss_pred: 0.049110
 61%|████████████████▍          | 122/200 [1:41:26<1:04:40, 49.75s/it]iteration 1221: loss: 60.846462, loss_kl: 0.153782, loss_recon: 0.606964, loss_pred: 0.087657
iteration 1222: loss: 61.216064, loss_kl: 0.095195, loss_recon: 0.610928, loss_pred: 0.084571
iteration 1223: loss: 61.423141, loss_kl: 0.331989, loss_recon: 0.612112, loss_pred: 0.077192
iteration 1224: loss: 61.368420, loss_kl: 0.087398, loss_recon: 0.612557, loss_pred: 0.077188
iteration 1225: loss: 61.256176, loss_kl: 0.274496, loss_recon: 0.610419, loss_pred: 0.102853
iteration 1226: loss: 61.096027, loss_kl: 0.183027, loss_recon: 0.609305, loss_pred: 0.091186
iteration 1227: loss: 61.445648, loss_kl: 0.085330, loss_recon: 0.613231, loss_pred: 0.087915
iteration 1228: loss: 59.281986, loss_kl: 0.149126, loss_recon: 0.591602, loss_pred: 0.061231
iteration 1229: loss: 60.335468, loss_kl: 0.028523, loss_recon: 0.602095, loss_pred: 0.114361
iteration 1230: loss: 59.705772, loss_kl: -0.182765, loss_recon: 0.596508, loss_pred: 0.129149
 62%|████████████████▌          | 123/200 [1:42:15<1:03:37, 49.58s/it]iteration 1231: loss: 60.423664, loss_kl: 0.051034, loss_recon: 0.603103, loss_pred: 0.090655
iteration 1232: loss: 60.936256, loss_kl: 0.083552, loss_recon: 0.608042, loss_pred: 0.094787
iteration 1233: loss: 60.906433, loss_kl: 0.117353, loss_recon: 0.607973, loss_pred: 0.056852
iteration 1234: loss: 61.521202, loss_kl: 0.043545, loss_recon: 0.614313, loss_pred: 0.070453
iteration 1235: loss: 60.663143, loss_kl: 0.108728, loss_recon: 0.605250, loss_pred: 0.089663
iteration 1236: loss: 60.191452, loss_kl: 0.213576, loss_recon: 0.599976, loss_pred: 0.098672
iteration 1237: loss: 60.223709, loss_kl: 0.111881, loss_recon: 0.601076, loss_pred: 0.066229
iteration 1238: loss: 61.747097, loss_kl: 0.114820, loss_recon: 0.616163, loss_pred: 0.079644
iteration 1239: loss: 60.435616, loss_kl: 0.049976, loss_recon: 0.603633, loss_pred: 0.050082
iteration 1240: loss: 62.482437, loss_kl: -0.144219, loss_recon: 0.624643, loss_pred: 0.082378
 62%|████████████████▋          | 124/200 [1:43:05<1:02:56, 49.69s/it]iteration 1241: loss: 59.490192, loss_kl: 0.025475, loss_recon: 0.593996, loss_pred: 0.078188
iteration 1242: loss: 61.536026, loss_kl: 0.199846, loss_recon: 0.613503, loss_pred: 0.088745
iteration 1243: loss: 60.418976, loss_kl: 0.094486, loss_recon: 0.603180, loss_pred: 0.055107
iteration 1244: loss: 60.710705, loss_kl: 0.228612, loss_recon: 0.605010, loss_pred: 0.098749
iteration 1245: loss: 61.564777, loss_kl: 0.155230, loss_recon: 0.614202, loss_pred: 0.069273
iteration 1246: loss: 61.182167, loss_kl: 0.080363, loss_recon: 0.610782, loss_pred: 0.064942
iteration 1247: loss: 61.063038, loss_kl: 0.068911, loss_recon: 0.609479, loss_pred: 0.081658
iteration 1248: loss: 59.846382, loss_kl: 0.116923, loss_recon: 0.596698, loss_pred: 0.119817
iteration 1249: loss: 61.736656, loss_kl: 0.101727, loss_recon: 0.616083, loss_pred: 0.078976
iteration 1250: loss: 61.384285, loss_kl: -0.018782, loss_recon: 0.613164, loss_pred: 0.076970
 62%|████████████████▉          | 125/200 [1:43:56<1:02:24, 49.93s/it]iteration 1251: loss: 61.583851, loss_kl: -0.032119, loss_recon: 0.614947, loss_pred: 0.105986
iteration 1252: loss: 61.565460, loss_kl: 0.080741, loss_recon: 0.614215, loss_pred: 0.101572
iteration 1253: loss: 59.717270, loss_kl: 0.049145, loss_recon: 0.596021, loss_pred: 0.089366
iteration 1254: loss: 61.665775, loss_kl: 0.145854, loss_recon: 0.614908, loss_pred: 0.098472
iteration 1255: loss: 60.613422, loss_kl: 0.119954, loss_recon: 0.604556, loss_pred: 0.094865
iteration 1256: loss: 59.290787, loss_kl: 0.183815, loss_recon: 0.591041, loss_pred: 0.090227
iteration 1257: loss: 60.324795, loss_kl: -0.020640, loss_recon: 0.602347, loss_pred: 0.100883
iteration 1258: loss: 62.576683, loss_kl: 0.034171, loss_recon: 0.624902, loss_pred: 0.068598
iteration 1259: loss: 60.147400, loss_kl: 0.078206, loss_recon: 0.600197, loss_pred: 0.086680
iteration 1260: loss: 62.152164, loss_kl: 0.304382, loss_recon: 0.619051, loss_pred: 0.087373
 63%|█████████████████          | 126/200 [1:44:46<1:01:46, 50.08s/it]iteration 1261: loss: 60.615406, loss_kl: 0.084137, loss_recon: 0.604568, loss_pred: 0.111135
iteration 1262: loss: 61.344841, loss_kl: 0.153851, loss_recon: 0.611701, loss_pred: 0.087949
iteration 1263: loss: 60.732487, loss_kl: 0.025570, loss_recon: 0.606557, loss_pred: 0.062332
iteration 1264: loss: 60.324097, loss_kl: 0.024555, loss_recon: 0.602438, loss_pred: 0.066462
iteration 1265: loss: 60.107330, loss_kl: 0.187105, loss_recon: 0.599566, loss_pred: 0.045090
iteration 1266: loss: 60.636803, loss_kl: 0.113424, loss_recon: 0.605191, loss_pred: 0.053692
iteration 1267: loss: 61.050953, loss_kl: 0.149362, loss_recon: 0.608582, loss_pred: 0.108427
iteration 1268: loss: 61.632767, loss_kl: -0.002344, loss_recon: 0.615542, loss_pred: 0.079934
iteration 1269: loss: 61.047585, loss_kl: 0.058286, loss_recon: 0.609300, loss_pred: 0.084691
iteration 1270: loss: 62.526287, loss_kl: 0.258654, loss_recon: 0.623165, loss_pred: 0.063813
 64%|█████████████████▏         | 127/200 [1:45:35<1:00:36, 49.82s/it]iteration 1271: loss: 62.187485, loss_kl: 0.330717, loss_recon: 0.618872, loss_pred: 0.100542
iteration 1272: loss: 59.997169, loss_kl: 0.058040, loss_recon: 0.598360, loss_pred: 0.126138
iteration 1273: loss: 61.047707, loss_kl: 0.192624, loss_recon: 0.608516, loss_pred: 0.079769
iteration 1274: loss: 60.298908, loss_kl: 0.055332, loss_recon: 0.601928, loss_pred: 0.072691
iteration 1275: loss: 61.204624, loss_kl: 0.189883, loss_recon: 0.610095, loss_pred: 0.080455
iteration 1276: loss: 60.357040, loss_kl: 0.046371, loss_recon: 0.602488, loss_pred: 0.080231
iteration 1277: loss: 61.181404, loss_kl: 0.094747, loss_recon: 0.610693, loss_pred: 0.054842
iteration 1278: loss: 61.451794, loss_kl: 0.034676, loss_recon: 0.613784, loss_pred: 0.052406
iteration 1279: loss: 60.720028, loss_kl: 0.100273, loss_recon: 0.606036, loss_pred: 0.055849
iteration 1280: loss: 60.467339, loss_kl: 0.144903, loss_recon: 0.602993, loss_pred: 0.080474
 64%|██████████████████▌          | 128/200 [1:46:25<59:40, 49.73s/it]iteration 1281: loss: 60.570557, loss_kl: 0.060718, loss_recon: 0.604693, loss_pred: 0.062137
iteration 1282: loss: 61.938793, loss_kl: 0.169118, loss_recon: 0.617468, loss_pred: 0.083152
iteration 1283: loss: 60.345695, loss_kl: 0.069460, loss_recon: 0.602040, loss_pred: 0.097023
iteration 1284: loss: 60.567368, loss_kl: 0.165452, loss_recon: 0.604098, loss_pred: 0.051087
iteration 1285: loss: 61.299454, loss_kl: 0.082574, loss_recon: 0.611508, loss_pred: 0.095463
iteration 1286: loss: 62.090565, loss_kl: 0.041265, loss_recon: 0.620028, loss_pred: 0.061178
iteration 1287: loss: 60.520405, loss_kl: 0.121219, loss_recon: 0.603858, loss_pred: 0.056600
iteration 1288: loss: 60.871174, loss_kl: 0.203712, loss_recon: 0.606699, loss_pred: 0.070190
iteration 1289: loss: 60.057846, loss_kl: 0.061960, loss_recon: 0.599249, loss_pred: 0.093069
iteration 1290: loss: 58.848270, loss_kl: 0.077171, loss_recon: 0.587149, loss_pred: 0.083717
 64%|██████████████████▋          | 129/200 [1:47:15<59:03, 49.92s/it]iteration 1291: loss: 61.124249, loss_kl: 0.079988, loss_recon: 0.609893, loss_pred: 0.080323
iteration 1292: loss: 61.309525, loss_kl: 0.260888, loss_recon: 0.610545, loss_pred: 0.076741
iteration 1293: loss: 59.814991, loss_kl: 0.179764, loss_recon: 0.595955, loss_pred: 0.096700
iteration 1294: loss: 61.121910, loss_kl: 0.042004, loss_recon: 0.610075, loss_pred: 0.085760
iteration 1295: loss: 59.916023, loss_kl: 0.203091, loss_recon: 0.596784, loss_pred: 0.098875
iteration 1296: loss: 61.157097, loss_kl: 0.221299, loss_recon: 0.609367, loss_pred: 0.069212
iteration 1297: loss: 61.312328, loss_kl: 0.051666, loss_recon: 0.611992, loss_pred: 0.077860
iteration 1298: loss: 60.513664, loss_kl: -0.015567, loss_recon: 0.604470, loss_pred: 0.077293
iteration 1299: loss: 60.977386, loss_kl: 0.139564, loss_recon: 0.607698, loss_pred: 0.112263
iteration 1300: loss: 62.204910, loss_kl: 0.130266, loss_recon: 0.620154, loss_pred: 0.100506
 65%|██████████████████▊          | 130/200 [1:48:07<59:05, 50.65s/it]iteration 1301: loss: 60.345802, loss_kl: 0.091945, loss_recon: 0.601425, loss_pred: 0.136820
iteration 1302: loss: 60.812866, loss_kl: 0.139943, loss_recon: 0.606051, loss_pred: 0.106605
iteration 1303: loss: 61.822243, loss_kl: 0.269399, loss_recon: 0.615568, loss_pred: 0.070699
iteration 1304: loss: 60.268604, loss_kl: 0.033800, loss_recon: 0.601483, loss_pred: 0.095916
iteration 1305: loss: 61.347656, loss_kl: 0.015834, loss_recon: 0.612305, loss_pred: 0.105677
iteration 1306: loss: 60.352272, loss_kl: 0.053343, loss_recon: 0.602324, loss_pred: 0.081331
iteration 1307: loss: 60.234379, loss_kl: 0.117449, loss_recon: 0.600587, loss_pred: 0.090766
iteration 1308: loss: 61.107731, loss_kl: 0.119255, loss_recon: 0.609540, loss_pred: 0.067512
iteration 1309: loss: 61.378853, loss_kl: -0.005484, loss_recon: 0.613077, loss_pred: 0.075145
iteration 1310: loss: 61.956306, loss_kl: 0.149501, loss_recon: 0.617940, loss_pred: 0.054220
 66%|██████████████████▉          | 131/200 [1:48:57<57:58, 50.41s/it]iteration 1311: loss: 60.291069, loss_kl: 0.107844, loss_recon: 0.601116, loss_pred: 0.097210
iteration 1312: loss: 61.418919, loss_kl: 0.143085, loss_recon: 0.612399, loss_pred: 0.069930
iteration 1313: loss: 60.832973, loss_kl: 0.124343, loss_recon: 0.606381, loss_pred: 0.100067
iteration 1314: loss: 62.189629, loss_kl: 0.122650, loss_recon: 0.619628, loss_pred: 0.133347
iteration 1315: loss: 60.395046, loss_kl: 0.120260, loss_recon: 0.602165, loss_pred: 0.086820
iteration 1316: loss: 59.531742, loss_kl: 0.067729, loss_recon: 0.593477, loss_pred: 0.132441
iteration 1317: loss: 60.515945, loss_kl: 0.175411, loss_recon: 0.602880, loss_pred: 0.094210
iteration 1318: loss: 61.413486, loss_kl: 0.047697, loss_recon: 0.613060, loss_pred: 0.071123
iteration 1319: loss: 61.766624, loss_kl: 0.014711, loss_recon: 0.616681, loss_pred: 0.087350
iteration 1320: loss: 60.410072, loss_kl: 0.117619, loss_recon: 0.602107, loss_pred: 0.109654
 66%|███████████████████▏         | 132/200 [1:49:47<56:53, 50.19s/it]iteration 1321: loss: 60.291035, loss_kl: 0.171507, loss_recon: 0.600404, loss_pred: 0.113129
iteration 1322: loss: 61.678711, loss_kl: 0.122731, loss_recon: 0.614987, loss_pred: 0.081592
iteration 1323: loss: 61.391182, loss_kl: 0.115965, loss_recon: 0.612432, loss_pred: 0.054976
iteration 1324: loss: 59.874504, loss_kl: 0.077860, loss_recon: 0.597518, loss_pred: 0.060229
iteration 1325: loss: 61.279682, loss_kl: 0.111608, loss_recon: 0.611234, loss_pred: 0.066794
iteration 1326: loss: 61.568588, loss_kl: 0.047394, loss_recon: 0.614702, loss_pred: 0.060378
iteration 1327: loss: 59.212704, loss_kl: 0.009729, loss_recon: 0.590620, loss_pred: 0.142863
iteration 1328: loss: 61.016251, loss_kl: -0.007149, loss_recon: 0.609331, loss_pred: 0.088914
iteration 1329: loss: 61.564907, loss_kl: 0.191789, loss_recon: 0.613202, loss_pred: 0.090851
iteration 1330: loss: 62.744740, loss_kl: -0.015230, loss_recon: 0.626017, loss_pred: 0.155241
 66%|███████████████████▎         | 133/200 [1:50:37<55:53, 50.05s/it]iteration 1331: loss: 61.446369, loss_kl: 0.032076, loss_recon: 0.612849, loss_pred: 0.134485
iteration 1332: loss: 61.105370, loss_kl: 0.050064, loss_recon: 0.609915, loss_pred: 0.071726
iteration 1333: loss: 61.111408, loss_kl: 0.032329, loss_recon: 0.609560, loss_pred: 0.128183
iteration 1334: loss: 59.696075, loss_kl: 0.046654, loss_recon: 0.594725, loss_pred: 0.184279
iteration 1335: loss: 60.938576, loss_kl: 0.014016, loss_recon: 0.607974, loss_pred: 0.129367
iteration 1336: loss: 62.071445, loss_kl: 0.105770, loss_recon: 0.619152, loss_pred: 0.067190
iteration 1337: loss: 61.893120, loss_kl: 0.138551, loss_recon: 0.616664, loss_pred: 0.110129
iteration 1338: loss: 60.296352, loss_kl: 0.023423, loss_recon: 0.601108, loss_pred: 0.165806
iteration 1339: loss: 60.262085, loss_kl: 0.134051, loss_recon: 0.600268, loss_pred: 0.122424
iteration 1340: loss: 58.441502, loss_kl: -0.208072, loss_recon: 0.584603, loss_pred: 0.156325
 67%|███████████████████▍         | 134/200 [1:51:26<54:53, 49.90s/it]iteration 1341: loss: 60.274532, loss_kl: 0.172588, loss_recon: 0.599827, loss_pred: 0.139705
iteration 1342: loss: 61.343071, loss_kl: 0.109248, loss_recon: 0.611355, loss_pred: 0.111307
iteration 1343: loss: 60.838898, loss_kl: 0.112438, loss_recon: 0.606683, loss_pred: 0.071560
iteration 1344: loss: 61.347298, loss_kl: 0.157159, loss_recon: 0.611047, loss_pred: 0.104079
iteration 1345: loss: 62.568565, loss_kl: 0.205712, loss_recon: 0.622387, loss_pred: 0.148573
iteration 1346: loss: 60.292641, loss_kl: 0.041093, loss_recon: 0.601305, loss_pred: 0.125940
iteration 1347: loss: 60.524548, loss_kl: 0.127584, loss_recon: 0.602992, loss_pred: 0.112925
iteration 1348: loss: 61.350761, loss_kl: 0.094769, loss_recon: 0.611653, loss_pred: 0.101949
iteration 1349: loss: 59.957085, loss_kl: 0.160582, loss_recon: 0.597070, loss_pred: 0.108555
iteration 1350: loss: 63.903214, loss_kl: 0.285468, loss_recon: 0.635667, loss_pred: 0.084994
 68%|███████████████████▌         | 135/200 [1:52:16<54:00, 49.85s/it]iteration 1351: loss: 61.445221, loss_kl: 0.120972, loss_recon: 0.612446, loss_pred: 0.089216
iteration 1352: loss: 61.270096, loss_kl: 0.160328, loss_recon: 0.610408, loss_pred: 0.081649
iteration 1353: loss: 61.082043, loss_kl: 0.124358, loss_recon: 0.608887, loss_pred: 0.078863
iteration 1354: loss: 60.851681, loss_kl: 0.237638, loss_recon: 0.605070, loss_pred: 0.125906
iteration 1355: loss: 60.411823, loss_kl: 0.107100, loss_recon: 0.602211, loss_pred: 0.092138
iteration 1356: loss: 59.776199, loss_kl: 0.072830, loss_recon: 0.596095, loss_pred: 0.099660
iteration 1357: loss: 61.845367, loss_kl: -0.031875, loss_recon: 0.618011, loss_pred: 0.073654
iteration 1358: loss: 60.753025, loss_kl: 0.108115, loss_recon: 0.605833, loss_pred: 0.070136
iteration 1359: loss: 61.255005, loss_kl: 0.117480, loss_recon: 0.610772, loss_pred: 0.069603
iteration 1360: loss: 60.697044, loss_kl: -0.047914, loss_recon: 0.606435, loss_pred: 0.097679
 68%|███████████████████▋         | 136/200 [1:53:05<53:02, 49.73s/it]iteration 1361: loss: 60.934151, loss_kl: 0.183315, loss_recon: 0.606680, loss_pred: 0.090062
iteration 1362: loss: 60.367783, loss_kl: 0.170301, loss_recon: 0.600968, loss_pred: 0.107451
iteration 1363: loss: 61.550358, loss_kl: 0.083925, loss_recon: 0.613639, loss_pred: 0.105851
iteration 1364: loss: 60.579475, loss_kl: 0.186005, loss_recon: 0.602955, loss_pred: 0.105309
iteration 1365: loss: 62.180782, loss_kl: 0.044768, loss_recon: 0.620579, loss_pred: 0.079911
iteration 1366: loss: 59.616287, loss_kl: 0.013135, loss_recon: 0.595023, loss_pred: 0.101379
iteration 1367: loss: 61.337704, loss_kl: 0.157994, loss_recon: 0.610829, loss_pred: 0.103105
iteration 1368: loss: 60.573013, loss_kl: 0.082450, loss_recon: 0.603806, loss_pred: 0.113227
iteration 1369: loss: 60.793671, loss_kl: -0.002055, loss_recon: 0.606888, loss_pred: 0.106871
iteration 1370: loss: 63.106743, loss_kl: -0.109589, loss_recon: 0.631320, loss_pred: 0.080025
 68%|███████████████████▊         | 137/200 [1:53:55<52:10, 49.69s/it]iteration 1371: loss: 61.160831, loss_kl: -0.051794, loss_recon: 0.611143, loss_pred: 0.098359
iteration 1372: loss: 60.368408, loss_kl: 0.075626, loss_recon: 0.601556, loss_pred: 0.137159
iteration 1373: loss: 60.675377, loss_kl: 0.110573, loss_recon: 0.604611, loss_pred: 0.103749
iteration 1374: loss: 59.738499, loss_kl: 0.054508, loss_recon: 0.596210, loss_pred: 0.062966
iteration 1375: loss: 60.004467, loss_kl: 0.131897, loss_recon: 0.597376, loss_pred: 0.134940
iteration 1376: loss: 61.062241, loss_kl: 0.195509, loss_recon: 0.607709, loss_pred: 0.095860
iteration 1377: loss: 60.438293, loss_kl: 0.118120, loss_recon: 0.602163, loss_pred: 0.103914
iteration 1378: loss: 62.653194, loss_kl: 0.154144, loss_recon: 0.624150, loss_pred: 0.084098
iteration 1379: loss: 61.010571, loss_kl: 0.133800, loss_recon: 0.607541, loss_pred: 0.122640
iteration 1380: loss: 65.077904, loss_kl: -0.043763, loss_recon: 0.649917, loss_pred: 0.129973
 69%|████████████████████         | 138/200 [1:54:46<51:47, 50.13s/it]iteration 1381: loss: 61.177387, loss_kl: 0.054236, loss_recon: 0.609349, loss_pred: 0.188286
iteration 1382: loss: 61.505566, loss_kl: 0.169475, loss_recon: 0.611446, loss_pred: 0.191524
iteration 1383: loss: 61.931984, loss_kl: 0.114626, loss_recon: 0.617262, loss_pred: 0.091126
iteration 1384: loss: 61.814240, loss_kl: 0.053553, loss_recon: 0.616776, loss_pred: 0.083048
iteration 1385: loss: 62.019699, loss_kl: 0.124297, loss_recon: 0.617588, loss_pred: 0.136586
iteration 1386: loss: 59.493279, loss_kl: 0.139056, loss_recon: 0.592034, loss_pred: 0.150838
iteration 1387: loss: 61.507236, loss_kl: 0.132923, loss_recon: 0.612684, loss_pred: 0.105959
iteration 1388: loss: 59.524555, loss_kl: 0.313970, loss_recon: 0.591326, loss_pred: 0.077968
iteration 1389: loss: 59.822968, loss_kl: 0.110106, loss_recon: 0.596116, loss_pred: 0.101262
iteration 1390: loss: 63.070271, loss_kl: -0.123535, loss_recon: 0.630364, loss_pred: 0.157405
 70%|████████████████████▏        | 139/200 [1:55:36<50:54, 50.08s/it]iteration 1391: loss: 60.439014, loss_kl: 0.024558, loss_recon: 0.602568, loss_pred: 0.157633
iteration 1392: loss: 60.768196, loss_kl: 0.030246, loss_recon: 0.605805, loss_pred: 0.157493
iteration 1393: loss: 60.379345, loss_kl: 0.114839, loss_recon: 0.601427, loss_pred: 0.121759
iteration 1394: loss: 61.687595, loss_kl: 0.050756, loss_recon: 0.615621, loss_pred: 0.074730
iteration 1395: loss: 61.549599, loss_kl: 0.162319, loss_recon: 0.613271, loss_pred: 0.060217
iteration 1396: loss: 61.234623, loss_kl: 0.086578, loss_recon: 0.610478, loss_pred: 0.100245
iteration 1397: loss: 61.439533, loss_kl: 0.107668, loss_recon: 0.612208, loss_pred: 0.111107
iteration 1398: loss: 60.890537, loss_kl: 0.069273, loss_recon: 0.606883, loss_pred: 0.132998
iteration 1399: loss: 60.131580, loss_kl: 0.134011, loss_recon: 0.598532, loss_pred: 0.144375
iteration 1400: loss: 60.743641, loss_kl: 0.154646, loss_recon: 0.604983, loss_pred: 0.090709
 70%|████████████████████▎        | 140/200 [1:56:26<49:57, 49.96s/it]iteration 1401: loss: 62.373394, loss_kl: 0.030410, loss_recon: 0.622463, loss_pred: 0.096733
iteration 1402: loss: 60.630329, loss_kl: 0.226766, loss_recon: 0.602102, loss_pred: 0.193397
iteration 1403: loss: 60.128143, loss_kl: 0.200969, loss_recon: 0.597426, loss_pred: 0.184607
iteration 1404: loss: 61.406250, loss_kl: -0.018945, loss_recon: 0.612894, loss_pred: 0.135754
iteration 1405: loss: 61.011086, loss_kl: 0.224291, loss_recon: 0.606309, loss_pred: 0.155935
iteration 1406: loss: 61.069363, loss_kl: 0.074442, loss_recon: 0.609187, loss_pred: 0.076229
iteration 1407: loss: 61.241386, loss_kl: 0.112845, loss_recon: 0.610040, loss_pred: 0.124547
iteration 1408: loss: 60.852432, loss_kl: 0.108817, loss_recon: 0.606167, loss_pred: 0.126908
iteration 1409: loss: 60.384632, loss_kl: 0.139676, loss_recon: 0.601471, loss_pred: 0.097851
iteration 1410: loss: 60.550980, loss_kl: 0.175196, loss_recon: 0.602322, loss_pred: 0.143608
 70%|████████████████████▍        | 141/200 [1:57:16<49:04, 49.91s/it]iteration 1411: loss: 60.663960, loss_kl: 0.032585, loss_recon: 0.604542, loss_pred: 0.177143
iteration 1412: loss: 61.536930, loss_kl: 0.117940, loss_recon: 0.612674, loss_pred: 0.151598
iteration 1413: loss: 61.648022, loss_kl: 0.104799, loss_recon: 0.614641, loss_pred: 0.079146
iteration 1414: loss: 60.254971, loss_kl: 0.067861, loss_recon: 0.600507, loss_pred: 0.136381
iteration 1415: loss: 61.145546, loss_kl: 0.161557, loss_recon: 0.609046, loss_pred: 0.079434
iteration 1416: loss: 59.486500, loss_kl: -0.031885, loss_recon: 0.594255, loss_pred: 0.092847
iteration 1417: loss: 61.479362, loss_kl: 0.067175, loss_recon: 0.613065, loss_pred: 0.105678
iteration 1418: loss: 61.604420, loss_kl: 0.075050, loss_recon: 0.614384, loss_pred: 0.090971
iteration 1419: loss: 60.872944, loss_kl: 0.138541, loss_recon: 0.606402, loss_pred: 0.094204
iteration 1420: loss: 60.310944, loss_kl: 0.119029, loss_recon: 0.600542, loss_pred: 0.137730
 71%|████████████████████▌        | 142/200 [1:58:06<48:16, 49.94s/it]iteration 1421: loss: 62.226803, loss_kl: -0.011072, loss_recon: 0.621006, loss_pred: 0.137306
iteration 1422: loss: 61.529751, loss_kl: 0.124175, loss_recon: 0.613072, loss_pred: 0.098397
iteration 1423: loss: 60.641960, loss_kl: -0.069126, loss_recon: 0.606055, loss_pred: 0.105570
iteration 1424: loss: 60.141926, loss_kl: 0.032373, loss_recon: 0.600149, loss_pred: 0.094646
iteration 1425: loss: 60.675007, loss_kl: 0.029802, loss_recon: 0.605579, loss_pred: 0.087276
iteration 1426: loss: 61.074471, loss_kl: 0.076314, loss_recon: 0.609112, loss_pred: 0.086955
iteration 1427: loss: 60.879669, loss_kl: 0.114694, loss_recon: 0.606578, loss_pred: 0.107199
iteration 1428: loss: 61.167961, loss_kl: 0.060944, loss_recon: 0.610584, loss_pred: 0.048652
iteration 1429: loss: 60.254536, loss_kl: 0.031093, loss_recon: 0.601220, loss_pred: 0.101418
iteration 1430: loss: 59.104061, loss_kl: 0.166686, loss_recon: 0.587594, loss_pred: 0.177964
 72%|████████████████████▋        | 143/200 [1:58:55<47:12, 49.69s/it]iteration 1431: loss: 59.818211, loss_kl: 0.064912, loss_recon: 0.596466, loss_pred: 0.106669
iteration 1432: loss: 62.348602, loss_kl: 0.190486, loss_recon: 0.620262, loss_pred: 0.131900
iteration 1433: loss: 61.026848, loss_kl: 0.074846, loss_recon: 0.608416, loss_pred: 0.110371
iteration 1434: loss: 61.564411, loss_kl: 0.160679, loss_recon: 0.613028, loss_pred: 0.100899
iteration 1435: loss: 60.249817, loss_kl: 0.099567, loss_recon: 0.600393, loss_pred: 0.110899
iteration 1436: loss: 61.497593, loss_kl: 0.104229, loss_recon: 0.613199, loss_pred: 0.073463
iteration 1437: loss: 61.954060, loss_kl: -0.011104, loss_recon: 0.618737, loss_pred: 0.091425
iteration 1438: loss: 59.136734, loss_kl: 0.089357, loss_recon: 0.589619, loss_pred: 0.085509
iteration 1439: loss: 60.534286, loss_kl: 0.034419, loss_recon: 0.604377, loss_pred: 0.062128
iteration 1440: loss: 60.405678, loss_kl: 0.023990, loss_recon: 0.603007, loss_pred: 0.080957
 72%|████████████████████▉        | 144/200 [1:59:44<46:16, 49.58s/it]iteration 1441: loss: 61.500835, loss_kl: 0.145503, loss_recon: 0.612726, loss_pred: 0.082720
iteration 1442: loss: 60.759445, loss_kl: 0.143303, loss_recon: 0.605352, loss_pred: 0.080933
iteration 1443: loss: 62.233002, loss_kl: 0.138154, loss_recon: 0.620142, loss_pred: 0.080638
iteration 1444: loss: 61.718246, loss_kl: 0.070440, loss_recon: 0.615797, loss_pred: 0.068146
iteration 1445: loss: 60.839420, loss_kl: 0.026749, loss_recon: 0.607159, loss_pred: 0.096805
iteration 1446: loss: 59.855881, loss_kl: 0.114120, loss_recon: 0.596582, loss_pred: 0.083528
iteration 1447: loss: 60.386055, loss_kl: 0.115367, loss_recon: 0.601709, loss_pred: 0.099796
iteration 1448: loss: 60.504238, loss_kl: 0.035834, loss_recon: 0.603580, loss_pred: 0.110442
iteration 1449: loss: 60.939983, loss_kl: 0.003898, loss_recon: 0.608388, loss_pred: 0.097270
iteration 1450: loss: 58.825138, loss_kl: 0.043592, loss_recon: 0.586169, loss_pred: 0.164647
 72%|█████████████████████        | 145/200 [2:00:33<45:23, 49.51s/it]iteration 1451: loss: 62.812958, loss_kl: 0.096196, loss_recon: 0.626374, loss_pred: 0.079392
iteration 1452: loss: 59.595627, loss_kl: 0.092680, loss_recon: 0.593498, loss_pred: 0.153112
iteration 1453: loss: 60.563240, loss_kl: 0.077141, loss_recon: 0.603937, loss_pred: 0.092396
iteration 1454: loss: 59.914795, loss_kl: 0.140444, loss_recon: 0.596874, loss_pred: 0.086948
iteration 1455: loss: 60.467426, loss_kl: 0.049374, loss_recon: 0.603285, loss_pred: 0.089562
iteration 1456: loss: 61.557175, loss_kl: 0.089396, loss_recon: 0.613646, loss_pred: 0.103222
iteration 1457: loss: 61.019093, loss_kl: 0.116539, loss_recon: 0.607957, loss_pred: 0.106900
iteration 1458: loss: 60.950863, loss_kl: 0.050140, loss_recon: 0.608073, loss_pred: 0.093393
iteration 1459: loss: 61.707813, loss_kl: 0.115305, loss_recon: 0.615047, loss_pred: 0.087824
iteration 1460: loss: 59.363342, loss_kl: -0.023757, loss_recon: 0.593351, loss_pred: 0.052007
 73%|█████████████████████▏       | 146/200 [2:01:24<44:57, 49.95s/it]iteration 1461: loss: 60.822971, loss_kl: 0.011043, loss_recon: 0.607493, loss_pred: 0.062580
iteration 1462: loss: 60.931271, loss_kl: 0.083139, loss_recon: 0.607595, loss_pred: 0.088634
iteration 1463: loss: 60.778496, loss_kl: 0.002011, loss_recon: 0.606711, loss_pred: 0.105423
iteration 1464: loss: 61.735363, loss_kl: 0.039028, loss_recon: 0.616232, loss_pred: 0.073162
iteration 1465: loss: 59.961975, loss_kl: 0.174036, loss_recon: 0.597123, loss_pred: 0.075603
iteration 1466: loss: 60.427326, loss_kl: 0.039812, loss_recon: 0.603037, loss_pred: 0.083764
iteration 1467: loss: 61.247238, loss_kl: 0.135054, loss_recon: 0.610463, loss_pred: 0.065837
iteration 1468: loss: 61.318478, loss_kl: 0.098638, loss_recon: 0.611583, loss_pred: 0.061513
iteration 1469: loss: 60.320156, loss_kl: 0.119366, loss_recon: 0.601251, loss_pred: 0.075738
iteration 1470: loss: 61.290508, loss_kl: 0.051432, loss_recon: 0.611544, loss_pred: 0.084631
 74%|█████████████████████▎       | 147/200 [2:02:14<44:08, 49.96s/it]iteration 1471: loss: 60.880573, loss_kl: 0.000667, loss_recon: 0.608172, loss_pred: 0.062724
iteration 1472: loss: 60.597450, loss_kl: 0.130764, loss_recon: 0.603954, loss_pred: 0.071249
iteration 1473: loss: 61.991913, loss_kl: 0.061382, loss_recon: 0.618529, loss_pred: 0.077648
iteration 1474: loss: 60.252766, loss_kl: 0.140169, loss_recon: 0.600534, loss_pred: 0.059236
iteration 1475: loss: 61.248753, loss_kl: 0.027469, loss_recon: 0.611366, loss_pred: 0.084680
iteration 1476: loss: 60.104736, loss_kl: 0.072217, loss_recon: 0.599759, loss_pred: 0.056634
iteration 1477: loss: 61.028057, loss_kl: 0.130333, loss_recon: 0.608181, loss_pred: 0.079667
iteration 1478: loss: 60.260429, loss_kl: 0.067672, loss_recon: 0.600777, loss_pred: 0.115070
iteration 1479: loss: 60.879288, loss_kl: 0.176702, loss_recon: 0.605781, loss_pred: 0.124473
iteration 1480: loss: 63.403461, loss_kl: -0.112165, loss_recon: 0.634543, loss_pred: 0.061324
 74%|█████████████████████▍       | 148/200 [2:03:04<43:09, 49.80s/it]iteration 1481: loss: 60.353470, loss_kl: 0.126194, loss_recon: 0.601401, loss_pred: 0.087195
iteration 1482: loss: 60.830311, loss_kl: 0.125333, loss_recon: 0.606128, loss_pred: 0.092133
iteration 1483: loss: 60.627586, loss_kl: 0.118380, loss_recon: 0.604041, loss_pred: 0.105059
iteration 1484: loss: 62.025955, loss_kl: 0.126200, loss_recon: 0.617532, loss_pred: 0.146551
iteration 1485: loss: 59.786663, loss_kl: -0.038590, loss_recon: 0.596827, loss_pred: 0.142551
iteration 1486: loss: 60.724415, loss_kl: 0.035115, loss_recon: 0.605924, loss_pred: 0.096929
iteration 1487: loss: 60.698933, loss_kl: 0.135521, loss_recon: 0.604657, loss_pred: 0.097744
iteration 1488: loss: 62.077194, loss_kl: 0.074417, loss_recon: 0.618163, loss_pred: 0.186486
iteration 1489: loss: 61.128323, loss_kl: 0.034960, loss_recon: 0.610157, loss_pred: 0.077646
iteration 1490: loss: 61.640404, loss_kl: 0.268073, loss_recon: 0.612815, loss_pred: 0.090858
 74%|█████████████████████▌       | 149/200 [2:03:53<42:17, 49.75s/it]iteration 1491: loss: 61.057667, loss_kl: 0.102045, loss_recon: 0.608526, loss_pred: 0.102993
iteration 1492: loss: 59.674397, loss_kl: 0.079221, loss_recon: 0.594985, loss_pred: 0.096657
iteration 1493: loss: 59.449131, loss_kl: 0.025680, loss_recon: 0.593410, loss_pred: 0.082447
iteration 1494: loss: 62.329044, loss_kl: -0.023879, loss_recon: 0.622831, loss_pred: 0.069863
iteration 1495: loss: 61.669003, loss_kl: 0.125144, loss_recon: 0.614197, loss_pred: 0.124200
iteration 1496: loss: 61.298328, loss_kl: -0.002272, loss_recon: 0.612247, loss_pred: 0.075919
iteration 1497: loss: 59.508434, loss_kl: 0.116656, loss_recon: 0.592795, loss_pred: 0.112289
iteration 1498: loss: 61.218403, loss_kl: 0.087613, loss_recon: 0.610297, loss_pred: 0.101092
iteration 1499: loss: 61.903641, loss_kl: 0.042573, loss_recon: 0.618077, loss_pred: 0.053395
iteration 1500: loss: 58.897896, loss_kl: 0.164601, loss_recon: 0.586067, loss_pred: 0.126596
 75%|█████████████████████▊       | 150/200 [2:04:44<41:35, 49.91s/it]iteration 1501: loss: 60.849476, loss_kl: 0.080488, loss_recon: 0.607450, loss_pred: 0.103653
iteration 1502: loss: 60.606026, loss_kl: 0.085457, loss_recon: 0.605202, loss_pred: 0.084959
iteration 1503: loss: 61.659260, loss_kl: -0.003044, loss_recon: 0.615836, loss_pred: 0.075668
iteration 1504: loss: 60.428207, loss_kl: 0.017829, loss_recon: 0.603374, loss_pred: 0.090676
iteration 1505: loss: 60.501724, loss_kl: 0.109556, loss_recon: 0.604111, loss_pred: 0.089556
iteration 1506: loss: 61.851368, loss_kl: 0.045578, loss_recon: 0.617610, loss_pred: 0.089915
iteration 1507: loss: 60.592197, loss_kl: 0.039853, loss_recon: 0.605198, loss_pred: 0.072019
iteration 1508: loss: 60.092445, loss_kl: 0.010400, loss_recon: 0.599953, loss_pred: 0.096996
iteration 1509: loss: 61.387302, loss_kl: 0.039651, loss_recon: 0.613208, loss_pred: 0.066112
iteration 1510: loss: 59.730206, loss_kl: 0.080369, loss_recon: 0.595513, loss_pred: 0.178143
 76%|█████████████████████▉       | 151/200 [2:05:33<40:43, 49.86s/it]iteration 1511: loss: 60.518654, loss_kl: 0.033037, loss_recon: 0.604224, loss_pred: 0.095962
iteration 1512: loss: 61.117001, loss_kl: 0.091025, loss_recon: 0.610442, loss_pred: 0.071928
iteration 1513: loss: 60.260986, loss_kl: 0.056925, loss_recon: 0.601283, loss_pred: 0.132140
iteration 1514: loss: 61.057217, loss_kl: -0.018629, loss_recon: 0.609091, loss_pred: 0.148336
iteration 1515: loss: 60.943211, loss_kl: 0.017340, loss_recon: 0.608824, loss_pred: 0.060631
iteration 1516: loss: 61.252693, loss_kl: 0.076455, loss_recon: 0.611586, loss_pred: 0.093307
iteration 1517: loss: 60.728676, loss_kl: 0.006836, loss_recon: 0.606570, loss_pred: 0.071618
iteration 1518: loss: 59.977451, loss_kl: 0.109173, loss_recon: 0.598915, loss_pred: 0.084818
iteration 1519: loss: 61.277306, loss_kl: 0.073875, loss_recon: 0.612051, loss_pred: 0.071500
iteration 1520: loss: 61.106266, loss_kl: 0.082291, loss_recon: 0.610476, loss_pred: 0.057813
 76%|██████████████████████       | 152/200 [2:06:23<39:48, 49.77s/it]iteration 1521: loss: 59.914211, loss_kl: 0.053309, loss_recon: 0.598154, loss_pred: 0.098313
iteration 1522: loss: 62.102264, loss_kl: 0.100925, loss_recon: 0.619997, loss_pred: 0.101538
iteration 1523: loss: 61.308773, loss_kl: 0.067433, loss_recon: 0.612513, loss_pred: 0.056794
iteration 1524: loss: 60.477116, loss_kl: 0.045363, loss_recon: 0.603720, loss_pred: 0.104684
iteration 1525: loss: 61.325676, loss_kl: 0.150532, loss_recon: 0.612281, loss_pred: 0.096083
iteration 1526: loss: 60.902161, loss_kl: 0.048944, loss_recon: 0.608283, loss_pred: 0.073411
iteration 1527: loss: 59.013611, loss_kl: 0.064660, loss_recon: 0.589147, loss_pred: 0.098271
iteration 1528: loss: 61.088020, loss_kl: 0.011203, loss_recon: 0.610280, loss_pred: 0.059871
iteration 1529: loss: 60.868164, loss_kl: 0.063824, loss_recon: 0.608017, loss_pred: 0.065870
iteration 1530: loss: 59.957447, loss_kl: 0.015481, loss_recon: 0.598935, loss_pred: 0.063841
 76%|██████████████████████▏      | 153/200 [2:07:13<39:01, 49.81s/it]iteration 1531: loss: 60.430721, loss_kl: 0.051683, loss_recon: 0.603325, loss_pred: 0.097728
iteration 1532: loss: 60.586182, loss_kl: 0.057319, loss_recon: 0.605157, loss_pred: 0.069871
iteration 1533: loss: 62.023243, loss_kl: 0.102099, loss_recon: 0.619395, loss_pred: 0.082674
iteration 1534: loss: 59.912861, loss_kl: 0.129170, loss_recon: 0.598475, loss_pred: 0.064088
iteration 1535: loss: 61.417942, loss_kl: 0.142672, loss_recon: 0.613589, loss_pred: 0.057665
iteration 1536: loss: 61.153133, loss_kl: 0.078387, loss_recon: 0.610747, loss_pred: 0.077692
iteration 1537: loss: 61.168598, loss_kl: 0.098343, loss_recon: 0.610872, loss_pred: 0.080407
iteration 1538: loss: 59.115215, loss_kl: 0.044266, loss_recon: 0.590351, loss_pred: 0.079707
iteration 1539: loss: 61.400410, loss_kl: 0.013442, loss_recon: 0.613284, loss_pred: 0.071847
iteration 1540: loss: 61.518059, loss_kl: -0.127247, loss_recon: 0.614277, loss_pred: 0.091626
 77%|██████████████████████▎      | 154/200 [2:08:03<38:17, 49.95s/it]iteration 1541: loss: 61.068527, loss_kl: 0.120850, loss_recon: 0.609882, loss_pred: 0.079157
iteration 1542: loss: 61.034569, loss_kl: 0.087852, loss_recon: 0.609447, loss_pred: 0.089016
iteration 1543: loss: 61.852093, loss_kl: 0.201408, loss_recon: 0.617791, loss_pred: 0.070968
iteration 1544: loss: 61.503468, loss_kl: 0.030881, loss_recon: 0.614091, loss_pred: 0.094015
iteration 1545: loss: 60.167294, loss_kl: -0.010585, loss_recon: 0.600505, loss_pred: 0.116913
iteration 1546: loss: 60.086689, loss_kl: -0.036316, loss_recon: 0.599948, loss_pred: 0.092228
iteration 1547: loss: 60.699570, loss_kl: -0.010497, loss_recon: 0.606377, loss_pred: 0.062004
iteration 1548: loss: 60.238529, loss_kl: 0.131056, loss_recon: 0.601723, loss_pred: 0.064955
iteration 1549: loss: 60.785805, loss_kl: -0.023347, loss_recon: 0.607266, loss_pred: 0.059461
iteration 1550: loss: 60.397919, loss_kl: 0.194727, loss_recon: 0.603327, loss_pred: 0.063251
 78%|██████████████████████▍      | 155/200 [2:08:53<37:24, 49.89s/it]iteration 1551: loss: 61.070690, loss_kl: -0.009665, loss_recon: 0.609807, loss_pred: 0.090096
iteration 1552: loss: 61.463097, loss_kl: -0.007973, loss_recon: 0.614044, loss_pred: 0.058729
iteration 1553: loss: 59.349644, loss_kl: 0.162214, loss_recon: 0.592419, loss_pred: 0.106124
iteration 1554: loss: 59.563267, loss_kl: 0.104312, loss_recon: 0.594123, loss_pred: 0.149901
iteration 1555: loss: 60.337162, loss_kl: 0.112190, loss_recon: 0.602427, loss_pred: 0.093311
iteration 1556: loss: 60.919788, loss_kl: 0.181447, loss_recon: 0.608624, loss_pred: 0.055556
iteration 1557: loss: 61.153183, loss_kl: 0.023701, loss_recon: 0.609989, loss_pred: 0.154023
iteration 1558: loss: 62.282333, loss_kl: 0.004358, loss_recon: 0.621613, loss_pred: 0.120972
iteration 1559: loss: 61.199879, loss_kl: 0.000414, loss_recon: 0.610914, loss_pred: 0.108460
iteration 1560: loss: 61.106213, loss_kl: 0.138932, loss_recon: 0.609271, loss_pred: 0.177749
 78%|██████████████████████▌      | 156/200 [2:09:43<36:35, 49.89s/it]iteration 1561: loss: 60.201813, loss_kl: 0.036391, loss_recon: 0.601264, loss_pred: 0.075069
iteration 1562: loss: 59.592518, loss_kl: 0.195009, loss_recon: 0.594827, loss_pred: 0.107849
iteration 1563: loss: 60.909641, loss_kl: 0.216811, loss_recon: 0.608205, loss_pred: 0.086989
iteration 1564: loss: 61.323761, loss_kl: 0.122690, loss_recon: 0.612271, loss_pred: 0.095452
iteration 1565: loss: 61.319496, loss_kl: -0.038051, loss_recon: 0.612077, loss_pred: 0.112212
iteration 1566: loss: 61.329319, loss_kl: 0.206797, loss_recon: 0.612615, loss_pred: 0.065760
iteration 1567: loss: 60.900280, loss_kl: 0.051507, loss_recon: 0.607704, loss_pred: 0.129403
iteration 1568: loss: 61.244102, loss_kl: 0.287525, loss_recon: 0.611011, loss_pred: 0.140151
iteration 1569: loss: 61.031792, loss_kl: 0.041192, loss_recon: 0.609620, loss_pred: 0.069361
iteration 1570: loss: 59.320850, loss_kl: 0.283767, loss_recon: 0.592838, loss_pred: 0.034185
 78%|██████████████████████▊      | 157/200 [2:10:33<35:43, 49.86s/it]iteration 1571: loss: 60.700130, loss_kl: 0.038828, loss_recon: 0.606468, loss_pred: 0.052962
iteration 1572: loss: 62.179939, loss_kl: 0.092070, loss_recon: 0.621254, loss_pred: 0.053592
iteration 1573: loss: 60.239021, loss_kl: -0.052573, loss_recon: 0.601650, loss_pred: 0.074511
iteration 1574: loss: 61.040390, loss_kl: 0.043108, loss_recon: 0.609704, loss_pred: 0.069588
iteration 1575: loss: 60.508282, loss_kl: 0.057914, loss_recon: 0.604106, loss_pred: 0.097098
iteration 1576: loss: 60.438984, loss_kl: 0.098014, loss_recon: 0.603752, loss_pred: 0.062833
iteration 1577: loss: 59.647263, loss_kl: 0.171092, loss_recon: 0.595798, loss_pred: 0.065794
iteration 1578: loss: 60.034801, loss_kl: 0.083673, loss_recon: 0.599687, loss_pred: 0.065271
iteration 1579: loss: 62.304165, loss_kl: 0.059632, loss_recon: 0.622361, loss_pred: 0.067505
iteration 1580: loss: 61.293617, loss_kl: 0.006035, loss_recon: 0.612209, loss_pred: 0.072644
 79%|██████████████████████▉      | 158/200 [2:11:22<34:52, 49.82s/it]iteration 1581: loss: 61.775066, loss_kl: 0.151040, loss_recon: 0.616802, loss_pred: 0.093366
iteration 1582: loss: 59.936153, loss_kl: 0.133560, loss_recon: 0.598605, loss_pred: 0.074291
iteration 1583: loss: 59.972694, loss_kl: 0.061516, loss_recon: 0.599037, loss_pred: 0.068408
iteration 1584: loss: 60.077221, loss_kl: 0.234419, loss_recon: 0.599588, loss_pred: 0.116053
iteration 1585: loss: 60.466061, loss_kl: 0.289042, loss_recon: 0.603625, loss_pred: 0.100673
iteration 1586: loss: 62.080807, loss_kl: -0.061111, loss_recon: 0.620012, loss_pred: 0.080201
iteration 1587: loss: 60.720402, loss_kl: 0.142208, loss_recon: 0.606336, loss_pred: 0.085341
iteration 1588: loss: 61.762821, loss_kl: 0.244034, loss_recon: 0.616861, loss_pred: 0.074263
iteration 1589: loss: 60.292702, loss_kl: 0.014442, loss_recon: 0.602064, loss_pred: 0.086177
iteration 1590: loss: 60.567295, loss_kl: -0.025915, loss_recon: 0.604922, loss_pred: 0.075347
 80%|███████████████████████      | 159/200 [2:12:12<33:56, 49.67s/it]iteration 1591: loss: 59.362553, loss_kl: 0.021538, loss_recon: 0.592967, loss_pred: 0.065660
iteration 1592: loss: 59.200821, loss_kl: 0.090011, loss_recon: 0.591236, loss_pred: 0.076329
iteration 1593: loss: 61.025715, loss_kl: 0.066558, loss_recon: 0.609701, loss_pred: 0.054978
iteration 1594: loss: 61.365124, loss_kl: 0.118574, loss_recon: 0.612971, loss_pred: 0.066879
iteration 1595: loss: 59.862942, loss_kl: 0.256238, loss_recon: 0.597864, loss_pred: 0.074024
iteration 1596: loss: 61.450890, loss_kl: -0.006448, loss_recon: 0.613937, loss_pred: 0.057292
iteration 1597: loss: 61.627609, loss_kl: 0.073356, loss_recon: 0.615500, loss_pred: 0.076838
iteration 1598: loss: 60.637646, loss_kl: 0.165154, loss_recon: 0.605516, loss_pred: 0.084393
iteration 1599: loss: 61.715874, loss_kl: 0.192082, loss_recon: 0.616441, loss_pred: 0.069895
iteration 1600: loss: 63.121529, loss_kl: -0.177388, loss_recon: 0.630604, loss_pred: 0.062942
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_159.pth
 80%|███████████████████████▏     | 160/200 [2:13:01<33:05, 49.63s/it]iteration 1601: loss: 60.116661, loss_kl: 0.044410, loss_recon: 0.600430, loss_pred: 0.073258
iteration 1602: loss: 61.077576, loss_kl: 0.201853, loss_recon: 0.610169, loss_pred: 0.058630
iteration 1603: loss: 60.318596, loss_kl: 0.183664, loss_recon: 0.602542, loss_pred: 0.062591
iteration 1604: loss: 61.405567, loss_kl: 0.096439, loss_recon: 0.613302, loss_pred: 0.074411
iteration 1605: loss: 61.532341, loss_kl: 0.089653, loss_recon: 0.614730, loss_pred: 0.058468
iteration 1606: loss: 60.735535, loss_kl: 0.154523, loss_recon: 0.606562, loss_pred: 0.077833
iteration 1607: loss: 60.506145, loss_kl: 0.060398, loss_recon: 0.604343, loss_pred: 0.071211
iteration 1608: loss: 60.917458, loss_kl: 0.100556, loss_recon: 0.608346, loss_pred: 0.081893
iteration 1609: loss: 60.027225, loss_kl: 0.040058, loss_recon: 0.599352, loss_pred: 0.091659
iteration 1610: loss: 61.065777, loss_kl: 0.325610, loss_recon: 0.609708, loss_pred: 0.091737
 80%|███████████████████████▎     | 161/200 [2:13:50<32:12, 49.54s/it]iteration 1611: loss: 61.384541, loss_kl: 0.145674, loss_recon: 0.613052, loss_pred: 0.077898
iteration 1612: loss: 60.039528, loss_kl: 0.149289, loss_recon: 0.599709, loss_pred: 0.067140
iteration 1613: loss: 61.116028, loss_kl: 0.047393, loss_recon: 0.610009, loss_pred: 0.114617
iteration 1614: loss: 61.234627, loss_kl: 0.275247, loss_recon: 0.611677, loss_pred: 0.064220
iteration 1615: loss: 60.426968, loss_kl: 0.246956, loss_recon: 0.603367, loss_pred: 0.087778
iteration 1616: loss: 60.335068, loss_kl: 0.163181, loss_recon: 0.602790, loss_pred: 0.054465
iteration 1617: loss: 62.240620, loss_kl: 0.106038, loss_recon: 0.621721, loss_pred: 0.067463
iteration 1618: loss: 59.043396, loss_kl: -0.008308, loss_recon: 0.589650, loss_pred: 0.078483
iteration 1619: loss: 60.595715, loss_kl: 0.330434, loss_recon: 0.605013, loss_pred: 0.091145
iteration 1620: loss: 61.808216, loss_kl: 0.090038, loss_recon: 0.617495, loss_pred: 0.057815
 81%|███████████████████████▍     | 162/200 [2:14:41<31:31, 49.77s/it]iteration 1621: loss: 60.363365, loss_kl: 0.096874, loss_recon: 0.602785, loss_pred: 0.083917
iteration 1622: loss: 60.695740, loss_kl: 0.010267, loss_recon: 0.606143, loss_pred: 0.081329
iteration 1623: loss: 60.853458, loss_kl: 0.024293, loss_recon: 0.607910, loss_pred: 0.062198
iteration 1624: loss: 59.991398, loss_kl: 0.149249, loss_recon: 0.599311, loss_pred: 0.058788
iteration 1625: loss: 60.985676, loss_kl: 0.147461, loss_recon: 0.609204, loss_pred: 0.063792
iteration 1626: loss: 61.194260, loss_kl: 0.371631, loss_recon: 0.611466, loss_pred: 0.043986
iteration 1627: loss: 60.464745, loss_kl: 0.027468, loss_recon: 0.603940, loss_pred: 0.070510
iteration 1628: loss: 61.831165, loss_kl: 0.190540, loss_recon: 0.617635, loss_pred: 0.065719
iteration 1629: loss: 61.234737, loss_kl: 0.221386, loss_recon: 0.611429, loss_pred: 0.089628
iteration 1630: loss: 59.316719, loss_kl: 0.276551, loss_recon: 0.592318, loss_pred: 0.082155
 82%|███████████████████████▋     | 163/200 [2:15:31<30:46, 49.91s/it]iteration 1631: loss: 59.850914, loss_kl: 0.197490, loss_recon: 0.597662, loss_pred: 0.074876
iteration 1632: loss: 60.264755, loss_kl: -0.021810, loss_recon: 0.601887, loss_pred: 0.077110
iteration 1633: loss: 62.448872, loss_kl: -0.012847, loss_recon: 0.623987, loss_pred: 0.050812
iteration 1634: loss: 61.931759, loss_kl: 0.078602, loss_recon: 0.618576, loss_pred: 0.070237
iteration 1635: loss: 61.055435, loss_kl: 0.248913, loss_recon: 0.609495, loss_pred: 0.093610
iteration 1636: loss: 59.267635, loss_kl: 0.157215, loss_recon: 0.591613, loss_pred: 0.098555
iteration 1637: loss: 61.667442, loss_kl: 0.140426, loss_recon: 0.615873, loss_pred: 0.073134
iteration 1638: loss: 59.476444, loss_kl: 0.027471, loss_recon: 0.594001, loss_pred: 0.075008
iteration 1639: loss: 60.891251, loss_kl: 0.095984, loss_recon: 0.608178, loss_pred: 0.068667
iteration 1640: loss: 61.383110, loss_kl: 0.462506, loss_recon: 0.612914, loss_pred: 0.068731
 82%|███████████████████████▊     | 164/200 [2:16:21<29:57, 49.92s/it]iteration 1641: loss: 61.380539, loss_kl: 0.029245, loss_recon: 0.613223, loss_pred: 0.055597
iteration 1642: loss: 59.884090, loss_kl: 0.170461, loss_recon: 0.597753, loss_pred: 0.093585
iteration 1643: loss: 61.088352, loss_kl: 0.079075, loss_recon: 0.609855, loss_pred: 0.095848
iteration 1644: loss: 60.175819, loss_kl: 0.107796, loss_recon: 0.600862, loss_pred: 0.079996
iteration 1645: loss: 61.980564, loss_kl: 0.045647, loss_recon: 0.619101, loss_pred: 0.066385
iteration 1646: loss: 60.774117, loss_kl: 0.011477, loss_recon: 0.606861, loss_pred: 0.087014
iteration 1647: loss: 62.245995, loss_kl: 0.102549, loss_recon: 0.621383, loss_pred: 0.098514
iteration 1648: loss: 60.820339, loss_kl: 0.122664, loss_recon: 0.607243, loss_pred: 0.085051
iteration 1649: loss: 58.831657, loss_kl: 0.073357, loss_recon: 0.587080, loss_pred: 0.117068
iteration 1650: loss: 60.517696, loss_kl: 0.145166, loss_recon: 0.604281, loss_pred: 0.076599
 82%|███████████████████████▉     | 165/200 [2:17:11<29:03, 49.82s/it]iteration 1651: loss: 61.634808, loss_kl: 0.125447, loss_recon: 0.615515, loss_pred: 0.067154
iteration 1652: loss: 61.101894, loss_kl: 0.219523, loss_recon: 0.610139, loss_pred: 0.059668
iteration 1653: loss: 61.523483, loss_kl: 0.182088, loss_recon: 0.614314, loss_pred: 0.068600
iteration 1654: loss: 61.434338, loss_kl: -0.020528, loss_recon: 0.613684, loss_pred: 0.068631
iteration 1655: loss: 59.625145, loss_kl: 0.101904, loss_recon: 0.595071, loss_pred: 0.104870
iteration 1656: loss: 59.618031, loss_kl: 0.162690, loss_recon: 0.594753, loss_pred: 0.121751
iteration 1657: loss: 59.540554, loss_kl: 0.142422, loss_recon: 0.594366, loss_pred: 0.085621
iteration 1658: loss: 61.263775, loss_kl: 0.113430, loss_recon: 0.611854, loss_pred: 0.063738
iteration 1659: loss: 61.752167, loss_kl: 0.114227, loss_recon: 0.616535, loss_pred: 0.083958
iteration 1660: loss: 60.443199, loss_kl: 0.148459, loss_recon: 0.603498, loss_pred: 0.074260
 83%|████████████████████████     | 166/200 [2:18:01<28:15, 49.86s/it]iteration 1661: loss: 60.180134, loss_kl: 0.063523, loss_recon: 0.601149, loss_pred: 0.054558
iteration 1662: loss: 60.715748, loss_kl: 0.106022, loss_recon: 0.606212, loss_pred: 0.076742
iteration 1663: loss: 61.540932, loss_kl: 0.040362, loss_recon: 0.614511, loss_pred: 0.083027
iteration 1664: loss: 60.887486, loss_kl: -0.072051, loss_recon: 0.608002, loss_pred: 0.099378
iteration 1665: loss: 60.695122, loss_kl: -0.046770, loss_recon: 0.606199, loss_pred: 0.083086
iteration 1666: loss: 59.731510, loss_kl: 0.045881, loss_recon: 0.596226, loss_pred: 0.101235
iteration 1667: loss: 60.390991, loss_kl: 0.033843, loss_recon: 0.603128, loss_pred: 0.072459
iteration 1668: loss: 61.166561, loss_kl: 0.061623, loss_recon: 0.610880, loss_pred: 0.068159
iteration 1669: loss: 61.867714, loss_kl: 0.109275, loss_recon: 0.617890, loss_pred: 0.060352
iteration 1670: loss: 60.633347, loss_kl: 0.108692, loss_recon: 0.605016, loss_pred: 0.113475
 84%|████████████████████████▏    | 167/200 [2:18:51<27:36, 50.20s/it]iteration 1671: loss: 60.251385, loss_kl: 0.019036, loss_recon: 0.601894, loss_pred: 0.058060
iteration 1672: loss: 62.667873, loss_kl: 0.065604, loss_recon: 0.626013, loss_pred: 0.052919
iteration 1673: loss: 61.103390, loss_kl: 0.010368, loss_recon: 0.610259, loss_pred: 0.075350
iteration 1674: loss: 60.820873, loss_kl: 0.026208, loss_recon: 0.607523, loss_pred: 0.063107
iteration 1675: loss: 60.819901, loss_kl: 0.083301, loss_recon: 0.607453, loss_pred: 0.057281
iteration 1676: loss: 60.277897, loss_kl: -0.069109, loss_recon: 0.602342, loss_pred: 0.058031
iteration 1677: loss: 60.658173, loss_kl: 0.062879, loss_recon: 0.605549, loss_pred: 0.090188
iteration 1678: loss: 60.667171, loss_kl: 0.058458, loss_recon: 0.605747, loss_pred: 0.080293
iteration 1679: loss: 60.250740, loss_kl: 0.059191, loss_recon: 0.601636, loss_pred: 0.074847
iteration 1680: loss: 59.192654, loss_kl: 0.056415, loss_recon: 0.590519, loss_pred: 0.129043
 84%|████████████████████████▎    | 168/200 [2:19:41<26:42, 50.07s/it]iteration 1681: loss: 60.090214, loss_kl: 0.077066, loss_recon: 0.599495, loss_pred: 0.121640
iteration 1682: loss: 61.786701, loss_kl: -0.012864, loss_recon: 0.616995, loss_pred: 0.090407
iteration 1683: loss: 61.065178, loss_kl: 0.075922, loss_recon: 0.609117, loss_pred: 0.134661
iteration 1684: loss: 60.460831, loss_kl: 0.044624, loss_recon: 0.603497, loss_pred: 0.100051
iteration 1685: loss: 61.212002, loss_kl: 0.084899, loss_recon: 0.610769, loss_pred: 0.114117
iteration 1686: loss: 60.664970, loss_kl: 0.098261, loss_recon: 0.605386, loss_pred: 0.102074
iteration 1687: loss: 61.287697, loss_kl: 0.090674, loss_recon: 0.611944, loss_pred: 0.070817
iteration 1688: loss: 61.160599, loss_kl: -0.013820, loss_recon: 0.610672, loss_pred: 0.096838
iteration 1689: loss: 59.752823, loss_kl: 0.130936, loss_recon: 0.596222, loss_pred: 0.098239
iteration 1690: loss: 61.504158, loss_kl: -0.035836, loss_recon: 0.614574, loss_pred: 0.055598
 84%|████████████████████████▌    | 169/200 [2:20:30<25:41, 49.73s/it]iteration 1691: loss: 59.888298, loss_kl: 0.004600, loss_recon: 0.598307, loss_pred: 0.056251
iteration 1692: loss: 61.033115, loss_kl: -0.041443, loss_recon: 0.609790, loss_pred: 0.066013
iteration 1693: loss: 60.703022, loss_kl: 0.001802, loss_recon: 0.606467, loss_pred: 0.055755
iteration 1694: loss: 61.031696, loss_kl: -0.003912, loss_recon: 0.609714, loss_pred: 0.061390
iteration 1695: loss: 61.277199, loss_kl: -0.032490, loss_recon: 0.611919, loss_pred: 0.094609
iteration 1696: loss: 59.615631, loss_kl: 0.042514, loss_recon: 0.595213, loss_pred: 0.082096
iteration 1697: loss: 62.074871, loss_kl: 0.018477, loss_recon: 0.620053, loss_pred: 0.064257
iteration 1698: loss: 60.772160, loss_kl: 0.170671, loss_recon: 0.606477, loss_pred: 0.075489
iteration 1699: loss: 60.572788, loss_kl: 0.046203, loss_recon: 0.604900, loss_pred: 0.069548
iteration 1700: loss: 60.654087, loss_kl: 0.033444, loss_recon: 0.605566, loss_pred: 0.087921
 85%|████████████████████████▋    | 170/200 [2:21:20<24:54, 49.83s/it]iteration 1701: loss: 61.582348, loss_kl: -0.001789, loss_recon: 0.614554, loss_pred: 0.127520
iteration 1702: loss: 60.303577, loss_kl: 0.051703, loss_recon: 0.602115, loss_pred: 0.075166
iteration 1703: loss: 60.666073, loss_kl: 0.055888, loss_recon: 0.605593, loss_pred: 0.088504
iteration 1704: loss: 60.204346, loss_kl: 0.091951, loss_recon: 0.600757, loss_pred: 0.098573
iteration 1705: loss: 60.813618, loss_kl: 0.099396, loss_recon: 0.607018, loss_pred: 0.079325
iteration 1706: loss: 60.939716, loss_kl: 0.032792, loss_recon: 0.607944, loss_pred: 0.134626
iteration 1707: loss: 60.834805, loss_kl: 0.017147, loss_recon: 0.607529, loss_pred: 0.076311
iteration 1708: loss: 61.411423, loss_kl: 0.104851, loss_recon: 0.613158, loss_pred: 0.061325
iteration 1709: loss: 60.923805, loss_kl: -0.035838, loss_recon: 0.608417, loss_pred: 0.093818
iteration 1710: loss: 61.118462, loss_kl: 0.030706, loss_recon: 0.610073, loss_pred: 0.101097
 86%|████████████████████████▊    | 171/200 [2:22:10<24:03, 49.76s/it]iteration 1711: loss: 60.177364, loss_kl: 0.110684, loss_recon: 0.600520, loss_pred: 0.084765
iteration 1712: loss: 60.963024, loss_kl: 0.098788, loss_recon: 0.608527, loss_pred: 0.074172
iteration 1713: loss: 62.005680, loss_kl: 0.016872, loss_recon: 0.619380, loss_pred: 0.061454
iteration 1714: loss: 61.400692, loss_kl: 0.026074, loss_recon: 0.613009, loss_pred: 0.090237
iteration 1715: loss: 60.823906, loss_kl: 0.060282, loss_recon: 0.607375, loss_pred: 0.064365
iteration 1716: loss: 60.793037, loss_kl: -0.019045, loss_recon: 0.607325, loss_pred: 0.067520
iteration 1717: loss: 59.624336, loss_kl: -0.025750, loss_recon: 0.595276, loss_pred: 0.106183
iteration 1718: loss: 60.635868, loss_kl: 0.010952, loss_recon: 0.605588, loss_pred: 0.073101
iteration 1719: loss: 61.054214, loss_kl: 0.093491, loss_recon: 0.609575, loss_pred: 0.062495
iteration 1720: loss: 60.704327, loss_kl: -0.026976, loss_recon: 0.605490, loss_pred: 0.165245
 86%|████████████████████████▉    | 172/200 [2:23:00<23:19, 49.98s/it]iteration 1721: loss: 61.906746, loss_kl: 0.058301, loss_recon: 0.618086, loss_pred: 0.074466
iteration 1722: loss: 60.395969, loss_kl: 0.059675, loss_recon: 0.603163, loss_pred: 0.055401
iteration 1723: loss: 59.909679, loss_kl: 0.024882, loss_recon: 0.598079, loss_pred: 0.091681
iteration 1724: loss: 61.467804, loss_kl: 0.045374, loss_recon: 0.613766, loss_pred: 0.072746
iteration 1725: loss: 61.239281, loss_kl: 0.071109, loss_recon: 0.611421, loss_pred: 0.068282
iteration 1726: loss: 61.407692, loss_kl: 0.052284, loss_recon: 0.613046, loss_pred: 0.081832
iteration 1727: loss: 59.913998, loss_kl: 0.137952, loss_recon: 0.597782, loss_pred: 0.079748
iteration 1728: loss: 60.551224, loss_kl: 0.084607, loss_recon: 0.604360, loss_pred: 0.080846
iteration 1729: loss: 60.700233, loss_kl: 0.034913, loss_recon: 0.605946, loss_pred: 0.091454
iteration 1730: loss: 60.935127, loss_kl: -0.094829, loss_recon: 0.609010, loss_pred: 0.072673
 86%|█████████████████████████    | 173/200 [2:23:50<22:25, 49.84s/it]iteration 1731: loss: 60.847950, loss_kl: 0.148893, loss_recon: 0.607067, loss_pred: 0.074956
iteration 1732: loss: 62.320770, loss_kl: 0.079063, loss_recon: 0.622271, loss_pred: 0.058413
iteration 1733: loss: 61.176117, loss_kl: 0.127877, loss_recon: 0.610439, loss_pred: 0.075224
iteration 1734: loss: 60.777943, loss_kl: 0.019317, loss_recon: 0.606728, loss_pred: 0.096490
iteration 1735: loss: 61.283264, loss_kl: 0.070643, loss_recon: 0.611671, loss_pred: 0.084702
iteration 1736: loss: 59.735825, loss_kl: 0.078626, loss_recon: 0.596206, loss_pred: 0.080239
iteration 1737: loss: 60.341785, loss_kl: 0.183496, loss_recon: 0.601614, loss_pred: 0.098662
iteration 1738: loss: 61.850868, loss_kl: 0.077080, loss_recon: 0.617322, loss_pred: 0.084355
iteration 1739: loss: 59.697220, loss_kl: 0.104661, loss_recon: 0.595676, loss_pred: 0.082939
iteration 1740: loss: 59.046932, loss_kl: 0.071526, loss_recon: 0.589346, loss_pred: 0.080448
 87%|█████████████████████████▏   | 174/200 [2:24:39<21:34, 49.77s/it]iteration 1741: loss: 60.237213, loss_kl: 0.087114, loss_recon: 0.600422, loss_pred: 0.152732
iteration 1742: loss: 59.965633, loss_kl: 0.095997, loss_recon: 0.598224, loss_pred: 0.096637
iteration 1743: loss: 61.741787, loss_kl: 0.106189, loss_recon: 0.616227, loss_pred: 0.067583
iteration 1744: loss: 60.230839, loss_kl: 0.070461, loss_recon: 0.601236, loss_pred: 0.073030
iteration 1745: loss: 61.639576, loss_kl: 0.079755, loss_recon: 0.614853, loss_pred: 0.115543
iteration 1746: loss: 60.783749, loss_kl: 0.050653, loss_recon: 0.606786, loss_pred: 0.080564
iteration 1747: loss: 61.162327, loss_kl: 0.107865, loss_recon: 0.610578, loss_pred: 0.052224
iteration 1748: loss: 61.241570, loss_kl: 0.098806, loss_recon: 0.611375, loss_pred: 0.056084
iteration 1749: loss: 60.271709, loss_kl: 0.090549, loss_recon: 0.601536, loss_pred: 0.074188
iteration 1750: loss: 61.002361, loss_kl: 0.147171, loss_recon: 0.608238, loss_pred: 0.107125
 88%|█████████████████████████▍   | 175/200 [2:25:30<20:51, 50.07s/it]iteration 1751: loss: 61.235218, loss_kl: 0.068850, loss_recon: 0.611377, loss_pred: 0.061359
iteration 1752: loss: 60.993816, loss_kl: 0.059252, loss_recon: 0.609024, loss_pred: 0.060362
iteration 1753: loss: 60.256050, loss_kl: 0.015524, loss_recon: 0.601639, loss_pred: 0.084041
iteration 1754: loss: 61.406330, loss_kl: 0.060866, loss_recon: 0.613038, loss_pred: 0.070556
iteration 1755: loss: 60.972538, loss_kl: 0.089213, loss_recon: 0.608117, loss_pred: 0.113979
iteration 1756: loss: 60.660748, loss_kl: 0.072931, loss_recon: 0.605363, loss_pred: 0.086156
iteration 1757: loss: 61.230564, loss_kl: -0.025200, loss_recon: 0.611899, loss_pred: 0.053926
iteration 1758: loss: 60.839294, loss_kl: 0.096268, loss_recon: 0.607037, loss_pred: 0.085073
iteration 1759: loss: 59.475151, loss_kl: 0.172861, loss_recon: 0.592392, loss_pred: 0.145231
iteration 1760: loss: 61.172298, loss_kl: -0.098353, loss_recon: 0.610836, loss_pred: 0.140331
 88%|█████████████████████████▌   | 176/200 [2:26:20<19:59, 49.97s/it]iteration 1761: loss: 59.796589, loss_kl: 0.109985, loss_recon: 0.596476, loss_pred: 0.086863
iteration 1762: loss: 62.097172, loss_kl: 0.148604, loss_recon: 0.619156, loss_pred: 0.097677
iteration 1763: loss: 61.391827, loss_kl: 0.097417, loss_recon: 0.612016, loss_pred: 0.135263
iteration 1764: loss: 60.193192, loss_kl: 0.173741, loss_recon: 0.599822, loss_pred: 0.112969
iteration 1765: loss: 60.689991, loss_kl: 0.037156, loss_recon: 0.606106, loss_pred: 0.058408
iteration 1766: loss: 60.458054, loss_kl: -0.043714, loss_recon: 0.603937, loss_pred: 0.089036
iteration 1767: loss: 60.556767, loss_kl: 0.107926, loss_recon: 0.603933, loss_pred: 0.102594
iteration 1768: loss: 61.318378, loss_kl: -0.022898, loss_recon: 0.612619, loss_pred: 0.069386
iteration 1769: loss: 60.740604, loss_kl: 0.073375, loss_recon: 0.606099, loss_pred: 0.089281
iteration 1770: loss: 61.252087, loss_kl: 0.021937, loss_recon: 0.611195, loss_pred: 0.120233
 88%|█████████████████████████▋   | 177/200 [2:27:12<19:24, 50.65s/it]iteration 1771: loss: 60.497829, loss_kl: 0.114699, loss_recon: 0.603173, loss_pred: 0.111202
iteration 1772: loss: 60.916275, loss_kl: 0.078583, loss_recon: 0.607267, loss_pred: 0.142100
iteration 1773: loss: 60.046921, loss_kl: 0.056217, loss_recon: 0.597732, loss_pred: 0.239735
iteration 1774: loss: 61.304649, loss_kl: 0.083363, loss_recon: 0.611862, loss_pred: 0.068105
iteration 1775: loss: 60.488380, loss_kl: 0.039562, loss_recon: 0.603709, loss_pred: 0.093599
iteration 1776: loss: 61.293724, loss_kl: 0.120968, loss_recon: 0.610825, loss_pred: 0.138210
iteration 1777: loss: 60.800510, loss_kl: 0.071096, loss_recon: 0.606208, loss_pred: 0.136790
iteration 1778: loss: 62.287071, loss_kl: 0.032226, loss_recon: 0.622064, loss_pred: 0.061254
iteration 1779: loss: 60.418591, loss_kl: 0.049268, loss_recon: 0.603294, loss_pred: 0.059416
iteration 1780: loss: 60.310497, loss_kl: 0.043179, loss_recon: 0.602009, loss_pred: 0.083541
 89%|█████████████████████████▊   | 178/200 [2:28:02<18:26, 50.28s/it]iteration 1781: loss: 60.913067, loss_kl: -0.027205, loss_recon: 0.608499, loss_pred: 0.080646
iteration 1782: loss: 60.615833, loss_kl: 0.019355, loss_recon: 0.604922, loss_pred: 0.111147
iteration 1783: loss: 59.985367, loss_kl: -0.035906, loss_recon: 0.599253, loss_pred: 0.083207
iteration 1784: loss: 60.753582, loss_kl: 0.188903, loss_recon: 0.605523, loss_pred: 0.079715
iteration 1785: loss: 60.912083, loss_kl: 0.034694, loss_recon: 0.608245, loss_pred: 0.065244
iteration 1786: loss: 62.758057, loss_kl: -0.011714, loss_recon: 0.626766, loss_pred: 0.088980
iteration 1787: loss: 60.526390, loss_kl: 0.161673, loss_recon: 0.603148, loss_pred: 0.107579
iteration 1788: loss: 61.770622, loss_kl: 0.219102, loss_recon: 0.615261, loss_pred: 0.103509
iteration 1789: loss: 58.863525, loss_kl: 0.129406, loss_recon: 0.586828, loss_pred: 0.097409
iteration 1790: loss: 62.886566, loss_kl: 0.109624, loss_recon: 0.627115, loss_pred: 0.104524
 90%|█████████████████████████▉   | 179/200 [2:28:53<17:40, 50.50s/it]iteration 1791: loss: 61.434353, loss_kl: 0.032679, loss_recon: 0.612388, loss_pred: 0.173218
iteration 1792: loss: 60.088219, loss_kl: -0.003619, loss_recon: 0.599727, loss_pred: 0.117989
iteration 1793: loss: 61.501068, loss_kl: 0.026044, loss_recon: 0.614045, loss_pred: 0.078823
iteration 1794: loss: 61.965069, loss_kl: 0.098846, loss_recon: 0.618419, loss_pred: 0.055596
iteration 1795: loss: 60.784409, loss_kl: 0.024443, loss_recon: 0.606943, loss_pred: 0.073375
iteration 1796: loss: 60.743946, loss_kl: 0.040008, loss_recon: 0.606180, loss_pred: 0.098628
iteration 1797: loss: 59.214016, loss_kl: 0.090422, loss_recon: 0.589830, loss_pred: 0.169267
iteration 1798: loss: 60.385777, loss_kl: -0.075965, loss_recon: 0.603235, loss_pred: 0.114217
iteration 1799: loss: 60.956112, loss_kl: 0.102373, loss_recon: 0.608234, loss_pred: 0.062792
iteration 1800: loss: 62.718334, loss_kl: -0.093070, loss_recon: 0.626588, loss_pred: 0.123116
 90%|██████████████████████████   | 180/200 [2:29:42<16:44, 50.23s/it]iteration 1801: loss: 60.926537, loss_kl: 0.081738, loss_recon: 0.607404, loss_pred: 0.127100
iteration 1802: loss: 61.148716, loss_kl: 0.056705, loss_recon: 0.610257, loss_pred: 0.081984
iteration 1803: loss: 61.493790, loss_kl: 0.066080, loss_recon: 0.613823, loss_pred: 0.063692
iteration 1804: loss: 60.443874, loss_kl: 0.037928, loss_recon: 0.603214, loss_pred: 0.095054
iteration 1805: loss: 60.528358, loss_kl: 0.026489, loss_recon: 0.604096, loss_pred: 0.099596
iteration 1806: loss: 60.969673, loss_kl: 0.085855, loss_recon: 0.608229, loss_pred: 0.084712
iteration 1807: loss: 60.915459, loss_kl: 0.056219, loss_recon: 0.607683, loss_pred: 0.106519
iteration 1808: loss: 61.429359, loss_kl: 0.123822, loss_recon: 0.612312, loss_pred: 0.108692
iteration 1809: loss: 60.404686, loss_kl: 0.130474, loss_recon: 0.602181, loss_pred: 0.092321
iteration 1810: loss: 60.963806, loss_kl: 0.142410, loss_recon: 0.607394, loss_pred: 0.121470
 90%|██████████████████████████▏  | 181/200 [2:30:32<15:51, 50.07s/it]iteration 1811: loss: 60.748917, loss_kl: 0.134076, loss_recon: 0.604853, loss_pred: 0.161398
iteration 1812: loss: 60.751732, loss_kl: 0.091165, loss_recon: 0.605403, loss_pred: 0.141902
iteration 1813: loss: 60.871254, loss_kl: 0.022835, loss_recon: 0.607491, loss_pred: 0.104704
iteration 1814: loss: 61.102711, loss_kl: -0.033337, loss_recon: 0.610458, loss_pred: 0.082326
iteration 1815: loss: 59.761341, loss_kl: 0.056195, loss_recon: 0.595688, loss_pred: 0.149679
iteration 1816: loss: 59.832710, loss_kl: 0.073091, loss_recon: 0.596666, loss_pred: 0.110342
iteration 1817: loss: 61.630924, loss_kl: 0.126053, loss_recon: 0.614505, loss_pred: 0.084288
iteration 1818: loss: 61.361439, loss_kl: 0.042965, loss_recon: 0.612644, loss_pred: 0.064286
iteration 1819: loss: 61.954762, loss_kl: -0.024924, loss_recon: 0.619258, loss_pred: 0.047989
iteration 1820: loss: 59.931644, loss_kl: -0.035332, loss_recon: 0.598543, loss_pred: 0.104318
 91%|██████████████████████████▍  | 182/200 [2:31:22<15:01, 50.06s/it]iteration 1821: loss: 60.409389, loss_kl: -0.017199, loss_recon: 0.603517, loss_pred: 0.071448
iteration 1822: loss: 59.686546, loss_kl: 0.106371, loss_recon: 0.594703, loss_pred: 0.130941
iteration 1823: loss: 61.777206, loss_kl: 0.046624, loss_recon: 0.616329, loss_pred: 0.106886
iteration 1824: loss: 60.024670, loss_kl: 0.038045, loss_recon: 0.599052, loss_pred: 0.088943
iteration 1825: loss: 60.748852, loss_kl: 0.057514, loss_recon: 0.606189, loss_pred: 0.083841
iteration 1826: loss: 60.938046, loss_kl: 0.155551, loss_recon: 0.607300, loss_pred: 0.083280
iteration 1827: loss: 61.581867, loss_kl: 0.052880, loss_recon: 0.614683, loss_pred: 0.071125
iteration 1828: loss: 60.567497, loss_kl: 0.044782, loss_recon: 0.604354, loss_pred: 0.096161
iteration 1829: loss: 62.168430, loss_kl: 0.091343, loss_recon: 0.620218, loss_pred: 0.073368
iteration 1830: loss: 62.235283, loss_kl: 0.159223, loss_recon: 0.620409, loss_pred: 0.066704
 92%|██████████████████████████▌  | 183/200 [2:32:12<14:08, 49.92s/it]iteration 1831: loss: 61.556538, loss_kl: -0.071583, loss_recon: 0.615285, loss_pred: 0.088314
iteration 1832: loss: 60.469036, loss_kl: 0.033849, loss_recon: 0.603739, loss_pred: 0.066609
iteration 1833: loss: 60.907909, loss_kl: -0.031530, loss_recon: 0.608594, loss_pred: 0.075018
iteration 1834: loss: 60.514626, loss_kl: 0.105675, loss_recon: 0.603630, loss_pred: 0.062682
iteration 1835: loss: 60.437569, loss_kl: 0.065580, loss_recon: 0.603164, loss_pred: 0.065946
iteration 1836: loss: 61.225765, loss_kl: -0.004270, loss_recon: 0.611786, loss_pred: 0.050739
iteration 1837: loss: 61.020214, loss_kl: 0.043386, loss_recon: 0.609210, loss_pred: 0.062721
iteration 1838: loss: 60.810875, loss_kl: 0.058482, loss_recon: 0.606781, loss_pred: 0.083525
iteration 1839: loss: 61.092239, loss_kl: 0.072580, loss_recon: 0.609196, loss_pred: 0.111583
iteration 1840: loss: 60.883595, loss_kl: 0.071215, loss_recon: 0.607652, loss_pred: 0.058455
 92%|██████████████████████████▋  | 184/200 [2:33:01<13:15, 49.69s/it]iteration 1841: loss: 61.762943, loss_kl: 0.033216, loss_recon: 0.616241, loss_pred: 0.109593
iteration 1842: loss: 59.808716, loss_kl: 0.076246, loss_recon: 0.596032, loss_pred: 0.138312
iteration 1843: loss: 60.315853, loss_kl: -0.024410, loss_recon: 0.602448, loss_pred: 0.092517
iteration 1844: loss: 60.846970, loss_kl: 0.075414, loss_recon: 0.607014, loss_pred: 0.079081
iteration 1845: loss: 59.751804, loss_kl: 0.047235, loss_recon: 0.596350, loss_pred: 0.075134
iteration 1846: loss: 61.358986, loss_kl: 0.092755, loss_recon: 0.611899, loss_pred: 0.087321
iteration 1847: loss: 61.508331, loss_kl: 0.017455, loss_recon: 0.614346, loss_pred: 0.058377
iteration 1848: loss: 61.311893, loss_kl: 0.109807, loss_recon: 0.611130, loss_pred: 0.102142
iteration 1849: loss: 60.917713, loss_kl: 0.006754, loss_recon: 0.607117, loss_pred: 0.200046
iteration 1850: loss: 62.219738, loss_kl: 0.152942, loss_recon: 0.620073, loss_pred: 0.077696
 92%|██████████████████████████▊  | 185/200 [2:33:52<12:32, 50.13s/it]iteration 1851: loss: 62.078079, loss_kl: 0.123072, loss_recon: 0.618755, loss_pred: 0.089276
iteration 1852: loss: 61.604729, loss_kl: 0.128705, loss_recon: 0.613942, loss_pred: 0.092056
iteration 1853: loss: 60.507408, loss_kl: 0.064116, loss_recon: 0.603545, loss_pred: 0.093839
iteration 1854: loss: 61.659111, loss_kl: 0.087188, loss_recon: 0.615107, loss_pred: 0.068107
iteration 1855: loss: 60.015980, loss_kl: 0.116802, loss_recon: 0.598200, loss_pred: 0.088443
iteration 1856: loss: 59.619457, loss_kl: 0.041832, loss_recon: 0.595046, loss_pred: 0.076348
iteration 1857: loss: 61.254120, loss_kl: 0.114078, loss_recon: 0.610846, loss_pred: 0.064437
iteration 1858: loss: 60.774029, loss_kl: -0.009041, loss_recon: 0.606925, loss_pred: 0.089875
iteration 1859: loss: 60.220024, loss_kl: 0.040001, loss_recon: 0.600650, loss_pred: 0.118213
iteration 1860: loss: 62.388725, loss_kl: 0.097361, loss_recon: 0.621825, loss_pred: 0.116553
 93%|██████████████████████████▉  | 186/200 [2:34:42<11:41, 50.08s/it]iteration 1861: loss: 60.995590, loss_kl: 0.088361, loss_recon: 0.607907, loss_pred: 0.120071
iteration 1862: loss: 62.348454, loss_kl: 0.002709, loss_recon: 0.622803, loss_pred: 0.065537
iteration 1863: loss: 61.253971, loss_kl: -0.012652, loss_recon: 0.611971, loss_pred: 0.069061
iteration 1864: loss: 60.198318, loss_kl: 0.048975, loss_recon: 0.600814, loss_pred: 0.069879
iteration 1865: loss: 61.557781, loss_kl: 0.073269, loss_recon: 0.614370, loss_pred: 0.050404
iteration 1866: loss: 60.325348, loss_kl: 0.138518, loss_recon: 0.601221, loss_pred: 0.070177
iteration 1867: loss: 60.351810, loss_kl: 0.172261, loss_recon: 0.601021, loss_pred: 0.084313
iteration 1868: loss: 61.166840, loss_kl: 0.089221, loss_recon: 0.610310, loss_pred: 0.050143
iteration 1869: loss: 59.928585, loss_kl: 0.126003, loss_recon: 0.597374, loss_pred: 0.070163
iteration 1870: loss: 60.374100, loss_kl: 0.079826, loss_recon: 0.601897, loss_pred: 0.107775
 94%|███████████████████████████  | 187/200 [2:35:32<10:51, 50.13s/it]iteration 1871: loss: 60.359570, loss_kl: 0.162251, loss_recon: 0.601303, loss_pred: 0.067041
iteration 1872: loss: 61.291523, loss_kl: 0.159264, loss_recon: 0.610702, loss_pred: 0.062019
iteration 1873: loss: 60.222652, loss_kl: 0.203008, loss_recon: 0.599445, loss_pred: 0.075196
iteration 1874: loss: 61.604450, loss_kl: 0.110490, loss_recon: 0.614258, loss_pred: 0.068136
iteration 1875: loss: 60.230743, loss_kl: 0.046657, loss_recon: 0.601152, loss_pred: 0.068908
iteration 1876: loss: 60.745262, loss_kl: 0.197525, loss_recon: 0.604778, loss_pred: 0.069949
iteration 1877: loss: 61.763615, loss_kl: 0.110474, loss_recon: 0.614495, loss_pred: 0.203606
iteration 1878: loss: 60.392834, loss_kl: 0.116303, loss_recon: 0.601752, loss_pred: 0.101294
iteration 1879: loss: 60.474342, loss_kl: -0.028217, loss_recon: 0.604206, loss_pred: 0.081943
iteration 1880: loss: 64.598610, loss_kl: -0.047012, loss_recon: 0.645720, loss_pred: 0.073591
 94%|███████████████████████████▎ | 188/200 [2:36:21<09:57, 49.79s/it]iteration 1881: loss: 61.334267, loss_kl: 0.017458, loss_recon: 0.611711, loss_pred: 0.145687
iteration 1882: loss: 60.476669, loss_kl: 0.060932, loss_recon: 0.603277, loss_pred: 0.088041
iteration 1883: loss: 60.979465, loss_kl: 0.082305, loss_recon: 0.608186, loss_pred: 0.078605
iteration 1884: loss: 61.087162, loss_kl: 0.247262, loss_recon: 0.607195, loss_pred: 0.120404
iteration 1885: loss: 62.936306, loss_kl: 0.103294, loss_recon: 0.627806, loss_pred: 0.052432
iteration 1886: loss: 59.919941, loss_kl: 0.100328, loss_recon: 0.597271, loss_pred: 0.092483
iteration 1887: loss: 59.162292, loss_kl: 0.065800, loss_recon: 0.589994, loss_pred: 0.097040
iteration 1888: loss: 61.849628, loss_kl: 0.153000, loss_recon: 0.616180, loss_pred: 0.078675
iteration 1889: loss: 60.237495, loss_kl: 0.028503, loss_recon: 0.601123, loss_pred: 0.096664
iteration 1890: loss: 60.653313, loss_kl: 0.128622, loss_recon: 0.604121, loss_pred: 0.112613
 94%|███████████████████████████▍ | 189/200 [2:37:11<09:08, 49.82s/it]iteration 1891: loss: 60.685604, loss_kl: 0.054964, loss_recon: 0.605602, loss_pred: 0.070454
iteration 1892: loss: 60.102840, loss_kl: 0.041174, loss_recon: 0.600016, loss_pred: 0.060058
iteration 1893: loss: 61.618885, loss_kl: 0.055969, loss_recon: 0.614688, loss_pred: 0.094154
iteration 1894: loss: 60.338829, loss_kl: 0.182077, loss_recon: 0.600681, loss_pred: 0.088624
iteration 1895: loss: 61.107426, loss_kl: 0.202879, loss_recon: 0.608500, loss_pred: 0.054572
iteration 1896: loss: 60.304989, loss_kl: 0.049557, loss_recon: 0.601798, loss_pred: 0.075677
iteration 1897: loss: 61.829926, loss_kl: 0.294104, loss_recon: 0.613983, loss_pred: 0.137486
iteration 1898: loss: 60.956207, loss_kl: 0.341050, loss_recon: 0.605146, loss_pred: 0.100530
iteration 1899: loss: 61.156120, loss_kl: 0.184823, loss_recon: 0.609075, loss_pred: 0.063798
iteration 1900: loss: 61.462337, loss_kl: 0.026863, loss_recon: 0.613337, loss_pred: 0.101798
 95%|███████████████████████████▌ | 190/200 [2:38:00<08:15, 49.56s/it]iteration 1901: loss: 60.623295, loss_kl: -0.003296, loss_recon: 0.604868, loss_pred: 0.139818
iteration 1902: loss: 61.417580, loss_kl: 0.049741, loss_recon: 0.612290, loss_pred: 0.138873
iteration 1903: loss: 61.435287, loss_kl: 0.115850, loss_recon: 0.611732, loss_pred: 0.146214
iteration 1904: loss: 61.681652, loss_kl: 0.030185, loss_recon: 0.615252, loss_pred: 0.126303
iteration 1905: loss: 60.286133, loss_kl: 0.080191, loss_recon: 0.601141, loss_pred: 0.091867
iteration 1906: loss: 61.019875, loss_kl: 0.097313, loss_recon: 0.608299, loss_pred: 0.092682
iteration 1907: loss: 60.429890, loss_kl: 0.047799, loss_recon: 0.602321, loss_pred: 0.149994
iteration 1908: loss: 59.839752, loss_kl: -0.027508, loss_recon: 0.597483, loss_pred: 0.118950
iteration 1909: loss: 60.517265, loss_kl: 0.026070, loss_recon: 0.604345, loss_pred: 0.056727
iteration 1910: loss: 62.742363, loss_kl: 0.073787, loss_recon: 0.625645, loss_pred: 0.104059
 96%|███████████████████████████▋ | 191/200 [2:38:50<07:26, 49.57s/it]iteration 1911: loss: 61.191879, loss_kl: 0.114238, loss_recon: 0.610018, loss_pred: 0.075832
iteration 1912: loss: 59.764965, loss_kl: 0.115778, loss_recon: 0.595966, loss_pred: 0.052634
iteration 1913: loss: 59.955853, loss_kl: 0.000081, loss_recon: 0.598856, loss_pred: 0.070165
iteration 1914: loss: 60.333900, loss_kl: 0.029727, loss_recon: 0.602202, loss_pred: 0.083943
iteration 1915: loss: 61.665630, loss_kl: 0.011174, loss_recon: 0.615950, loss_pred: 0.059507
iteration 1916: loss: 60.660427, loss_kl: 0.015573, loss_recon: 0.605980, loss_pred: 0.046892
iteration 1917: loss: 62.611389, loss_kl: 0.097107, loss_recon: 0.624736, loss_pred: 0.040706
iteration 1918: loss: 60.731045, loss_kl: 0.010495, loss_recon: 0.606462, loss_pred: 0.074378
iteration 1919: loss: 60.934296, loss_kl: 0.058665, loss_recon: 0.607928, loss_pred: 0.082824
iteration 1920: loss: 61.005360, loss_kl: 0.024650, loss_recon: 0.609004, loss_pred: 0.080331
 96%|███████████████████████████▊ | 192/200 [2:39:40<06:38, 49.76s/it]iteration 1921: loss: 60.031464, loss_kl: 0.073055, loss_recon: 0.598254, loss_pred: 0.133011
iteration 1922: loss: 60.047131, loss_kl: 0.140692, loss_recon: 0.597074, loss_pred: 0.199051
iteration 1923: loss: 59.803200, loss_kl: 0.039703, loss_recon: 0.596166, loss_pred: 0.146912
iteration 1924: loss: 61.836926, loss_kl: 0.158115, loss_recon: 0.615634, loss_pred: 0.115392
iteration 1925: loss: 61.003551, loss_kl: 0.043695, loss_recon: 0.608180, loss_pred: 0.141839
iteration 1926: loss: 60.549946, loss_kl: 0.084131, loss_recon: 0.603788, loss_pred: 0.087043
iteration 1927: loss: 60.982071, loss_kl: 0.211487, loss_recon: 0.606609, loss_pred: 0.109710
iteration 1928: loss: 61.315208, loss_kl: 0.099433, loss_recon: 0.610953, loss_pred: 0.120482
iteration 1929: loss: 61.965450, loss_kl: 0.157518, loss_recon: 0.617209, loss_pred: 0.087025
iteration 1930: loss: 63.481098, loss_kl: -0.007626, loss_recon: 0.634256, loss_pred: 0.063142
 96%|███████████████████████████▉ | 193/200 [2:40:29<05:46, 49.54s/it]iteration 1931: loss: 61.158268, loss_kl: 0.065130, loss_recon: 0.609856, loss_pred: 0.107490
iteration 1932: loss: 61.860535, loss_kl: 0.074089, loss_recon: 0.617097, loss_pred: 0.076701
iteration 1933: loss: 60.348610, loss_kl: 0.113679, loss_recon: 0.600852, loss_pred: 0.149771
iteration 1934: loss: 61.970825, loss_kl: 0.017632, loss_recon: 0.618437, loss_pred: 0.109482
iteration 1935: loss: 59.752323, loss_kl: 0.059285, loss_recon: 0.595941, loss_pred: 0.098967
iteration 1936: loss: 60.171440, loss_kl: 0.047936, loss_recon: 0.600031, loss_pred: 0.120363
iteration 1937: loss: 60.603867, loss_kl: 0.082788, loss_recon: 0.603962, loss_pred: 0.124832
iteration 1938: loss: 61.721333, loss_kl: 0.062495, loss_recon: 0.615417, loss_pred: 0.117132
iteration 1939: loss: 60.685726, loss_kl: 0.055246, loss_recon: 0.605282, loss_pred: 0.102330
iteration 1940: loss: 61.963734, loss_kl: 0.234726, loss_recon: 0.616801, loss_pred: 0.048913
 97%|████████████████████████████▏| 194/200 [2:41:20<04:59, 49.95s/it]iteration 1941: loss: 60.520241, loss_kl: 0.075986, loss_recon: 0.602541, loss_pred: 0.190202
iteration 1942: loss: 60.981056, loss_kl: -0.060592, loss_recon: 0.608583, loss_pred: 0.183335
iteration 1943: loss: 60.458363, loss_kl: 0.000001, loss_recon: 0.603556, loss_pred: 0.102741
iteration 1944: loss: 59.709541, loss_kl: 0.167042, loss_recon: 0.592289, loss_pred: 0.313557
iteration 1945: loss: 63.028992, loss_kl: 0.022081, loss_recon: 0.628303, loss_pred: 0.176645
iteration 1946: loss: 61.243404, loss_kl: 0.038189, loss_recon: 0.611143, loss_pred: 0.090911
iteration 1947: loss: 61.475235, loss_kl: 0.007878, loss_recon: 0.613419, loss_pred: 0.125478
iteration 1948: loss: 60.181316, loss_kl: 0.095927, loss_recon: 0.599735, loss_pred: 0.111916
iteration 1949: loss: 61.164806, loss_kl: 0.109107, loss_recon: 0.609834, loss_pred: 0.072256
iteration 1950: loss: 61.966660, loss_kl: 0.243811, loss_recon: 0.616274, loss_pred: 0.095415
 98%|████████████████████████████▎| 195/200 [2:42:10<04:09, 49.98s/it]iteration 1951: loss: 60.221424, loss_kl: 0.090666, loss_recon: 0.600371, loss_pred: 0.093689
iteration 1952: loss: 61.544952, loss_kl: 0.107059, loss_recon: 0.613329, loss_pred: 0.105007
iteration 1953: loss: 60.378464, loss_kl: 0.164170, loss_recon: 0.600822, loss_pred: 0.132116
iteration 1954: loss: 60.294209, loss_kl: 0.059766, loss_recon: 0.601180, loss_pred: 0.116443
iteration 1955: loss: 61.757462, loss_kl: 0.023935, loss_recon: 0.616269, loss_pred: 0.106625
iteration 1956: loss: 60.666359, loss_kl: 0.066442, loss_recon: 0.605226, loss_pred: 0.077326
iteration 1957: loss: 60.447002, loss_kl: 0.103354, loss_recon: 0.602728, loss_pred: 0.070878
iteration 1958: loss: 61.210793, loss_kl: 0.253873, loss_recon: 0.608657, loss_pred: 0.091180
iteration 1959: loss: 61.331230, loss_kl: 0.141404, loss_recon: 0.610748, loss_pred: 0.114990
iteration 1960: loss: 62.834381, loss_kl: 0.224459, loss_recon: 0.625387, loss_pred: 0.071246
 98%|████████████████████████████▍| 196/200 [2:42:59<03:19, 49.90s/it]iteration 1961: loss: 60.752682, loss_kl: 0.070669, loss_recon: 0.606059, loss_pred: 0.076066
iteration 1962: loss: 60.694202, loss_kl: 0.058841, loss_recon: 0.605450, loss_pred: 0.090348
iteration 1963: loss: 61.834408, loss_kl: 0.064443, loss_recon: 0.617125, loss_pred: 0.057468
iteration 1964: loss: 60.853172, loss_kl: 0.173355, loss_recon: 0.606070, loss_pred: 0.072772
iteration 1965: loss: 61.739437, loss_kl: 0.046394, loss_recon: 0.616333, loss_pred: 0.059729
iteration 1966: loss: 60.071301, loss_kl: 0.050139, loss_recon: 0.599424, loss_pred: 0.078742
iteration 1967: loss: 61.133957, loss_kl: 0.089752, loss_recon: 0.609865, loss_pred: 0.057733
iteration 1968: loss: 60.116196, loss_kl: 0.046658, loss_recon: 0.599984, loss_pred: 0.071097
iteration 1969: loss: 59.595123, loss_kl: 0.019785, loss_recon: 0.595053, loss_pred: 0.069998
iteration 1970: loss: 63.457180, loss_kl: -0.134040, loss_recon: 0.635258, loss_pred: 0.065428
 98%|████████████████████████████▌| 197/200 [2:43:49<02:29, 49.85s/it]iteration 1971: loss: 59.884628, loss_kl: -0.059388, loss_recon: 0.598314, loss_pred: 0.112577
iteration 1972: loss: 61.746937, loss_kl: 0.236021, loss_recon: 0.614316, loss_pred: 0.079280
iteration 1973: loss: 61.525230, loss_kl: 0.018060, loss_recon: 0.613618, loss_pred: 0.145402
iteration 1974: loss: 61.761395, loss_kl: 0.174807, loss_recon: 0.614417, loss_pred: 0.144845
iteration 1975: loss: 60.968903, loss_kl: 0.060292, loss_recon: 0.608629, loss_pred: 0.045671
iteration 1976: loss: 60.351151, loss_kl: 0.029991, loss_recon: 0.602297, loss_pred: 0.091485
iteration 1977: loss: 59.351768, loss_kl: -0.046341, loss_recon: 0.592290, loss_pred: 0.169062
iteration 1978: loss: 61.355217, loss_kl: 0.026422, loss_recon: 0.612320, loss_pred: 0.096845
iteration 1979: loss: 60.838509, loss_kl: 0.066402, loss_recon: 0.607046, loss_pred: 0.067472
iteration 1980: loss: 61.812275, loss_kl: -0.103381, loss_recon: 0.618371, loss_pred: 0.078532
 99%|████████████████████████████▋| 198/200 [2:44:39<01:39, 49.91s/it]iteration 1981: loss: 60.265396, loss_kl: 0.283190, loss_recon: 0.599142, loss_pred: 0.068016
iteration 1982: loss: 61.511169, loss_kl: -0.002221, loss_recon: 0.614312, loss_pred: 0.082172
iteration 1983: loss: 60.105873, loss_kl: 0.141676, loss_recon: 0.598634, loss_pred: 0.100791
iteration 1984: loss: 59.215759, loss_kl: 0.049687, loss_recon: 0.590615, loss_pred: 0.104586
iteration 1985: loss: 62.297970, loss_kl: 0.089314, loss_recon: 0.620865, loss_pred: 0.122160
iteration 1986: loss: 63.117138, loss_kl: -0.021494, loss_recon: 0.630352, loss_pred: 0.103461
iteration 1987: loss: 60.815414, loss_kl: 0.006613, loss_recon: 0.607172, loss_pred: 0.091588
iteration 1988: loss: 60.328648, loss_kl: 0.229262, loss_recon: 0.599820, loss_pred: 0.117359
iteration 1989: loss: 61.004955, loss_kl: 0.117445, loss_recon: 0.608194, loss_pred: 0.068091
iteration 1990: loss: 59.299374, loss_kl: 0.052170, loss_recon: 0.591533, loss_pred: 0.093924
100%|████████████████████████████▊| 199/200 [2:45:30<00:50, 50.28s/it]iteration 1991: loss: 61.458004, loss_kl: 0.037853, loss_recon: 0.613405, loss_pred: 0.079682
iteration 1992: loss: 60.518192, loss_kl: 0.023170, loss_recon: 0.604322, loss_pred: 0.062842
iteration 1993: loss: 60.739491, loss_kl: 0.036701, loss_recon: 0.606163, loss_pred: 0.086495
iteration 1994: loss: 60.566811, loss_kl: 0.091399, loss_recon: 0.603860, loss_pred: 0.089410
iteration 1995: loss: 60.965885, loss_kl: 0.278583, loss_recon: 0.606209, loss_pred: 0.066434
iteration 1996: loss: 61.296669, loss_kl: 0.076663, loss_recon: 0.611509, loss_pred: 0.069080
iteration 1997: loss: 60.912228, loss_kl: 0.088134, loss_recon: 0.607499, loss_pred: 0.074205
iteration 1998: loss: 61.533211, loss_kl: 0.061853, loss_recon: 0.614072, loss_pred: 0.064153
iteration 1999: loss: 59.848896, loss_kl: 0.008179, loss_recon: 0.597482, loss_pred: 0.092516
iteration 2000: loss: 60.047058, loss_kl: 0.037542, loss_recon: 0.599167, loss_pred: 0.092830
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_199.pth
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_199.pth
100%|████████████████████████████▊| 199/200 [2:46:21<00:50, 50.16s/it]
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design2', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], net_path=False, vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.001, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=64, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, number_of_samplings=6, num_classes=2, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[64, 64, 64]', distributed=False)
TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234
4 test iterations per epoch
0it [00:00, ?it/s]0it [00:15, ?it/s]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 386, in <module>
    inferrer[dataset_name](args, net, test_save_path)
  File "/home/mhashemi/TransVNet/test.py", line 167, in inferrer_mat2
    formatting_tuple = tuple([name_batch[i]] + [metric_batch[i, j] for j in range(metric_avg.shape[1])])
  File "/home/mhashemi/TransVNet/test.py", line 167, in <listcomp>
    formatting_tuple = tuple([name_batch[i]] + [metric_batch[i, j] for j in range(metric_avg.shape[1])])
IndexError: index 9 is out of bounds for dimension 1 with size 9
