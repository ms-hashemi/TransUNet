/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=True, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.003, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
10 iterations per epoch. 2000 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 69.472481, loss_kl: 17.353088, loss_recon: 0.693148, loss_pred: 1.402912
iteration 2: loss: 70.599266, loss_kl: 33.296181, loss_recon: 0.703608, loss_pred: 2.051579
iteration 3: loss: 70.523499, loss_kl: 606.220093, loss_recon: 0.685147, loss_pred: 14.025747
iteration 4: loss: 70.278221, loss_kl: 847.341675, loss_recon: 0.690665, loss_pred: 3.644108
iteration 5: loss: 69.463318, loss_kl: 315.903503, loss_recon: 0.688041, loss_pred: 3.433075
iteration 6: loss: 68.980110, loss_kl: 155.912521, loss_recon: 0.684592, loss_pred: 3.650376
iteration 7: loss: 70.081512, loss_kl: 174.569580, loss_recon: 0.695731, loss_pred: 3.338049
iteration 8: loss: 69.345505, loss_kl: 375.035645, loss_recon: 0.687337, loss_pred: 2.367763
iteration 9: loss: 68.704315, loss_kl: 522.901428, loss_recon: 0.679035, loss_pred: 2.778631
iteration 10: loss: 68.536057, loss_kl: 630.066284, loss_recon: 0.675995, loss_pred: 3.064978
  0%|▏                              | 1/200 [00:54<3:01:18, 54.67s/it]iteration 11: loss: 68.929253, loss_kl: 658.419922, loss_recon: 0.679653, loss_pred: 3.055877
iteration 12: loss: 66.229614, loss_kl: 614.280640, loss_recon: 0.653799, loss_pred: 2.354269
iteration 13: loss: 65.236984, loss_kl: 611.046570, loss_recon: 0.644751, loss_pred: 1.507966
iteration 14: loss: 64.456902, loss_kl: 590.694092, loss_recon: 0.637503, loss_pred: 1.158788
iteration 15: loss: 63.995975, loss_kl: 580.941895, loss_recon: 0.633017, loss_pred: 1.133021
iteration 16: loss: 62.608768, loss_kl: 516.876892, loss_recon: 0.619798, loss_pred: 1.121003
iteration 17: loss: 64.521805, loss_kl: 394.797302, loss_recon: 0.640201, loss_pred: 1.069421
iteration 18: loss: 63.481270, loss_kl: 455.654663, loss_recon: 0.628818, loss_pred: 1.437756
iteration 19: loss: 64.070969, loss_kl: 485.409485, loss_recon: 0.634715, loss_pred: 1.140105
iteration 20: loss: 62.744846, loss_kl: 438.133881, loss_recon: 0.621994, loss_pred: 1.073294
  1%|▎                              | 2/200 [01:44<2:50:35, 51.69s/it]iteration 21: loss: 63.508205, loss_kl: 367.233032, loss_recon: 0.630549, loss_pred: 0.861031
iteration 22: loss: 62.919899, loss_kl: 318.867462, loss_recon: 0.624977, loss_pred: 1.033291
iteration 23: loss: 62.902367, loss_kl: 317.854431, loss_recon: 0.625107, loss_pred: 0.737972
iteration 24: loss: 62.331860, loss_kl: 351.668976, loss_recon: 0.619051, loss_pred: 0.750470
iteration 25: loss: 61.511257, loss_kl: 375.132019, loss_recon: 0.610413, loss_pred: 0.947917
iteration 26: loss: 61.138653, loss_kl: 344.111450, loss_recon: 0.607134, loss_pred: 0.811895
iteration 27: loss: 61.538033, loss_kl: 300.264648, loss_recon: 0.611861, loss_pred: 0.516369
iteration 28: loss: 62.635685, loss_kl: 257.503754, loss_recon: 0.623465, loss_pred: 0.317235
iteration 29: loss: 62.736229, loss_kl: 221.134430, loss_recon: 0.624683, loss_pred: 0.468329
iteration 30: loss: 62.348122, loss_kl: 235.557251, loss_recon: 0.620615, loss_pred: 0.510397
  2%|▍                              | 3/200 [02:34<2:46:55, 50.84s/it]iteration 31: loss: 61.521324, loss_kl: 250.126099, loss_recon: 0.612072, loss_pred: 0.640337
iteration 32: loss: 60.866871, loss_kl: 254.462677, loss_recon: 0.605415, loss_pred: 0.709025
iteration 33: loss: 62.153553, loss_kl: 235.362442, loss_recon: 0.618801, loss_pred: 0.380677
iteration 34: loss: 63.002426, loss_kl: 215.002243, loss_recon: 0.627552, loss_pred: 0.322526
iteration 35: loss: 63.010311, loss_kl: 187.068237, loss_recon: 0.627905, loss_pred: 0.327234
iteration 36: loss: 60.884979, loss_kl: 171.121124, loss_recon: 0.606713, loss_pred: 0.425517
iteration 37: loss: 62.987503, loss_kl: 159.323456, loss_recon: 0.627820, loss_pred: 0.461872
iteration 38: loss: 61.086143, loss_kl: 227.902527, loss_recon: 0.608123, loss_pred: 0.459054
iteration 39: loss: 62.168198, loss_kl: 322.479126, loss_recon: 0.617931, loss_pred: 0.525826
iteration 40: loss: 61.237164, loss_kl: 397.447388, loss_recon: 0.607763, loss_pred: 0.634430
  2%|▌                              | 4/200 [03:26<2:48:08, 51.47s/it]iteration 41: loss: 61.274429, loss_kl: 434.968689, loss_recon: 0.607834, loss_pred: 0.560408
iteration 42: loss: 61.852551, loss_kl: 450.175110, loss_recon: 0.613722, loss_pred: 0.302009
iteration 43: loss: 61.477852, loss_kl: 427.736389, loss_recon: 0.610118, loss_pred: 0.382867
iteration 44: loss: 62.593498, loss_kl: 386.124634, loss_recon: 0.621486, loss_pred: 0.588236
iteration 45: loss: 61.478107, loss_kl: 349.204529, loss_recon: 0.610679, loss_pred: 0.610369
iteration 46: loss: 63.482864, loss_kl: 299.649780, loss_recon: 0.631101, loss_pred: 0.731016
iteration 47: loss: 61.637096, loss_kl: 269.711060, loss_recon: 0.613310, loss_pred: 0.363371
iteration 48: loss: 61.181190, loss_kl: 234.361816, loss_recon: 0.609178, loss_pred: 0.289909
iteration 49: loss: 61.606728, loss_kl: 211.232086, loss_recon: 0.613645, loss_pred: 0.310231
iteration 50: loss: 62.493671, loss_kl: 182.328079, loss_recon: 0.622585, loss_pred: 0.528073
  2%|▊                              | 5/200 [04:16<2:45:24, 50.89s/it]iteration 51: loss: 61.951180, loss_kl: 173.042587, loss_recon: 0.617205, loss_pred: 0.576494
iteration 52: loss: 62.635250, loss_kl: 160.786743, loss_recon: 0.624261, loss_pred: 0.483987
iteration 53: loss: 61.910080, loss_kl: 158.050781, loss_recon: 0.616999, loss_pred: 0.520899
iteration 54: loss: 60.610512, loss_kl: 153.756241, loss_recon: 0.603746, loss_pred: 0.821859
iteration 55: loss: 62.757248, loss_kl: 141.877136, loss_recon: 0.625688, loss_pred: 0.465482
iteration 56: loss: 62.333530, loss_kl: 131.712402, loss_recon: 0.621337, loss_pred: 0.680880
iteration 57: loss: 61.204594, loss_kl: 124.452141, loss_recon: 0.610018, loss_pred: 0.783232
iteration 58: loss: 61.481091, loss_kl: 118.492645, loss_recon: 0.613104, loss_pred: 0.521801
iteration 59: loss: 60.963318, loss_kl: 119.681000, loss_recon: 0.608072, loss_pred: 0.364498
iteration 60: loss: 60.353054, loss_kl: 121.356392, loss_recon: 0.601922, loss_pred: 0.394855
  3%|▉                              | 6/200 [05:05<2:42:47, 50.35s/it]iteration 61: loss: 60.577236, loss_kl: 129.269547, loss_recon: 0.604168, loss_pred: 0.312097
iteration 62: loss: 60.930450, loss_kl: 140.097076, loss_recon: 0.607545, loss_pred: 0.358505
iteration 63: loss: 62.984825, loss_kl: 151.787598, loss_recon: 0.627906, loss_pred: 0.424595
iteration 64: loss: 61.822968, loss_kl: 162.122650, loss_recon: 0.616136, loss_pred: 0.472342
iteration 65: loss: 61.858101, loss_kl: 168.229065, loss_recon: 0.616501, loss_pred: 0.397888
iteration 66: loss: 61.580410, loss_kl: 170.247681, loss_recon: 0.613807, loss_pred: 0.294988
iteration 67: loss: 61.558475, loss_kl: 171.887146, loss_recon: 0.613565, loss_pred: 0.300504
iteration 68: loss: 61.771805, loss_kl: 168.134674, loss_recon: 0.615529, loss_pred: 0.507730
iteration 69: loss: 61.087986, loss_kl: 159.653961, loss_recon: 0.608851, loss_pred: 0.431842
iteration 70: loss: 61.461628, loss_kl: 149.109879, loss_recon: 0.612795, loss_pred: 0.330076
  4%|█                              | 7/200 [05:56<2:42:00, 50.37s/it]iteration 71: loss: 60.822426, loss_kl: 140.276764, loss_recon: 0.606618, loss_pred: 0.203903
iteration 72: loss: 61.119938, loss_kl: 134.304657, loss_recon: 0.609549, loss_pred: 0.307026
iteration 73: loss: 62.219246, loss_kl: 125.838356, loss_recon: 0.620680, loss_pred: 0.254272
iteration 74: loss: 61.394089, loss_kl: 119.609741, loss_recon: 0.612377, loss_pred: 0.368077
iteration 75: loss: 61.240196, loss_kl: 111.765480, loss_recon: 0.610954, loss_pred: 0.330074
iteration 76: loss: 61.702423, loss_kl: 103.873932, loss_recon: 0.615702, loss_pred: 0.283168
iteration 77: loss: 60.560898, loss_kl: 93.518059, loss_recon: 0.604399, loss_pred: 0.274784
iteration 78: loss: 61.376434, loss_kl: 93.370323, loss_recon: 0.612588, loss_pred: 0.242200
iteration 79: loss: 61.903980, loss_kl: 107.160118, loss_recon: 0.617770, loss_pred: 0.198027
iteration 80: loss: 61.250393, loss_kl: 122.606224, loss_recon: 0.611095, loss_pred: 0.182391
  4%|█▏                             | 8/200 [06:47<2:41:51, 50.58s/it]iteration 81: loss: 62.182949, loss_kl: 131.710495, loss_recon: 0.620380, loss_pred: 0.132728
iteration 82: loss: 60.990685, loss_kl: 144.929123, loss_recon: 0.608271, loss_pred: 0.186591
iteration 83: loss: 62.996010, loss_kl: 144.963257, loss_recon: 0.628299, loss_pred: 0.211782
iteration 84: loss: 60.846764, loss_kl: 146.524887, loss_recon: 0.606751, loss_pred: 0.251012
iteration 85: loss: 60.828320, loss_kl: 146.277161, loss_recon: 0.606618, loss_pred: 0.202278
iteration 86: loss: 63.017902, loss_kl: 133.803650, loss_recon: 0.628609, loss_pred: 0.231860
iteration 87: loss: 59.988686, loss_kl: 118.114914, loss_recon: 0.598548, loss_pred: 0.157288
iteration 88: loss: 61.574608, loss_kl: 105.379791, loss_recon: 0.614406, loss_pred: 0.286148
iteration 89: loss: 60.738319, loss_kl: 120.422424, loss_recon: 0.605990, loss_pred: 0.189214
iteration 90: loss: 60.783607, loss_kl: 135.304520, loss_recon: 0.606135, loss_pred: 0.347790
  4%|█▍                             | 9/200 [07:37<2:40:48, 50.52s/it]iteration 91: loss: 61.118916, loss_kl: 144.915741, loss_recon: 0.609436, loss_pred: 0.303836
iteration 92: loss: 61.307640, loss_kl: 150.189484, loss_recon: 0.611398, loss_pred: 0.176479
iteration 93: loss: 61.334839, loss_kl: 151.964539, loss_recon: 0.611646, loss_pred: 0.183223
iteration 94: loss: 62.330143, loss_kl: 143.297119, loss_recon: 0.621684, loss_pred: 0.183974
iteration 95: loss: 61.564995, loss_kl: 135.115479, loss_recon: 0.614119, loss_pred: 0.180221
iteration 96: loss: 61.376579, loss_kl: 121.814911, loss_recon: 0.612319, loss_pred: 0.229008
iteration 97: loss: 60.763016, loss_kl: 113.071945, loss_recon: 0.606324, loss_pred: 0.175230
iteration 98: loss: 61.740307, loss_kl: 106.439789, loss_recon: 0.616176, loss_pred: 0.162754
iteration 99: loss: 59.468658, loss_kl: 110.874092, loss_recon: 0.593180, loss_pred: 0.397895
iteration 100: loss: 60.606117, loss_kl: 117.262383, loss_recon: 0.604640, loss_pred: 0.248634
  5%|█▌                            | 10/200 [08:26<2:38:34, 50.08s/it]iteration 101: loss: 60.687553, loss_kl: 117.554794, loss_recon: 0.605453, loss_pred: 0.246575
iteration 102: loss: 61.080540, loss_kl: 112.896576, loss_recon: 0.609368, loss_pred: 0.308520
iteration 103: loss: 60.802841, loss_kl: 107.441292, loss_recon: 0.606722, loss_pred: 0.231782
iteration 104: loss: 61.330975, loss_kl: 100.355728, loss_recon: 0.612066, loss_pred: 0.240426
iteration 105: loss: 61.298244, loss_kl: 93.060715, loss_recon: 0.611855, loss_pred: 0.196575
iteration 106: loss: 61.173576, loss_kl: 93.788109, loss_recon: 0.610605, loss_pred: 0.192434
iteration 107: loss: 60.874405, loss_kl: 88.271164, loss_recon: 0.607658, loss_pred: 0.203371
iteration 108: loss: 61.112339, loss_kl: 86.381058, loss_recon: 0.610065, loss_pred: 0.194213
iteration 109: loss: 61.653252, loss_kl: 87.892578, loss_recon: 0.615373, loss_pred: 0.280482
iteration 110: loss: 62.522705, loss_kl: 91.480560, loss_recon: 0.624122, loss_pred: 0.189821
  6%|█▋                            | 11/200 [09:16<2:37:13, 49.91s/it]iteration 111: loss: 62.100368, loss_kl: 107.152946, loss_recon: 0.619658, loss_pred: 0.274161
iteration 112: loss: 61.778755, loss_kl: 119.033516, loss_recon: 0.616297, loss_pred: 0.300441
iteration 113: loss: 61.129261, loss_kl: 131.420105, loss_recon: 0.609751, loss_pred: 0.227866
iteration 114: loss: 60.200012, loss_kl: 140.711823, loss_recon: 0.600402, loss_pred: 0.191190
iteration 115: loss: 61.015713, loss_kl: 146.179993, loss_recon: 0.608505, loss_pred: 0.189861
iteration 116: loss: 61.076077, loss_kl: 147.557068, loss_recon: 0.609119, loss_pred: 0.166510
iteration 117: loss: 61.297981, loss_kl: 136.768936, loss_recon: 0.611417, loss_pred: 0.195418
iteration 118: loss: 60.708164, loss_kl: 129.327881, loss_recon: 0.605590, loss_pred: 0.198239
iteration 119: loss: 61.825291, loss_kl: 116.619873, loss_recon: 0.616770, loss_pred: 0.316509
iteration 120: loss: 61.543613, loss_kl: 112.608841, loss_recon: 0.614087, loss_pred: 0.223163
  6%|█▊                            | 12/200 [10:05<2:35:38, 49.67s/it]iteration 121: loss: 60.932735, loss_kl: 109.898598, loss_recon: 0.608080, loss_pred: 0.148758
iteration 122: loss: 59.638229, loss_kl: 113.115150, loss_recon: 0.594902, loss_pred: 0.348761
iteration 123: loss: 62.001450, loss_kl: 114.369606, loss_recon: 0.618580, loss_pred: 0.290845
iteration 124: loss: 61.552200, loss_kl: 115.735725, loss_recon: 0.614023, loss_pred: 0.342067
iteration 125: loss: 61.610268, loss_kl: 110.942993, loss_recon: 0.614802, loss_pred: 0.191296
iteration 126: loss: 61.169357, loss_kl: 105.922729, loss_recon: 0.610459, loss_pred: 0.175498
iteration 127: loss: 62.172020, loss_kl: 91.655914, loss_recon: 0.620555, loss_pred: 0.249020
iteration 128: loss: 61.031353, loss_kl: 90.193436, loss_recon: 0.609199, loss_pred: 0.212474
iteration 129: loss: 61.450016, loss_kl: 85.810104, loss_recon: 0.613451, loss_pred: 0.191222
iteration 130: loss: 60.932102, loss_kl: 84.485023, loss_recon: 0.608198, loss_pred: 0.277886
  6%|█▉                            | 13/200 [10:55<2:35:04, 49.76s/it]iteration 131: loss: 60.458221, loss_kl: 80.320572, loss_recon: 0.603541, loss_pred: 0.237496
iteration 132: loss: 59.540123, loss_kl: 77.170212, loss_recon: 0.594411, loss_pred: 0.218800
iteration 133: loss: 61.208866, loss_kl: 75.925789, loss_recon: 0.611176, loss_pred: 0.153579
iteration 134: loss: 62.950310, loss_kl: 84.386673, loss_recon: 0.628313, loss_pred: 0.345873
iteration 135: loss: 61.246475, loss_kl: 88.519508, loss_recon: 0.611373, loss_pred: 0.206935
iteration 136: loss: 60.377605, loss_kl: 90.402878, loss_recon: 0.602675, loss_pred: 0.196760
iteration 137: loss: 62.547688, loss_kl: 98.412285, loss_recon: 0.624296, loss_pred: 0.196587
iteration 138: loss: 62.120655, loss_kl: 102.183640, loss_recon: 0.619967, loss_pred: 0.217656
iteration 139: loss: 61.572559, loss_kl: 103.585114, loss_recon: 0.614447, loss_pred: 0.242652
iteration 140: loss: 60.997677, loss_kl: 104.067703, loss_recon: 0.608700, loss_pred: 0.235622
  7%|██                            | 14/200 [11:47<2:36:10, 50.38s/it]iteration 141: loss: 61.958153, loss_kl: 102.332527, loss_recon: 0.618266, loss_pred: 0.292379
iteration 142: loss: 59.945007, loss_kl: 98.857193, loss_recon: 0.598250, loss_pred: 0.211104
iteration 143: loss: 61.933300, loss_kl: 91.352982, loss_recon: 0.618215, loss_pred: 0.204105
iteration 144: loss: 60.881699, loss_kl: 96.712631, loss_recon: 0.607634, loss_pred: 0.215786
iteration 145: loss: 61.032757, loss_kl: 95.912155, loss_recon: 0.609102, loss_pred: 0.266915
iteration 146: loss: 61.508148, loss_kl: 99.447548, loss_recon: 0.613811, loss_pred: 0.275834
iteration 147: loss: 61.593914, loss_kl: 105.809349, loss_recon: 0.614531, loss_pred: 0.349859
iteration 148: loss: 61.853748, loss_kl: 115.209221, loss_recon: 0.617204, loss_pred: 0.181602
iteration 149: loss: 60.938034, loss_kl: 117.633667, loss_recon: 0.607948, loss_pred: 0.256143
iteration 150: loss: 62.240398, loss_kl: 123.490387, loss_recon: 0.620971, loss_pred: 0.198558
  8%|██▎                           | 15/200 [12:37<2:35:38, 50.48s/it]iteration 151: loss: 60.728809, loss_kl: 122.448578, loss_recon: 0.605851, loss_pred: 0.212122
iteration 152: loss: 61.540821, loss_kl: 111.427826, loss_recon: 0.613973, loss_pred: 0.321147
iteration 153: loss: 61.395237, loss_kl: 107.045807, loss_recon: 0.612664, loss_pred: 0.218432
iteration 154: loss: 59.901814, loss_kl: 104.313911, loss_recon: 0.597798, loss_pred: 0.177082
iteration 155: loss: 60.729607, loss_kl: 95.870926, loss_recon: 0.606196, loss_pred: 0.141478
iteration 156: loss: 60.072666, loss_kl: 87.931511, loss_recon: 0.599541, loss_pred: 0.306816
iteration 157: loss: 62.022915, loss_kl: 91.742355, loss_recon: 0.619131, loss_pred: 0.180872
iteration 158: loss: 64.016190, loss_kl: 90.616226, loss_recon: 0.639049, loss_pred: 0.206655
iteration 159: loss: 60.449047, loss_kl: 95.165161, loss_recon: 0.603203, loss_pred: 0.336318
iteration 160: loss: 61.069893, loss_kl: 99.928017, loss_recon: 0.609542, loss_pred: 0.158096
  8%|██▍                           | 16/200 [13:27<2:33:45, 50.14s/it]iteration 161: loss: 62.125477, loss_kl: 99.906059, loss_recon: 0.620079, loss_pred: 0.176426
iteration 162: loss: 61.599411, loss_kl: 100.177078, loss_recon: 0.614798, loss_pred: 0.194410
iteration 163: loss: 60.750946, loss_kl: 100.521370, loss_recon: 0.606315, loss_pred: 0.189573
iteration 164: loss: 60.602634, loss_kl: 97.757294, loss_recon: 0.604904, loss_pred: 0.144501
iteration 165: loss: 60.606167, loss_kl: 96.011940, loss_recon: 0.604917, loss_pred: 0.184871
iteration 166: loss: 61.888638, loss_kl: 93.514854, loss_recon: 0.617728, loss_pred: 0.223647
iteration 167: loss: 59.693241, loss_kl: 97.316330, loss_recon: 0.595672, loss_pred: 0.286811
iteration 168: loss: 60.777645, loss_kl: 100.816307, loss_recon: 0.606601, loss_pred: 0.167021
iteration 169: loss: 61.984600, loss_kl: 98.074417, loss_recon: 0.618692, loss_pred: 0.173040
iteration 170: loss: 61.263992, loss_kl: 89.253128, loss_recon: 0.611527, loss_pred: 0.220576
  8%|██▌                           | 17/200 [14:19<2:34:50, 50.77s/it]iteration 171: loss: 60.774677, loss_kl: 97.713989, loss_recon: 0.606600, loss_pred: 0.169387
iteration 172: loss: 60.642635, loss_kl: 105.033073, loss_recon: 0.605188, loss_pred: 0.188046
iteration 173: loss: 62.702579, loss_kl: 109.973511, loss_recon: 0.625769, loss_pred: 0.156984
iteration 174: loss: 61.356304, loss_kl: 110.513687, loss_recon: 0.612239, loss_pred: 0.219077
iteration 175: loss: 61.433144, loss_kl: 108.213928, loss_recon: 0.613093, loss_pred: 0.156751
iteration 176: loss: 60.904537, loss_kl: 107.084061, loss_recon: 0.607759, loss_pred: 0.215270
iteration 177: loss: 60.772690, loss_kl: 105.752594, loss_recon: 0.606361, loss_pred: 0.308341
iteration 178: loss: 61.029667, loss_kl: 105.361923, loss_recon: 0.608992, loss_pred: 0.250890
iteration 179: loss: 60.426434, loss_kl: 104.077835, loss_recon: 0.603063, loss_pred: 0.160166
iteration 180: loss: 62.153339, loss_kl: 110.319832, loss_recon: 0.620220, loss_pred: 0.210651
  9%|██▋                           | 18/200 [15:09<2:33:11, 50.50s/it]iteration 181: loss: 60.659328, loss_kl: 110.081360, loss_recon: 0.605270, loss_pred: 0.222553
iteration 182: loss: 61.359055, loss_kl: 108.461067, loss_recon: 0.612314, loss_pred: 0.191661
iteration 183: loss: 61.419655, loss_kl: 103.907166, loss_recon: 0.612962, loss_pred: 0.195377
iteration 184: loss: 59.961811, loss_kl: 102.423233, loss_recon: 0.598412, loss_pred: 0.181438
iteration 185: loss: 60.754745, loss_kl: 98.248444, loss_recon: 0.606445, loss_pred: 0.119985
iteration 186: loss: 59.823578, loss_kl: 93.864403, loss_recon: 0.597151, loss_pred: 0.146633
iteration 187: loss: 62.169865, loss_kl: 88.226105, loss_recon: 0.620637, loss_pred: 0.179791
iteration 188: loss: 61.665668, loss_kl: 77.981903, loss_recon: 0.615660, loss_pred: 0.216936
iteration 189: loss: 61.486858, loss_kl: 87.618454, loss_recon: 0.613865, loss_pred: 0.126954
iteration 190: loss: 61.036133, loss_kl: 93.150490, loss_recon: 0.609203, loss_pred: 0.226347
 10%|██▊                           | 19/200 [15:59<2:31:54, 50.35s/it]iteration 191: loss: 60.385113, loss_kl: 93.698570, loss_recon: 0.602635, loss_pred: 0.278823
iteration 192: loss: 61.665115, loss_kl: 92.507477, loss_recon: 0.615471, loss_pred: 0.255128
iteration 193: loss: 61.925121, loss_kl: 97.229263, loss_recon: 0.618008, loss_pred: 0.271057
iteration 194: loss: 62.312908, loss_kl: 97.540977, loss_recon: 0.621894, loss_pred: 0.259558
iteration 195: loss: 60.887516, loss_kl: 95.997208, loss_recon: 0.607774, loss_pred: 0.141608
iteration 196: loss: 59.233570, loss_kl: 92.576256, loss_recon: 0.591220, loss_pred: 0.190377
iteration 197: loss: 60.537922, loss_kl: 90.234909, loss_recon: 0.604318, loss_pred: 0.159271
iteration 198: loss: 62.087181, loss_kl: 93.291481, loss_recon: 0.619754, loss_pred: 0.185367
iteration 199: loss: 59.695259, loss_kl: 87.205780, loss_recon: 0.595894, loss_pred: 0.186390
iteration 200: loss: 62.819641, loss_kl: 80.717155, loss_recon: 0.627141, loss_pred: 0.248161
 10%|███                           | 20/200 [16:49<2:30:43, 50.24s/it]iteration 201: loss: 61.256107, loss_kl: 90.032745, loss_recon: 0.611496, loss_pred: 0.164449
iteration 202: loss: 60.073166, loss_kl: 100.651756, loss_recon: 0.599423, loss_pred: 0.301960
iteration 203: loss: 61.406612, loss_kl: 107.179405, loss_recon: 0.612747, loss_pred: 0.247153
iteration 204: loss: 60.001057, loss_kl: 113.820824, loss_recon: 0.598454, loss_pred: 0.418147
iteration 205: loss: 61.758900, loss_kl: 114.512115, loss_recon: 0.616307, loss_pred: 0.137246
iteration 206: loss: 62.065277, loss_kl: 110.824699, loss_recon: 0.619373, loss_pred: 0.171613
iteration 207: loss: 61.284645, loss_kl: 107.314201, loss_recon: 0.611561, loss_pred: 0.212250
iteration 208: loss: 61.558022, loss_kl: 102.659943, loss_recon: 0.614323, loss_pred: 0.230415
iteration 209: loss: 61.142433, loss_kl: 100.970703, loss_recon: 0.610177, loss_pred: 0.237497
iteration 210: loss: 59.975067, loss_kl: 99.193214, loss_recon: 0.598420, loss_pred: 0.338508
 10%|███▏                          | 21/200 [17:39<2:29:47, 50.21s/it]iteration 211: loss: 61.740868, loss_kl: 95.096680, loss_recon: 0.616246, loss_pred: 0.211690
iteration 212: loss: 61.145584, loss_kl: 96.736496, loss_recon: 0.610241, loss_pred: 0.247426
iteration 213: loss: 61.149464, loss_kl: 92.700012, loss_recon: 0.610271, loss_pred: 0.296699
iteration 214: loss: 60.569618, loss_kl: 90.572876, loss_recon: 0.604571, loss_pred: 0.219656
iteration 215: loss: 60.502117, loss_kl: 86.980896, loss_recon: 0.603931, loss_pred: 0.220381
iteration 216: loss: 61.528873, loss_kl: 82.426315, loss_recon: 0.614312, loss_pred: 0.152634
iteration 217: loss: 62.826523, loss_kl: 77.935966, loss_recon: 0.627295, loss_pred: 0.191103
iteration 218: loss: 60.087715, loss_kl: 79.308281, loss_recon: 0.599866, loss_pred: 0.218296
iteration 219: loss: 61.048843, loss_kl: 80.054398, loss_recon: 0.609524, loss_pred: 0.164019
iteration 220: loss: 60.898548, loss_kl: 81.645721, loss_recon: 0.608019, loss_pred: 0.150220
 11%|███▎                          | 22/200 [18:29<2:29:07, 50.26s/it]iteration 221: loss: 62.328526, loss_kl: 83.808197, loss_recon: 0.622286, loss_pred: 0.161389
iteration 222: loss: 60.894047, loss_kl: 83.866081, loss_recon: 0.607955, loss_pred: 0.146950
iteration 223: loss: 60.137337, loss_kl: 92.378029, loss_recon: 0.600067, loss_pred: 0.382116
iteration 224: loss: 60.510754, loss_kl: 92.282227, loss_recon: 0.603974, loss_pred: 0.210455
iteration 225: loss: 60.322411, loss_kl: 94.193161, loss_recon: 0.602041, loss_pred: 0.241045
iteration 226: loss: 60.935822, loss_kl: 93.686241, loss_recon: 0.608222, loss_pred: 0.199665
iteration 227: loss: 61.740349, loss_kl: 87.307068, loss_recon: 0.616270, loss_pred: 0.260109
iteration 228: loss: 62.076069, loss_kl: 87.862152, loss_recon: 0.619703, loss_pred: 0.179288
iteration 229: loss: 60.931377, loss_kl: 87.914619, loss_recon: 0.608263, loss_pred: 0.171912
iteration 230: loss: 60.630100, loss_kl: 86.997887, loss_recon: 0.605258, loss_pred: 0.173543
 12%|███▍                          | 23/200 [19:19<2:27:41, 50.07s/it]iteration 231: loss: 60.375305, loss_kl: 92.449829, loss_recon: 0.602654, loss_pred: 0.174464
iteration 232: loss: 60.753067, loss_kl: 98.571884, loss_recon: 0.606256, loss_pred: 0.288899
iteration 233: loss: 59.454742, loss_kl: 101.421944, loss_recon: 0.593144, loss_pred: 0.389097
iteration 234: loss: 62.043941, loss_kl: 101.976608, loss_recon: 0.619199, loss_pred: 0.220229
iteration 235: loss: 60.440865, loss_kl: 99.376808, loss_recon: 0.603167, loss_pred: 0.248183
iteration 236: loss: 60.915432, loss_kl: 91.722626, loss_recon: 0.608014, loss_pred: 0.222867
iteration 237: loss: 61.354576, loss_kl: 91.365402, loss_recon: 0.612463, loss_pred: 0.169121
iteration 238: loss: 61.882702, loss_kl: 92.294266, loss_recon: 0.617763, loss_pred: 0.140904
iteration 239: loss: 60.960728, loss_kl: 89.039185, loss_recon: 0.608439, loss_pred: 0.277733
iteration 240: loss: 61.492687, loss_kl: 82.575470, loss_recon: 0.613796, loss_pred: 0.305208
 12%|███▌                          | 24/200 [20:10<2:27:43, 50.36s/it]iteration 241: loss: 60.287037, loss_kl: 82.313339, loss_recon: 0.601703, loss_pred: 0.344477
iteration 242: loss: 61.428272, loss_kl: 81.445740, loss_recon: 0.613317, loss_pred: 0.151537
iteration 243: loss: 62.131912, loss_kl: 76.502541, loss_recon: 0.620221, loss_pred: 0.332659
iteration 244: loss: 62.441612, loss_kl: 70.057442, loss_recon: 0.623284, loss_pred: 0.431849
iteration 245: loss: 59.897850, loss_kl: 91.597580, loss_recon: 0.597699, loss_pred: 0.363698
iteration 246: loss: 61.911552, loss_kl: 120.183670, loss_recon: 0.617337, loss_pred: 0.576828
iteration 247: loss: 60.820187, loss_kl: 148.864563, loss_recon: 0.605928, loss_pred: 0.785505
iteration 248: loss: 60.710003, loss_kl: 172.790985, loss_recon: 0.604888, loss_pred: 0.484078
iteration 249: loss: 61.357983, loss_kl: 181.421890, loss_recon: 0.611607, loss_pred: 0.158823
iteration 250: loss: 61.465401, loss_kl: 182.671799, loss_recon: 0.612500, loss_pred: 0.326908
 12%|███▊                          | 25/200 [20:59<2:25:44, 49.97s/it]slurmstepd: error: *** JOB 4543403 ON nova21-gpu-6 CANCELLED AT 2023-06-24T20:55:05 ***
