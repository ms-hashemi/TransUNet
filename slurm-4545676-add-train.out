/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.001, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
10 iterations per epoch. 2000 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 88.283737, loss_kl: 19.004042, loss_recon: 0.693148, loss_pred: 18.778866
iteration 2: loss: 88.627975, loss_kl: 43.887756, loss_recon: 0.692319, loss_pred: 18.957203
iteration 3: loss: 85.382446, loss_kl: 18.164606, loss_recon: 0.688735, loss_pred: 16.327320
iteration 4: loss: 87.317451, loss_kl: 26.947212, loss_recon: 0.689069, loss_pred: 18.141039
iteration 5: loss: 86.391388, loss_kl: 21.673954, loss_recon: 0.684397, loss_pred: 17.734947
iteration 6: loss: 84.453560, loss_kl: 17.282078, loss_recon: 0.690922, loss_pred: 15.188520
iteration 7: loss: 88.484009, loss_kl: 19.353943, loss_recon: 0.684594, loss_pred: 19.831032
iteration 8: loss: 89.310074, loss_kl: 24.544971, loss_recon: 0.683280, loss_pred: 20.736595
iteration 9: loss: 82.150208, loss_kl: 27.647827, loss_recon: 0.691854, loss_pred: 12.688309
iteration 10: loss: 86.483139, loss_kl: 47.709698, loss_recon: 0.688168, loss_pred: 17.189196
  0%|▏                              | 1/200 [00:54<3:00:10, 54.32s/it]iteration 11: loss: 81.945793, loss_kl: 52.712925, loss_recon: 0.688871, loss_pred: 12.531608
iteration 12: loss: 82.431335, loss_kl: 58.286148, loss_recon: 0.690200, loss_pred: 12.828442
iteration 13: loss: 83.787971, loss_kl: 68.419891, loss_recon: 0.688653, loss_pred: 14.238443
iteration 14: loss: 81.548866, loss_kl: 67.206512, loss_recon: 0.690664, loss_pred: 11.810440
iteration 15: loss: 82.641487, loss_kl: 65.406876, loss_recon: 0.685812, loss_pred: 13.406211
iteration 16: loss: 83.833069, loss_kl: 58.512505, loss_recon: 0.683436, loss_pred: 14.904358
iteration 17: loss: 79.124725, loss_kl: 58.088039, loss_recon: 0.687139, loss_pred: 9.829924
iteration 18: loss: 81.065353, loss_kl: 50.255016, loss_recon: 0.681692, loss_pred: 12.393602
iteration 19: loss: 75.638977, loss_kl: 55.120880, loss_recon: 0.673753, loss_pred: 7.712432
iteration 20: loss: 75.333282, loss_kl: 53.295780, loss_recon: 0.668345, loss_pred: 7.965793
  1%|▎                              | 2/200 [01:43<2:48:52, 51.17s/it]iteration 21: loss: 73.320755, loss_kl: 56.957474, loss_recon: 0.652902, loss_pred: 7.461027
iteration 22: loss: 71.671623, loss_kl: 67.083611, loss_recon: 0.651714, loss_pred: 5.829424
iteration 23: loss: 71.025589, loss_kl: 83.437805, loss_recon: 0.642787, loss_pred: 5.912525
iteration 24: loss: 69.027763, loss_kl: 86.643990, loss_recon: 0.639115, loss_pred: 4.249852
iteration 25: loss: 68.544495, loss_kl: 96.149963, loss_recon: 0.624211, loss_pred: 5.161888
iteration 26: loss: 69.631500, loss_kl: 98.273361, loss_recon: 0.647433, loss_pred: 3.905466
iteration 27: loss: 67.968773, loss_kl: 85.896362, loss_recon: 0.639072, loss_pred: 3.202577
iteration 28: loss: 68.105698, loss_kl: 66.394341, loss_recon: 0.655277, loss_pred: 1.914079
iteration 29: loss: 68.262527, loss_kl: 49.375969, loss_recon: 0.634100, loss_pred: 4.358730
iteration 30: loss: 68.586540, loss_kl: 41.737663, loss_recon: 0.639814, loss_pred: 4.187720
  2%|▍                              | 3/200 [02:32<2:44:53, 50.22s/it]iteration 31: loss: 66.432053, loss_kl: 51.996262, loss_recon: 0.638660, loss_pred: 2.046108
iteration 32: loss: 65.722084, loss_kl: 60.539013, loss_recon: 0.631471, loss_pred: 1.969568
iteration 33: loss: 67.538551, loss_kl: 70.349167, loss_recon: 0.641117, loss_pred: 2.723314
iteration 34: loss: 68.131142, loss_kl: 59.280838, loss_recon: 0.652264, loss_pred: 2.311936
iteration 35: loss: 65.123077, loss_kl: 55.143845, loss_recon: 0.632302, loss_pred: 1.341401
iteration 36: loss: 66.245193, loss_kl: 52.629963, loss_recon: 0.627822, loss_pred: 2.936680
iteration 37: loss: 65.754959, loss_kl: 47.986526, loss_recon: 0.632367, loss_pred: 2.038439
iteration 38: loss: 64.467079, loss_kl: 51.268860, loss_recon: 0.625358, loss_pred: 1.418577
iteration 39: loss: 66.810936, loss_kl: 49.343681, loss_recon: 0.639606, loss_pred: 2.356867
iteration 40: loss: 66.513039, loss_kl: 47.631935, loss_recon: 0.636252, loss_pred: 2.411566
  2%|▌                              | 4/200 [03:21<2:42:48, 49.84s/it]iteration 41: loss: 63.990540, loss_kl: 44.478043, loss_recon: 0.621199, loss_pred: 1.425874
iteration 42: loss: 63.928520, loss_kl: 45.048569, loss_recon: 0.623415, loss_pred: 1.136548
iteration 43: loss: 63.744591, loss_kl: 42.737465, loss_recon: 0.612984, loss_pred: 2.018770
iteration 44: loss: 65.801971, loss_kl: 45.387398, loss_recon: 0.643188, loss_pred: 1.029346
iteration 45: loss: 65.457466, loss_kl: 42.670490, loss_recon: 0.641549, loss_pred: 0.875829
iteration 46: loss: 64.133011, loss_kl: 41.950172, loss_recon: 0.619221, loss_pred: 1.791425
iteration 47: loss: 63.332355, loss_kl: 41.638939, loss_recon: 0.619953, loss_pred: 0.920685
iteration 48: loss: 63.409908, loss_kl: 42.409321, loss_recon: 0.613078, loss_pred: 1.678016
iteration 49: loss: 62.992016, loss_kl: 45.718285, loss_recon: 0.612163, loss_pred: 1.318580
iteration 50: loss: 66.238976, loss_kl: 47.401516, loss_recon: 0.628127, loss_pred: 2.952269
  2%|▊                              | 5/200 [04:10<2:40:47, 49.47s/it]iteration 51: loss: 62.263161, loss_kl: 47.502769, loss_recon: 0.604288, loss_pred: 1.359341
iteration 52: loss: 63.936230, loss_kl: 46.648472, loss_recon: 0.626594, loss_pred: 0.810374
iteration 53: loss: 63.349239, loss_kl: 41.691628, loss_recon: 0.613721, loss_pred: 1.560226
iteration 54: loss: 62.618584, loss_kl: 38.345547, loss_recon: 0.602861, loss_pred: 1.949069
iteration 55: loss: 62.930538, loss_kl: 36.361752, loss_recon: 0.615149, loss_pred: 1.052054
iteration 56: loss: 63.717281, loss_kl: 40.483627, loss_recon: 0.625829, loss_pred: 0.729595
iteration 57: loss: 63.232967, loss_kl: 38.093090, loss_recon: 0.614784, loss_pred: 1.373675
iteration 58: loss: 62.075100, loss_kl: 39.076591, loss_recon: 0.605527, loss_pred: 1.131630
iteration 59: loss: 64.603096, loss_kl: 37.649033, loss_recon: 0.624649, loss_pred: 1.761719
iteration 60: loss: 63.547153, loss_kl: 38.905872, loss_recon: 0.620773, loss_pred: 1.080799
  3%|▉                              | 6/200 [05:00<2:40:03, 49.50s/it]iteration 61: loss: 63.238136, loss_kl: 33.296944, loss_recon: 0.618146, loss_pred: 1.090580
iteration 62: loss: 62.395103, loss_kl: 37.289787, loss_recon: 0.605049, loss_pred: 1.517353
iteration 63: loss: 63.520573, loss_kl: 35.699310, loss_recon: 0.615299, loss_pred: 1.633686
iteration 64: loss: 63.082638, loss_kl: 38.269455, loss_recon: 0.614766, loss_pred: 1.223338
iteration 65: loss: 63.706772, loss_kl: 41.766087, loss_recon: 0.619362, loss_pred: 1.352890
iteration 66: loss: 63.660297, loss_kl: 43.442326, loss_recon: 0.614659, loss_pred: 1.759963
iteration 67: loss: 62.691692, loss_kl: 38.313675, loss_recon: 0.612048, loss_pred: 1.103722
iteration 68: loss: 62.744999, loss_kl: 37.751842, loss_recon: 0.612496, loss_pred: 1.117928
iteration 69: loss: 62.738892, loss_kl: 34.727982, loss_recon: 0.613512, loss_pred: 1.040443
iteration 70: loss: 64.661072, loss_kl: 37.085243, loss_recon: 0.609910, loss_pred: 3.299253
  4%|█                              | 7/200 [05:49<2:39:35, 49.61s/it]iteration 71: loss: 62.585472, loss_kl: 34.959023, loss_recon: 0.612998, loss_pred: 0.936093
iteration 72: loss: 62.724442, loss_kl: 35.683411, loss_recon: 0.613713, loss_pred: 0.996329
iteration 73: loss: 62.648514, loss_kl: 34.295002, loss_recon: 0.615272, loss_pred: 0.778378
iteration 74: loss: 62.079010, loss_kl: 37.809689, loss_recon: 0.610224, loss_pred: 0.678489
iteration 75: loss: 62.433887, loss_kl: 40.394653, loss_recon: 0.609439, loss_pred: 1.086027
iteration 76: loss: 61.138847, loss_kl: 37.358803, loss_recon: 0.602164, loss_pred: 0.548893
iteration 77: loss: 62.761868, loss_kl: 36.077148, loss_recon: 0.614060, loss_pred: 0.995117
iteration 78: loss: 63.655640, loss_kl: 34.971649, loss_recon: 0.621975, loss_pred: 1.108446
iteration 79: loss: 62.896889, loss_kl: 33.267239, loss_recon: 0.610864, loss_pred: 1.477860
iteration 80: loss: 64.217171, loss_kl: 31.693134, loss_recon: 0.606247, loss_pred: 3.275504
  4%|█▏                             | 8/200 [06:40<2:39:34, 49.87s/it]iteration 81: loss: 62.323780, loss_kl: 37.843201, loss_recon: 0.598653, loss_pred: 2.079997
iteration 82: loss: 62.201424, loss_kl: 38.964645, loss_recon: 0.609714, loss_pred: 0.840348
iteration 83: loss: 63.521660, loss_kl: 40.306091, loss_recon: 0.606997, loss_pred: 2.418922
iteration 84: loss: 64.607361, loss_kl: 35.656845, loss_recon: 0.618531, loss_pred: 2.397645
iteration 85: loss: 65.838593, loss_kl: 35.924889, loss_recon: 0.629410, loss_pred: 2.538390
iteration 86: loss: 61.719715, loss_kl: 32.338711, loss_recon: 0.602772, loss_pred: 1.119118
iteration 87: loss: 63.161034, loss_kl: 33.253235, loss_recon: 0.618221, loss_pred: 1.006406
iteration 88: loss: 63.396080, loss_kl: 30.174854, loss_recon: 0.622187, loss_pred: 0.875580
iteration 89: loss: 63.793911, loss_kl: 30.605194, loss_recon: 0.617730, loss_pred: 1.714877
iteration 90: loss: 64.800873, loss_kl: 31.789644, loss_recon: 0.629394, loss_pred: 1.543567
  4%|█▍                             | 9/200 [07:29<2:37:57, 49.62s/it]iteration 91: loss: 62.963715, loss_kl: 28.873732, loss_recon: 0.618795, loss_pred: 0.795472
iteration 92: loss: 63.320969, loss_kl: 31.992474, loss_recon: 0.615254, loss_pred: 1.475653
iteration 93: loss: 62.135986, loss_kl: 29.666533, loss_recon: 0.607235, loss_pred: 1.115847
iteration 94: loss: 62.006683, loss_kl: 31.973125, loss_recon: 0.609471, loss_pred: 0.739850
iteration 95: loss: 61.675507, loss_kl: 32.964233, loss_recon: 0.600104, loss_pred: 1.335418
iteration 96: loss: 63.257755, loss_kl: 30.595127, loss_recon: 0.617711, loss_pred: 1.180730
iteration 97: loss: 64.518410, loss_kl: 33.814331, loss_recon: 0.626071, loss_pred: 1.573138
iteration 98: loss: 62.443413, loss_kl: 34.783298, loss_recon: 0.613855, loss_pred: 0.710035
iteration 99: loss: 62.625595, loss_kl: 35.882683, loss_recon: 0.612190, loss_pred: 1.047726
iteration 100: loss: 62.258415, loss_kl: 37.427353, loss_recon: 0.592747, loss_pred: 2.609475
  5%|█▌                            | 10/200 [08:18<2:36:29, 49.42s/it]iteration 101: loss: 62.630737, loss_kl: 33.605770, loss_recon: 0.613986, loss_pred: 0.896053
iteration 102: loss: 61.505058, loss_kl: 34.015404, loss_recon: 0.605825, loss_pred: 0.582440
iteration 103: loss: 63.171547, loss_kl: 38.019142, loss_recon: 0.614937, loss_pred: 1.297626
iteration 104: loss: 62.068619, loss_kl: 33.977432, loss_recon: 0.609319, loss_pred: 0.796919
iteration 105: loss: 63.362396, loss_kl: 35.322250, loss_recon: 0.617822, loss_pred: 1.226998
iteration 106: loss: 61.757118, loss_kl: 34.786552, loss_recon: 0.604752, loss_pred: 0.934101
iteration 107: loss: 63.181133, loss_kl: 32.058472, loss_recon: 0.616080, loss_pred: 1.252554
iteration 108: loss: 62.924603, loss_kl: 31.414608, loss_recon: 0.617254, loss_pred: 0.885029
iteration 109: loss: 62.190331, loss_kl: 29.681229, loss_recon: 0.610628, loss_pred: 0.830696
iteration 110: loss: 64.297676, loss_kl: 29.301256, loss_recon: 0.629000, loss_pred: 1.104706
  6%|█▋                            | 11/200 [09:07<2:35:09, 49.26s/it]iteration 111: loss: 61.696754, loss_kl: 31.508350, loss_recon: 0.598737, loss_pred: 1.508002
iteration 112: loss: 62.908527, loss_kl: 31.793869, loss_recon: 0.615269, loss_pred: 1.063707
iteration 113: loss: 64.135376, loss_kl: 33.709320, loss_recon: 0.614929, loss_pred: 2.305380
iteration 114: loss: 62.848682, loss_kl: 32.317528, loss_recon: 0.615568, loss_pred: 0.968669
iteration 115: loss: 64.648857, loss_kl: 31.072849, loss_recon: 0.623776, loss_pred: 1.960516
iteration 116: loss: 61.755257, loss_kl: 34.367706, loss_recon: 0.604107, loss_pred: 1.000865
iteration 117: loss: 62.391495, loss_kl: 29.974487, loss_recon: 0.613444, loss_pred: 0.747358
iteration 118: loss: 63.121643, loss_kl: 30.109650, loss_recon: 0.617265, loss_pred: 1.094016
iteration 119: loss: 62.071400, loss_kl: 26.934343, loss_recon: 0.607623, loss_pred: 1.039753
iteration 120: loss: 67.101578, loss_kl: 26.808414, loss_recon: 0.653404, loss_pred: 1.493068
  6%|█▊                            | 12/200 [09:56<2:34:49, 49.41s/it]iteration 121: loss: 62.115303, loss_kl: 27.071857, loss_recon: 0.600899, loss_pred: 1.754692
iteration 122: loss: 63.248165, loss_kl: 27.582649, loss_recon: 0.620825, loss_pred: 0.889824
iteration 123: loss: 61.804512, loss_kl: 29.941044, loss_recon: 0.608919, loss_pred: 0.613164
iteration 124: loss: 60.971600, loss_kl: 33.820927, loss_recon: 0.601567, loss_pred: 0.476660
iteration 125: loss: 63.150429, loss_kl: 29.993855, loss_recon: 0.615759, loss_pred: 1.274588
iteration 126: loss: 62.004204, loss_kl: 30.540726, loss_recon: 0.607001, loss_pred: 0.998663
iteration 127: loss: 63.257637, loss_kl: 32.182068, loss_recon: 0.616808, loss_pred: 1.255011
iteration 128: loss: 63.586426, loss_kl: 30.267994, loss_recon: 0.619764, loss_pred: 1.307318
iteration 129: loss: 63.394112, loss_kl: 29.023304, loss_recon: 0.623757, loss_pred: 0.728208
iteration 130: loss: 64.058777, loss_kl: 28.132311, loss_recon: 0.608765, loss_pred: 2.901001
  6%|█▉                            | 13/200 [10:46<2:33:56, 49.39s/it]iteration 131: loss: 63.792248, loss_kl: 26.892616, loss_recon: 0.611158, loss_pred: 1.342538
iteration 132: loss: 63.604801, loss_kl: 29.047262, loss_recon: 0.611137, loss_pred: 1.050320
iteration 133: loss: 63.341919, loss_kl: 25.395733, loss_recon: 0.611156, loss_pred: 0.966707
iteration 134: loss: 65.687065, loss_kl: 28.463913, loss_recon: 0.624245, loss_pred: 1.850772
iteration 135: loss: 64.615280, loss_kl: 24.561609, loss_recon: 0.622299, loss_pred: 1.167085
iteration 136: loss: 63.490948, loss_kl: 25.811882, loss_recon: 0.611875, loss_pred: 1.023143
iteration 137: loss: 63.791756, loss_kl: 24.394409, loss_recon: 0.611195, loss_pred: 1.462324
iteration 138: loss: 63.649914, loss_kl: 24.668365, loss_recon: 0.613852, loss_pred: 1.041182
iteration 139: loss: 63.131424, loss_kl: 24.200218, loss_recon: 0.602469, loss_pred: 1.684201
iteration 140: loss: 64.507141, loss_kl: 25.687389, loss_recon: 0.620923, loss_pred: 1.140798
  7%|██                            | 14/200 [11:35<2:32:58, 49.34s/it]iteration 141: loss: 63.594482, loss_kl: 27.043550, loss_recon: 0.603758, loss_pred: 0.806412
iteration 142: loss: 65.119102, loss_kl: 26.552341, loss_recon: 0.617495, loss_pred: 1.001131
iteration 143: loss: 65.060875, loss_kl: 27.747536, loss_recon: 0.613167, loss_pred: 1.269070
iteration 144: loss: 63.902515, loss_kl: 25.983425, loss_recon: 0.608852, loss_pred: 0.699578
iteration 145: loss: 63.559742, loss_kl: 23.389469, loss_recon: 0.606720, loss_pred: 0.801427
iteration 146: loss: 64.132858, loss_kl: 25.368700, loss_recon: 0.610587, loss_pred: 0.811223
iteration 147: loss: 63.729473, loss_kl: 21.169769, loss_recon: 0.611233, loss_pred: 0.717807
iteration 148: loss: 65.108597, loss_kl: 19.471601, loss_recon: 0.623113, loss_pred: 1.060448
iteration 149: loss: 65.340958, loss_kl: 22.339071, loss_recon: 0.618950, loss_pred: 1.453326
iteration 150: loss: 63.585175, loss_kl: 19.624929, loss_recon: 0.606604, loss_pred: 1.174195
  8%|██▎                           | 15/200 [12:25<2:32:33, 49.48s/it]iteration 151: loss: 64.789398, loss_kl: 19.118280, loss_recon: 0.616504, loss_pred: 0.676556
iteration 152: loss: 65.051109, loss_kl: 18.235050, loss_recon: 0.619699, loss_pred: 0.732495
iteration 153: loss: 64.065392, loss_kl: 17.559202, loss_recon: 0.610988, loss_pred: 0.704982
iteration 154: loss: 65.811447, loss_kl: 19.605820, loss_recon: 0.610779, loss_pred: 2.208271
iteration 155: loss: 64.503143, loss_kl: 18.622738, loss_recon: 0.613726, loss_pred: 0.731953
iteration 156: loss: 64.192123, loss_kl: 19.230682, loss_recon: 0.608211, loss_pred: 0.894140
iteration 157: loss: 63.417637, loss_kl: 20.101942, loss_recon: 0.599753, loss_pred: 0.853209
iteration 158: loss: 65.520126, loss_kl: 18.478699, loss_recon: 0.623887, loss_pred: 0.751376
iteration 159: loss: 64.082802, loss_kl: 19.893364, loss_recon: 0.610019, loss_pred: 0.518622
iteration 160: loss: 64.404846, loss_kl: 21.141689, loss_recon: 0.602304, loss_pred: 1.451435
  8%|██▍                           | 16/200 [13:14<2:31:10, 49.30s/it]iteration 161: loss: 65.342949, loss_kl: 17.472883, loss_recon: 0.614254, loss_pred: 0.975109
iteration 162: loss: 63.758827, loss_kl: 16.691048, loss_recon: 0.599120, loss_pred: 1.036094
iteration 163: loss: 64.396820, loss_kl: 17.838074, loss_recon: 0.607334, loss_pred: 0.659498
iteration 164: loss: 65.125381, loss_kl: 19.980644, loss_recon: 0.608389, loss_pred: 0.921712
iteration 165: loss: 66.544243, loss_kl: 16.850725, loss_recon: 0.625139, loss_pred: 1.192641
iteration 166: loss: 65.438545, loss_kl: 13.888104, loss_recon: 0.624543, loss_pred: 0.645443
iteration 167: loss: 65.025330, loss_kl: 15.237515, loss_recon: 0.596718, loss_pred: 2.787562
iteration 168: loss: 66.633598, loss_kl: 16.440222, loss_recon: 0.632848, loss_pred: 0.580232
iteration 169: loss: 64.703873, loss_kl: 14.311691, loss_recon: 0.615391, loss_pred: 0.754631
iteration 170: loss: 63.070957, loss_kl: 15.136263, loss_recon: 0.595304, loss_pred: 0.991639
  8%|██▌                           | 17/200 [14:03<2:29:56, 49.16s/it]iteration 171: loss: 66.843307, loss_kl: 13.879200, loss_recon: 0.628589, loss_pred: 1.097553
iteration 172: loss: 65.818054, loss_kl: 13.426203, loss_recon: 0.617733, loss_pred: 1.252077
iteration 173: loss: 65.297272, loss_kl: 13.352644, loss_recon: 0.613641, loss_pred: 1.155793
iteration 174: loss: 66.117699, loss_kl: 12.843768, loss_recon: 0.619078, loss_pred: 1.538437
iteration 175: loss: 65.572464, loss_kl: 13.235150, loss_recon: 0.612748, loss_pred: 1.544742
iteration 176: loss: 64.952843, loss_kl: 12.306149, loss_recon: 0.616292, loss_pred: 0.763922
iteration 177: loss: 64.645317, loss_kl: 14.046984, loss_recon: 0.607320, loss_pred: 0.991570
iteration 178: loss: 64.855919, loss_kl: 14.411539, loss_recon: 0.609950, loss_pred: 0.863321
iteration 179: loss: 63.636906, loss_kl: 15.588897, loss_recon: 0.595092, loss_pred: 0.885187
iteration 180: loss: 66.577553, loss_kl: 11.512975, loss_recon: 0.614045, loss_pred: 2.778393
  9%|██▋                           | 18/200 [14:52<2:29:42, 49.35s/it]iteration 181: loss: 64.690224, loss_kl: 11.856786, loss_recon: 0.611956, loss_pred: 0.558898
iteration 182: loss: 64.902443, loss_kl: 11.864238, loss_recon: 0.610464, loss_pred: 0.918427
iteration 183: loss: 64.631882, loss_kl: 11.711658, loss_recon: 0.609132, loss_pred: 0.818912
iteration 184: loss: 63.971077, loss_kl: 11.334866, loss_recon: 0.594878, loss_pred: 1.676772
iteration 185: loss: 63.575367, loss_kl: 11.438021, loss_recon: 0.599737, loss_pred: 0.769609
iteration 186: loss: 65.854958, loss_kl: 10.723818, loss_recon: 0.620369, loss_pred: 1.162840
iteration 187: loss: 64.519585, loss_kl: 13.258120, loss_recon: 0.605756, loss_pred: 0.661249
iteration 188: loss: 65.517525, loss_kl: 11.422150, loss_recon: 0.620534, loss_pred: 0.635998
iteration 189: loss: 65.888687, loss_kl: 10.628597, loss_recon: 0.627793, loss_pred: 0.477746
iteration 190: loss: 65.549400, loss_kl: 7.923904, loss_recon: 0.618775, loss_pred: 1.709946
 10%|██▊                           | 19/200 [15:43<2:30:13, 49.80s/it]iteration 191: loss: 65.475052, loss_kl: 10.057053, loss_recon: 0.616217, loss_pred: 0.964959
iteration 192: loss: 65.976997, loss_kl: 10.077923, loss_recon: 0.620406, loss_pred: 1.042005
iteration 193: loss: 64.887421, loss_kl: 10.007097, loss_recon: 0.610342, loss_pred: 0.979146
iteration 194: loss: 65.731216, loss_kl: 11.000939, loss_recon: 0.618014, loss_pred: 0.770346
iteration 195: loss: 64.714035, loss_kl: 11.572750, loss_recon: 0.606502, loss_pred: 0.740191
iteration 196: loss: 66.162178, loss_kl: 10.860810, loss_recon: 0.619704, loss_pred: 1.072557
iteration 197: loss: 65.179878, loss_kl: 11.976543, loss_recon: 0.611731, loss_pred: 0.567142
iteration 198: loss: 63.917751, loss_kl: 10.261021, loss_recon: 0.598612, loss_pred: 1.109634
iteration 199: loss: 64.234810, loss_kl: 10.619654, loss_recon: 0.604448, loss_pred: 0.740008
iteration 200: loss: 68.637291, loss_kl: 10.793199, loss_recon: 0.635274, loss_pred: 2.010037
 10%|███                           | 20/200 [16:32<2:28:37, 49.54s/it]iteration 201: loss: 64.787712, loss_kl: 9.915997, loss_recon: 0.604010, loss_pred: 1.146172
iteration 202: loss: 66.157639, loss_kl: 9.065495, loss_recon: 0.618183, loss_pred: 1.376735
iteration 203: loss: 64.789215, loss_kl: 7.728549, loss_recon: 0.616774, loss_pred: 0.586127
iteration 204: loss: 65.329720, loss_kl: 8.916176, loss_recon: 0.618960, loss_pred: 0.519880
iteration 205: loss: 64.424484, loss_kl: 10.292354, loss_recon: 0.602307, loss_pred: 0.830212
iteration 206: loss: 64.658028, loss_kl: 8.833386, loss_recon: 0.613367, loss_pred: 0.434596
iteration 207: loss: 64.620918, loss_kl: 8.987011, loss_recon: 0.609839, loss_pred: 0.700034
iteration 208: loss: 64.300697, loss_kl: 9.217512, loss_recon: 0.605483, loss_pred: 0.740091
iteration 209: loss: 65.358406, loss_kl: 7.510254, loss_recon: 0.617957, loss_pred: 1.108315
iteration 210: loss: 66.054718, loss_kl: 7.646497, loss_recon: 0.621920, loss_pred: 1.363858
 10%|███▏                          | 21/200 [17:22<2:27:47, 49.54s/it]iteration 211: loss: 66.307365, loss_kl: 7.517241, loss_recon: 0.617464, loss_pred: 1.806679
iteration 212: loss: 64.935555, loss_kl: 8.804710, loss_recon: 0.601305, loss_pred: 1.578985
iteration 213: loss: 65.433449, loss_kl: 8.603644, loss_recon: 0.616849, loss_pred: 0.596176
iteration 214: loss: 65.339470, loss_kl: 10.165836, loss_recon: 0.604897, loss_pred: 1.124962
iteration 215: loss: 64.180611, loss_kl: 9.194592, loss_recon: 0.601238, loss_pred: 0.687956
iteration 216: loss: 66.007309, loss_kl: 8.020720, loss_recon: 0.624256, loss_pred: 0.642937
iteration 217: loss: 64.021088, loss_kl: 6.866408, loss_recon: 0.600505, loss_pred: 1.454690
iteration 218: loss: 66.289818, loss_kl: 7.644952, loss_recon: 0.622355, loss_pred: 1.253224
iteration 219: loss: 65.688026, loss_kl: 8.034514, loss_recon: 0.621329, loss_pred: 0.611315
iteration 220: loss: 67.664291, loss_kl: 7.633134, loss_recon: 0.622796, loss_pred: 2.587914
 11%|███▎                          | 22/200 [18:11<2:26:32, 49.40s/it]iteration 221: loss: 64.554115, loss_kl: 8.099401, loss_recon: 0.604719, loss_pred: 0.793819
iteration 222: loss: 64.088562, loss_kl: 8.094073, loss_recon: 0.600975, loss_pred: 0.704904
iteration 223: loss: 66.129616, loss_kl: 7.773315, loss_recon: 0.621624, loss_pred: 0.811296
iteration 224: loss: 63.871929, loss_kl: 7.408227, loss_recon: 0.603943, loss_pred: 0.469922
iteration 225: loss: 64.933662, loss_kl: 7.202738, loss_recon: 0.615790, loss_pred: 0.430308
iteration 226: loss: 66.025307, loss_kl: 6.693577, loss_recon: 0.628943, loss_pred: 0.413441
iteration 227: loss: 65.786652, loss_kl: 7.410551, loss_recon: 0.615448, loss_pred: 1.233206
iteration 228: loss: 64.589157, loss_kl: 6.578983, loss_recon: 0.606848, loss_pred: 1.233335
iteration 229: loss: 64.434776, loss_kl: 6.055417, loss_recon: 0.609599, loss_pred: 1.016356
iteration 230: loss: 65.789597, loss_kl: 6.997193, loss_recon: 0.618438, loss_pred: 1.104916
 12%|███▍                          | 23/200 [19:00<2:25:25, 49.30s/it]iteration 231: loss: 65.402473, loss_kl: 6.501328, loss_recon: 0.617722, loss_pred: 0.733300
iteration 232: loss: 64.681580, loss_kl: 6.561182, loss_recon: 0.609284, loss_pred: 0.829541
iteration 233: loss: 65.186707, loss_kl: 7.142997, loss_recon: 0.610473, loss_pred: 0.956457
iteration 234: loss: 65.532822, loss_kl: 6.996485, loss_recon: 0.616018, loss_pred: 0.813339
iteration 235: loss: 64.530846, loss_kl: 6.510674, loss_recon: 0.610960, loss_pred: 0.533646
iteration 236: loss: 65.316223, loss_kl: 6.660025, loss_recon: 0.613185, loss_pred: 1.030064
iteration 237: loss: 64.178429, loss_kl: 6.381418, loss_recon: 0.606162, loss_pred: 0.718679
iteration 238: loss: 63.687134, loss_kl: 6.440465, loss_recon: 0.600487, loss_pred: 0.768569
iteration 239: loss: 63.879276, loss_kl: 6.040540, loss_recon: 0.607363, loss_pred: 0.451313
iteration 240: loss: 68.672188, loss_kl: 6.171214, loss_recon: 0.642308, loss_pred: 1.691483
 12%|███▌                          | 24/200 [19:49<2:24:06, 49.13s/it]iteration 241: loss: 64.718361, loss_kl: 6.618129, loss_recon: 0.610914, loss_pred: 0.415885
iteration 242: loss: 63.160690, loss_kl: 5.164045, loss_recon: 0.600108, loss_pred: 0.644285
iteration 243: loss: 63.596745, loss_kl: 5.531879, loss_recon: 0.599930, loss_pred: 0.919668
iteration 244: loss: 64.672325, loss_kl: 6.951679, loss_recon: 0.606543, loss_pred: 0.645107
iteration 245: loss: 64.865540, loss_kl: 5.685785, loss_recon: 0.615396, loss_pred: 0.567214
iteration 246: loss: 65.656540, loss_kl: 5.116817, loss_recon: 0.626598, loss_pred: 0.514096
iteration 247: loss: 64.778481, loss_kl: 5.725481, loss_recon: 0.614784, loss_pred: 0.522088
iteration 248: loss: 65.112129, loss_kl: 6.370741, loss_recon: 0.613811, loss_pred: 0.639958
iteration 249: loss: 64.746887, loss_kl: 4.789674, loss_recon: 0.619427, loss_pred: 0.480265
iteration 250: loss: 64.371872, loss_kl: 6.634035, loss_recon: 0.600076, loss_pred: 1.145444
 12%|███▊                          | 25/200 [20:38<2:23:40, 49.26s/it]iteration 251: loss: 65.296967, loss_kl: 5.492162, loss_recon: 0.615485, loss_pred: 0.866167
iteration 252: loss: 63.783848, loss_kl: 3.945240, loss_recon: 0.605565, loss_pred: 1.156848
iteration 253: loss: 66.359932, loss_kl: 5.125188, loss_recon: 0.627527, loss_pred: 0.917512
iteration 254: loss: 63.924774, loss_kl: 5.749250, loss_recon: 0.604011, loss_pred: 0.506489
iteration 255: loss: 63.152237, loss_kl: 6.015278, loss_recon: 0.593446, loss_pred: 0.650831
iteration 256: loss: 65.834183, loss_kl: 5.674221, loss_recon: 0.622476, loss_pred: 0.608748
iteration 257: loss: 63.308292, loss_kl: 4.601430, loss_recon: 0.602041, loss_pred: 0.689384
iteration 258: loss: 64.609283, loss_kl: 5.129295, loss_recon: 0.606364, loss_pred: 1.281032
iteration 259: loss: 64.127029, loss_kl: 4.517184, loss_recon: 0.610499, loss_pred: 0.706527
iteration 260: loss: 66.658249, loss_kl: 3.781849, loss_recon: 0.633180, loss_pred: 1.355548
 13%|███▉                          | 26/200 [21:28<2:23:22, 49.44s/it]iteration 261: loss: 63.800869, loss_kl: 4.857326, loss_recon: 0.604649, loss_pred: 0.594529
iteration 262: loss: 63.573666, loss_kl: 3.907315, loss_recon: 0.600719, loss_pred: 1.296520
iteration 263: loss: 65.025597, loss_kl: 5.233314, loss_recon: 0.616065, loss_pred: 0.465442
iteration 264: loss: 64.518867, loss_kl: 4.553631, loss_recon: 0.614030, loss_pred: 0.545835
iteration 265: loss: 63.716633, loss_kl: 3.820524, loss_recon: 0.610617, loss_pred: 0.498595
iteration 266: loss: 64.933380, loss_kl: 4.502593, loss_recon: 0.619381, loss_pred: 0.453980
iteration 267: loss: 64.432037, loss_kl: 4.591254, loss_recon: 0.606172, loss_pred: 1.223538
iteration 268: loss: 63.836166, loss_kl: 4.806200, loss_recon: 0.603360, loss_pred: 0.787529
iteration 269: loss: 65.229790, loss_kl: 4.315112, loss_recon: 0.620435, loss_pred: 0.750820
iteration 270: loss: 64.404121, loss_kl: 4.150981, loss_recon: 0.611019, loss_pred: 0.959449
 14%|████                          | 27/200 [22:17<2:22:14, 49.33s/it]iteration 271: loss: 63.082134, loss_kl: 3.326577, loss_recon: 0.604930, loss_pred: 0.579844
iteration 272: loss: 64.155136, loss_kl: 4.811900, loss_recon: 0.606177, loss_pred: 0.631069
iteration 273: loss: 63.812771, loss_kl: 4.214036, loss_recon: 0.608675, loss_pred: 0.399950
iteration 274: loss: 65.360374, loss_kl: 4.447359, loss_recon: 0.622952, loss_pred: 0.378940
iteration 275: loss: 64.135399, loss_kl: 4.410641, loss_recon: 0.607018, loss_pred: 0.769577
iteration 276: loss: 64.779358, loss_kl: 3.768407, loss_recon: 0.619818, loss_pred: 0.521421
iteration 277: loss: 64.246414, loss_kl: 3.344568, loss_recon: 0.613670, loss_pred: 0.859272
iteration 278: loss: 63.765770, loss_kl: 4.674456, loss_recon: 0.605806, loss_pred: 0.361840
iteration 279: loss: 63.436497, loss_kl: 4.603730, loss_recon: 0.600754, loss_pred: 0.580489
iteration 280: loss: 66.107582, loss_kl: 3.801548, loss_recon: 0.632395, loss_pred: 0.571979
 14%|████▏                         | 28/200 [23:07<2:21:49, 49.47s/it]iteration 281: loss: 63.956017, loss_kl: 3.991301, loss_recon: 0.608176, loss_pred: 0.569640
iteration 282: loss: 63.722851, loss_kl: 3.809802, loss_recon: 0.607072, loss_pred: 0.563652
iteration 283: loss: 64.568085, loss_kl: 3.691457, loss_recon: 0.615257, loss_pred: 0.666614
iteration 284: loss: 63.282082, loss_kl: 3.648364, loss_recon: 0.600793, loss_pred: 0.854718
iteration 285: loss: 63.809692, loss_kl: 3.163359, loss_recon: 0.613334, loss_pred: 0.440350
iteration 286: loss: 65.405060, loss_kl: 3.617326, loss_recon: 0.627371, loss_pred: 0.339865
iteration 287: loss: 63.673260, loss_kl: 3.616080, loss_recon: 0.609403, loss_pred: 0.405613
iteration 288: loss: 63.595993, loss_kl: 4.190088, loss_recon: 0.603150, loss_pred: 0.584260
iteration 289: loss: 63.800510, loss_kl: 3.608935, loss_recon: 0.610039, loss_pred: 0.473893
iteration 290: loss: 66.467323, loss_kl: 3.737836, loss_recon: 0.632027, loss_pred: 0.858961
 14%|████▎                         | 29/200 [23:56<2:20:34, 49.32s/it]iteration 291: loss: 64.369202, loss_kl: 3.845226, loss_recon: 0.613139, loss_pred: 0.428275
iteration 292: loss: 64.469421, loss_kl: 3.523859, loss_recon: 0.613866, loss_pred: 0.675357
iteration 293: loss: 64.826042, loss_kl: 4.010489, loss_recon: 0.615742, loss_pred: 0.511874
iteration 294: loss: 64.183327, loss_kl: 3.845848, loss_recon: 0.611257, loss_pred: 0.430121
iteration 295: loss: 62.622524, loss_kl: 3.487395, loss_recon: 0.596686, loss_pred: 0.571378
iteration 296: loss: 63.336037, loss_kl: 3.673095, loss_recon: 0.604322, loss_pred: 0.394375
iteration 297: loss: 63.492500, loss_kl: 2.836990, loss_recon: 0.611394, loss_pred: 0.414884
iteration 298: loss: 63.705864, loss_kl: 2.596426, loss_recon: 0.613767, loss_pred: 0.555287
iteration 299: loss: 64.126404, loss_kl: 2.870472, loss_recon: 0.616995, loss_pred: 0.465790
iteration 300: loss: 63.460930, loss_kl: 3.316462, loss_recon: 0.604782, loss_pred: 0.716880
 15%|████▌                         | 30/200 [24:46<2:20:04, 49.44s/it]iteration 301: loss: 63.366089, loss_kl: 2.894794, loss_recon: 0.608360, loss_pred: 0.437709
iteration 302: loss: 64.119591, loss_kl: 3.509757, loss_recon: 0.612849, loss_pred: 0.297808
iteration 303: loss: 62.396473, loss_kl: 2.858279, loss_recon: 0.595331, loss_pred: 0.797450
iteration 304: loss: 63.374840, loss_kl: 2.827823, loss_recon: 0.608624, loss_pred: 0.468451
iteration 305: loss: 64.451042, loss_kl: 3.949314, loss_recon: 0.609069, loss_pred: 0.689536
iteration 306: loss: 62.768818, loss_kl: 3.098770, loss_recon: 0.601105, loss_pred: 0.418509
iteration 307: loss: 65.501358, loss_kl: 2.216331, loss_recon: 0.635284, loss_pred: 0.370966
iteration 308: loss: 65.258034, loss_kl: 3.139027, loss_recon: 0.625324, loss_pred: 0.456776
iteration 309: loss: 63.686344, loss_kl: 3.022793, loss_recon: 0.609408, loss_pred: 0.560708
iteration 310: loss: 61.962128, loss_kl: 3.258482, loss_recon: 0.589727, loss_pred: 0.634149
 16%|████▋                         | 31/200 [25:35<2:18:52, 49.31s/it]iteration 311: loss: 64.390083, loss_kl: 3.273857, loss_recon: 0.612135, loss_pred: 0.680565
iteration 312: loss: 63.018929, loss_kl: 2.849813, loss_recon: 0.602297, loss_pred: 0.616571
iteration 313: loss: 63.912209, loss_kl: 3.188499, loss_recon: 0.609916, loss_pred: 0.489700
iteration 314: loss: 63.618763, loss_kl: 3.061348, loss_recon: 0.607162, loss_pred: 0.568619
iteration 315: loss: 64.888306, loss_kl: 2.571267, loss_recon: 0.621896, loss_pred: 0.738386
iteration 316: loss: 63.339252, loss_kl: 2.109864, loss_recon: 0.608291, loss_pred: 0.901574
iteration 317: loss: 63.273033, loss_kl: 2.744216, loss_recon: 0.603714, loss_pred: 0.809398
iteration 318: loss: 64.843124, loss_kl: 2.615620, loss_recon: 0.619887, loss_pred: 0.860262
iteration 319: loss: 62.779678, loss_kl: 2.609422, loss_recon: 0.602762, loss_pred: 0.514079
iteration 320: loss: 64.330292, loss_kl: 1.675297, loss_recon: 0.627343, loss_pred: 0.318702
 16%|████▊                         | 32/200 [26:23<2:17:45, 49.20s/it]iteration 321: loss: 65.013977, loss_kl: 2.658841, loss_recon: 0.613860, loss_pred: 1.495553
iteration 322: loss: 64.119545, loss_kl: 2.562530, loss_recon: 0.614642, loss_pred: 0.600232
iteration 323: loss: 64.113007, loss_kl: 1.690916, loss_recon: 0.613058, loss_pred: 1.451080
iteration 324: loss: 64.285904, loss_kl: 2.409650, loss_recon: 0.619068, loss_pred: 0.446606
iteration 325: loss: 65.416199, loss_kl: 3.037309, loss_recon: 0.618626, loss_pred: 1.117673
iteration 326: loss: 62.606865, loss_kl: 2.092388, loss_recon: 0.596485, loss_pred: 1.280307
iteration 327: loss: 63.957443, loss_kl: 2.128665, loss_recon: 0.611710, loss_pred: 1.079238
iteration 328: loss: 63.042641, loss_kl: 2.502284, loss_recon: 0.600908, loss_pred: 0.944962
iteration 329: loss: 63.371521, loss_kl: 2.177616, loss_recon: 0.613148, loss_pred: 0.310311
iteration 330: loss: 63.992149, loss_kl: 2.168177, loss_recon: 0.602259, loss_pred: 2.027423
 16%|████▉                         | 33/200 [27:12<2:16:43, 49.12s/it]iteration 331: loss: 62.115891, loss_kl: 1.853955, loss_recon: 0.596103, loss_pred: 0.945283
iteration 332: loss: 64.077835, loss_kl: 2.242795, loss_recon: 0.613115, loss_pred: 0.878803
iteration 333: loss: 63.747948, loss_kl: 2.577825, loss_recon: 0.604709, loss_pred: 1.107521
iteration 334: loss: 62.654808, loss_kl: 1.945864, loss_recon: 0.604499, loss_pred: 0.567316
iteration 335: loss: 64.338501, loss_kl: 2.123873, loss_recon: 0.618101, loss_pred: 0.740925
iteration 336: loss: 64.019760, loss_kl: 2.480994, loss_recon: 0.602519, loss_pred: 1.679890
iteration 337: loss: 63.958775, loss_kl: 2.303375, loss_recon: 0.614217, loss_pred: 0.598599
iteration 338: loss: 64.596161, loss_kl: 2.363004, loss_recon: 0.611030, loss_pred: 1.504429
iteration 339: loss: 63.669617, loss_kl: 2.095657, loss_recon: 0.612745, loss_pred: 0.631406
iteration 340: loss: 67.156532, loss_kl: 2.424211, loss_recon: 0.645296, loss_pred: 0.586696
 17%|█████                         | 34/200 [28:02<2:16:23, 49.30s/it]iteration 341: loss: 65.601425, loss_kl: 2.148211, loss_recon: 0.609808, loss_pred: 2.727588
iteration 342: loss: 64.162468, loss_kl: 1.910429, loss_recon: 0.615770, loss_pred: 0.902007
iteration 343: loss: 64.359520, loss_kl: 2.685298, loss_recon: 0.611624, loss_pred: 0.830823
iteration 344: loss: 64.495377, loss_kl: 2.058484, loss_recon: 0.610390, loss_pred: 1.642418
iteration 345: loss: 63.396088, loss_kl: 2.158726, loss_recon: 0.610623, loss_pred: 0.431570
iteration 346: loss: 63.424862, loss_kl: 1.939527, loss_recon: 0.610599, loss_pred: 0.655842
iteration 347: loss: 63.454681, loss_kl: 2.152715, loss_recon: 0.605741, loss_pred: 0.983603
iteration 348: loss: 62.638866, loss_kl: 1.458441, loss_recon: 0.609149, loss_pred: 0.438791
iteration 349: loss: 64.524841, loss_kl: 2.310875, loss_recon: 0.616661, loss_pred: 0.822379
iteration 350: loss: 61.529289, loss_kl: 2.118501, loss_recon: 0.590256, loss_pred: 0.636831
 18%|█████▎                        | 35/200 [28:52<2:15:45, 49.37s/it]iteration 351: loss: 66.314941, loss_kl: 2.441650, loss_recon: 0.636381, loss_pred: 0.428536
iteration 352: loss: 63.891335, loss_kl: 1.762187, loss_recon: 0.614219, loss_pred: 0.846848
iteration 353: loss: 62.900593, loss_kl: 2.252263, loss_recon: 0.597469, loss_pred: 1.079790
iteration 354: loss: 63.515202, loss_kl: 1.431726, loss_recon: 0.618416, loss_pred: 0.355304
iteration 355: loss: 63.647038, loss_kl: 2.242547, loss_recon: 0.605718, loss_pred: 1.010342
iteration 356: loss: 62.229694, loss_kl: 1.368973, loss_recon: 0.605052, loss_pred: 0.463893
iteration 357: loss: 62.849083, loss_kl: 2.021191, loss_recon: 0.606564, loss_pred: 0.331530
iteration 358: loss: 63.295990, loss_kl: 1.755867, loss_recon: 0.610527, loss_pred: 0.626473
iteration 359: loss: 61.421082, loss_kl: 1.619394, loss_recon: 0.592216, loss_pred: 0.708346
iteration 360: loss: 64.440422, loss_kl: 2.395632, loss_recon: 0.615765, loss_pred: 0.658041
 18%|█████▍                        | 36/200 [29:42<2:15:49, 49.69s/it]iteration 361: loss: 63.385925, loss_kl: 1.508875, loss_recon: 0.609490, loss_pred: 0.987768
iteration 362: loss: 63.919365, loss_kl: 1.891305, loss_recon: 0.613968, loss_pred: 0.706158
iteration 363: loss: 63.800289, loss_kl: 1.712737, loss_recon: 0.618328, loss_pred: 0.322586
iteration 364: loss: 62.959057, loss_kl: 1.924311, loss_recon: 0.600971, loss_pred: 1.013860
iteration 365: loss: 64.337700, loss_kl: 1.904323, loss_recon: 0.620609, loss_pred: 0.447883
iteration 366: loss: 63.506401, loss_kl: 1.650853, loss_recon: 0.611498, loss_pred: 0.771125
iteration 367: loss: 62.277843, loss_kl: 1.836441, loss_recon: 0.599474, loss_pred: 0.566707
iteration 368: loss: 63.100143, loss_kl: 1.948483, loss_recon: 0.607484, loss_pred: 0.480380
iteration 369: loss: 63.078606, loss_kl: 1.656045, loss_recon: 0.609858, loss_pred: 0.502302
iteration 370: loss: 63.733459, loss_kl: 1.138139, loss_recon: 0.622146, loss_pred: 0.425752
 18%|█████▌                        | 37/200 [30:32<2:14:54, 49.66s/it]iteration 371: loss: 61.943462, loss_kl: 1.097370, loss_recon: 0.603608, loss_pred: 0.485259
iteration 372: loss: 62.537552, loss_kl: 0.827600, loss_recon: 0.614030, loss_pred: 0.306987
iteration 373: loss: 62.451321, loss_kl: 1.409182, loss_recon: 0.606868, loss_pred: 0.355349
iteration 374: loss: 63.762238, loss_kl: 1.356269, loss_recon: 0.620314, loss_pred: 0.374615
iteration 375: loss: 61.582165, loss_kl: 1.102169, loss_recon: 0.599831, loss_pred: 0.496911
iteration 376: loss: 62.395756, loss_kl: 1.828815, loss_recon: 0.600685, loss_pred: 0.498470
iteration 377: loss: 64.006798, loss_kl: 2.083591, loss_recon: 0.614534, loss_pred: 0.469813
iteration 378: loss: 64.930664, loss_kl: 1.418697, loss_recon: 0.631038, loss_pred: 0.408135
iteration 379: loss: 61.370102, loss_kl: 1.136163, loss_recon: 0.594985, loss_pred: 0.735481
iteration 380: loss: 65.258011, loss_kl: 1.301830, loss_recon: 0.635805, loss_pred: 0.375683
 19%|█████▋                        | 38/200 [31:21<2:13:55, 49.60s/it]iteration 381: loss: 62.485157, loss_kl: 1.294726, loss_recon: 0.608473, loss_pred: 0.343149
iteration 382: loss: 62.866730, loss_kl: 1.469346, loss_recon: 0.610589, loss_pred: 0.338448
iteration 383: loss: 63.794250, loss_kl: 1.421682, loss_recon: 0.620319, loss_pred: 0.340651
iteration 384: loss: 62.335690, loss_kl: 1.602288, loss_recon: 0.602786, loss_pred: 0.454764
iteration 385: loss: 62.417900, loss_kl: 1.364704, loss_recon: 0.605216, loss_pred: 0.531550
iteration 386: loss: 63.087410, loss_kl: 0.975948, loss_recon: 0.616795, loss_pred: 0.431972
iteration 387: loss: 62.567276, loss_kl: 0.989788, loss_recon: 0.610812, loss_pred: 0.496248
iteration 388: loss: 62.059147, loss_kl: 1.532742, loss_recon: 0.597973, loss_pred: 0.729086
iteration 389: loss: 63.361126, loss_kl: 1.389908, loss_recon: 0.616136, loss_pred: 0.357610
iteration 390: loss: 63.650745, loss_kl: 1.232300, loss_recon: 0.619169, loss_pred: 0.501568
 20%|█████▊                        | 39/200 [32:11<2:12:53, 49.52s/it]iteration 391: loss: 61.825939, loss_kl: 1.031286, loss_recon: 0.603327, loss_pred: 0.461954
iteration 392: loss: 62.472984, loss_kl: 1.296019, loss_recon: 0.605117, loss_pred: 0.665228
iteration 393: loss: 62.297112, loss_kl: 1.293655, loss_recon: 0.606824, loss_pred: 0.321040
iteration 394: loss: 64.318069, loss_kl: 1.245964, loss_recon: 0.624324, loss_pred: 0.639687
iteration 395: loss: 62.843578, loss_kl: 1.201445, loss_recon: 0.612337, loss_pred: 0.408483
iteration 396: loss: 63.077953, loss_kl: 1.145156, loss_recon: 0.612739, loss_pred: 0.658873
iteration 397: loss: 61.511009, loss_kl: 1.058039, loss_recon: 0.598663, loss_pred: 0.586700
iteration 398: loss: 62.740974, loss_kl: 1.070543, loss_recon: 0.612737, loss_pred: 0.396692
iteration 399: loss: 63.395348, loss_kl: 1.227148, loss_recon: 0.616281, loss_pred: 0.540117
iteration 400: loss: 65.149956, loss_kl: 1.771208, loss_recon: 0.626886, loss_pred: 0.690153
 20%|██████                        | 40/200 [33:00<2:12:11, 49.57s/it]iteration 401: loss: 62.247898, loss_kl: 1.028654, loss_recon: 0.607429, loss_pred: 0.476351
iteration 402: loss: 62.663582, loss_kl: 1.086926, loss_recon: 0.612480, loss_pred: 0.328639
iteration 403: loss: 62.344269, loss_kl: 1.539449, loss_recon: 0.603844, loss_pred: 0.420465
iteration 404: loss: 61.415276, loss_kl: 0.993461, loss_recon: 0.599727, loss_pred: 0.449114
iteration 405: loss: 63.085518, loss_kl: 0.998320, loss_recon: 0.617306, loss_pred: 0.356559
iteration 406: loss: 62.040478, loss_kl: 1.135480, loss_recon: 0.605169, loss_pred: 0.388060
iteration 407: loss: 62.952824, loss_kl: 1.054863, loss_recon: 0.615996, loss_pred: 0.298409
iteration 408: loss: 64.861435, loss_kl: 0.898524, loss_recon: 0.637602, loss_pred: 0.202703
iteration 409: loss: 61.165157, loss_kl: 1.317584, loss_recon: 0.592967, loss_pred: 0.550880
iteration 410: loss: 63.089397, loss_kl: 0.483118, loss_recon: 0.623420, loss_pred: 0.264270
 20%|██████▏                       | 41/200 [33:50<2:11:17, 49.54s/it]iteration 411: loss: 62.672443, loss_kl: 1.484653, loss_recon: 0.608869, loss_pred: 0.300938
iteration 412: loss: 62.888031, loss_kl: 0.867855, loss_recon: 0.617152, loss_pred: 0.304978
iteration 413: loss: 62.343044, loss_kl: 0.921484, loss_recon: 0.611531, loss_pred: 0.268430
iteration 414: loss: 61.946358, loss_kl: 0.507548, loss_recon: 0.610205, loss_pred: 0.418295
iteration 415: loss: 62.285988, loss_kl: 0.877679, loss_recon: 0.609103, loss_pred: 0.497968
iteration 416: loss: 62.482738, loss_kl: 0.969190, loss_recon: 0.610266, loss_pred: 0.486941
iteration 417: loss: 62.696289, loss_kl: 1.033857, loss_recon: 0.612862, loss_pred: 0.376211
iteration 418: loss: 62.264484, loss_kl: 1.117799, loss_recon: 0.607406, loss_pred: 0.406097
iteration 419: loss: 62.202087, loss_kl: 1.056626, loss_recon: 0.607476, loss_pred: 0.397909
iteration 420: loss: 61.755562, loss_kl: 1.059488, loss_recon: 0.602556, loss_pred: 0.440473
 21%|██████▎                       | 42/200 [34:39<2:10:03, 49.39s/it]iteration 421: loss: 61.156155, loss_kl: 1.026991, loss_recon: 0.596838, loss_pred: 0.445366
iteration 422: loss: 62.701988, loss_kl: 0.930923, loss_recon: 0.614226, loss_pred: 0.348484
iteration 423: loss: 62.678787, loss_kl: 1.150540, loss_recon: 0.611234, loss_pred: 0.404806
iteration 424: loss: 62.451378, loss_kl: 1.199366, loss_recon: 0.608868, loss_pred: 0.365248
iteration 425: loss: 62.961723, loss_kl: 1.076203, loss_recon: 0.615300, loss_pred: 0.355564
iteration 426: loss: 61.900520, loss_kl: 0.916705, loss_recon: 0.604819, loss_pred: 0.501916
iteration 427: loss: 62.676174, loss_kl: 1.087794, loss_recon: 0.612682, loss_pred: 0.320177
iteration 428: loss: 63.050385, loss_kl: 1.330120, loss_recon: 0.614012, loss_pred: 0.319064
iteration 429: loss: 62.340141, loss_kl: 1.220772, loss_recon: 0.607543, loss_pred: 0.365049
iteration 430: loss: 63.313881, loss_kl: 0.524046, loss_recon: 0.623681, loss_pred: 0.421779
 22%|██████▍                       | 43/200 [35:28<2:08:55, 49.27s/it]iteration 431: loss: 63.041744, loss_kl: 0.587371, loss_recon: 0.621837, loss_pred: 0.270696
iteration 432: loss: 61.688519, loss_kl: 0.645851, loss_recon: 0.604848, loss_pred: 0.557900
iteration 433: loss: 62.317707, loss_kl: 0.479104, loss_recon: 0.614374, loss_pred: 0.401196
iteration 434: loss: 62.239132, loss_kl: 0.765608, loss_recon: 0.610189, loss_pred: 0.454612
iteration 435: loss: 61.967964, loss_kl: 0.892857, loss_recon: 0.607694, loss_pred: 0.305749
iteration 436: loss: 61.295872, loss_kl: 0.622456, loss_recon: 0.602677, loss_pred: 0.405713
iteration 437: loss: 63.211086, loss_kl: 1.449273, loss_recon: 0.613519, loss_pred: 0.409913
iteration 438: loss: 63.365601, loss_kl: 1.374582, loss_recon: 0.615149, loss_pred: 0.476100
iteration 439: loss: 62.005653, loss_kl: 1.013067, loss_recon: 0.607086, loss_pred: 0.283962
iteration 440: loss: 61.364708, loss_kl: 0.906127, loss_recon: 0.597349, loss_pred: 0.723645
 22%|██████▌                       | 44/200 [36:18<2:08:41, 49.50s/it]iteration 441: loss: 62.246124, loss_kl: 1.090455, loss_recon: 0.607944, loss_pred: 0.361234
iteration 442: loss: 61.424145, loss_kl: 0.797981, loss_recon: 0.602704, loss_pred: 0.355735
iteration 443: loss: 62.936687, loss_kl: 0.782057, loss_recon: 0.618627, loss_pred: 0.291929
iteration 444: loss: 61.764717, loss_kl: 0.721536, loss_recon: 0.605634, loss_pred: 0.479746
iteration 445: loss: 62.248497, loss_kl: 0.962226, loss_recon: 0.609865, loss_pred: 0.299731
iteration 446: loss: 61.932556, loss_kl: 0.906408, loss_recon: 0.606527, loss_pred: 0.373472
iteration 447: loss: 62.506241, loss_kl: 0.925057, loss_recon: 0.612471, loss_pred: 0.334103
iteration 448: loss: 62.507301, loss_kl: 0.846810, loss_recon: 0.613131, loss_pred: 0.347350
iteration 449: loss: 62.070641, loss_kl: 0.721254, loss_recon: 0.610565, loss_pred: 0.292839
iteration 450: loss: 63.319157, loss_kl: 0.763042, loss_recon: 0.622099, loss_pred: 0.346266
 22%|██████▊                       | 45/200 [37:07<2:07:25, 49.33s/it]iteration 451: loss: 63.112507, loss_kl: 1.074255, loss_recon: 0.616605, loss_pred: 0.377737
iteration 452: loss: 62.390198, loss_kl: 0.939818, loss_recon: 0.611144, loss_pred: 0.336008
iteration 453: loss: 62.818470, loss_kl: 0.738814, loss_recon: 0.617911, loss_pred: 0.288521
iteration 454: loss: 62.494789, loss_kl: 0.837829, loss_recon: 0.612886, loss_pred: 0.368348
iteration 455: loss: 62.202778, loss_kl: 0.658067, loss_recon: 0.612745, loss_pred: 0.270216
iteration 456: loss: 61.968296, loss_kl: 0.853680, loss_recon: 0.607786, loss_pred: 0.335979
iteration 457: loss: 61.211262, loss_kl: 0.872291, loss_recon: 0.598799, loss_pred: 0.459059
iteration 458: loss: 62.330112, loss_kl: 0.842701, loss_recon: 0.611509, loss_pred: 0.336551
iteration 459: loss: 60.491062, loss_kl: 0.847163, loss_recon: 0.591451, loss_pred: 0.498848
iteration 460: loss: 61.903912, loss_kl: 0.628343, loss_recon: 0.607268, loss_pred: 0.548799
 23%|██████▉                       | 46/200 [37:56<2:06:50, 49.42s/it]iteration 461: loss: 60.820560, loss_kl: 0.749420, loss_recon: 0.596025, loss_pred: 0.468603
iteration 462: loss: 62.730980, loss_kl: 0.673306, loss_recon: 0.618056, loss_pred: 0.252078
iteration 463: loss: 61.795506, loss_kl: 0.625950, loss_recon: 0.606958, loss_pred: 0.473718
iteration 464: loss: 62.493233, loss_kl: 0.803290, loss_recon: 0.612676, loss_pred: 0.422375
iteration 465: loss: 62.167236, loss_kl: 0.715940, loss_recon: 0.608980, loss_pred: 0.553303
iteration 466: loss: 61.436974, loss_kl: 0.927709, loss_recon: 0.601588, loss_pred: 0.350476
iteration 467: loss: 62.380688, loss_kl: 0.783916, loss_recon: 0.613372, loss_pred: 0.259573
iteration 468: loss: 62.891609, loss_kl: 0.801913, loss_recon: 0.617259, loss_pred: 0.363782
iteration 469: loss: 61.614166, loss_kl: 0.768318, loss_recon: 0.605925, loss_pred: 0.253378
iteration 470: loss: 63.126251, loss_kl: 0.615474, loss_recon: 0.621713, loss_pred: 0.339432
 24%|███████                       | 47/200 [38:45<2:05:45, 49.31s/it]iteration 471: loss: 63.292946, loss_kl: 0.608503, loss_recon: 0.624353, loss_pred: 0.249133
iteration 472: loss: 61.075352, loss_kl: 0.424417, loss_recon: 0.603640, loss_pred: 0.286931
iteration 473: loss: 63.016800, loss_kl: 0.928293, loss_recon: 0.617843, loss_pred: 0.304178
iteration 474: loss: 61.693436, loss_kl: 0.640157, loss_recon: 0.606129, loss_pred: 0.440367
iteration 475: loss: 61.385063, loss_kl: 0.608490, loss_recon: 0.602753, loss_pred: 0.501269
iteration 476: loss: 61.973907, loss_kl: 0.729406, loss_recon: 0.609897, loss_pred: 0.254789
iteration 477: loss: 61.458317, loss_kl: 0.340206, loss_recon: 0.607590, loss_pred: 0.359062
iteration 478: loss: 62.024364, loss_kl: 0.667489, loss_recon: 0.609678, loss_pred: 0.389040
iteration 479: loss: 60.812981, loss_kl: 0.758204, loss_recon: 0.596087, loss_pred: 0.446126
iteration 480: loss: 64.094704, loss_kl: 1.077932, loss_recon: 0.627229, loss_pred: 0.293870
 24%|███████▏                      | 48/200 [39:34<2:04:37, 49.19s/it]iteration 481: loss: 60.784542, loss_kl: 0.433161, loss_recon: 0.599413, loss_pred: 0.410046
iteration 482: loss: 61.731201, loss_kl: 0.615330, loss_recon: 0.607491, loss_pred: 0.366745
iteration 483: loss: 61.283390, loss_kl: 0.517030, loss_recon: 0.604481, loss_pred: 0.318265
iteration 484: loss: 61.889297, loss_kl: 0.605456, loss_recon: 0.610124, loss_pred: 0.271405
iteration 485: loss: 61.333935, loss_kl: 0.480831, loss_recon: 0.605357, loss_pred: 0.317393
iteration 486: loss: 62.236004, loss_kl: 0.572701, loss_recon: 0.613442, loss_pred: 0.319111
iteration 487: loss: 62.271606, loss_kl: 0.417426, loss_recon: 0.615894, loss_pred: 0.264819
iteration 488: loss: 62.081329, loss_kl: 0.357802, loss_recon: 0.612242, loss_pred: 0.499299
iteration 489: loss: 62.658226, loss_kl: 0.625905, loss_recon: 0.616673, loss_pred: 0.365046
iteration 490: loss: 63.629730, loss_kl: 0.714157, loss_recon: 0.625199, loss_pred: 0.395717
 24%|███████▎                      | 49/200 [40:23<2:03:37, 49.12s/it]iteration 491: loss: 61.128521, loss_kl: 0.613509, loss_recon: 0.601497, loss_pred: 0.365342
iteration 492: loss: 61.827671, loss_kl: 0.413151, loss_recon: 0.610737, loss_pred: 0.340793
iteration 493: loss: 62.045006, loss_kl: 0.451800, loss_recon: 0.611154, loss_pred: 0.477846
iteration 494: loss: 61.902500, loss_kl: 0.717366, loss_recon: 0.608803, loss_pred: 0.304803
iteration 495: loss: 60.953373, loss_kl: 0.689301, loss_recon: 0.597467, loss_pred: 0.517377
iteration 496: loss: 63.267242, loss_kl: 0.774674, loss_recon: 0.622387, loss_pred: 0.253846
iteration 497: loss: 61.935398, loss_kl: 0.659436, loss_recon: 0.606492, loss_pred: 0.626759
iteration 498: loss: 62.244125, loss_kl: 0.417881, loss_recon: 0.615737, loss_pred: 0.252518
iteration 499: loss: 62.279190, loss_kl: 0.549231, loss_recon: 0.613317, loss_pred: 0.398215
iteration 500: loss: 64.759209, loss_kl: 1.147254, loss_recon: 0.625934, loss_pred: 1.018603
 25%|███████▌                      | 50/200 [41:13<2:03:30, 49.40s/it]iteration 501: loss: 62.147617, loss_kl: 0.744144, loss_recon: 0.616003, loss_pred: 0.539862
iteration 502: loss: 61.067112, loss_kl: 0.466387, loss_recon: 0.607467, loss_pred: 0.315777
iteration 503: loss: 62.069683, loss_kl: 0.425790, loss_recon: 0.617145, loss_pred: 0.350884
iteration 504: loss: 61.027435, loss_kl: 0.510738, loss_recon: 0.606219, loss_pred: 0.400411
iteration 505: loss: 60.221981, loss_kl: 0.694819, loss_recon: 0.599312, loss_pred: 0.283828
iteration 506: loss: 61.159355, loss_kl: 0.581646, loss_recon: 0.605414, loss_pred: 0.612112
iteration 507: loss: 60.678757, loss_kl: 0.763362, loss_recon: 0.603508, loss_pred: 0.320281
iteration 508: loss: 61.882175, loss_kl: 0.668358, loss_recon: 0.614634, loss_pred: 0.412072
iteration 509: loss: 61.421017, loss_kl: 0.629555, loss_recon: 0.611468, loss_pred: 0.267915
iteration 510: loss: 65.113487, loss_kl: 0.665103, loss_recon: 0.644992, loss_pred: 0.607594
 26%|███████▋                      | 51/200 [42:03<2:02:36, 49.37s/it]iteration 511: loss: 61.984825, loss_kl: 0.424388, loss_recon: 0.616907, loss_pred: 0.289858
iteration 512: loss: 62.404816, loss_kl: 0.577641, loss_recon: 0.618839, loss_pred: 0.515187
iteration 513: loss: 61.379253, loss_kl: 0.462499, loss_recon: 0.609691, loss_pred: 0.405532
iteration 514: loss: 60.780899, loss_kl: 0.514885, loss_recon: 0.601882, loss_pred: 0.587561
iteration 515: loss: 61.540447, loss_kl: 0.302500, loss_recon: 0.612712, loss_pred: 0.266209
iteration 516: loss: 61.382061, loss_kl: 0.410649, loss_recon: 0.606018, loss_pred: 0.776193
iteration 517: loss: 61.279259, loss_kl: 0.538571, loss_recon: 0.608085, loss_pred: 0.465384
iteration 518: loss: 60.955139, loss_kl: 0.317910, loss_recon: 0.605401, loss_pred: 0.411839
iteration 519: loss: 60.464176, loss_kl: 1.152272, loss_recon: 0.600726, loss_pred: 0.380018
iteration 520: loss: 62.783752, loss_kl: 0.617430, loss_recon: 0.621085, loss_pred: 0.669108
 26%|███████▊                      | 52/200 [42:53<2:02:21, 49.60s/it]iteration 521: loss: 62.384453, loss_kl: 0.763911, loss_recon: 0.619796, loss_pred: 0.397242
iteration 522: loss: 61.118690, loss_kl: 0.714643, loss_recon: 0.608809, loss_pred: 0.230631
iteration 523: loss: 62.146595, loss_kl: 0.398669, loss_recon: 0.618150, loss_pred: 0.327560
iteration 524: loss: 60.653023, loss_kl: 0.694094, loss_recon: 0.601292, loss_pred: 0.516913
iteration 525: loss: 61.961975, loss_kl: 0.520502, loss_recon: 0.615930, loss_pred: 0.363752
iteration 526: loss: 61.554714, loss_kl: 0.419278, loss_recon: 0.612450, loss_pred: 0.305528
iteration 527: loss: 61.043755, loss_kl: 0.935541, loss_recon: 0.605308, loss_pred: 0.503565
iteration 528: loss: 60.828232, loss_kl: 0.619625, loss_recon: 0.604114, loss_pred: 0.410595
iteration 529: loss: 60.149082, loss_kl: 0.563538, loss_recon: 0.598120, loss_pred: 0.331483
iteration 530: loss: 61.971478, loss_kl: 0.777814, loss_recon: 0.615110, loss_pred: 0.452705
 26%|███████▉                      | 53/200 [43:43<2:01:55, 49.77s/it]iteration 531: loss: 62.140457, loss_kl: 0.422667, loss_recon: 0.618789, loss_pred: 0.257364
iteration 532: loss: 61.302807, loss_kl: 0.479707, loss_recon: 0.609329, loss_pred: 0.365146
iteration 533: loss: 61.997696, loss_kl: 0.513693, loss_recon: 0.617878, loss_pred: 0.204752
iteration 534: loss: 60.643456, loss_kl: 1.032838, loss_recon: 0.603016, loss_pred: 0.331511
iteration 535: loss: 61.454456, loss_kl: 0.926918, loss_recon: 0.611457, loss_pred: 0.299490
iteration 536: loss: 60.940880, loss_kl: 0.442667, loss_recon: 0.606975, loss_pred: 0.238910
iteration 537: loss: 60.906654, loss_kl: 0.533535, loss_recon: 0.604829, loss_pred: 0.418413
iteration 538: loss: 59.917519, loss_kl: 0.539908, loss_recon: 0.596720, loss_pred: 0.240139
iteration 539: loss: 61.356861, loss_kl: 0.728568, loss_recon: 0.609846, loss_pred: 0.364957
iteration 540: loss: 61.081120, loss_kl: 0.745660, loss_recon: 0.607548, loss_pred: 0.318843
 27%|████████                      | 54/200 [44:33<2:01:17, 49.85s/it]iteration 541: loss: 61.542019, loss_kl: 0.601419, loss_recon: 0.611107, loss_pred: 0.425270
iteration 542: loss: 59.647625, loss_kl: 0.876878, loss_recon: 0.593989, loss_pred: 0.239984
iteration 543: loss: 61.685692, loss_kl: 0.808606, loss_recon: 0.613405, loss_pred: 0.337136
iteration 544: loss: 61.469505, loss_kl: 0.502681, loss_recon: 0.612436, loss_pred: 0.220840
iteration 545: loss: 61.757599, loss_kl: 0.800369, loss_recon: 0.615669, loss_pred: 0.182714
iteration 546: loss: 60.085720, loss_kl: 0.547085, loss_recon: 0.596214, loss_pred: 0.458850
iteration 547: loss: 60.610275, loss_kl: 0.674237, loss_recon: 0.603764, loss_pred: 0.227115
iteration 548: loss: 61.617443, loss_kl: 0.857661, loss_recon: 0.612850, loss_pred: 0.323891
iteration 549: loss: 62.567116, loss_kl: 0.452880, loss_recon: 0.623356, loss_pred: 0.227001
iteration 550: loss: 62.622208, loss_kl: -0.040268, loss_recon: 0.615770, loss_pred: 1.045649
 28%|████████▎                     | 55/200 [45:22<2:00:17, 49.78s/it]iteration 551: loss: 62.078732, loss_kl: 0.542049, loss_recon: 0.618689, loss_pred: 0.204390
iteration 552: loss: 61.571392, loss_kl: 0.609855, loss_recon: 0.610814, loss_pred: 0.483853
iteration 553: loss: 60.629913, loss_kl: 0.492308, loss_recon: 0.603867, loss_pred: 0.238284
iteration 554: loss: 61.257469, loss_kl: 0.622070, loss_recon: 0.607004, loss_pred: 0.550809
iteration 555: loss: 59.943943, loss_kl: 0.616063, loss_recon: 0.596234, loss_pred: 0.314426
iteration 556: loss: 62.772972, loss_kl: 0.872591, loss_recon: 0.622597, loss_pred: 0.504568
iteration 557: loss: 60.842159, loss_kl: 0.861781, loss_recon: 0.604011, loss_pred: 0.432401
iteration 558: loss: 60.567875, loss_kl: 0.862337, loss_recon: 0.601642, loss_pred: 0.395031
iteration 559: loss: 62.264061, loss_kl: 0.478455, loss_recon: 0.616695, loss_pred: 0.589813
iteration 560: loss: 59.665558, loss_kl: 0.394545, loss_recon: 0.591190, loss_pred: 0.542599
 28%|████████▍                     | 56/200 [46:11<1:58:53, 49.54s/it]iteration 561: loss: 61.427650, loss_kl: 0.793808, loss_recon: 0.608217, loss_pred: 0.597968
iteration 562: loss: 62.269596, loss_kl: 0.576959, loss_recon: 0.618420, loss_pred: 0.421856
iteration 563: loss: 60.940884, loss_kl: 0.494793, loss_recon: 0.605645, loss_pred: 0.371388
iteration 564: loss: 61.429504, loss_kl: 0.560340, loss_recon: 0.608069, loss_pred: 0.616982
iteration 565: loss: 60.622005, loss_kl: 0.818659, loss_recon: 0.604250, loss_pred: 0.188848
iteration 566: loss: 62.953487, loss_kl: 0.523396, loss_recon: 0.622828, loss_pred: 0.665471
iteration 567: loss: 59.758507, loss_kl: 0.570190, loss_recon: 0.594924, loss_pred: 0.260371
iteration 568: loss: 61.485741, loss_kl: 0.768268, loss_recon: 0.611934, loss_pred: 0.284677
iteration 569: loss: 62.115349, loss_kl: 0.590674, loss_recon: 0.616395, loss_pred: 0.469913
iteration 570: loss: 62.150097, loss_kl: 1.159198, loss_recon: 0.614071, loss_pred: 0.731428
 28%|████████▌                     | 57/200 [47:01<1:57:52, 49.46s/it]iteration 571: loss: 62.214638, loss_kl: 1.191444, loss_recon: 0.613934, loss_pred: 0.809369
iteration 572: loss: 62.113461, loss_kl: 0.450170, loss_recon: 0.618293, loss_pred: 0.279700
iteration 573: loss: 60.943909, loss_kl: 0.861797, loss_recon: 0.605144, loss_pred: 0.420936
iteration 574: loss: 61.753693, loss_kl: 0.582819, loss_recon: 0.614141, loss_pred: 0.333737
iteration 575: loss: 60.918381, loss_kl: 0.929217, loss_recon: 0.606842, loss_pred: 0.224905
iteration 576: loss: 61.353092, loss_kl: 0.571792, loss_recon: 0.609384, loss_pred: 0.408992
iteration 577: loss: 61.456692, loss_kl: 0.788704, loss_recon: 0.611293, loss_pred: 0.319532
iteration 578: loss: 61.400814, loss_kl: 0.664452, loss_recon: 0.611398, loss_pred: 0.254385
iteration 579: loss: 60.302673, loss_kl: 0.781595, loss_recon: 0.599168, loss_pred: 0.378064
iteration 580: loss: 60.982212, loss_kl: 0.972409, loss_recon: 0.606383, loss_pred: 0.334157
 29%|████████▋                     | 58/200 [47:50<1:57:11, 49.52s/it]iteration 581: loss: 60.237602, loss_kl: 0.889927, loss_recon: 0.595255, loss_pred: 0.703200
iteration 582: loss: 62.156158, loss_kl: 0.601424, loss_recon: 0.619488, loss_pred: 0.201377
iteration 583: loss: 61.132252, loss_kl: 0.691738, loss_recon: 0.608024, loss_pred: 0.322898
iteration 584: loss: 62.483524, loss_kl: 0.792601, loss_recon: 0.621621, loss_pred: 0.313504
iteration 585: loss: 60.684708, loss_kl: 0.483216, loss_recon: 0.603772, loss_pred: 0.302629
iteration 586: loss: 61.501324, loss_kl: 1.257341, loss_recon: 0.610336, loss_pred: 0.455156
iteration 587: loss: 61.249413, loss_kl: 0.873711, loss_recon: 0.609564, loss_pred: 0.284235
iteration 588: loss: 60.770332, loss_kl: 0.786778, loss_recon: 0.605261, loss_pred: 0.236381
iteration 589: loss: 60.041264, loss_kl: 0.827691, loss_recon: 0.596645, loss_pred: 0.368526
iteration 590: loss: 63.972061, loss_kl: 0.903303, loss_recon: 0.636963, loss_pred: 0.266757
 30%|████████▊                     | 59/200 [48:40<1:56:43, 49.67s/it]iteration 591: loss: 60.697880, loss_kl: 0.748956, loss_recon: 0.603944, loss_pred: 0.296017
iteration 592: loss: 60.843102, loss_kl: 0.553539, loss_recon: 0.605899, loss_pred: 0.247622
iteration 593: loss: 61.811600, loss_kl: 0.794412, loss_recon: 0.615719, loss_pred: 0.231705
iteration 594: loss: 61.876842, loss_kl: 0.766830, loss_recon: 0.616538, loss_pred: 0.215352
iteration 595: loss: 61.344662, loss_kl: 0.796379, loss_recon: 0.610071, loss_pred: 0.329570
iteration 596: loss: 60.329712, loss_kl: 0.877810, loss_recon: 0.601131, loss_pred: 0.207878
iteration 597: loss: 61.753498, loss_kl: 1.072850, loss_recon: 0.613039, loss_pred: 0.438849
iteration 598: loss: 61.452744, loss_kl: 0.748163, loss_recon: 0.612189, loss_pred: 0.226394
iteration 599: loss: 61.662849, loss_kl: 0.673087, loss_recon: 0.613501, loss_pred: 0.306061
iteration 600: loss: 60.687103, loss_kl: 0.937623, loss_recon: 0.601039, loss_pred: 0.573852
 30%|█████████                     | 60/200 [49:30<1:55:32, 49.51s/it]iteration 601: loss: 60.551704, loss_kl: 1.081797, loss_recon: 0.602958, loss_pred: 0.245082
iteration 602: loss: 60.638176, loss_kl: 0.859841, loss_recon: 0.601213, loss_pred: 0.508320
iteration 603: loss: 62.015774, loss_kl: 0.801766, loss_recon: 0.616862, loss_pred: 0.321577
iteration 604: loss: 61.923523, loss_kl: 0.559790, loss_recon: 0.616149, loss_pred: 0.302988
iteration 605: loss: 61.902428, loss_kl: 0.757285, loss_recon: 0.616556, loss_pred: 0.239231
iteration 606: loss: 61.648762, loss_kl: 0.612535, loss_recon: 0.614166, loss_pred: 0.226003
iteration 607: loss: 60.215626, loss_kl: 0.569771, loss_recon: 0.596933, loss_pred: 0.516623
iteration 608: loss: 61.803844, loss_kl: 0.898949, loss_recon: 0.611998, loss_pred: 0.595017
iteration 609: loss: 61.418716, loss_kl: 0.777889, loss_recon: 0.610233, loss_pred: 0.387627
iteration 610: loss: 63.203522, loss_kl: 0.786600, loss_recon: 0.625858, loss_pred: 0.609824
 30%|█████████▏                    | 61/200 [50:19<1:54:21, 49.36s/it]iteration 611: loss: 59.627625, loss_kl: 0.888155, loss_recon: 0.590738, loss_pred: 0.544933
iteration 612: loss: 61.727108, loss_kl: 0.499289, loss_recon: 0.613975, loss_pred: 0.324622
iteration 613: loss: 60.824238, loss_kl: 0.832063, loss_recon: 0.604639, loss_pred: 0.351994
iteration 614: loss: 60.954601, loss_kl: 0.628964, loss_recon: 0.606891, loss_pred: 0.259209
iteration 615: loss: 60.349369, loss_kl: 1.152502, loss_recon: 0.599917, loss_pred: 0.346193
iteration 616: loss: 61.685909, loss_kl: 0.848283, loss_recon: 0.614771, loss_pred: 0.200345
iteration 617: loss: 62.418850, loss_kl: 0.773230, loss_recon: 0.622218, loss_pred: 0.189346
iteration 618: loss: 61.951336, loss_kl: 0.641814, loss_recon: 0.617527, loss_pred: 0.192239
iteration 619: loss: 61.429859, loss_kl: 0.806679, loss_recon: 0.610916, loss_pred: 0.330196
iteration 620: loss: 61.536777, loss_kl: 0.933245, loss_recon: 0.610644, loss_pred: 0.462999
 31%|█████████▎                    | 62/200 [51:08<1:53:37, 49.40s/it]iteration 621: loss: 60.875961, loss_kl: 0.824488, loss_recon: 0.606264, loss_pred: 0.241300
iteration 622: loss: 61.735424, loss_kl: 0.949479, loss_recon: 0.614735, loss_pred: 0.252399
iteration 623: loss: 61.904480, loss_kl: 0.635602, loss_recon: 0.616101, loss_pred: 0.288047
iteration 624: loss: 59.713566, loss_kl: 1.103429, loss_recon: 0.594630, loss_pred: 0.239495
iteration 625: loss: 60.020054, loss_kl: 1.283628, loss_recon: 0.597510, loss_pred: 0.256184
iteration 626: loss: 61.380352, loss_kl: 0.917519, loss_recon: 0.611749, loss_pred: 0.196273
iteration 627: loss: 62.348125, loss_kl: 0.775844, loss_recon: 0.621177, loss_pred: 0.222703
iteration 628: loss: 61.829811, loss_kl: 0.590548, loss_recon: 0.616326, loss_pred: 0.191263
iteration 629: loss: 60.773727, loss_kl: 0.476717, loss_recon: 0.604797, loss_pred: 0.289243
iteration 630: loss: 60.350891, loss_kl: 0.729381, loss_recon: 0.600400, loss_pred: 0.303580
 32%|█████████▍                    | 63/200 [51:57<1:52:25, 49.24s/it]iteration 631: loss: 62.136776, loss_kl: 0.705415, loss_recon: 0.616339, loss_pred: 0.467881
iteration 632: loss: 61.263233, loss_kl: 0.552478, loss_recon: 0.610180, loss_pred: 0.217840
iteration 633: loss: 61.900517, loss_kl: 0.628599, loss_recon: 0.615615, loss_pred: 0.307841
iteration 634: loss: 61.609138, loss_kl: 0.831371, loss_recon: 0.612073, loss_pred: 0.360637
iteration 635: loss: 60.518566, loss_kl: 1.232656, loss_recon: 0.601438, loss_pred: 0.313621
iteration 636: loss: 60.886288, loss_kl: 0.798506, loss_recon: 0.602970, loss_pred: 0.549678
iteration 637: loss: 61.916046, loss_kl: 0.634285, loss_recon: 0.617374, loss_pred: 0.147167
iteration 638: loss: 61.353535, loss_kl: 0.718442, loss_recon: 0.608690, loss_pred: 0.448942
iteration 639: loss: 61.008396, loss_kl: 0.946855, loss_recon: 0.605000, loss_pred: 0.461478
iteration 640: loss: 62.425983, loss_kl: 1.186650, loss_recon: 0.619808, loss_pred: 0.386325
 32%|█████████▌                    | 64/200 [52:46<1:51:28, 49.18s/it]iteration 641: loss: 61.283646, loss_kl: 1.015637, loss_recon: 0.605280, loss_pred: 0.665092
iteration 642: loss: 62.029819, loss_kl: 1.000641, loss_recon: 0.615144, loss_pred: 0.426209
iteration 643: loss: 61.178047, loss_kl: 0.680214, loss_recon: 0.608201, loss_pred: 0.297274
iteration 644: loss: 62.635010, loss_kl: 0.983885, loss_recon: 0.617865, loss_pred: 0.760776
iteration 645: loss: 60.911293, loss_kl: 0.636202, loss_recon: 0.606065, loss_pred: 0.248015
iteration 646: loss: 61.172455, loss_kl: 0.962341, loss_recon: 0.607008, loss_pred: 0.385847
iteration 647: loss: 60.455963, loss_kl: 0.728296, loss_recon: 0.597602, loss_pred: 0.630779
iteration 648: loss: 61.952599, loss_kl: 0.634206, loss_recon: 0.616984, loss_pred: 0.197597
iteration 649: loss: 61.553158, loss_kl: 0.693596, loss_recon: 0.609043, loss_pred: 0.586974
iteration 650: loss: 62.508781, loss_kl: 0.906217, loss_recon: 0.619732, loss_pred: 0.454705
 32%|█████████▊                    | 65/200 [53:35<1:50:34, 49.14s/it]iteration 651: loss: 60.719292, loss_kl: 0.855800, loss_recon: 0.603683, loss_pred: 0.240780
iteration 652: loss: 61.282116, loss_kl: 0.847910, loss_recon: 0.606626, loss_pred: 0.510264
iteration 653: loss: 61.836220, loss_kl: 0.725367, loss_recon: 0.614463, loss_pred: 0.296469
iteration 654: loss: 59.495735, loss_kl: 0.700498, loss_recon: 0.590816, loss_pred: 0.323872
iteration 655: loss: 60.700531, loss_kl: 0.719373, loss_recon: 0.601412, loss_pred: 0.466631
iteration 656: loss: 61.181648, loss_kl: 0.600431, loss_recon: 0.609209, loss_pred: 0.183430
iteration 657: loss: 62.882141, loss_kl: 0.789379, loss_recon: 0.624409, loss_pred: 0.339552
iteration 658: loss: 62.814625, loss_kl: 0.778872, loss_recon: 0.623755, loss_pred: 0.338792
iteration 659: loss: 60.032612, loss_kl: 0.619610, loss_recon: 0.597437, loss_pred: 0.209146
iteration 660: loss: 61.682667, loss_kl: 1.250680, loss_recon: 0.612130, loss_pred: 0.308575
 33%|█████████▉                    | 66/200 [54:24<1:49:40, 49.11s/it]iteration 661: loss: 61.771645, loss_kl: 0.634379, loss_recon: 0.613139, loss_pred: 0.350881
iteration 662: loss: 61.177666, loss_kl: 0.651694, loss_recon: 0.608188, loss_pred: 0.249087
iteration 663: loss: 60.065880, loss_kl: 0.790132, loss_recon: 0.596707, loss_pred: 0.262081
iteration 664: loss: 61.434776, loss_kl: 1.052483, loss_recon: 0.607323, loss_pred: 0.525236
iteration 665: loss: 61.218773, loss_kl: 0.607344, loss_recon: 0.609183, loss_pred: 0.198244
iteration 666: loss: 62.313114, loss_kl: 0.580949, loss_recon: 0.619430, loss_pred: 0.272251
iteration 667: loss: 61.931530, loss_kl: 0.722304, loss_recon: 0.614252, loss_pred: 0.384651
iteration 668: loss: 60.837246, loss_kl: 0.522710, loss_recon: 0.605368, loss_pred: 0.212370
iteration 669: loss: 59.901405, loss_kl: 0.565255, loss_recon: 0.595379, loss_pred: 0.268338
iteration 670: loss: 64.852402, loss_kl: 0.936363, loss_recon: 0.639505, loss_pred: 0.744201
 34%|██████████                    | 67/200 [55:13<1:48:49, 49.09s/it]iteration 671: loss: 62.309700, loss_kl: 0.638970, loss_recon: 0.619884, loss_pred: 0.188434
iteration 672: loss: 62.837570, loss_kl: 0.591025, loss_recon: 0.621320, loss_pred: 0.582648
iteration 673: loss: 62.573559, loss_kl: 0.759123, loss_recon: 0.622515, loss_pred: 0.164127
iteration 674: loss: 59.900917, loss_kl: 0.398721, loss_recon: 0.595853, loss_pred: 0.232643
iteration 675: loss: 60.894913, loss_kl: 0.953874, loss_recon: 0.603541, loss_pred: 0.342418
iteration 676: loss: 60.185570, loss_kl: 0.933942, loss_recon: 0.597406, loss_pred: 0.250733
iteration 677: loss: 61.266285, loss_kl: 1.072243, loss_recon: 0.608575, loss_pred: 0.185730
iteration 678: loss: 60.220940, loss_kl: 0.384375, loss_recon: 0.595181, loss_pred: 0.622930
iteration 679: loss: 61.833984, loss_kl: 0.505365, loss_recon: 0.614287, loss_pred: 0.300177
iteration 680: loss: 61.408165, loss_kl: 1.055919, loss_recon: 0.607947, loss_pred: 0.393881
 34%|██████████▏                   | 68/200 [56:03<1:48:24, 49.28s/it]iteration 681: loss: 61.453609, loss_kl: 0.752210, loss_recon: 0.609725, loss_pred: 0.294870
iteration 682: loss: 61.342949, loss_kl: 0.618641, loss_recon: 0.609754, loss_pred: 0.214420
iteration 683: loss: 61.765018, loss_kl: 0.526982, loss_recon: 0.614102, loss_pred: 0.224333
iteration 684: loss: 62.236183, loss_kl: 0.682058, loss_recon: 0.617400, loss_pred: 0.327297
iteration 685: loss: 60.925343, loss_kl: 0.531241, loss_recon: 0.605745, loss_pred: 0.219299
iteration 686: loss: 60.575756, loss_kl: 0.552448, loss_recon: 0.602413, loss_pred: 0.197672
iteration 687: loss: 59.936111, loss_kl: 0.761506, loss_recon: 0.594114, loss_pred: 0.336193
iteration 688: loss: 61.988815, loss_kl: 0.661177, loss_recon: 0.616069, loss_pred: 0.218162
iteration 689: loss: 60.687889, loss_kl: 0.642076, loss_recon: 0.603793, loss_pred: 0.149608
iteration 690: loss: 63.733757, loss_kl: 0.866100, loss_recon: 0.633648, loss_pred: 0.154500
 34%|██████████▎                   | 69/200 [56:52<1:47:21, 49.17s/it]iteration 691: loss: 60.097179, loss_kl: 0.866582, loss_recon: 0.596623, loss_pred: 0.186042
iteration 692: loss: 61.516235, loss_kl: 0.622932, loss_recon: 0.611845, loss_pred: 0.152806
iteration 693: loss: 60.505413, loss_kl: 0.766189, loss_recon: 0.599906, loss_pred: 0.294715
iteration 694: loss: 60.897984, loss_kl: 0.900759, loss_recon: 0.604708, loss_pred: 0.168449
iteration 695: loss: 61.915203, loss_kl: 0.780916, loss_recon: 0.614947, loss_pred: 0.196187
iteration 696: loss: 62.520557, loss_kl: 0.840560, loss_recon: 0.620469, loss_pred: 0.232233
iteration 697: loss: 61.646553, loss_kl: 0.696065, loss_recon: 0.612443, loss_pred: 0.202356
iteration 698: loss: 60.790184, loss_kl: 0.640076, loss_recon: 0.603900, loss_pred: 0.216307
iteration 699: loss: 61.585579, loss_kl: 0.416289, loss_recon: 0.611974, loss_pred: 0.268660
iteration 700: loss: 62.106499, loss_kl: 0.516522, loss_recon: 0.615823, loss_pred: 0.375840
 35%|██████████▌                   | 70/200 [57:41<1:46:17, 49.06s/it]iteration 701: loss: 61.745686, loss_kl: 0.847318, loss_recon: 0.612773, loss_pred: 0.191520
iteration 702: loss: 61.144863, loss_kl: 0.447868, loss_recon: 0.607836, loss_pred: 0.214870
iteration 703: loss: 61.062706, loss_kl: 0.325155, loss_recon: 0.607697, loss_pred: 0.186758
iteration 704: loss: 60.888554, loss_kl: 0.491102, loss_recon: 0.605274, loss_pred: 0.200623
iteration 705: loss: 61.760044, loss_kl: 0.416492, loss_recon: 0.614396, loss_pred: 0.184307
iteration 706: loss: 59.929630, loss_kl: 0.766912, loss_recon: 0.592274, loss_pred: 0.451573
iteration 707: loss: 61.972412, loss_kl: 0.517037, loss_recon: 0.616284, loss_pred: 0.175091
iteration 708: loss: 61.770279, loss_kl: 0.563625, loss_recon: 0.614123, loss_pred: 0.173775
iteration 709: loss: 60.847271, loss_kl: 0.731470, loss_recon: 0.604173, loss_pred: 0.190957
iteration 710: loss: 60.903328, loss_kl: 0.811121, loss_recon: 0.604307, loss_pred: 0.207565
 36%|██████████▋                   | 71/200 [58:30<1:45:51, 49.23s/it]iteration 711: loss: 61.461296, loss_kl: 0.549315, loss_recon: 0.610896, loss_pred: 0.170388
iteration 712: loss: 62.111607, loss_kl: 0.507774, loss_recon: 0.617139, loss_pred: 0.211696
iteration 713: loss: 61.885780, loss_kl: 0.465137, loss_recon: 0.615875, loss_pred: 0.127807
iteration 714: loss: 60.824577, loss_kl: 0.806739, loss_recon: 0.603105, loss_pred: 0.218519
iteration 715: loss: 60.324440, loss_kl: 0.555709, loss_recon: 0.598900, loss_pred: 0.230837
iteration 716: loss: 60.735916, loss_kl: 0.631241, loss_recon: 0.603066, loss_pred: 0.198018
iteration 717: loss: 60.065289, loss_kl: 0.668533, loss_recon: 0.596337, loss_pred: 0.186668
iteration 718: loss: 61.970806, loss_kl: 0.466812, loss_recon: 0.615674, loss_pred: 0.232383
iteration 719: loss: 61.695759, loss_kl: 0.461252, loss_recon: 0.613586, loss_pred: 0.168176
iteration 720: loss: 63.078644, loss_kl: 0.235446, loss_recon: 0.628425, loss_pred: 0.149835
 36%|██████████▊                   | 72/200 [59:19<1:44:55, 49.18s/it]iteration 721: loss: 61.578449, loss_kl: 0.581738, loss_recon: 0.611928, loss_pred: 0.149487
iteration 722: loss: 61.065159, loss_kl: 0.582290, loss_recon: 0.606504, loss_pred: 0.178307
iteration 723: loss: 61.824017, loss_kl: 0.586409, loss_recon: 0.614113, loss_pred: 0.174655
iteration 724: loss: 60.636459, loss_kl: 0.546526, loss_recon: 0.602200, loss_pred: 0.194553
iteration 725: loss: 62.681984, loss_kl: 0.527773, loss_recon: 0.622838, loss_pred: 0.183907
iteration 726: loss: 61.170574, loss_kl: 0.686763, loss_recon: 0.606742, loss_pred: 0.217525
iteration 727: loss: 60.793346, loss_kl: 0.751929, loss_recon: 0.602731, loss_pred: 0.214940
iteration 728: loss: 60.809044, loss_kl: 0.596632, loss_recon: 0.603793, loss_pred: 0.187546
iteration 729: loss: 60.991608, loss_kl: 0.633071, loss_recon: 0.605760, loss_pred: 0.158555
iteration 730: loss: 61.594318, loss_kl: 0.594018, loss_recon: 0.611272, loss_pred: 0.225909
 36%|██████████▏                 | 73/200 [1:00:09<1:44:22, 49.31s/it]iteration 731: loss: 62.010399, loss_kl: 0.489796, loss_recon: 0.615897, loss_pred: 0.202439
iteration 732: loss: 60.363602, loss_kl: 0.707647, loss_recon: 0.598443, loss_pred: 0.203940
iteration 733: loss: 61.214813, loss_kl: 0.454166, loss_recon: 0.608225, loss_pred: 0.189950
iteration 734: loss: 61.405247, loss_kl: 0.568225, loss_recon: 0.609675, loss_pred: 0.184506
iteration 735: loss: 61.179466, loss_kl: 0.685459, loss_recon: 0.606527, loss_pred: 0.221356
iteration 736: loss: 61.392723, loss_kl: 0.518894, loss_recon: 0.609178, loss_pred: 0.243701
iteration 737: loss: 60.560066, loss_kl: 0.641855, loss_recon: 0.601008, loss_pred: 0.173278
iteration 738: loss: 61.859512, loss_kl: 0.438715, loss_recon: 0.612572, loss_pred: 0.406858
iteration 739: loss: 61.822937, loss_kl: 0.394947, loss_recon: 0.614811, loss_pred: 0.165825
iteration 740: loss: 61.475227, loss_kl: 0.097601, loss_recon: 0.610219, loss_pred: 0.409818
 37%|██████████▎                 | 74/200 [1:00:58<1:43:30, 49.29s/it]iteration 741: loss: 61.412376, loss_kl: 0.303949, loss_recon: 0.610999, loss_pred: 0.165039
iteration 742: loss: 61.617561, loss_kl: 0.448958, loss_recon: 0.611436, loss_pred: 0.256102
iteration 743: loss: 60.940819, loss_kl: 0.562010, loss_recon: 0.604411, loss_pred: 0.226996
iteration 744: loss: 60.582016, loss_kl: 0.570398, loss_recon: 0.601030, loss_pred: 0.202215
iteration 745: loss: 62.025440, loss_kl: 0.562497, loss_recon: 0.615232, loss_pred: 0.229303
iteration 746: loss: 60.650135, loss_kl: 0.618418, loss_recon: 0.601998, loss_pred: 0.150286
iteration 747: loss: 61.821690, loss_kl: 0.510758, loss_recon: 0.613517, loss_pred: 0.222163
iteration 748: loss: 60.840164, loss_kl: 0.561281, loss_recon: 0.603948, loss_pred: 0.172980
iteration 749: loss: 61.065201, loss_kl: 0.320701, loss_recon: 0.607211, loss_pred: 0.188465
iteration 750: loss: 64.246613, loss_kl: 0.426628, loss_recon: 0.632477, loss_pred: 0.791945
 38%|██████████▌                 | 75/200 [1:01:47<1:42:25, 49.17s/it]iteration 751: loss: 61.097744, loss_kl: 0.630233, loss_recon: 0.605492, loss_pred: 0.217790
iteration 752: loss: 61.370502, loss_kl: 0.553828, loss_recon: 0.608006, loss_pred: 0.279279
iteration 753: loss: 62.631126, loss_kl: 0.443539, loss_recon: 0.621872, loss_pred: 0.211169
iteration 754: loss: 60.494915, loss_kl: 0.442367, loss_recon: 0.599271, loss_pred: 0.335697
iteration 755: loss: 60.720993, loss_kl: 0.614976, loss_recon: 0.602113, loss_pred: 0.186988
iteration 756: loss: 60.555801, loss_kl: 0.415671, loss_recon: 0.601055, loss_pred: 0.232142
iteration 757: loss: 63.058575, loss_kl: 0.442372, loss_recon: 0.626140, loss_pred: 0.212383
iteration 758: loss: 60.553417, loss_kl: 0.542226, loss_recon: 0.601065, loss_pred: 0.162325
iteration 759: loss: 60.812248, loss_kl: 0.622673, loss_recon: 0.602400, loss_pred: 0.245518
iteration 760: loss: 63.703789, loss_kl: 0.657311, loss_recon: 0.631148, loss_pred: 0.244017
 38%|██████████▋                 | 76/200 [1:02:36<1:41:38, 49.18s/it]iteration 761: loss: 61.721691, loss_kl: 0.494325, loss_recon: 0.612232, loss_pred: 0.219454
iteration 762: loss: 61.693939, loss_kl: 0.650252, loss_recon: 0.611530, loss_pred: 0.173974
iteration 763: loss: 60.375721, loss_kl: 0.591298, loss_recon: 0.598300, loss_pred: 0.211960
iteration 764: loss: 61.051727, loss_kl: 0.367984, loss_recon: 0.606556, loss_pred: 0.188399
iteration 765: loss: 61.964558, loss_kl: 0.481241, loss_recon: 0.615189, loss_pred: 0.174050
iteration 766: loss: 61.246834, loss_kl: 0.543546, loss_recon: 0.607189, loss_pred: 0.221126
iteration 767: loss: 60.761681, loss_kl: 0.452404, loss_recon: 0.602799, loss_pred: 0.226412
iteration 768: loss: 61.399559, loss_kl: 0.554027, loss_recon: 0.607561, loss_pred: 0.330764
iteration 769: loss: 61.732105, loss_kl: 0.561409, loss_recon: 0.612505, loss_pred: 0.164774
iteration 770: loss: 61.281734, loss_kl: 0.905527, loss_recon: 0.604233, loss_pred: 0.347367
 38%|██████████▊                 | 77/200 [1:03:25<1:40:51, 49.20s/it]iteration 771: loss: 61.292213, loss_kl: 0.450167, loss_recon: 0.608213, loss_pred: 0.199016
iteration 772: loss: 62.021294, loss_kl: 0.242123, loss_recon: 0.615828, loss_pred: 0.292210
iteration 773: loss: 59.733974, loss_kl: 0.452035, loss_recon: 0.592584, loss_pred: 0.202567
iteration 774: loss: 61.457325, loss_kl: 0.419854, loss_recon: 0.608201, loss_pred: 0.383594
iteration 775: loss: 60.382290, loss_kl: 0.490906, loss_recon: 0.598567, loss_pred: 0.229089
iteration 776: loss: 61.214619, loss_kl: 0.433831, loss_recon: 0.607221, loss_pred: 0.230443
iteration 777: loss: 61.593891, loss_kl: 0.255895, loss_recon: 0.611824, loss_pred: 0.256891
iteration 778: loss: 61.215050, loss_kl: 0.576238, loss_recon: 0.606983, loss_pred: 0.168674
iteration 779: loss: 62.662983, loss_kl: 0.472371, loss_recon: 0.621320, loss_pred: 0.245671
iteration 780: loss: 62.206631, loss_kl: 0.099022, loss_recon: 0.618788, loss_pred: 0.267998
 39%|██████████▉                 | 78/200 [1:04:15<1:40:31, 49.44s/it]iteration 781: loss: 60.214046, loss_kl: 0.582206, loss_recon: 0.595909, loss_pred: 0.248397
iteration 782: loss: 60.847275, loss_kl: 0.401313, loss_recon: 0.604054, loss_pred: 0.183580
iteration 783: loss: 62.479240, loss_kl: 0.401242, loss_recon: 0.619744, loss_pred: 0.246642
iteration 784: loss: 60.824890, loss_kl: 0.440271, loss_recon: 0.603406, loss_pred: 0.200944
iteration 785: loss: 61.710690, loss_kl: 0.482927, loss_recon: 0.610583, loss_pred: 0.341617
iteration 786: loss: 61.715141, loss_kl: 0.204340, loss_recon: 0.614170, loss_pred: 0.166676
iteration 787: loss: 61.015938, loss_kl: 0.491845, loss_recon: 0.602968, loss_pred: 0.402553
iteration 788: loss: 61.812805, loss_kl: 0.419647, loss_recon: 0.613400, loss_pred: 0.202746
iteration 789: loss: 60.858212, loss_kl: 0.378940, loss_recon: 0.603585, loss_pred: 0.255812
iteration 790: loss: 62.374783, loss_kl: 0.479183, loss_recon: 0.618073, loss_pred: 0.259100
 40%|███████████                 | 79/200 [1:05:05<1:39:35, 49.38s/it]iteration 791: loss: 60.209919, loss_kl: 0.448329, loss_recon: 0.597350, loss_pred: 0.168652
iteration 792: loss: 59.885540, loss_kl: 0.280118, loss_recon: 0.594258, loss_pred: 0.268324
iteration 793: loss: 61.730770, loss_kl: 0.572473, loss_recon: 0.611057, loss_pred: 0.233991
iteration 794: loss: 62.331894, loss_kl: 0.621836, loss_recon: 0.617343, loss_pred: 0.172801
iteration 795: loss: 61.148998, loss_kl: 0.450844, loss_recon: 0.605418, loss_pred: 0.299172
iteration 796: loss: 61.382713, loss_kl: 0.364943, loss_recon: 0.609673, loss_pred: 0.166124
iteration 797: loss: 62.079605, loss_kl: 0.493058, loss_recon: 0.615554, loss_pred: 0.187372
iteration 798: loss: 62.702377, loss_kl: 0.360942, loss_recon: 0.620672, loss_pred: 0.388629
iteration 799: loss: 61.114414, loss_kl: 0.552876, loss_recon: 0.605605, loss_pred: 0.176157
iteration 800: loss: 61.834152, loss_kl: 0.556062, loss_recon: 0.611845, loss_pred: 0.269741
 40%|███████████▏                | 80/200 [1:05:54<1:38:35, 49.30s/it]iteration 801: loss: 61.388763, loss_kl: 0.407249, loss_recon: 0.609500, loss_pred: 0.144354
iteration 802: loss: 62.310688, loss_kl: 0.260989, loss_recon: 0.619156, loss_pred: 0.206490
iteration 803: loss: 61.129627, loss_kl: 0.513598, loss_recon: 0.605493, loss_pred: 0.209074
iteration 804: loss: 60.452686, loss_kl: 0.331361, loss_recon: 0.599829, loss_pred: 0.230312
iteration 805: loss: 61.291309, loss_kl: 0.428541, loss_recon: 0.607778, loss_pred: 0.203764
iteration 806: loss: 61.457493, loss_kl: 0.412170, loss_recon: 0.609992, loss_pred: 0.160407
iteration 807: loss: 60.925941, loss_kl: 0.412087, loss_recon: 0.603932, loss_pred: 0.234858
iteration 808: loss: 60.170639, loss_kl: 0.305192, loss_recon: 0.596906, loss_pred: 0.259483
iteration 809: loss: 61.734390, loss_kl: 0.531357, loss_recon: 0.611909, loss_pred: 0.159387
iteration 810: loss: 63.505627, loss_kl: 0.175621, loss_recon: 0.632691, loss_pred: 0.109627
 40%|███████████▎                | 81/200 [1:06:44<1:38:06, 49.47s/it]iteration 811: loss: 62.603642, loss_kl: 0.446429, loss_recon: 0.621129, loss_pred: 0.150355
iteration 812: loss: 62.629341, loss_kl: 0.389066, loss_recon: 0.622128, loss_pred: 0.119883
iteration 813: loss: 60.305473, loss_kl: 0.596228, loss_recon: 0.596611, loss_pred: 0.189802
iteration 814: loss: 61.038357, loss_kl: 0.446831, loss_recon: 0.605208, loss_pred: 0.176886
iteration 815: loss: 59.142334, loss_kl: 0.187525, loss_recon: 0.587630, loss_pred: 0.236340
iteration 816: loss: 62.250069, loss_kl: 0.613578, loss_recon: 0.616445, loss_pred: 0.137728
iteration 817: loss: 61.980694, loss_kl: 0.477967, loss_recon: 0.613635, loss_pred: 0.252834
iteration 818: loss: 60.402821, loss_kl: 0.328763, loss_recon: 0.599895, loss_pred: 0.162657
iteration 819: loss: 61.360332, loss_kl: 0.362885, loss_recon: 0.607862, loss_pred: 0.297455
iteration 820: loss: 63.225540, loss_kl: 0.533693, loss_recon: 0.626502, loss_pred: 0.168432
 41%|███████████▍                | 82/200 [1:07:34<1:37:33, 49.61s/it]iteration 821: loss: 61.009235, loss_kl: 0.410484, loss_recon: 0.603653, loss_pred: 0.314721
iteration 822: loss: 61.072620, loss_kl: 0.279110, loss_recon: 0.606551, loss_pred: 0.193705
iteration 823: loss: 62.527340, loss_kl: 0.238888, loss_recon: 0.620444, loss_pred: 0.291350
iteration 824: loss: 60.220242, loss_kl: 0.400952, loss_recon: 0.594856, loss_pred: 0.413112
iteration 825: loss: 61.933319, loss_kl: 0.312849, loss_recon: 0.613676, loss_pred: 0.314779
iteration 826: loss: 62.500824, loss_kl: 0.335802, loss_recon: 0.618263, loss_pred: 0.405249
iteration 827: loss: 61.335228, loss_kl: 0.457791, loss_recon: 0.607677, loss_pred: 0.200418
iteration 828: loss: 60.942612, loss_kl: 0.354044, loss_recon: 0.601452, loss_pred: 0.513511
iteration 829: loss: 61.003345, loss_kl: 0.323771, loss_recon: 0.605430, loss_pred: 0.200700
iteration 830: loss: 62.212421, loss_kl: 0.546141, loss_recon: 0.614386, loss_pred: 0.335857
 42%|███████████▌                | 83/200 [1:08:23<1:36:26, 49.45s/it]iteration 831: loss: 62.374474, loss_kl: 0.549526, loss_recon: 0.614429, loss_pred: 0.469139
iteration 832: loss: 61.967896, loss_kl: 0.234485, loss_recon: 0.615415, loss_pred: 0.229050
iteration 833: loss: 61.732151, loss_kl: 0.343814, loss_recon: 0.604419, loss_pred: 1.000861
iteration 834: loss: 61.333939, loss_kl: 0.347247, loss_recon: 0.608121, loss_pred: 0.229604
iteration 835: loss: 61.314686, loss_kl: 0.370232, loss_recon: 0.604045, loss_pred: 0.598606
iteration 836: loss: 61.280663, loss_kl: 0.253676, loss_recon: 0.607444, loss_pred: 0.322779
iteration 837: loss: 62.131889, loss_kl: 0.270123, loss_recon: 0.615615, loss_pred: 0.343085
iteration 838: loss: 61.272411, loss_kl: 0.412926, loss_recon: 0.602509, loss_pred: 0.673994
iteration 839: loss: 60.797421, loss_kl: 0.237368, loss_recon: 0.603633, loss_pred: 0.234330
iteration 840: loss: 62.795696, loss_kl: 0.131117, loss_recon: 0.619061, loss_pred: 0.779244
 42%|███████████▊                | 84/200 [1:09:14<1:36:26, 49.88s/it]iteration 841: loss: 61.426987, loss_kl: 0.212442, loss_recon: 0.608960, loss_pred: 0.343781
iteration 842: loss: 61.777363, loss_kl: 0.249430, loss_recon: 0.612484, loss_pred: 0.309154
iteration 843: loss: 62.459965, loss_kl: 0.252372, loss_recon: 0.617212, loss_pred: 0.516355
iteration 844: loss: 63.193439, loss_kl: 0.232582, loss_recon: 0.626718, loss_pred: 0.316699
iteration 845: loss: 61.851257, loss_kl: 0.356214, loss_recon: 0.611413, loss_pred: 0.396090
iteration 846: loss: 60.681545, loss_kl: 0.494471, loss_recon: 0.597736, loss_pred: 0.472204
iteration 847: loss: 60.553982, loss_kl: 0.426942, loss_recon: 0.599593, loss_pred: 0.218469
iteration 848: loss: 61.269585, loss_kl: 0.464162, loss_recon: 0.605174, loss_pred: 0.343211
iteration 849: loss: 60.407970, loss_kl: 0.622658, loss_recon: 0.595042, loss_pred: 0.355120
iteration 850: loss: 61.203285, loss_kl: 0.161239, loss_recon: 0.607958, loss_pred: 0.265402
 42%|███████████▉                | 85/200 [1:10:04<1:35:51, 50.01s/it]iteration 851: loss: 61.478413, loss_kl: 0.402508, loss_recon: 0.609047, loss_pred: 0.203090
iteration 852: loss: 61.822990, loss_kl: 0.403282, loss_recon: 0.611040, loss_pred: 0.347629
iteration 853: loss: 62.087696, loss_kl: 0.419533, loss_recon: 0.614445, loss_pred: 0.256844
iteration 854: loss: 60.022118, loss_kl: 0.017700, loss_recon: 0.597436, loss_pred: 0.262223
iteration 855: loss: 62.123177, loss_kl: 0.264692, loss_recon: 0.615882, loss_pred: 0.291291
iteration 856: loss: 59.616188, loss_kl: 0.378299, loss_recon: 0.589876, loss_pred: 0.280211
iteration 857: loss: 61.073418, loss_kl: 0.260688, loss_recon: 0.605807, loss_pred: 0.252628
iteration 858: loss: 61.406784, loss_kl: 0.269232, loss_recon: 0.608427, loss_pred: 0.316191
iteration 859: loss: 61.719677, loss_kl: 0.332889, loss_recon: 0.612470, loss_pred: 0.166132
iteration 860: loss: 64.036995, loss_kl: 0.297714, loss_recon: 0.632328, loss_pred: 0.530101
 43%|████████████                | 86/200 [1:10:53<1:34:20, 49.65s/it]iteration 861: loss: 62.490200, loss_kl: 0.189694, loss_recon: 0.620467, loss_pred: 0.261300
iteration 862: loss: 59.538082, loss_kl: 0.151011, loss_recon: 0.592045, loss_pred: 0.188599
iteration 863: loss: 61.693638, loss_kl: 0.190758, loss_recon: 0.612009, loss_pred: 0.309573
iteration 864: loss: 61.599209, loss_kl: 0.341082, loss_recon: 0.610031, loss_pred: 0.268485
iteration 865: loss: 61.855335, loss_kl: 0.390263, loss_recon: 0.612251, loss_pred: 0.255418
iteration 866: loss: 61.328629, loss_kl: 0.216549, loss_recon: 0.607373, loss_pred: 0.383367
iteration 867: loss: 61.608063, loss_kl: 0.287422, loss_recon: 0.610758, loss_pred: 0.256218
iteration 868: loss: 61.969761, loss_kl: 0.350589, loss_recon: 0.612213, loss_pred: 0.411733
iteration 869: loss: 59.990746, loss_kl: 0.301362, loss_recon: 0.593855, loss_pred: 0.315808
iteration 870: loss: 61.156940, loss_kl: 0.187452, loss_recon: 0.608091, loss_pred: 0.167817
 44%|████████████▏               | 87/200 [1:11:44<1:34:33, 50.21s/it]iteration 871: loss: 62.039291, loss_kl: 0.374202, loss_recon: 0.613971, loss_pred: 0.268012
iteration 872: loss: 63.161972, loss_kl: 0.280692, loss_recon: 0.626181, loss_pred: 0.263148
iteration 873: loss: 60.618504, loss_kl: 0.401518, loss_recon: 0.600073, loss_pred: 0.209718
iteration 874: loss: 60.702271, loss_kl: 0.462182, loss_recon: 0.599444, loss_pred: 0.295711
iteration 875: loss: 61.022263, loss_kl: 0.280212, loss_recon: 0.602933, loss_pred: 0.448799
iteration 876: loss: 60.783627, loss_kl: 0.410094, loss_recon: 0.600936, loss_pred: 0.279883
iteration 877: loss: 61.557972, loss_kl: 0.288150, loss_recon: 0.611424, loss_pred: 0.127410
iteration 878: loss: 61.390530, loss_kl: 0.263476, loss_recon: 0.609154, loss_pred: 0.211684
iteration 879: loss: 61.804199, loss_kl: 0.373140, loss_recon: 0.611606, loss_pred: 0.270443
iteration 880: loss: 61.275925, loss_kl: 0.274673, loss_recon: 0.607583, loss_pred: 0.242939
 44%|████████████▎               | 88/200 [1:12:35<1:34:02, 50.38s/it]iteration 881: loss: 61.887836, loss_kl: 0.312749, loss_recon: 0.613667, loss_pred: 0.208382
iteration 882: loss: 60.464565, loss_kl: 0.260625, loss_recon: 0.598979, loss_pred: 0.306060
iteration 883: loss: 62.351742, loss_kl: 0.244714, loss_recon: 0.618205, loss_pred: 0.286488
iteration 884: loss: 62.139591, loss_kl: 0.120693, loss_recon: 0.617952, loss_pred: 0.223676
iteration 885: loss: 62.218678, loss_kl: 0.273766, loss_recon: 0.617059, loss_pred: 0.239006
iteration 886: loss: 59.768345, loss_kl: 0.257616, loss_recon: 0.590724, loss_pred: 0.438360
iteration 887: loss: 60.370846, loss_kl: 0.228957, loss_recon: 0.599682, loss_pred: 0.173701
iteration 888: loss: 60.982025, loss_kl: 0.296649, loss_recon: 0.604570, loss_pred: 0.228355
iteration 889: loss: 61.410099, loss_kl: 0.187186, loss_recon: 0.609416, loss_pred: 0.281323
iteration 890: loss: 61.427574, loss_kl: 0.204600, loss_recon: 0.609897, loss_pred: 0.233224
 44%|████████████▍               | 89/200 [1:13:24<1:32:27, 49.98s/it]iteration 891: loss: 60.603271, loss_kl: 0.244721, loss_recon: 0.601225, loss_pred: 0.236050
iteration 892: loss: 61.538830, loss_kl: 0.082233, loss_recon: 0.612555, loss_pred: 0.201077
iteration 893: loss: 61.467648, loss_kl: 0.252838, loss_recon: 0.609515, loss_pred: 0.263273
iteration 894: loss: 62.336578, loss_kl: 0.271758, loss_recon: 0.619171, loss_pred: 0.147739
iteration 895: loss: 59.500587, loss_kl: 0.251928, loss_recon: 0.589757, loss_pred: 0.273004
iteration 896: loss: 61.237286, loss_kl: 0.254410, loss_recon: 0.607717, loss_pred: 0.211189
iteration 897: loss: 61.536606, loss_kl: 0.166185, loss_recon: 0.612086, loss_pred: 0.161773
iteration 898: loss: 60.593555, loss_kl: 0.189239, loss_recon: 0.602703, loss_pred: 0.134008
iteration 899: loss: 61.490128, loss_kl: 0.243501, loss_recon: 0.611136, loss_pred: 0.133008
iteration 900: loss: 63.004990, loss_kl: 0.104850, loss_recon: 0.627785, loss_pred: 0.121603
 45%|████████████▌               | 90/200 [1:14:13<1:31:12, 49.75s/it]iteration 901: loss: 60.442585, loss_kl: 0.229017, loss_recon: 0.600654, loss_pred: 0.148150
iteration 902: loss: 61.124668, loss_kl: 0.307912, loss_recon: 0.606322, loss_pred: 0.184510
iteration 903: loss: 62.434170, loss_kl: 0.228337, loss_recon: 0.620656, loss_pred: 0.140264
iteration 904: loss: 62.257748, loss_kl: 0.366448, loss_recon: 0.617552, loss_pred: 0.136130
iteration 905: loss: 61.275265, loss_kl: 0.287640, loss_recon: 0.608486, loss_pred: 0.139009
iteration 906: loss: 61.617817, loss_kl: 0.404338, loss_recon: 0.609451, loss_pred: 0.268412
iteration 907: loss: 61.109241, loss_kl: 0.309904, loss_recon: 0.606104, loss_pred: 0.188966
iteration 908: loss: 61.150562, loss_kl: 0.250313, loss_recon: 0.606969, loss_pred: 0.203308
iteration 909: loss: 60.354218, loss_kl: 0.305879, loss_recon: 0.598338, loss_pred: 0.214537
iteration 910: loss: 60.800747, loss_kl: 0.535364, loss_recon: 0.600867, loss_pred: 0.178690
 46%|████████████▋               | 91/200 [1:15:04<1:30:41, 49.92s/it]iteration 911: loss: 60.896145, loss_kl: 0.392174, loss_recon: 0.603118, loss_pred: 0.192159
iteration 912: loss: 60.864857, loss_kl: 0.239002, loss_recon: 0.603981, loss_pred: 0.227742
iteration 913: loss: 60.897655, loss_kl: 0.158501, loss_recon: 0.605227, loss_pred: 0.216437
iteration 914: loss: 61.217888, loss_kl: 0.111275, loss_recon: 0.609539, loss_pred: 0.152716
iteration 915: loss: 61.682724, loss_kl: 0.308849, loss_recon: 0.612025, loss_pred: 0.171378
iteration 916: loss: 61.406090, loss_kl: 0.185600, loss_recon: 0.609977, loss_pred: 0.222753
iteration 917: loss: 61.610703, loss_kl: 0.290168, loss_recon: 0.611754, loss_pred: 0.145154
iteration 918: loss: 61.721790, loss_kl: 0.265542, loss_recon: 0.612975, loss_pred: 0.158791
iteration 919: loss: 60.187386, loss_kl: 0.189467, loss_recon: 0.597293, loss_pred: 0.268572
iteration 920: loss: 61.901669, loss_kl: 0.352149, loss_recon: 0.613924, loss_pred: 0.157133
 46%|████████████▉               | 92/200 [1:15:54<1:30:19, 50.18s/it]iteration 921: loss: 61.062473, loss_kl: 0.227448, loss_recon: 0.606396, loss_pred: 0.195381
iteration 922: loss: 60.445614, loss_kl: 0.259552, loss_recon: 0.599158, loss_pred: 0.270227
iteration 923: loss: 61.083321, loss_kl: 0.228590, loss_recon: 0.606844, loss_pred: 0.170323
iteration 924: loss: 62.214264, loss_kl: 0.081702, loss_recon: 0.618930, loss_pred: 0.239576
iteration 925: loss: 61.370697, loss_kl: 0.313844, loss_recon: 0.607864, loss_pred: 0.270417
iteration 926: loss: 60.502441, loss_kl: 0.297033, loss_recon: 0.598818, loss_pred: 0.323603
iteration 927: loss: 62.737331, loss_kl: 0.400492, loss_recon: 0.621998, loss_pred: 0.137045
iteration 928: loss: 61.427773, loss_kl: 0.082344, loss_recon: 0.610642, loss_pred: 0.281193
iteration 929: loss: 59.997963, loss_kl: 0.195741, loss_recon: 0.596296, loss_pred: 0.172632
iteration 930: loss: 62.438583, loss_kl: -0.019314, loss_recon: 0.623187, loss_pred: 0.139200
 46%|█████████████               | 93/200 [1:16:44<1:29:00, 49.91s/it]iteration 931: loss: 62.410034, loss_kl: 0.222349, loss_recon: 0.619359, loss_pred: 0.251748
iteration 932: loss: 61.491749, loss_kl: 0.170878, loss_recon: 0.611710, loss_pred: 0.149825
iteration 933: loss: 60.610142, loss_kl: 0.259759, loss_recon: 0.601502, loss_pred: 0.200172
iteration 934: loss: 61.503830, loss_kl: 0.373664, loss_recon: 0.609463, loss_pred: 0.183845
iteration 935: loss: 61.149361, loss_kl: 0.327892, loss_recon: 0.606976, loss_pred: 0.123839
iteration 936: loss: 60.244461, loss_kl: 0.217153, loss_recon: 0.598641, loss_pred: 0.163191
iteration 937: loss: 61.952858, loss_kl: 0.145271, loss_recon: 0.616445, loss_pred: 0.163119
iteration 938: loss: 61.075581, loss_kl: 0.140156, loss_recon: 0.607424, loss_pred: 0.192993
iteration 939: loss: 60.011974, loss_kl: 0.170636, loss_recon: 0.596237, loss_pred: 0.217639
iteration 940: loss: 63.637554, loss_kl: 0.231935, loss_recon: 0.630563, loss_pred: 0.349308
 47%|█████████████▏              | 94/200 [1:17:33<1:27:57, 49.79s/it]iteration 941: loss: 60.358070, loss_kl: 0.162725, loss_recon: 0.598989, loss_pred: 0.296441
iteration 942: loss: 61.961494, loss_kl: 0.184619, loss_recon: 0.615646, loss_pred: 0.212270
iteration 943: loss: 61.170647, loss_kl: 0.269697, loss_recon: 0.605975, loss_pred: 0.303488
iteration 944: loss: 61.331055, loss_kl: 0.095468, loss_recon: 0.610801, loss_pred: 0.155479
iteration 945: loss: 63.067993, loss_kl: 0.330258, loss_recon: 0.624725, loss_pred: 0.265246
iteration 946: loss: 60.697144, loss_kl: 0.094664, loss_recon: 0.598978, loss_pred: 0.704683
iteration 947: loss: 61.430458, loss_kl: 0.214827, loss_recon: 0.609496, loss_pred: 0.266018
iteration 948: loss: 62.326550, loss_kl: 0.170165, loss_recon: 0.618603, loss_pred: 0.296079
iteration 949: loss: 60.671539, loss_kl: 0.262990, loss_recon: 0.601130, loss_pred: 0.295585
iteration 950: loss: 60.778122, loss_kl: 0.135048, loss_recon: 0.602112, loss_pred: 0.431915
 48%|█████████████▎              | 95/200 [1:18:23<1:26:57, 49.69s/it]iteration 951: loss: 61.934063, loss_kl: 0.253228, loss_recon: 0.614968, loss_pred: 0.184061
iteration 952: loss: 61.513992, loss_kl: 0.118374, loss_recon: 0.610628, loss_pred: 0.332796
iteration 953: loss: 60.128460, loss_kl: 0.296482, loss_recon: 0.595160, loss_pred: 0.315930
iteration 954: loss: 61.041050, loss_kl: 0.226457, loss_recon: 0.605623, loss_pred: 0.252336
iteration 955: loss: 61.632774, loss_kl: 0.161989, loss_recon: 0.612259, loss_pred: 0.244902
iteration 956: loss: 61.870445, loss_kl: 0.298912, loss_recon: 0.613170, loss_pred: 0.254490
iteration 957: loss: 61.461243, loss_kl: 0.188522, loss_recon: 0.611375, loss_pred: 0.135206
iteration 958: loss: 61.713974, loss_kl: 0.397200, loss_recon: 0.610938, loss_pred: 0.222989
iteration 959: loss: 60.401470, loss_kl: 0.180452, loss_recon: 0.598686, loss_pred: 0.352419
iteration 960: loss: 59.959587, loss_kl: -0.016590, loss_recon: 0.597306, loss_pred: 0.245618
 48%|█████████████▍              | 96/200 [1:19:12<1:25:54, 49.57s/it]iteration 961: loss: 61.318352, loss_kl: 0.070251, loss_recon: 0.611021, loss_pred: 0.145976
iteration 962: loss: 61.850777, loss_kl: 0.157338, loss_recon: 0.614938, loss_pred: 0.199589
iteration 963: loss: 61.739113, loss_kl: 0.192062, loss_recon: 0.613918, loss_pred: 0.155238
iteration 964: loss: 61.121037, loss_kl: 0.232523, loss_recon: 0.607506, loss_pred: 0.137890
iteration 965: loss: 61.290283, loss_kl: 0.171100, loss_recon: 0.609373, loss_pred: 0.181849
iteration 966: loss: 60.254322, loss_kl: 0.175535, loss_recon: 0.598631, loss_pred: 0.215725
iteration 967: loss: 61.409344, loss_kl: 0.112590, loss_recon: 0.610640, loss_pred: 0.232760
iteration 968: loss: 61.166714, loss_kl: 0.198626, loss_recon: 0.607323, loss_pred: 0.235798
iteration 969: loss: 60.822636, loss_kl: 0.090260, loss_recon: 0.605745, loss_pred: 0.157845
iteration 970: loss: 59.923531, loss_kl: 0.288145, loss_recon: 0.593858, loss_pred: 0.249596
 48%|█████████████▌              | 97/200 [1:20:01<1:24:43, 49.36s/it]iteration 971: loss: 62.030884, loss_kl: 0.210268, loss_recon: 0.616710, loss_pred: 0.149652
iteration 972: loss: 60.310890, loss_kl: 0.187586, loss_recon: 0.598897, loss_pred: 0.233561
iteration 973: loss: 60.869427, loss_kl: 0.104886, loss_recon: 0.605456, loss_pred: 0.218961
iteration 974: loss: 62.639797, loss_kl: 0.225838, loss_recon: 0.622787, loss_pred: 0.135245
iteration 975: loss: 61.730114, loss_kl: 0.097495, loss_recon: 0.614481, loss_pred: 0.184551
iteration 976: loss: 61.320892, loss_kl: 0.244910, loss_recon: 0.609234, loss_pred: 0.152625
iteration 977: loss: 59.685268, loss_kl: 0.201198, loss_recon: 0.591892, loss_pred: 0.294837
iteration 978: loss: 61.394215, loss_kl: 0.315883, loss_recon: 0.609123, loss_pred: 0.166017
iteration 979: loss: 61.115620, loss_kl: 0.206847, loss_recon: 0.607510, loss_pred: 0.157755
iteration 980: loss: 60.489491, loss_kl: 0.208854, loss_recon: 0.597263, loss_pred: 0.554374
 49%|█████████████▋              | 98/200 [1:20:50<1:23:48, 49.30s/it]iteration 981: loss: 61.117233, loss_kl: 0.097950, loss_recon: 0.608442, loss_pred: 0.175076
iteration 982: loss: 60.983303, loss_kl: 0.170013, loss_recon: 0.605107, loss_pred: 0.302635
iteration 983: loss: 61.680244, loss_kl: 0.061073, loss_recon: 0.614294, loss_pred: 0.189787
iteration 984: loss: 60.453068, loss_kl: 0.218052, loss_recon: 0.600130, loss_pred: 0.221974
iteration 985: loss: 61.021812, loss_kl: 0.241413, loss_recon: 0.605945, loss_pred: 0.185864
iteration 986: loss: 60.586983, loss_kl: 0.256600, loss_recon: 0.601612, loss_pred: 0.169183
iteration 987: loss: 61.666210, loss_kl: 0.066936, loss_recon: 0.614131, loss_pred: 0.186211
iteration 988: loss: 60.953629, loss_kl: 0.294218, loss_recon: 0.604477, loss_pred: 0.211747
iteration 989: loss: 61.366409, loss_kl: 0.203975, loss_recon: 0.609994, loss_pred: 0.163061
iteration 990: loss: 63.706089, loss_kl: 0.195150, loss_recon: 0.632202, loss_pred: 0.290756
 50%|█████████████▊              | 99/200 [1:21:39<1:23:02, 49.33s/it]iteration 991: loss: 61.226456, loss_kl: 0.201142, loss_recon: 0.608583, loss_pred: 0.167001
iteration 992: loss: 60.317974, loss_kl: 0.114109, loss_recon: 0.600492, loss_pred: 0.154669
iteration 993: loss: 61.803074, loss_kl: 0.125190, loss_recon: 0.614336, loss_pred: 0.244304
iteration 994: loss: 60.907036, loss_kl: 0.318294, loss_recon: 0.604342, loss_pred: 0.154513
iteration 995: loss: 60.995533, loss_kl: 0.249087, loss_recon: 0.605075, loss_pred: 0.238927
iteration 996: loss: 61.419399, loss_kl: 0.071316, loss_recon: 0.612143, loss_pred: 0.133806
iteration 997: loss: 61.697964, loss_kl: 0.371896, loss_recon: 0.610218, loss_pred: 0.304252
iteration 998: loss: 61.072090, loss_kl: 0.049224, loss_recon: 0.608579, loss_pred: 0.164966
iteration 999: loss: 60.509407, loss_kl: 0.260413, loss_recon: 0.599770, loss_pred: 0.272028
iteration 1000: loss: 63.655888, loss_kl: -0.031798, loss_recon: 0.633720, loss_pred: 0.315671
 50%|█████████████▌             | 100/200 [1:22:29<1:22:16, 49.36s/it]iteration 1001: loss: 61.306137, loss_kl: 0.337082, loss_recon: 0.609691, loss_pred: 0.333707
iteration 1002: loss: 61.163765, loss_kl: 0.273291, loss_recon: 0.608714, loss_pred: 0.289624
iteration 1003: loss: 61.740940, loss_kl: 0.175777, loss_recon: 0.615138, loss_pred: 0.225382
iteration 1004: loss: 60.616035, loss_kl: 0.235522, loss_recon: 0.602340, loss_pred: 0.379658
iteration 1005: loss: 61.184391, loss_kl: 0.297284, loss_recon: 0.610337, loss_pred: 0.147755
iteration 1006: loss: 60.324238, loss_kl: 0.278581, loss_recon: 0.599608, loss_pred: 0.360633
iteration 1007: loss: 61.569489, loss_kl: 0.314115, loss_recon: 0.613733, loss_pred: 0.193086
iteration 1008: loss: 60.848721, loss_kl: 0.096953, loss_recon: 0.606959, loss_pred: 0.151867
iteration 1009: loss: 60.722733, loss_kl: 0.170543, loss_recon: 0.604641, loss_pred: 0.256943
iteration 1010: loss: 60.176197, loss_kl: 0.180069, loss_recon: 0.598994, loss_pred: 0.274953
 50%|█████████████▋             | 101/200 [1:23:18<1:21:30, 49.40s/it]iteration 1011: loss: 61.519123, loss_kl: 0.312775, loss_recon: 0.613472, loss_pred: 0.168786
iteration 1012: loss: 61.859383, loss_kl: 0.192985, loss_recon: 0.616643, loss_pred: 0.193136
iteration 1013: loss: 60.669514, loss_kl: 0.281855, loss_recon: 0.604230, loss_pred: 0.243734
iteration 1014: loss: 60.733311, loss_kl: 0.186717, loss_recon: 0.605876, loss_pred: 0.143807
iteration 1015: loss: 61.990028, loss_kl: 0.248978, loss_recon: 0.617709, loss_pred: 0.216671
iteration 1016: loss: 61.027515, loss_kl: 0.152485, loss_recon: 0.608340, loss_pred: 0.192029
iteration 1017: loss: 59.744530, loss_kl: 0.292960, loss_recon: 0.595217, loss_pred: 0.219895
iteration 1018: loss: 60.983818, loss_kl: 0.258150, loss_recon: 0.607849, loss_pred: 0.196377
iteration 1019: loss: 60.367767, loss_kl: 0.285772, loss_recon: 0.601196, loss_pred: 0.245301
iteration 1020: loss: 59.678185, loss_kl: 0.253261, loss_recon: 0.594642, loss_pred: 0.211418
 51%|█████████████▊             | 102/200 [1:24:08<1:20:42, 49.41s/it]iteration 1021: loss: 61.380093, loss_kl: 0.287596, loss_recon: 0.612339, loss_pred: 0.143323
iteration 1022: loss: 62.155285, loss_kl: 0.181634, loss_recon: 0.618404, loss_pred: 0.313075
iteration 1023: loss: 61.701107, loss_kl: 0.321983, loss_recon: 0.615201, loss_pred: 0.177823
iteration 1024: loss: 60.683170, loss_kl: 0.263400, loss_recon: 0.605065, loss_pred: 0.174075
iteration 1025: loss: 60.828495, loss_kl: 0.159694, loss_recon: 0.605707, loss_pred: 0.256161
iteration 1026: loss: 60.977982, loss_kl: 0.216986, loss_recon: 0.607174, loss_pred: 0.258412
iteration 1027: loss: 60.895958, loss_kl: 0.176607, loss_recon: 0.606942, loss_pred: 0.200003
iteration 1028: loss: 60.988663, loss_kl: 0.152616, loss_recon: 0.608734, loss_pred: 0.113744
iteration 1029: loss: 59.718575, loss_kl: 0.198977, loss_recon: 0.594966, loss_pred: 0.219999
iteration 1030: loss: 59.800022, loss_kl: 0.233951, loss_recon: 0.594165, loss_pred: 0.381216
 52%|█████████████▉             | 103/200 [1:24:57<1:19:48, 49.36s/it]iteration 1031: loss: 61.825386, loss_kl: 0.305277, loss_recon: 0.616034, loss_pred: 0.218949
iteration 1032: loss: 61.500877, loss_kl: 0.260805, loss_recon: 0.613095, loss_pred: 0.188811
iteration 1033: loss: 61.031742, loss_kl: 0.189110, loss_recon: 0.608381, loss_pred: 0.191769
iteration 1034: loss: 62.549133, loss_kl: 0.285153, loss_recon: 0.623944, loss_pred: 0.151843
iteration 1035: loss: 59.551468, loss_kl: 0.194556, loss_recon: 0.593474, loss_pred: 0.202083
iteration 1036: loss: 61.229858, loss_kl: 0.206432, loss_recon: 0.610527, loss_pred: 0.175058
iteration 1037: loss: 60.643990, loss_kl: 0.218120, loss_recon: 0.604798, loss_pred: 0.162052
iteration 1038: loss: 59.897560, loss_kl: 0.187888, loss_recon: 0.597256, loss_pred: 0.170096
iteration 1039: loss: 60.630112, loss_kl: 0.146737, loss_recon: 0.605121, loss_pred: 0.116503
iteration 1040: loss: 60.172241, loss_kl: 0.273647, loss_recon: 0.598807, loss_pred: 0.288792
 52%|██████████████             | 104/200 [1:25:46<1:19:04, 49.42s/it]iteration 1041: loss: 61.612473, loss_kl: 0.281262, loss_recon: 0.614457, loss_pred: 0.163917
iteration 1042: loss: 60.897602, loss_kl: 0.262075, loss_recon: 0.607644, loss_pred: 0.130560
iteration 1043: loss: 62.367126, loss_kl: 0.183541, loss_recon: 0.622399, loss_pred: 0.125366
iteration 1044: loss: 60.647095, loss_kl: 0.243502, loss_recon: 0.605060, loss_pred: 0.138637
iteration 1045: loss: 59.782650, loss_kl: 0.232371, loss_recon: 0.596162, loss_pred: 0.164111
iteration 1046: loss: 60.470314, loss_kl: 0.202618, loss_recon: 0.602847, loss_pred: 0.183544
iteration 1047: loss: 60.012280, loss_kl: 0.170073, loss_recon: 0.598554, loss_pred: 0.155140
iteration 1048: loss: 61.795921, loss_kl: 0.348746, loss_recon: 0.616821, loss_pred: 0.110327
iteration 1049: loss: 60.391392, loss_kl: 0.248995, loss_recon: 0.602152, loss_pred: 0.173745
iteration 1050: loss: 61.084400, loss_kl: -0.132321, loss_recon: 0.609666, loss_pred: 0.119161
 52%|██████████████▏            | 105/200 [1:26:36<1:18:06, 49.33s/it]iteration 1051: loss: 60.410122, loss_kl: 0.316062, loss_recon: 0.602632, loss_pred: 0.143762
iteration 1052: loss: 60.946518, loss_kl: 0.355759, loss_recon: 0.607687, loss_pred: 0.174290
iteration 1053: loss: 61.179722, loss_kl: 0.205138, loss_recon: 0.610278, loss_pred: 0.149869
iteration 1054: loss: 60.863918, loss_kl: 0.264338, loss_recon: 0.607171, loss_pred: 0.144136
iteration 1055: loss: 60.615086, loss_kl: 0.227889, loss_recon: 0.604305, loss_pred: 0.182329
iteration 1056: loss: 60.302376, loss_kl: 0.237661, loss_recon: 0.601098, loss_pred: 0.190153
iteration 1057: loss: 61.477390, loss_kl: 0.160549, loss_recon: 0.613420, loss_pred: 0.133800
iteration 1058: loss: 61.190540, loss_kl: 0.132333, loss_recon: 0.610248, loss_pred: 0.164450
iteration 1059: loss: 61.401443, loss_kl: 0.203248, loss_recon: 0.612288, loss_pred: 0.170598
iteration 1060: loss: 63.146683, loss_kl: 0.374766, loss_recon: 0.626056, loss_pred: 0.537331
 53%|██████████████▎            | 106/200 [1:27:25<1:17:17, 49.34s/it]iteration 1061: loss: 59.581173, loss_kl: 0.278867, loss_recon: 0.594216, loss_pred: 0.156814
iteration 1062: loss: 61.497334, loss_kl: 0.168486, loss_recon: 0.612913, loss_pred: 0.204367
iteration 1063: loss: 61.753113, loss_kl: 0.121631, loss_recon: 0.616193, loss_pred: 0.132566
iteration 1064: loss: 59.331841, loss_kl: 0.303321, loss_recon: 0.591360, loss_pred: 0.192833
iteration 1065: loss: 61.105427, loss_kl: 0.209564, loss_recon: 0.609478, loss_pred: 0.155575
iteration 1066: loss: 61.939976, loss_kl: 0.255587, loss_recon: 0.617680, loss_pred: 0.169394
iteration 1067: loss: 60.344208, loss_kl: 0.251040, loss_recon: 0.601469, loss_pred: 0.194769
iteration 1068: loss: 62.038731, loss_kl: 0.369366, loss_recon: 0.618037, loss_pred: 0.231325
iteration 1069: loss: 61.916706, loss_kl: 0.124805, loss_recon: 0.617358, loss_pred: 0.179661
iteration 1070: loss: 58.992500, loss_kl: 0.534920, loss_recon: 0.586892, loss_pred: 0.297983
 54%|██████████████▍            | 107/200 [1:28:14<1:16:24, 49.29s/it]iteration 1071: loss: 60.043053, loss_kl: 0.269242, loss_recon: 0.596969, loss_pred: 0.343489
iteration 1072: loss: 61.494888, loss_kl: 0.292724, loss_recon: 0.613259, loss_pred: 0.166021
iteration 1073: loss: 61.155663, loss_kl: 0.249864, loss_recon: 0.608515, loss_pred: 0.301650
iteration 1074: loss: 60.701046, loss_kl: 0.350861, loss_recon: 0.604637, loss_pred: 0.233816
iteration 1075: loss: 61.296844, loss_kl: 0.157860, loss_recon: 0.610575, loss_pred: 0.237786
iteration 1076: loss: 61.448414, loss_kl: 0.214463, loss_recon: 0.611054, loss_pred: 0.340901
iteration 1077: loss: 60.832779, loss_kl: 0.174092, loss_recon: 0.606134, loss_pred: 0.217650
iteration 1078: loss: 62.278599, loss_kl: 0.183579, loss_recon: 0.619887, loss_pred: 0.288062
iteration 1079: loss: 60.929539, loss_kl: 0.159850, loss_recon: 0.608068, loss_pred: 0.121129
iteration 1080: loss: 60.709492, loss_kl: 0.537096, loss_recon: 0.604398, loss_pred: 0.264330
 54%|██████████████▌            | 108/200 [1:29:04<1:16:01, 49.58s/it]iteration 1081: loss: 61.418873, loss_kl: 0.401274, loss_recon: 0.611772, loss_pred: 0.237630
iteration 1082: loss: 59.818897, loss_kl: 0.286760, loss_recon: 0.596854, loss_pred: 0.130586
iteration 1083: loss: 60.858730, loss_kl: 0.197045, loss_recon: 0.605025, loss_pred: 0.354258
iteration 1084: loss: 60.920345, loss_kl: 0.189451, loss_recon: 0.607913, loss_pred: 0.127183
iteration 1085: loss: 61.644772, loss_kl: 0.098269, loss_recon: 0.612806, loss_pred: 0.363157
iteration 1086: loss: 61.280415, loss_kl: 0.254385, loss_recon: 0.610144, loss_pred: 0.263481
iteration 1087: loss: 61.719383, loss_kl: 0.120109, loss_recon: 0.615019, loss_pred: 0.216329
iteration 1088: loss: 60.843040, loss_kl: 0.271739, loss_recon: 0.603953, loss_pred: 0.445007
iteration 1089: loss: 61.938961, loss_kl: 0.235478, loss_recon: 0.617913, loss_pred: 0.145260
iteration 1090: loss: 60.236210, loss_kl: -0.060971, loss_recon: 0.598243, loss_pred: 0.412511
 55%|██████████████▋            | 109/200 [1:29:53<1:14:50, 49.34s/it]iteration 1091: loss: 61.300621, loss_kl: 0.260380, loss_recon: 0.609665, loss_pred: 0.331555
iteration 1092: loss: 61.383106, loss_kl: 0.211041, loss_recon: 0.612555, loss_pred: 0.125459
iteration 1093: loss: 62.222816, loss_kl: 0.161366, loss_recon: 0.620314, loss_pred: 0.189768
iteration 1094: loss: 60.098259, loss_kl: 0.204489, loss_recon: 0.598603, loss_pred: 0.235867
iteration 1095: loss: 61.735500, loss_kl: 0.232251, loss_recon: 0.615602, loss_pred: 0.173000
iteration 1096: loss: 60.938896, loss_kl: 0.233831, loss_recon: 0.607077, loss_pred: 0.228875
iteration 1097: loss: 60.247330, loss_kl: 0.256117, loss_recon: 0.601079, loss_pred: 0.136865
iteration 1098: loss: 59.393127, loss_kl: 0.353324, loss_recon: 0.591231, loss_pred: 0.266536
iteration 1099: loss: 61.362072, loss_kl: 0.204855, loss_recon: 0.612315, loss_pred: 0.128541
iteration 1100: loss: 60.485596, loss_kl: 0.284051, loss_recon: 0.603360, loss_pred: 0.146728
 55%|██████████████▊            | 110/200 [1:30:42<1:13:51, 49.24s/it]iteration 1101: loss: 60.336018, loss_kl: 0.245299, loss_recon: 0.601894, loss_pred: 0.144191
iteration 1102: loss: 59.855812, loss_kl: 0.473897, loss_recon: 0.597050, loss_pred: 0.146084
iteration 1103: loss: 61.635250, loss_kl: 0.148714, loss_recon: 0.614876, loss_pred: 0.146170
iteration 1104: loss: 60.060429, loss_kl: 0.252469, loss_recon: 0.598971, loss_pred: 0.160796
iteration 1105: loss: 60.978550, loss_kl: 0.244612, loss_recon: 0.608488, loss_pred: 0.127330
iteration 1106: loss: 61.112331, loss_kl: 0.280206, loss_recon: 0.609678, loss_pred: 0.141754
iteration 1107: loss: 62.759411, loss_kl: 0.251613, loss_recon: 0.626378, loss_pred: 0.119044
iteration 1108: loss: 60.813850, loss_kl: 0.200575, loss_recon: 0.606919, loss_pred: 0.119978
iteration 1109: loss: 60.804108, loss_kl: 0.230517, loss_recon: 0.606718, loss_pred: 0.130021
iteration 1110: loss: 60.168709, loss_kl: 0.071154, loss_recon: 0.600622, loss_pred: 0.105819
 56%|██████████████▉            | 111/200 [1:31:31<1:12:56, 49.18s/it]iteration 1111: loss: 61.513657, loss_kl: 0.246881, loss_recon: 0.613737, loss_pred: 0.137511
iteration 1112: loss: 60.606724, loss_kl: 0.234845, loss_recon: 0.604315, loss_pred: 0.172895
iteration 1113: loss: 60.776768, loss_kl: 0.235318, loss_recon: 0.606685, loss_pred: 0.105935
iteration 1114: loss: 60.080135, loss_kl: 0.247023, loss_recon: 0.599524, loss_pred: 0.125286
iteration 1115: loss: 60.351810, loss_kl: 0.276489, loss_recon: 0.601724, loss_pred: 0.176657
iteration 1116: loss: 60.734188, loss_kl: 0.135019, loss_recon: 0.605948, loss_pred: 0.138002
iteration 1117: loss: 60.598503, loss_kl: 0.276431, loss_recon: 0.604590, loss_pred: 0.136769
iteration 1118: loss: 61.238251, loss_kl: 0.385761, loss_recon: 0.611288, loss_pred: 0.105570
iteration 1119: loss: 61.309563, loss_kl: 0.111331, loss_recon: 0.612127, loss_pred: 0.095770
iteration 1120: loss: 62.925621, loss_kl: 0.055227, loss_recon: 0.627377, loss_pred: 0.187393
 56%|███████████████            | 112/200 [1:32:20<1:12:05, 49.15s/it]iteration 1121: loss: 61.350124, loss_kl: 0.214759, loss_recon: 0.611874, loss_pred: 0.160530
iteration 1122: loss: 60.068516, loss_kl: 0.397307, loss_recon: 0.599279, loss_pred: 0.136612
iteration 1123: loss: 60.479958, loss_kl: 0.140274, loss_recon: 0.603297, loss_pred: 0.148808
iteration 1124: loss: 61.746098, loss_kl: 0.302304, loss_recon: 0.616268, loss_pred: 0.116268
iteration 1125: loss: 60.799374, loss_kl: 0.403058, loss_recon: 0.606739, loss_pred: 0.121445
iteration 1126: loss: 60.826653, loss_kl: -0.015118, loss_recon: 0.606464, loss_pred: 0.180439
iteration 1127: loss: 61.284584, loss_kl: 0.318861, loss_recon: 0.611541, loss_pred: 0.127336
iteration 1128: loss: 61.778439, loss_kl: 0.311878, loss_recon: 0.615595, loss_pred: 0.215863
iteration 1129: loss: 60.331703, loss_kl: 0.367269, loss_recon: 0.601808, loss_pred: 0.147271
iteration 1130: loss: 60.501369, loss_kl: 0.233849, loss_recon: 0.602807, loss_pred: 0.218358
 56%|███████████████▎           | 113/200 [1:33:09<1:11:12, 49.11s/it]iteration 1131: loss: 60.362732, loss_kl: 0.261692, loss_recon: 0.601793, loss_pred: 0.170443
iteration 1132: loss: 60.952499, loss_kl: 0.333483, loss_recon: 0.608018, loss_pred: 0.134146
iteration 1133: loss: 61.005981, loss_kl: 0.485261, loss_recon: 0.607908, loss_pred: 0.191151
iteration 1134: loss: 61.877243, loss_kl: 0.178205, loss_recon: 0.617695, loss_pred: 0.098858
iteration 1135: loss: 61.491814, loss_kl: 0.303962, loss_recon: 0.612793, loss_pred: 0.197429
iteration 1136: loss: 60.235691, loss_kl: 0.383065, loss_recon: 0.600867, loss_pred: 0.130029
iteration 1137: loss: 62.030113, loss_kl: 0.283716, loss_recon: 0.617834, loss_pred: 0.232678
iteration 1138: loss: 61.804344, loss_kl: 0.344459, loss_recon: 0.615460, loss_pred: 0.241308
iteration 1139: loss: 60.045094, loss_kl: 0.460576, loss_recon: 0.598835, loss_pred: 0.138702
iteration 1140: loss: 58.649124, loss_kl: 0.222235, loss_recon: 0.580744, loss_pred: 0.563686
 57%|███████████████▍           | 114/200 [1:33:58<1:10:17, 49.04s/it]iteration 1141: loss: 61.867195, loss_kl: 0.324836, loss_recon: 0.617062, loss_pred: 0.132012
iteration 1142: loss: 61.986134, loss_kl: 0.203735, loss_recon: 0.617289, loss_pred: 0.239092
iteration 1143: loss: 60.635715, loss_kl: 0.336864, loss_recon: 0.604366, loss_pred: 0.169084
iteration 1144: loss: 60.136768, loss_kl: 0.467446, loss_recon: 0.599519, loss_pred: 0.143177
iteration 1145: loss: 60.662689, loss_kl: 0.236042, loss_recon: 0.604253, loss_pred: 0.216295
iteration 1146: loss: 60.481129, loss_kl: 0.219753, loss_recon: 0.603567, loss_pred: 0.104781
iteration 1147: loss: 61.534931, loss_kl: 0.296048, loss_recon: 0.612673, loss_pred: 0.241201
iteration 1148: loss: 60.947998, loss_kl: 0.184647, loss_recon: 0.607322, loss_pred: 0.199289
iteration 1149: loss: 61.248348, loss_kl: 0.315852, loss_recon: 0.610504, loss_pred: 0.169737
iteration 1150: loss: 59.724949, loss_kl: 0.234021, loss_recon: 0.589904, loss_pred: 0.713711
 57%|███████████████▌           | 115/200 [1:34:47<1:09:28, 49.04s/it]iteration 1151: loss: 61.962715, loss_kl: 0.324379, loss_recon: 0.616452, loss_pred: 0.275733
iteration 1152: loss: 61.326885, loss_kl: 0.354982, loss_recon: 0.607246, loss_pred: 0.556564
iteration 1153: loss: 60.850605, loss_kl: 0.277763, loss_recon: 0.606516, loss_pred: 0.163264
iteration 1154: loss: 62.409332, loss_kl: 0.262109, loss_recon: 0.618173, loss_pred: 0.558299
iteration 1155: loss: 59.174107, loss_kl: 0.313411, loss_recon: 0.589052, loss_pred: 0.228582
iteration 1156: loss: 62.325237, loss_kl: 0.351172, loss_recon: 0.618531, loss_pred: 0.426865
iteration 1157: loss: 60.348721, loss_kl: 0.257433, loss_recon: 0.601134, loss_pred: 0.202186
iteration 1158: loss: 61.453999, loss_kl: 0.189105, loss_recon: 0.611726, loss_pred: 0.257039
iteration 1159: loss: 60.290134, loss_kl: 0.177998, loss_recon: 0.597412, loss_pred: 0.526017
iteration 1160: loss: 62.255550, loss_kl: 0.166154, loss_recon: 0.620562, loss_pred: 0.177907
 58%|███████████████▋           | 116/200 [1:35:37<1:09:01, 49.31s/it]iteration 1161: loss: 60.941143, loss_kl: 0.282829, loss_recon: 0.605261, loss_pred: 0.367398
iteration 1162: loss: 61.894501, loss_kl: 0.166486, loss_recon: 0.616390, loss_pred: 0.227433
iteration 1163: loss: 59.493206, loss_kl: 0.185939, loss_recon: 0.592222, loss_pred: 0.239709
iteration 1164: loss: 62.731632, loss_kl: 0.213030, loss_recon: 0.625240, loss_pred: 0.171733
iteration 1165: loss: 61.277027, loss_kl: 0.221606, loss_recon: 0.609939, loss_pred: 0.245857
iteration 1166: loss: 60.077972, loss_kl: 0.148868, loss_recon: 0.597613, loss_pred: 0.291579
iteration 1167: loss: 60.797363, loss_kl: 0.203032, loss_recon: 0.605083, loss_pred: 0.254891
iteration 1168: loss: 61.879524, loss_kl: 0.357987, loss_recon: 0.616688, loss_pred: 0.150431
iteration 1169: loss: 61.151470, loss_kl: 0.206724, loss_recon: 0.608913, loss_pred: 0.225308
iteration 1170: loss: 57.878990, loss_kl: 0.343791, loss_recon: 0.572731, loss_pred: 0.547975
 58%|███████████████▊           | 117/200 [1:36:26<1:08:04, 49.21s/it]iteration 1171: loss: 60.828220, loss_kl: 0.226469, loss_recon: 0.603314, loss_pred: 0.449749
iteration 1172: loss: 60.112091, loss_kl: 0.269641, loss_recon: 0.596685, loss_pred: 0.387497
iteration 1173: loss: 61.755947, loss_kl: 0.327662, loss_recon: 0.615399, loss_pred: 0.147880
iteration 1174: loss: 61.820694, loss_kl: 0.270245, loss_recon: 0.614239, loss_pred: 0.340568
iteration 1175: loss: 60.169628, loss_kl: 0.246577, loss_recon: 0.597665, loss_pred: 0.351844
iteration 1176: loss: 61.287548, loss_kl: 0.341479, loss_recon: 0.610481, loss_pred: 0.168458
iteration 1177: loss: 62.310947, loss_kl: 0.211729, loss_recon: 0.619586, loss_pred: 0.308328
iteration 1178: loss: 60.459797, loss_kl: 0.120346, loss_recon: 0.602951, loss_pred: 0.139679
iteration 1179: loss: 61.351906, loss_kl: 0.188319, loss_recon: 0.611305, loss_pred: 0.182194
iteration 1180: loss: 61.253746, loss_kl: 0.480134, loss_recon: 0.609774, loss_pred: 0.176504
 59%|███████████████▉           | 118/200 [1:37:15<1:07:09, 49.14s/it]iteration 1181: loss: 60.613548, loss_kl: 0.263473, loss_recon: 0.604243, loss_pred: 0.123972
iteration 1182: loss: 60.763828, loss_kl: 0.056446, loss_recon: 0.606079, loss_pred: 0.141995
iteration 1183: loss: 61.087822, loss_kl: 0.190781, loss_recon: 0.609037, loss_pred: 0.136931
iteration 1184: loss: 61.190346, loss_kl: 0.098938, loss_recon: 0.609479, loss_pred: 0.217988
iteration 1185: loss: 60.750805, loss_kl: 0.213952, loss_recon: 0.604998, loss_pred: 0.198049
iteration 1186: loss: 60.821075, loss_kl: 0.274163, loss_recon: 0.606503, loss_pred: 0.102857
iteration 1187: loss: 60.880791, loss_kl: 0.189662, loss_recon: 0.606796, loss_pred: 0.154270
iteration 1188: loss: 60.955372, loss_kl: 0.273423, loss_recon: 0.607057, loss_pred: 0.181993
iteration 1189: loss: 61.979023, loss_kl: 0.261722, loss_recon: 0.617570, loss_pred: 0.157258
iteration 1190: loss: 60.755035, loss_kl: 0.166418, loss_recon: 0.602933, loss_pred: 0.420556
 60%|████████████████           | 119/200 [1:38:04<1:06:24, 49.19s/it]iteration 1191: loss: 61.321453, loss_kl: 0.349548, loss_recon: 0.610985, loss_pred: 0.122573
iteration 1192: loss: 62.075188, loss_kl: 0.246272, loss_recon: 0.617639, loss_pred: 0.240594
iteration 1193: loss: 59.422287, loss_kl: 0.413489, loss_recon: 0.590590, loss_pred: 0.244572
iteration 1194: loss: 61.485222, loss_kl: 0.254562, loss_recon: 0.612312, loss_pred: 0.180945
iteration 1195: loss: 60.843979, loss_kl: 0.276282, loss_recon: 0.605426, loss_pred: 0.221993
iteration 1196: loss: 61.431625, loss_kl: 0.169718, loss_recon: 0.612639, loss_pred: 0.118991
iteration 1197: loss: 59.869762, loss_kl: 0.194841, loss_recon: 0.595863, loss_pred: 0.227555
iteration 1198: loss: 60.206036, loss_kl: 0.239166, loss_recon: 0.599393, loss_pred: 0.198092
iteration 1199: loss: 62.393993, loss_kl: 0.358864, loss_recon: 0.621643, loss_pred: 0.126604
iteration 1200: loss: 62.282772, loss_kl: 0.266509, loss_recon: 0.619759, loss_pred: 0.230339
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_119.pth
 60%|████████████████▏          | 120/200 [1:38:54<1:05:43, 49.29s/it]iteration 1201: loss: 60.723934, loss_kl: 0.274081, loss_recon: 0.604633, loss_pred: 0.171030
iteration 1202: loss: 60.363571, loss_kl: 0.383533, loss_recon: 0.600672, loss_pred: 0.171047
iteration 1203: loss: 60.426186, loss_kl: 0.193026, loss_recon: 0.602050, loss_pred: 0.158141
iteration 1204: loss: 60.156193, loss_kl: 0.141068, loss_recon: 0.599761, loss_pred: 0.134035
iteration 1205: loss: 60.058842, loss_kl: 0.214991, loss_recon: 0.598439, loss_pred: 0.144660
iteration 1206: loss: 61.412125, loss_kl: 0.145534, loss_recon: 0.612060, loss_pred: 0.158542
iteration 1207: loss: 61.491493, loss_kl: 0.199280, loss_recon: 0.613057, loss_pred: 0.120691
iteration 1208: loss: 61.743435, loss_kl: 0.163452, loss_recon: 0.615357, loss_pred: 0.154289
iteration 1209: loss: 61.872253, loss_kl: 0.087905, loss_recon: 0.616940, loss_pred: 0.149572
iteration 1210: loss: 62.804623, loss_kl: 0.073298, loss_recon: 0.626604, loss_pred: 0.120242
 60%|████████████████▎          | 121/200 [1:39:44<1:05:09, 49.49s/it]iteration 1211: loss: 61.306923, loss_kl: 0.084706, loss_recon: 0.611174, loss_pred: 0.158494
iteration 1212: loss: 60.400402, loss_kl: 0.185971, loss_recon: 0.601706, loss_pred: 0.161633
iteration 1213: loss: 59.783180, loss_kl: 0.270323, loss_recon: 0.595550, loss_pred: 0.129123
iteration 1214: loss: 61.037582, loss_kl: 0.169497, loss_recon: 0.608486, loss_pred: 0.126829
iteration 1215: loss: 60.246033, loss_kl: 0.209731, loss_recon: 0.599939, loss_pred: 0.175298
iteration 1216: loss: 61.019417, loss_kl: 0.257482, loss_recon: 0.608030, loss_pred: 0.122035
iteration 1217: loss: 62.343529, loss_kl: 0.150971, loss_recon: 0.621802, loss_pred: 0.108012
iteration 1218: loss: 61.414082, loss_kl: 0.251006, loss_recon: 0.611672, loss_pred: 0.154925
iteration 1219: loss: 60.616646, loss_kl: -0.047177, loss_recon: 0.605111, loss_pred: 0.122847
iteration 1220: loss: 64.303406, loss_kl: 0.090745, loss_recon: 0.640642, loss_pred: 0.205950
 61%|████████████████▍          | 122/200 [1:40:33<1:04:09, 49.35s/it]iteration 1221: loss: 61.106941, loss_kl: 0.197549, loss_recon: 0.608910, loss_pred: 0.135716
iteration 1222: loss: 60.988449, loss_kl: 0.144055, loss_recon: 0.608063, loss_pred: 0.123625
iteration 1223: loss: 62.042919, loss_kl: 0.221786, loss_recon: 0.618173, loss_pred: 0.135590
iteration 1224: loss: 60.182121, loss_kl: 0.198944, loss_recon: 0.599240, loss_pred: 0.177359
iteration 1225: loss: 59.237148, loss_kl: 0.253234, loss_recon: 0.589728, loss_pred: 0.161540
iteration 1226: loss: 62.250423, loss_kl: 0.243129, loss_recon: 0.620025, loss_pred: 0.149215
iteration 1227: loss: 61.649689, loss_kl: 0.173629, loss_recon: 0.614678, loss_pred: 0.111401
iteration 1228: loss: 59.527851, loss_kl: 0.253377, loss_recon: 0.592769, loss_pred: 0.148061
iteration 1229: loss: 61.230953, loss_kl: 0.157312, loss_recon: 0.610419, loss_pred: 0.125147
iteration 1230: loss: 62.127953, loss_kl: 0.236742, loss_recon: 0.618788, loss_pred: 0.153014
 62%|████████████████▌          | 123/200 [1:41:22<1:03:10, 49.23s/it]iteration 1231: loss: 62.382282, loss_kl: 0.133891, loss_recon: 0.621127, loss_pred: 0.209958
iteration 1232: loss: 60.530697, loss_kl: 0.177613, loss_recon: 0.602984, loss_pred: 0.153172
iteration 1233: loss: 60.903358, loss_kl: 0.184978, loss_recon: 0.606475, loss_pred: 0.173401
iteration 1234: loss: 60.262119, loss_kl: 0.284992, loss_recon: 0.599162, loss_pred: 0.218971
iteration 1235: loss: 61.892262, loss_kl: 0.226008, loss_recon: 0.616764, loss_pred: 0.115113
iteration 1236: loss: 60.634468, loss_kl: 0.279173, loss_recon: 0.603150, loss_pred: 0.195097
iteration 1237: loss: 61.312489, loss_kl: 0.102925, loss_recon: 0.611643, loss_pred: 0.102360
iteration 1238: loss: 60.193012, loss_kl: 0.139268, loss_recon: 0.599905, loss_pred: 0.140423
iteration 1239: loss: 60.870888, loss_kl: 0.088844, loss_recon: 0.606832, loss_pred: 0.148089
iteration 1240: loss: 61.998760, loss_kl: 0.227835, loss_recon: 0.616865, loss_pred: 0.210784
 62%|████████████████▋          | 124/200 [1:42:11<1:02:22, 49.24s/it]iteration 1241: loss: 62.443657, loss_kl: 0.119164, loss_recon: 0.622656, loss_pred: 0.120188
iteration 1242: loss: 60.135941, loss_kl: 0.150958, loss_recon: 0.597919, loss_pred: 0.270748
iteration 1243: loss: 60.687271, loss_kl: 0.177591, loss_recon: 0.604529, loss_pred: 0.148178
iteration 1244: loss: 60.784393, loss_kl: 0.182947, loss_recon: 0.605667, loss_pred: 0.128960
iteration 1245: loss: 60.517868, loss_kl: 0.281932, loss_recon: 0.602257, loss_pred: 0.155361
iteration 1246: loss: 61.188515, loss_kl: 0.205990, loss_recon: 0.609568, loss_pred: 0.131814
iteration 1247: loss: 61.413269, loss_kl: 0.206813, loss_recon: 0.611779, loss_pred: 0.134982
iteration 1248: loss: 60.966888, loss_kl: 0.142088, loss_recon: 0.607465, loss_pred: 0.151432
iteration 1249: loss: 61.224194, loss_kl: 0.123277, loss_recon: 0.610206, loss_pred: 0.143739
iteration 1250: loss: 59.957359, loss_kl: 0.151070, loss_recon: 0.596968, loss_pred: 0.187308
 62%|████████████████▉          | 125/200 [1:43:00<1:01:31, 49.22s/it]iteration 1251: loss: 60.642010, loss_kl: 0.021088, loss_recon: 0.605012, loss_pred: 0.129783
iteration 1252: loss: 59.774208, loss_kl: 0.256997, loss_recon: 0.594826, loss_pred: 0.156751
iteration 1253: loss: 61.951149, loss_kl: 0.170975, loss_recon: 0.617227, loss_pred: 0.138751
iteration 1254: loss: 60.553116, loss_kl: 0.232305, loss_recon: 0.602986, loss_pred: 0.132575
iteration 1255: loss: 60.171040, loss_kl: 0.204955, loss_recon: 0.598648, loss_pred: 0.198728
iteration 1256: loss: 60.890076, loss_kl: 0.183334, loss_recon: 0.606006, loss_pred: 0.193294
iteration 1257: loss: 62.117657, loss_kl: 0.178086, loss_recon: 0.617933, loss_pred: 0.230849
iteration 1258: loss: 61.125065, loss_kl: -0.025862, loss_recon: 0.610249, loss_pred: 0.113721
iteration 1259: loss: 61.563469, loss_kl: 0.143956, loss_recon: 0.612867, loss_pred: 0.201243
iteration 1260: loss: 61.227531, loss_kl: 0.226759, loss_recon: 0.608657, loss_pred: 0.242796
 63%|█████████████████          | 126/200 [1:43:51<1:01:05, 49.54s/it]iteration 1261: loss: 59.609741, loss_kl: 0.158172, loss_recon: 0.593818, loss_pred: 0.138657
iteration 1262: loss: 61.018379, loss_kl: 0.248269, loss_recon: 0.607281, loss_pred: 0.150167
iteration 1263: loss: 60.461105, loss_kl: 0.217035, loss_recon: 0.601797, loss_pred: 0.158956
iteration 1264: loss: 61.727139, loss_kl: 0.079646, loss_recon: 0.614992, loss_pred: 0.183015
iteration 1265: loss: 61.762814, loss_kl: 0.147370, loss_recon: 0.615791, loss_pred: 0.100530
iteration 1266: loss: 61.273571, loss_kl: 0.272816, loss_recon: 0.609975, loss_pred: 0.122120
iteration 1267: loss: 60.600414, loss_kl: 0.187634, loss_recon: 0.603906, loss_pred: 0.103939
iteration 1268: loss: 61.620258, loss_kl: 0.137472, loss_recon: 0.613957, loss_pred: 0.146947
iteration 1269: loss: 61.580273, loss_kl: 0.036739, loss_recon: 0.614193, loss_pred: 0.140280
iteration 1270: loss: 59.194012, loss_kl: 0.057048, loss_recon: 0.590183, loss_pred: 0.143499
 64%|█████████████████▏         | 127/200 [1:44:41<1:00:37, 49.83s/it]iteration 1271: loss: 61.587433, loss_kl: 0.333738, loss_recon: 0.612579, loss_pred: 0.127953
iteration 1272: loss: 60.644024, loss_kl: 0.081965, loss_recon: 0.604857, loss_pred: 0.108825
iteration 1273: loss: 62.177376, loss_kl: 0.141288, loss_recon: 0.619079, loss_pred: 0.184111
iteration 1274: loss: 60.792210, loss_kl: 0.060906, loss_recon: 0.605833, loss_pred: 0.172149
iteration 1275: loss: 60.628166, loss_kl: 0.181636, loss_recon: 0.603841, loss_pred: 0.134382
iteration 1276: loss: 59.850048, loss_kl: 0.183074, loss_recon: 0.596020, loss_pred: 0.137488
iteration 1277: loss: 61.894352, loss_kl: 0.149799, loss_recon: 0.616576, loss_pred: 0.146304
iteration 1278: loss: 60.937420, loss_kl: 0.178983, loss_recon: 0.607036, loss_pred: 0.125761
iteration 1279: loss: 60.914371, loss_kl: 0.241793, loss_recon: 0.606095, loss_pred: 0.158821
iteration 1280: loss: 59.185150, loss_kl: 0.161590, loss_recon: 0.589316, loss_pred: 0.155967
 64%|██████████████████▌          | 128/200 [1:45:30<59:30, 49.59s/it]iteration 1281: loss: 61.038483, loss_kl: 0.134980, loss_recon: 0.607801, loss_pred: 0.171538
iteration 1282: loss: 60.776134, loss_kl: 0.254353, loss_recon: 0.604904, loss_pred: 0.122024
iteration 1283: loss: 60.997196, loss_kl: 0.117730, loss_recon: 0.607979, loss_pred: 0.123495
iteration 1284: loss: 61.084270, loss_kl: 0.212417, loss_recon: 0.608241, loss_pred: 0.123470
iteration 1285: loss: 61.291149, loss_kl: 0.160551, loss_recon: 0.610134, loss_pred: 0.174372
iteration 1286: loss: 61.568890, loss_kl: 0.060743, loss_recon: 0.613957, loss_pred: 0.134145
iteration 1287: loss: 59.792263, loss_kl: 0.063450, loss_recon: 0.595965, loss_pred: 0.154899
iteration 1288: loss: 61.246555, loss_kl: 0.150736, loss_recon: 0.610433, loss_pred: 0.106270
iteration 1289: loss: 61.100948, loss_kl: 0.125932, loss_recon: 0.608492, loss_pred: 0.170730
iteration 1290: loss: 60.347580, loss_kl: 0.009575, loss_recon: 0.602459, loss_pred: 0.095522
 64%|██████████████████▋          | 129/200 [1:46:19<58:23, 49.34s/it]iteration 1291: loss: 61.052433, loss_kl: 0.189749, loss_recon: 0.607339, loss_pred: 0.188921
iteration 1292: loss: 61.053757, loss_kl: 0.267957, loss_recon: 0.607051, loss_pred: 0.165592
iteration 1293: loss: 62.118710, loss_kl: 0.164139, loss_recon: 0.618887, loss_pred: 0.117903
iteration 1294: loss: 60.849339, loss_kl: 0.050781, loss_recon: 0.606940, loss_pred: 0.120658
iteration 1295: loss: 60.432003, loss_kl: 0.183600, loss_recon: 0.601438, loss_pred: 0.162760
iteration 1296: loss: 60.993446, loss_kl: 0.164518, loss_recon: 0.607447, loss_pred: 0.136359
iteration 1297: loss: 60.683144, loss_kl: 0.054325, loss_recon: 0.605020, loss_pred: 0.144080
iteration 1298: loss: 61.033199, loss_kl: 0.037198, loss_recon: 0.608598, loss_pred: 0.147951
iteration 1299: loss: 60.149826, loss_kl: 0.182045, loss_recon: 0.599144, loss_pred: 0.111024
iteration 1300: loss: 61.723309, loss_kl: 0.195585, loss_recon: 0.614264, loss_pred: 0.163286
 65%|██████████████████▊          | 130/200 [1:47:09<57:47, 49.54s/it]iteration 1301: loss: 62.176361, loss_kl: 0.215946, loss_recon: 0.618905, loss_pred: 0.129790
iteration 1302: loss: 60.606152, loss_kl: 0.225067, loss_recon: 0.603261, loss_pred: 0.117341
iteration 1303: loss: 61.147892, loss_kl: 0.208129, loss_recon: 0.608746, loss_pred: 0.122896
iteration 1304: loss: 60.875252, loss_kl: 0.163504, loss_recon: 0.605875, loss_pred: 0.169577
iteration 1305: loss: 61.567474, loss_kl: 0.237880, loss_recon: 0.612774, loss_pred: 0.118152
iteration 1306: loss: 60.942883, loss_kl: 0.140580, loss_recon: 0.606887, loss_pred: 0.152614
iteration 1307: loss: 59.775513, loss_kl: 0.122697, loss_recon: 0.595411, loss_pred: 0.145720
iteration 1308: loss: 60.940746, loss_kl: 0.195664, loss_recon: 0.606638, loss_pred: 0.135516
iteration 1309: loss: 61.165695, loss_kl: 0.107217, loss_recon: 0.609066, loss_pred: 0.181633
iteration 1310: loss: 59.806881, loss_kl: 0.091205, loss_recon: 0.595454, loss_pred: 0.195612
 66%|██████████████████▉          | 131/200 [1:47:59<57:04, 49.63s/it]iteration 1311: loss: 61.262302, loss_kl: 0.169655, loss_recon: 0.609536, loss_pred: 0.179334
iteration 1312: loss: 60.061066, loss_kl: 0.177906, loss_recon: 0.598207, loss_pred: 0.104685
iteration 1313: loss: 61.003464, loss_kl: 0.336467, loss_recon: 0.606031, loss_pred: 0.143888
iteration 1314: loss: 61.388466, loss_kl: 0.173308, loss_recon: 0.611497, loss_pred: 0.106656
iteration 1315: loss: 61.021431, loss_kl: 0.149351, loss_recon: 0.607115, loss_pred: 0.196023
iteration 1316: loss: 60.807491, loss_kl: 0.174748, loss_recon: 0.605453, loss_pred: 0.129002
iteration 1317: loss: 61.857578, loss_kl: 0.247007, loss_recon: 0.615612, loss_pred: 0.108055
iteration 1318: loss: 60.701035, loss_kl: 0.102246, loss_recon: 0.604866, loss_pred: 0.136486
iteration 1319: loss: 60.392700, loss_kl: 0.163741, loss_recon: 0.601289, loss_pred: 0.138934
iteration 1320: loss: 61.987995, loss_kl: -0.090960, loss_recon: 0.618674, loss_pred: 0.189986
 66%|███████████████████▏         | 132/200 [1:48:48<55:58, 49.39s/it]iteration 1321: loss: 61.009239, loss_kl: 0.207344, loss_recon: 0.607140, loss_pred: 0.128946
iteration 1322: loss: 61.425114, loss_kl: 0.098491, loss_recon: 0.611844, loss_pred: 0.161771
iteration 1323: loss: 60.526844, loss_kl: 0.172449, loss_recon: 0.602638, loss_pred: 0.124761
iteration 1324: loss: 60.507710, loss_kl: 0.152913, loss_recon: 0.602307, loss_pred: 0.154385
iteration 1325: loss: 59.204704, loss_kl: 0.189898, loss_recon: 0.589442, loss_pred: 0.108202
iteration 1326: loss: 60.136238, loss_kl: 0.095239, loss_recon: 0.598320, loss_pred: 0.227822
iteration 1327: loss: 61.130405, loss_kl: 0.154801, loss_recon: 0.608941, loss_pred: 0.112161
iteration 1328: loss: 61.704498, loss_kl: 0.117521, loss_recon: 0.614068, loss_pred: 0.203472
iteration 1329: loss: 62.492455, loss_kl: 0.130015, loss_recon: 0.622462, loss_pred: 0.141999
iteration 1330: loss: 63.930725, loss_kl: 0.087665, loss_recon: 0.637070, loss_pred: 0.153373
 66%|███████████████████▎         | 133/200 [1:49:37<55:06, 49.35s/it]iteration 1331: loss: 59.491638, loss_kl: 0.042360, loss_recon: 0.593194, loss_pred: 0.136572
iteration 1332: loss: 61.199341, loss_kl: 0.148126, loss_recon: 0.609322, loss_pred: 0.142446
iteration 1333: loss: 60.449902, loss_kl: 0.124336, loss_recon: 0.601769, loss_pred: 0.168326
iteration 1334: loss: 61.268642, loss_kl: 0.181919, loss_recon: 0.609812, loss_pred: 0.134330
iteration 1335: loss: 61.244053, loss_kl: 0.105497, loss_recon: 0.610200, loss_pred: 0.135297
iteration 1336: loss: 60.567364, loss_kl: 0.224913, loss_recon: 0.602051, loss_pred: 0.173025
iteration 1337: loss: 61.715786, loss_kl: 0.205351, loss_recon: 0.614195, loss_pred: 0.123463
iteration 1338: loss: 61.168770, loss_kl: 0.049698, loss_recon: 0.609706, loss_pred: 0.156387
iteration 1339: loss: 61.128613, loss_kl: 0.193495, loss_recon: 0.607973, loss_pred: 0.168451
iteration 1340: loss: 63.793297, loss_kl: 0.025679, loss_recon: 0.636526, loss_pred: 0.119052
 67%|███████████████████▍         | 134/200 [1:50:26<54:07, 49.20s/it]iteration 1341: loss: 61.907806, loss_kl: 0.206370, loss_recon: 0.615931, loss_pred: 0.132822
iteration 1342: loss: 60.662949, loss_kl: 0.125650, loss_recon: 0.604402, loss_pred: 0.112007
iteration 1343: loss: 60.843975, loss_kl: 0.200216, loss_recon: 0.605279, loss_pred: 0.139638
iteration 1344: loss: 61.105331, loss_kl: 0.193359, loss_recon: 0.608083, loss_pred: 0.126616
iteration 1345: loss: 61.166225, loss_kl: 0.102619, loss_recon: 0.609778, loss_pred: 0.097990
iteration 1346: loss: 60.657127, loss_kl: 0.108491, loss_recon: 0.604172, loss_pred: 0.144367
iteration 1347: loss: 60.847130, loss_kl: 0.154022, loss_recon: 0.606082, loss_pred: 0.103199
iteration 1348: loss: 60.562279, loss_kl: 0.167484, loss_recon: 0.602643, loss_pred: 0.150379
iteration 1349: loss: 60.826714, loss_kl: 0.204478, loss_recon: 0.605171, loss_pred: 0.129441
iteration 1350: loss: 61.308300, loss_kl: 0.281945, loss_recon: 0.609108, loss_pred: 0.149061
 68%|███████████████████▌         | 135/200 [1:51:15<53:29, 49.37s/it]iteration 1351: loss: 63.596207, loss_kl: 0.139201, loss_recon: 0.633656, loss_pred: 0.102453
iteration 1352: loss: 61.077175, loss_kl: 0.157961, loss_recon: 0.608219, loss_pred: 0.109855
iteration 1353: loss: 60.543030, loss_kl: 0.181353, loss_recon: 0.602342, loss_pred: 0.141842
iteration 1354: loss: 60.558765, loss_kl: 0.190452, loss_recon: 0.602669, loss_pred: 0.116455
iteration 1355: loss: 59.988186, loss_kl: 0.105273, loss_recon: 0.597591, loss_pred: 0.132158
iteration 1356: loss: 60.813007, loss_kl: 0.176688, loss_recon: 0.605349, loss_pred: 0.115373
iteration 1357: loss: 61.284016, loss_kl: 0.066580, loss_recon: 0.610683, loss_pred: 0.154376
iteration 1358: loss: 61.083191, loss_kl: 0.140138, loss_recon: 0.608471, loss_pred: 0.107092
iteration 1359: loss: 59.294468, loss_kl: 0.067856, loss_recon: 0.591237, loss_pred: 0.108321
iteration 1360: loss: 61.968090, loss_kl: -0.002407, loss_recon: 0.618574, loss_pred: 0.112914
 68%|███████████████████▋         | 136/200 [1:52:05<52:40, 49.38s/it]iteration 1361: loss: 60.382729, loss_kl: 0.134815, loss_recon: 0.601152, loss_pred: 0.138044
iteration 1362: loss: 60.668396, loss_kl: 0.057086, loss_recon: 0.604682, loss_pred: 0.145340
iteration 1363: loss: 61.780704, loss_kl: 0.196195, loss_recon: 0.614828, loss_pred: 0.109521
iteration 1364: loss: 60.935612, loss_kl: 0.189976, loss_recon: 0.605924, loss_pred: 0.160762
iteration 1365: loss: 60.708370, loss_kl: 0.069878, loss_recon: 0.604488, loss_pred: 0.192483
iteration 1366: loss: 61.222397, loss_kl: 0.120718, loss_recon: 0.609527, loss_pred: 0.153731
iteration 1367: loss: 61.248756, loss_kl: 0.166326, loss_recon: 0.609690, loss_pred: 0.120056
iteration 1368: loss: 61.154408, loss_kl: 0.101519, loss_recon: 0.609053, loss_pred: 0.151587
iteration 1369: loss: 61.215122, loss_kl: 0.062843, loss_recon: 0.610218, loss_pred: 0.132944
iteration 1370: loss: 60.925709, loss_kl: 0.088341, loss_recon: 0.606806, loss_pred: 0.160262
 68%|███████████████████▊         | 137/200 [1:52:54<51:50, 49.38s/it]iteration 1371: loss: 61.628849, loss_kl: 0.002364, loss_recon: 0.614954, loss_pred: 0.131043
iteration 1372: loss: 60.792686, loss_kl: 0.156389, loss_recon: 0.604996, loss_pred: 0.136675
iteration 1373: loss: 60.419277, loss_kl: 0.216705, loss_recon: 0.600125, loss_pred: 0.190074
iteration 1374: loss: 59.972900, loss_kl: 0.146246, loss_recon: 0.596413, loss_pred: 0.185362
iteration 1375: loss: 61.943844, loss_kl: 0.119607, loss_recon: 0.616212, loss_pred: 0.203013
iteration 1376: loss: 61.482323, loss_kl: 0.248147, loss_recon: 0.611084, loss_pred: 0.125802
iteration 1377: loss: 60.587749, loss_kl: 0.240571, loss_recon: 0.601234, loss_pred: 0.223788
iteration 1378: loss: 61.403187, loss_kl: 0.064801, loss_recon: 0.611716, loss_pred: 0.166779
iteration 1379: loss: 61.461754, loss_kl: 0.062017, loss_recon: 0.612229, loss_pred: 0.176804
iteration 1380: loss: 59.692303, loss_kl: 0.031438, loss_recon: 0.591694, loss_pred: 0.491486
 69%|████████████████████         | 138/200 [1:53:44<51:14, 49.59s/it]iteration 1381: loss: 62.002697, loss_kl: 0.145784, loss_recon: 0.613346, loss_pred: 0.522367
iteration 1382: loss: 61.651848, loss_kl: 0.111551, loss_recon: 0.611534, loss_pred: 0.386894
iteration 1383: loss: 59.701767, loss_kl: 0.169980, loss_recon: 0.593179, loss_pred: 0.213849
iteration 1384: loss: 62.348263, loss_kl: 0.167479, loss_recon: 0.616697, loss_pred: 0.511042
iteration 1385: loss: 61.299854, loss_kl: 0.181073, loss_recon: 0.609300, loss_pred: 0.188750
iteration 1386: loss: 61.912533, loss_kl: 0.133774, loss_recon: 0.613882, loss_pred: 0.390509
iteration 1387: loss: 60.548782, loss_kl: 0.096589, loss_recon: 0.602763, loss_pred: 0.175917
iteration 1388: loss: 59.685108, loss_kl: 0.103335, loss_recon: 0.593193, loss_pred: 0.262508
iteration 1389: loss: 62.424538, loss_kl: 0.156160, loss_recon: 0.618532, loss_pred: 0.415214
iteration 1390: loss: 60.419930, loss_kl: 0.057423, loss_recon: 0.600496, loss_pred: 0.312949
 70%|████████████████████▏        | 139/200 [1:54:33<50:17, 49.46s/it]iteration 1391: loss: 60.776722, loss_kl: 0.169896, loss_recon: 0.602271, loss_pred: 0.379719
iteration 1392: loss: 60.369934, loss_kl: 0.059346, loss_recon: 0.601341, loss_pred: 0.176530
iteration 1393: loss: 61.353264, loss_kl: 0.074519, loss_recon: 0.610048, loss_pred: 0.273992
iteration 1394: loss: 61.828861, loss_kl: 0.051514, loss_recon: 0.615268, loss_pred: 0.250580
iteration 1395: loss: 61.211559, loss_kl: 0.143857, loss_recon: 0.609495, loss_pred: 0.118199
iteration 1396: loss: 61.382717, loss_kl: 0.152697, loss_recon: 0.610012, loss_pred: 0.228856
iteration 1397: loss: 60.847073, loss_kl: 0.128325, loss_recon: 0.603896, loss_pred: 0.329192
iteration 1398: loss: 61.198025, loss_kl: 0.106873, loss_recon: 0.609704, loss_pred: 0.120729
iteration 1399: loss: 60.511093, loss_kl: 0.235193, loss_recon: 0.599837, loss_pred: 0.292180
iteration 1400: loss: 62.901695, loss_kl: 0.262468, loss_recon: 0.624079, loss_pred: 0.231362
 70%|████████████████████▎        | 140/200 [1:55:23<49:21, 49.36s/it]iteration 1401: loss: 62.402508, loss_kl: 0.115934, loss_recon: 0.620257, loss_pred: 0.260892
iteration 1402: loss: 62.378117, loss_kl: 0.165189, loss_recon: 0.619905, loss_pred: 0.222477
iteration 1403: loss: 58.967037, loss_kl: 0.190626, loss_recon: 0.585866, loss_pred: 0.189776
iteration 1404: loss: 61.842903, loss_kl: 0.006552, loss_recon: 0.615525, loss_pred: 0.283827
iteration 1405: loss: 60.664230, loss_kl: 0.132470, loss_recon: 0.603789, loss_pred: 0.152863
iteration 1406: loss: 59.903576, loss_kl: 0.114308, loss_recon: 0.595818, loss_pred: 0.207472
iteration 1407: loss: 61.546204, loss_kl: 0.122924, loss_recon: 0.612148, loss_pred: 0.208441
iteration 1408: loss: 61.512081, loss_kl: 0.036230, loss_recon: 0.613054, loss_pred: 0.170458
iteration 1409: loss: 60.869961, loss_kl: 0.207415, loss_recon: 0.604814, loss_pred: 0.181143
iteration 1410: loss: 60.837181, loss_kl: 0.174883, loss_recon: 0.604965, loss_pred: 0.165772
 70%|████████████████████▍        | 141/200 [1:56:12<48:37, 49.44s/it]iteration 1411: loss: 62.252804, loss_kl: 0.095029, loss_recon: 0.619903, loss_pred: 0.167482
iteration 1412: loss: 60.416401, loss_kl: 0.078824, loss_recon: 0.601296, loss_pred: 0.207929
iteration 1413: loss: 61.213055, loss_kl: 0.058902, loss_recon: 0.610240, loss_pred: 0.130144
iteration 1414: loss: 61.111885, loss_kl: 0.133209, loss_recon: 0.607687, loss_pred: 0.209955
iteration 1415: loss: 60.174305, loss_kl: 0.120105, loss_recon: 0.598848, loss_pred: 0.169358
iteration 1416: loss: 61.366394, loss_kl: 0.129763, loss_recon: 0.610787, loss_pred: 0.157912
iteration 1417: loss: 60.381451, loss_kl: 0.116268, loss_recon: 0.598672, loss_pred: 0.398014
iteration 1418: loss: 60.845303, loss_kl: 0.100360, loss_recon: 0.605830, loss_pred: 0.161916
iteration 1419: loss: 61.236504, loss_kl: 0.100801, loss_recon: 0.609512, loss_pred: 0.184524
iteration 1420: loss: 61.362961, loss_kl: 0.192604, loss_recon: 0.610333, loss_pred: 0.137084
 71%|████████████████████▌        | 142/200 [1:57:02<47:51, 49.50s/it]iteration 1421: loss: 59.958675, loss_kl: 0.005744, loss_recon: 0.598003, loss_pred: 0.152627
iteration 1422: loss: 60.265373, loss_kl: 0.119704, loss_recon: 0.599383, loss_pred: 0.207320
iteration 1423: loss: 61.048820, loss_kl: 0.132840, loss_recon: 0.607661, loss_pred: 0.149886
iteration 1424: loss: 60.368320, loss_kl: 0.088122, loss_recon: 0.601129, loss_pred: 0.167341
iteration 1425: loss: 62.619221, loss_kl: 0.029544, loss_recon: 0.624027, loss_pred: 0.186944
iteration 1426: loss: 60.947781, loss_kl: 0.037510, loss_recon: 0.607606, loss_pred: 0.149679
iteration 1427: loss: 61.180595, loss_kl: 0.098079, loss_recon: 0.609066, loss_pred: 0.175884
iteration 1428: loss: 60.360023, loss_kl: 0.130792, loss_recon: 0.601201, loss_pred: 0.109146
iteration 1429: loss: 62.016319, loss_kl: 0.084139, loss_recon: 0.617690, loss_pred: 0.163211
iteration 1430: loss: 61.649349, loss_kl: 0.112850, loss_recon: 0.613425, loss_pred: 0.193993
 72%|████████████████████▋        | 143/200 [1:57:51<46:57, 49.44s/it]iteration 1431: loss: 60.890507, loss_kl: 0.117211, loss_recon: 0.606572, loss_pred: 0.116129
iteration 1432: loss: 61.581512, loss_kl: 0.110412, loss_recon: 0.612954, loss_pred: 0.175692
iteration 1433: loss: 62.365509, loss_kl: 0.120772, loss_recon: 0.620937, loss_pred: 0.151031
iteration 1434: loss: 62.329613, loss_kl: 0.213774, loss_recon: 0.619669, loss_pred: 0.148919
iteration 1435: loss: 60.764774, loss_kl: 0.166733, loss_recon: 0.604624, loss_pred: 0.135627
iteration 1436: loss: 61.077267, loss_kl: 0.152004, loss_recon: 0.606937, loss_pred: 0.231528
iteration 1437: loss: 62.093159, loss_kl: 0.058113, loss_recon: 0.619138, loss_pred: 0.121197
iteration 1438: loss: 59.510811, loss_kl: 0.223536, loss_recon: 0.590294, loss_pred: 0.257845
iteration 1439: loss: 59.792908, loss_kl: 0.128507, loss_recon: 0.594647, loss_pred: 0.199723
iteration 1440: loss: 59.445343, loss_kl: 0.064529, loss_recon: 0.592349, loss_pred: 0.145878
 72%|████████████████████▉        | 144/200 [1:58:41<46:20, 49.65s/it]iteration 1441: loss: 62.661663, loss_kl: 0.084800, loss_recon: 0.623725, loss_pred: 0.204405
iteration 1442: loss: 61.948380, loss_kl: 0.211668, loss_recon: 0.616034, loss_pred: 0.133311
iteration 1443: loss: 60.960533, loss_kl: 0.212804, loss_recon: 0.605284, loss_pred: 0.219301
iteration 1444: loss: 59.395420, loss_kl: 0.054107, loss_recon: 0.591956, loss_pred: 0.145722
iteration 1445: loss: 60.928310, loss_kl: 0.063832, loss_recon: 0.606730, loss_pred: 0.191495
iteration 1446: loss: 60.864571, loss_kl: 0.084750, loss_recon: 0.606479, loss_pred: 0.131921
iteration 1447: loss: 61.478634, loss_kl: 0.164230, loss_recon: 0.611646, loss_pred: 0.149771
iteration 1448: loss: 61.012852, loss_kl: 0.078695, loss_recon: 0.606319, loss_pred: 0.302291
iteration 1449: loss: 60.356445, loss_kl: 0.168660, loss_recon: 0.600445, loss_pred: 0.143286
iteration 1450: loss: 60.468555, loss_kl: -0.063678, loss_recon: 0.603370, loss_pred: 0.195210
 72%|█████████████████████        | 145/200 [1:59:32<45:48, 49.96s/it]iteration 1451: loss: 61.122395, loss_kl: 0.180244, loss_recon: 0.608051, loss_pred: 0.137092
iteration 1452: loss: 60.927876, loss_kl: 0.235425, loss_recon: 0.605448, loss_pred: 0.147611
iteration 1453: loss: 61.717232, loss_kl: 0.076752, loss_recon: 0.614360, loss_pred: 0.204454
iteration 1454: loss: 60.053051, loss_kl: 0.135160, loss_recon: 0.597448, loss_pred: 0.173050
iteration 1455: loss: 60.419598, loss_kl: 0.111862, loss_recon: 0.601328, loss_pred: 0.174914
iteration 1456: loss: 62.444332, loss_kl: 0.091877, loss_recon: 0.622207, loss_pred: 0.131787
iteration 1457: loss: 60.943848, loss_kl: 0.066280, loss_recon: 0.607109, loss_pred: 0.166667
iteration 1458: loss: 60.516365, loss_kl: 0.035236, loss_recon: 0.603365, loss_pred: 0.144635
iteration 1459: loss: 60.982361, loss_kl: 0.102894, loss_recon: 0.607253, loss_pred: 0.154142
iteration 1460: loss: 60.318878, loss_kl: -0.023727, loss_recon: 0.601563, loss_pred: 0.186261
 73%|█████████████████████▏       | 146/200 [2:00:21<44:48, 49.78s/it]iteration 1461: loss: 60.142464, loss_kl: 0.077321, loss_recon: 0.598724, loss_pred: 0.192792
iteration 1462: loss: 61.840290, loss_kl: 0.159834, loss_recon: 0.614995, loss_pred: 0.180986
iteration 1463: loss: 60.239761, loss_kl: 0.079783, loss_recon: 0.600153, loss_pred: 0.144687
iteration 1464: loss: 61.541901, loss_kl: 0.109895, loss_recon: 0.612706, loss_pred: 0.161396
iteration 1465: loss: 61.205528, loss_kl: 0.127087, loss_recon: 0.609576, loss_pred: 0.120852
iteration 1466: loss: 61.617298, loss_kl: 0.098904, loss_recon: 0.614099, loss_pred: 0.108475
iteration 1467: loss: 60.398354, loss_kl: 0.137449, loss_recon: 0.601047, loss_pred: 0.156175
iteration 1468: loss: 61.601849, loss_kl: 0.117536, loss_recon: 0.613810, loss_pred: 0.103321
iteration 1469: loss: 59.873379, loss_kl: 0.117193, loss_recon: 0.594916, loss_pred: 0.264622
iteration 1470: loss: 62.319283, loss_kl: 0.244884, loss_recon: 0.619275, loss_pred: 0.146908
 74%|█████████████████████▎       | 147/200 [2:01:12<44:11, 50.03s/it]iteration 1471: loss: 60.963615, loss_kl: 0.171350, loss_recon: 0.606574, loss_pred: 0.134880
iteration 1472: loss: 61.513199, loss_kl: 0.163574, loss_recon: 0.612270, loss_pred: 0.122610
iteration 1473: loss: 60.895042, loss_kl: 0.133426, loss_recon: 0.606261, loss_pred: 0.135490
iteration 1474: loss: 59.701466, loss_kl: 0.175862, loss_recon: 0.593066, loss_pred: 0.219001
iteration 1475: loss: 61.101631, loss_kl: 0.111386, loss_recon: 0.609044, loss_pred: 0.085882
iteration 1476: loss: 60.540909, loss_kl: 0.127710, loss_recon: 0.602786, loss_pred: 0.134571
iteration 1477: loss: 61.391502, loss_kl: 0.101917, loss_recon: 0.611185, loss_pred: 0.171088
iteration 1478: loss: 61.227203, loss_kl: 0.166575, loss_recon: 0.609313, loss_pred: 0.129294
iteration 1479: loss: 61.338356, loss_kl: 0.172303, loss_recon: 0.609195, loss_pred: 0.246577
iteration 1480: loss: 63.711948, loss_kl: 0.089651, loss_recon: 0.634735, loss_pred: 0.148809
 74%|█████████████████████▍       | 148/200 [2:02:02<43:18, 49.97s/it]iteration 1481: loss: 59.889084, loss_kl: 0.178402, loss_recon: 0.595100, loss_pred: 0.200685
iteration 1482: loss: 61.239113, loss_kl: 0.151519, loss_recon: 0.609236, loss_pred: 0.163997
iteration 1483: loss: 60.877110, loss_kl: 0.141622, loss_recon: 0.606037, loss_pred: 0.131781
iteration 1484: loss: 62.181454, loss_kl: 0.088168, loss_recon: 0.619436, loss_pred: 0.149731
iteration 1485: loss: 60.420567, loss_kl: 0.044870, loss_recon: 0.602422, loss_pred: 0.133487
iteration 1486: loss: 62.021828, loss_kl: 0.113031, loss_recon: 0.618044, loss_pred: 0.104438
iteration 1487: loss: 61.472382, loss_kl: 0.142630, loss_recon: 0.611551, loss_pred: 0.174635
iteration 1488: loss: 59.866009, loss_kl: 0.130169, loss_recon: 0.595926, loss_pred: 0.143218
iteration 1489: loss: 61.097012, loss_kl: 0.125800, loss_recon: 0.608232, loss_pred: 0.148025
iteration 1490: loss: 61.381660, loss_kl: 0.260580, loss_recon: 0.609735, loss_pred: 0.147627
 74%|█████████████████████▌       | 149/200 [2:02:51<42:11, 49.65s/it]iteration 1491: loss: 60.916817, loss_kl: 0.221832, loss_recon: 0.605137, loss_pred: 0.181316
iteration 1492: loss: 59.649887, loss_kl: 0.000818, loss_recon: 0.594862, loss_pred: 0.162915
iteration 1493: loss: 60.680428, loss_kl: 0.029086, loss_recon: 0.605249, loss_pred: 0.126465
iteration 1494: loss: 60.300587, loss_kl: 0.034647, loss_recon: 0.601344, loss_pred: 0.131502
iteration 1495: loss: 61.352570, loss_kl: 0.121154, loss_recon: 0.610880, loss_pred: 0.143394
iteration 1496: loss: 62.312943, loss_kl: 0.099602, loss_recon: 0.620991, loss_pred: 0.114234
iteration 1497: loss: 60.937191, loss_kl: 0.123599, loss_recon: 0.606921, loss_pred: 0.121461
iteration 1498: loss: 61.469215, loss_kl: 0.163939, loss_recon: 0.611374, loss_pred: 0.167824
iteration 1499: loss: 61.129581, loss_kl: 0.045647, loss_recon: 0.609872, loss_pred: 0.096784
iteration 1500: loss: 60.303570, loss_kl: 0.107404, loss_recon: 0.599767, loss_pred: 0.219473
 75%|█████████████████████▊       | 150/200 [2:03:41<41:36, 49.93s/it]iteration 1501: loss: 62.125309, loss_kl: 0.127045, loss_recon: 0.619968, loss_pred: 0.127253
iteration 1502: loss: 61.274551, loss_kl: 0.120442, loss_recon: 0.610644, loss_pred: 0.208942
iteration 1503: loss: 61.128914, loss_kl: 0.079383, loss_recon: 0.610003, loss_pred: 0.127835
iteration 1504: loss: 59.583126, loss_kl: 0.128550, loss_recon: 0.593711, loss_pred: 0.210783
iteration 1505: loss: 60.251015, loss_kl: 0.082877, loss_recon: 0.601268, loss_pred: 0.123375
iteration 1506: loss: 60.900337, loss_kl: 0.056945, loss_recon: 0.607913, loss_pred: 0.108445
iteration 1507: loss: 60.911327, loss_kl: 0.101987, loss_recon: 0.607887, loss_pred: 0.121602
iteration 1508: loss: 60.916168, loss_kl: 0.076803, loss_recon: 0.607790, loss_pred: 0.136436
iteration 1509: loss: 61.196438, loss_kl: 0.137090, loss_recon: 0.610449, loss_pred: 0.150206
iteration 1510: loss: 58.400200, loss_kl: 0.139959, loss_recon: 0.582724, loss_pred: 0.126396
 76%|█████████████████████▉       | 151/200 [2:04:31<40:42, 49.85s/it]iteration 1511: loss: 60.348473, loss_kl: 0.138010, loss_recon: 0.602177, loss_pred: 0.129432
iteration 1512: loss: 60.149345, loss_kl: 0.220408, loss_recon: 0.600460, loss_pred: 0.101092
iteration 1513: loss: 59.936157, loss_kl: 0.180870, loss_recon: 0.598463, loss_pred: 0.088098
iteration 1514: loss: 60.946774, loss_kl: 0.094879, loss_recon: 0.608349, loss_pred: 0.110946
iteration 1515: loss: 61.899723, loss_kl: 0.047176, loss_recon: 0.617616, loss_pred: 0.137647
iteration 1516: loss: 59.667137, loss_kl: 0.055440, loss_recon: 0.595851, loss_pred: 0.081532
iteration 1517: loss: 61.274048, loss_kl: 0.128819, loss_recon: 0.611731, loss_pred: 0.099671
iteration 1518: loss: 61.797905, loss_kl: 0.122610, loss_recon: 0.616940, loss_pred: 0.102697
iteration 1519: loss: 61.793316, loss_kl: 0.150171, loss_recon: 0.616918, loss_pred: 0.100050
iteration 1520: loss: 59.810471, loss_kl: 0.268638, loss_recon: 0.595904, loss_pred: 0.217399
 76%|██████████████████████       | 152/200 [2:05:20<39:46, 49.73s/it]iteration 1521: loss: 61.285809, loss_kl: 0.082522, loss_recon: 0.611727, loss_pred: 0.112285
iteration 1522: loss: 61.190815, loss_kl: 0.190772, loss_recon: 0.610599, loss_pred: 0.128968
iteration 1523: loss: 60.660175, loss_kl: 0.074024, loss_recon: 0.605560, loss_pred: 0.103414
iteration 1524: loss: 60.621578, loss_kl: 0.130676, loss_recon: 0.605179, loss_pred: 0.102357
iteration 1525: loss: 61.186138, loss_kl: 0.181944, loss_recon: 0.610755, loss_pred: 0.108855
iteration 1526: loss: 60.330311, loss_kl: 0.103172, loss_recon: 0.602315, loss_pred: 0.097778
iteration 1527: loss: 60.225842, loss_kl: 0.069184, loss_recon: 0.601247, loss_pred: 0.100425
iteration 1528: loss: 60.334869, loss_kl: 0.075871, loss_recon: 0.602089, loss_pred: 0.125229
iteration 1529: loss: 60.981823, loss_kl: 0.090593, loss_recon: 0.608802, loss_pred: 0.100758
iteration 1530: loss: 61.346153, loss_kl: 0.050255, loss_recon: 0.612319, loss_pred: 0.113750
 76%|██████████████████████▏      | 153/200 [2:06:10<38:53, 49.64s/it]iteration 1531: loss: 61.296394, loss_kl: 0.184281, loss_recon: 0.611513, loss_pred: 0.143270
iteration 1532: loss: 60.616604, loss_kl: 0.189072, loss_recon: 0.605035, loss_pred: 0.111176
iteration 1533: loss: 62.030952, loss_kl: 0.091245, loss_recon: 0.618481, loss_pred: 0.181929
iteration 1534: loss: 59.333382, loss_kl: 0.188771, loss_recon: 0.591831, loss_pred: 0.148391
iteration 1535: loss: 61.309433, loss_kl: 0.151340, loss_recon: 0.611774, loss_pred: 0.130536
iteration 1536: loss: 60.674969, loss_kl: 0.092028, loss_recon: 0.605390, loss_pred: 0.135061
iteration 1537: loss: 61.217106, loss_kl: 0.046310, loss_recon: 0.610997, loss_pred: 0.116945
iteration 1538: loss: 60.775021, loss_kl: 0.163127, loss_recon: 0.606839, loss_pred: 0.089462
iteration 1539: loss: 60.258602, loss_kl: 0.086282, loss_recon: 0.601383, loss_pred: 0.119462
iteration 1540: loss: 60.922092, loss_kl: 0.026845, loss_recon: 0.607702, loss_pred: 0.151580
 77%|██████████████████████▎      | 154/200 [2:07:00<38:04, 49.67s/it]iteration 1541: loss: 60.800426, loss_kl: 0.140638, loss_recon: 0.607025, loss_pred: 0.096517
iteration 1542: loss: 60.389095, loss_kl: 0.167325, loss_recon: 0.602440, loss_pred: 0.143387
iteration 1543: loss: 60.588993, loss_kl: 0.185333, loss_recon: 0.604993, loss_pred: 0.087820
iteration 1544: loss: 60.486912, loss_kl: 0.092608, loss_recon: 0.603641, loss_pred: 0.121898
iteration 1545: loss: 60.449543, loss_kl: 0.032416, loss_recon: 0.603470, loss_pred: 0.102235
iteration 1546: loss: 61.244953, loss_kl: 0.121537, loss_recon: 0.611485, loss_pred: 0.095277
iteration 1547: loss: 60.984627, loss_kl: 0.026320, loss_recon: 0.608826, loss_pred: 0.101735
iteration 1548: loss: 60.383286, loss_kl: 0.066625, loss_recon: 0.602708, loss_pred: 0.111844
iteration 1549: loss: 61.595814, loss_kl: 0.049208, loss_recon: 0.615039, loss_pred: 0.091435
iteration 1550: loss: 63.448761, loss_kl: 0.338621, loss_recon: 0.632743, loss_pred: 0.171035
 78%|██████████████████████▍      | 155/200 [2:07:49<37:11, 49.58s/it]iteration 1551: loss: 61.162376, loss_kl: 0.054926, loss_recon: 0.609837, loss_pred: 0.178135
iteration 1552: loss: 60.105980, loss_kl: 0.017172, loss_recon: 0.599702, loss_pred: 0.135621
iteration 1553: loss: 60.865452, loss_kl: 0.125344, loss_recon: 0.606609, loss_pred: 0.203328
iteration 1554: loss: 60.936005, loss_kl: 0.129234, loss_recon: 0.608369, loss_pred: 0.097775
iteration 1555: loss: 61.288063, loss_kl: 0.087882, loss_recon: 0.611071, loss_pred: 0.180112
iteration 1556: loss: 61.449581, loss_kl: 0.162940, loss_recon: 0.613185, loss_pred: 0.129459
iteration 1557: loss: 61.368809, loss_kl: 0.076937, loss_recon: 0.612422, loss_pred: 0.125816
iteration 1558: loss: 60.615856, loss_kl: 0.105322, loss_recon: 0.603782, loss_pred: 0.236633
iteration 1559: loss: 60.947540, loss_kl: 0.066291, loss_recon: 0.608273, loss_pred: 0.119589
iteration 1560: loss: 61.597359, loss_kl: 0.138501, loss_recon: 0.614314, loss_pred: 0.164597
 78%|██████████████████████▌      | 156/200 [2:08:38<36:14, 49.42s/it]iteration 1561: loss: 61.963436, loss_kl: 0.056912, loss_recon: 0.618471, loss_pred: 0.115723
iteration 1562: loss: 60.970348, loss_kl: 0.103202, loss_recon: 0.607954, loss_pred: 0.173895
iteration 1563: loss: 59.948975, loss_kl: 0.086899, loss_recon: 0.597994, loss_pred: 0.148702
iteration 1564: loss: 59.919250, loss_kl: 0.117870, loss_recon: 0.596897, loss_pred: 0.228395
iteration 1565: loss: 61.041981, loss_kl: 0.015109, loss_recon: 0.609051, loss_pred: 0.136699
iteration 1566: loss: 61.298965, loss_kl: 0.175713, loss_recon: 0.611516, loss_pred: 0.145609
iteration 1567: loss: 60.952671, loss_kl: 0.162088, loss_recon: 0.607222, loss_pred: 0.228802
iteration 1568: loss: 61.301159, loss_kl: 0.156247, loss_recon: 0.611950, loss_pred: 0.104597
iteration 1569: loss: 61.007240, loss_kl: 0.192941, loss_recon: 0.608129, loss_pred: 0.192438
iteration 1570: loss: 60.242764, loss_kl: 0.293071, loss_recon: 0.600523, loss_pred: 0.187556
 78%|██████████████████████▊      | 157/200 [2:09:27<35:16, 49.22s/it]iteration 1571: loss: 60.923035, loss_kl: 0.181116, loss_recon: 0.607697, loss_pred: 0.151524
iteration 1572: loss: 61.254013, loss_kl: -0.042525, loss_recon: 0.609474, loss_pred: 0.307028
iteration 1573: loss: 61.921658, loss_kl: 0.080046, loss_recon: 0.618228, loss_pred: 0.098069
iteration 1574: loss: 59.765270, loss_kl: 0.046626, loss_recon: 0.596268, loss_pred: 0.137963
iteration 1575: loss: 60.951942, loss_kl: 0.104781, loss_recon: 0.607936, loss_pred: 0.157262
iteration 1576: loss: 60.205875, loss_kl: 0.122535, loss_recon: 0.600607, loss_pred: 0.143906
iteration 1577: loss: 60.863194, loss_kl: 0.241232, loss_recon: 0.607717, loss_pred: 0.089119
iteration 1578: loss: 60.915752, loss_kl: 0.069770, loss_recon: 0.608103, loss_pred: 0.104742
iteration 1579: loss: 61.397091, loss_kl: 0.144035, loss_recon: 0.613144, loss_pred: 0.081239
iteration 1580: loss: 60.828690, loss_kl: 0.027352, loss_recon: 0.606133, loss_pred: 0.215111
 79%|██████████████████████▉      | 158/200 [2:10:16<34:25, 49.18s/it]iteration 1581: loss: 60.480442, loss_kl: 0.142231, loss_recon: 0.603400, loss_pred: 0.139050
iteration 1582: loss: 61.510582, loss_kl: 0.162976, loss_recon: 0.613777, loss_pred: 0.131206
iteration 1583: loss: 61.500500, loss_kl: 0.196957, loss_recon: 0.612671, loss_pred: 0.231451
iteration 1584: loss: 59.447796, loss_kl: 0.164936, loss_recon: 0.593520, loss_pred: 0.094111
iteration 1585: loss: 61.399376, loss_kl: 0.267438, loss_recon: 0.612627, loss_pred: 0.134043
iteration 1586: loss: 60.667656, loss_kl: 0.210382, loss_recon: 0.604824, loss_pred: 0.183136
iteration 1587: loss: 60.441631, loss_kl: 0.215586, loss_recon: 0.603136, loss_pred: 0.125903
iteration 1588: loss: 61.253704, loss_kl: 0.240832, loss_recon: 0.610792, loss_pred: 0.172102
iteration 1589: loss: 61.260597, loss_kl: 0.139810, loss_recon: 0.611560, loss_pred: 0.103222
iteration 1590: loss: 61.889599, loss_kl: 0.232320, loss_recon: 0.617697, loss_pred: 0.117534
 80%|███████████████████████      | 159/200 [2:11:06<33:46, 49.42s/it]iteration 1591: loss: 59.743053, loss_kl: 0.056391, loss_recon: 0.595043, loss_pred: 0.238164
iteration 1592: loss: 60.691917, loss_kl: 0.158307, loss_recon: 0.605577, loss_pred: 0.132612
iteration 1593: loss: 60.876968, loss_kl: 0.117309, loss_recon: 0.606487, loss_pred: 0.227077
iteration 1594: loss: 61.909565, loss_kl: 0.091192, loss_recon: 0.617914, loss_pred: 0.117232
iteration 1595: loss: 60.762562, loss_kl: 0.197097, loss_recon: 0.605276, loss_pred: 0.232968
iteration 1596: loss: 60.333099, loss_kl: 0.044725, loss_recon: 0.601682, loss_pred: 0.164410
iteration 1597: loss: 60.111542, loss_kl: 0.079806, loss_recon: 0.599334, loss_pred: 0.177350
iteration 1598: loss: 60.856350, loss_kl: 0.138068, loss_recon: 0.606848, loss_pred: 0.170121
iteration 1599: loss: 61.986324, loss_kl: 0.189753, loss_recon: 0.618620, loss_pred: 0.122437
iteration 1600: loss: 64.329269, loss_kl: 0.097588, loss_recon: 0.639319, loss_pred: 0.396383
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_159.pth
 80%|███████████████████████▏     | 160/200 [2:11:55<32:52, 49.32s/it]iteration 1601: loss: 62.329887, loss_kl: 0.065164, loss_recon: 0.621370, loss_pred: 0.192216
iteration 1602: loss: 60.755150, loss_kl: 0.232245, loss_recon: 0.606713, loss_pred: 0.081507
iteration 1603: loss: 60.602711, loss_kl: 0.047291, loss_recon: 0.604211, loss_pred: 0.181122
iteration 1604: loss: 59.818874, loss_kl: 0.146401, loss_recon: 0.596954, loss_pred: 0.122029
iteration 1605: loss: 60.555866, loss_kl: 0.129123, loss_recon: 0.604224, loss_pred: 0.132209
iteration 1606: loss: 60.928185, loss_kl: 0.109413, loss_recon: 0.607999, loss_pred: 0.127164
iteration 1607: loss: 60.269615, loss_kl: 0.128447, loss_recon: 0.601038, loss_pred: 0.164563
iteration 1608: loss: 60.789906, loss_kl: 0.111973, loss_recon: 0.606396, loss_pred: 0.149206
iteration 1609: loss: 61.457294, loss_kl: 0.032937, loss_recon: 0.613163, loss_pred: 0.140699
iteration 1610: loss: 61.873623, loss_kl: 0.154940, loss_recon: 0.617564, loss_pred: 0.115654
 80%|███████████████████████▎     | 161/200 [2:12:45<32:08, 49.44s/it]iteration 1611: loss: 60.821266, loss_kl: 0.139547, loss_recon: 0.606787, loss_pred: 0.141174
iteration 1612: loss: 60.496632, loss_kl: 0.201200, loss_recon: 0.603421, loss_pred: 0.152519
iteration 1613: loss: 60.342216, loss_kl: 0.070146, loss_recon: 0.602023, loss_pred: 0.139252
iteration 1614: loss: 60.099064, loss_kl: 0.146096, loss_recon: 0.599365, loss_pred: 0.161057
iteration 1615: loss: 61.644794, loss_kl: 0.283573, loss_recon: 0.615492, loss_pred: 0.092762
iteration 1616: loss: 61.654907, loss_kl: 0.204715, loss_recon: 0.614816, loss_pred: 0.171244
iteration 1617: loss: 61.302879, loss_kl: 0.150536, loss_recon: 0.611434, loss_pred: 0.157950
iteration 1618: loss: 61.450294, loss_kl: 0.007986, loss_recon: 0.613366, loss_pred: 0.113584
iteration 1619: loss: 60.622746, loss_kl: 0.306290, loss_recon: 0.604474, loss_pred: 0.172309
iteration 1620: loss: 58.390537, loss_kl: 0.040231, loss_recon: 0.582455, loss_pred: 0.144669
 81%|███████████████████████▍     | 162/200 [2:13:34<31:15, 49.34s/it]iteration 1621: loss: 60.765751, loss_kl: 0.120024, loss_recon: 0.605467, loss_pred: 0.217841
iteration 1622: loss: 60.544205, loss_kl: 0.138326, loss_recon: 0.604174, loss_pred: 0.125388
iteration 1623: loss: 61.262444, loss_kl: 0.076356, loss_recon: 0.611372, loss_pred: 0.124468
iteration 1624: loss: 60.814522, loss_kl: 0.209756, loss_recon: 0.605826, loss_pred: 0.229861
iteration 1625: loss: 61.625633, loss_kl: 0.155305, loss_recon: 0.615048, loss_pred: 0.119263
iteration 1626: loss: 60.952030, loss_kl: 0.217857, loss_recon: 0.607327, loss_pred: 0.217138
iteration 1627: loss: 60.622082, loss_kl: 0.027456, loss_recon: 0.604765, loss_pred: 0.145279
iteration 1628: loss: 61.036655, loss_kl: 0.134520, loss_recon: 0.609048, loss_pred: 0.130532
iteration 1629: loss: 59.838097, loss_kl: 0.141502, loss_recon: 0.595285, loss_pred: 0.308192
iteration 1630: loss: 61.983841, loss_kl: 0.224072, loss_recon: 0.618859, loss_pred: 0.095665
 82%|███████████████████████▋     | 163/200 [2:14:24<30:39, 49.71s/it]iteration 1631: loss: 60.743599, loss_kl: 0.145406, loss_recon: 0.603457, loss_pred: 0.390704
iteration 1632: loss: 59.384644, loss_kl: 0.064298, loss_recon: 0.591447, loss_pred: 0.236756
iteration 1633: loss: 60.525604, loss_kl: 0.081886, loss_recon: 0.603373, loss_pred: 0.184199
iteration 1634: loss: 61.791256, loss_kl: 0.195866, loss_recon: 0.614885, loss_pred: 0.292994
iteration 1635: loss: 60.272789, loss_kl: 0.255540, loss_recon: 0.601503, loss_pred: 0.109861
iteration 1636: loss: 62.576538, loss_kl: 0.078865, loss_recon: 0.623746, loss_pred: 0.198029
iteration 1637: loss: 60.851673, loss_kl: 0.281334, loss_recon: 0.606249, loss_pred: 0.212846
iteration 1638: loss: 61.625481, loss_kl: 0.088262, loss_recon: 0.615452, loss_pred: 0.075929
iteration 1639: loss: 61.459805, loss_kl: 0.175203, loss_recon: 0.612964, loss_pred: 0.154755
iteration 1640: loss: 59.682705, loss_kl: 0.558232, loss_recon: 0.594119, loss_pred: 0.243087
 82%|███████████████████████▊     | 164/200 [2:15:13<29:42, 49.52s/it]iteration 1641: loss: 59.985935, loss_kl: 0.073699, loss_recon: 0.598714, loss_pred: 0.107920
iteration 1642: loss: 60.658241, loss_kl: 0.183236, loss_recon: 0.605068, loss_pred: 0.135119
iteration 1643: loss: 62.118641, loss_kl: 0.138070, loss_recon: 0.620059, loss_pred: 0.100410
iteration 1644: loss: 60.652733, loss_kl: 0.078725, loss_recon: 0.605214, loss_pred: 0.124298
iteration 1645: loss: 60.621567, loss_kl: 0.083745, loss_recon: 0.604921, loss_pred: 0.121999
iteration 1646: loss: 62.811832, loss_kl: 0.134068, loss_recon: 0.626870, loss_pred: 0.112858
iteration 1647: loss: 60.714489, loss_kl: 0.190547, loss_recon: 0.605926, loss_pred: 0.104872
iteration 1648: loss: 60.637241, loss_kl: 0.173591, loss_recon: 0.605212, loss_pred: 0.100547
iteration 1649: loss: 59.238716, loss_kl: 0.252745, loss_recon: 0.591077, loss_pred: 0.108438
iteration 1650: loss: 63.326366, loss_kl: 0.224991, loss_recon: 0.630783, loss_pred: 0.227969
 82%|███████████████████████▉     | 165/200 [2:16:03<28:53, 49.53s/it]iteration 1651: loss: 59.431694, loss_kl: 0.197606, loss_recon: 0.592756, loss_pred: 0.130634
iteration 1652: loss: 60.574612, loss_kl: 0.266482, loss_recon: 0.604019, loss_pred: 0.138415
iteration 1653: loss: 60.516956, loss_kl: 0.252291, loss_recon: 0.603615, loss_pred: 0.122959
iteration 1654: loss: 60.407227, loss_kl: 0.148672, loss_recon: 0.602371, loss_pred: 0.150935
iteration 1655: loss: 62.168510, loss_kl: 0.069392, loss_recon: 0.620468, loss_pred: 0.112766
iteration 1656: loss: 61.852356, loss_kl: 0.104891, loss_recon: 0.617063, loss_pred: 0.132575
iteration 1657: loss: 60.889450, loss_kl: 0.309430, loss_recon: 0.607203, loss_pred: 0.129340
iteration 1658: loss: 60.701839, loss_kl: 0.106131, loss_recon: 0.605675, loss_pred: 0.120710
iteration 1659: loss: 61.126259, loss_kl: 0.142886, loss_recon: 0.609765, loss_pred: 0.131351
iteration 1660: loss: 60.156891, loss_kl: 0.260517, loss_recon: 0.599721, loss_pred: 0.151277
 83%|████████████████████████     | 166/200 [2:16:52<27:58, 49.37s/it]iteration 1661: loss: 61.713917, loss_kl: 0.070974, loss_recon: 0.615802, loss_pred: 0.121732
iteration 1662: loss: 60.947926, loss_kl: 0.134552, loss_recon: 0.608196, loss_pred: 0.105684
iteration 1663: loss: 60.393574, loss_kl: 0.103066, loss_recon: 0.602491, loss_pred: 0.127138
iteration 1664: loss: 61.395859, loss_kl: 0.115750, loss_recon: 0.612174, loss_pred: 0.159005
iteration 1665: loss: 59.960396, loss_kl: 0.107500, loss_recon: 0.598054, loss_pred: 0.136877
iteration 1666: loss: 61.698414, loss_kl: 0.097652, loss_recon: 0.615542, loss_pred: 0.127726
iteration 1667: loss: 60.044952, loss_kl: 0.186058, loss_recon: 0.597185, loss_pred: 0.295117
iteration 1668: loss: 60.914764, loss_kl: 0.126142, loss_recon: 0.607766, loss_pred: 0.116965
iteration 1669: loss: 59.939842, loss_kl: 0.108337, loss_recon: 0.597815, loss_pred: 0.140142
iteration 1670: loss: 64.198563, loss_kl: 0.096476, loss_recon: 0.640540, loss_pred: 0.128284
 84%|████████████████████████▏    | 167/200 [2:17:41<27:05, 49.27s/it]iteration 1671: loss: 60.760799, loss_kl: 0.147975, loss_recon: 0.606023, loss_pred: 0.127701
iteration 1672: loss: 60.862419, loss_kl: 0.195279, loss_recon: 0.606504, loss_pred: 0.171428
iteration 1673: loss: 60.677380, loss_kl: 0.056761, loss_recon: 0.605471, loss_pred: 0.118477
iteration 1674: loss: 61.691872, loss_kl: 0.082947, loss_recon: 0.615391, loss_pred: 0.135552
iteration 1675: loss: 60.068497, loss_kl: 0.107672, loss_recon: 0.598874, loss_pred: 0.158741
iteration 1676: loss: 61.219933, loss_kl: 0.067647, loss_recon: 0.610755, loss_pred: 0.130373
iteration 1677: loss: 60.704666, loss_kl: 0.132529, loss_recon: 0.605878, loss_pred: 0.089349
iteration 1678: loss: 61.372093, loss_kl: 0.137003, loss_recon: 0.612458, loss_pred: 0.097751
iteration 1679: loss: 59.972576, loss_kl: 0.084948, loss_recon: 0.598613, loss_pred: 0.093565
iteration 1680: loss: 61.034943, loss_kl: 0.190077, loss_recon: 0.608503, loss_pred: 0.145063
 84%|████████████████████████▎    | 168/200 [2:18:31<26:20, 49.40s/it]iteration 1681: loss: 60.810062, loss_kl: -0.023702, loss_recon: 0.607052, loss_pred: 0.110714
iteration 1682: loss: 60.809113, loss_kl: 0.059657, loss_recon: 0.606837, loss_pred: 0.110687
iteration 1683: loss: 61.409744, loss_kl: 0.146108, loss_recon: 0.612982, loss_pred: 0.075360
iteration 1684: loss: 61.073303, loss_kl: 0.152276, loss_recon: 0.609288, loss_pred: 0.106767
iteration 1685: loss: 61.320541, loss_kl: 0.148085, loss_recon: 0.611860, loss_pred: 0.097866
iteration 1686: loss: 61.486607, loss_kl: 0.091399, loss_recon: 0.613657, loss_pred: 0.098320
iteration 1687: loss: 60.554562, loss_kl: 0.178725, loss_recon: 0.604141, loss_pred: 0.096206
iteration 1688: loss: 60.183548, loss_kl: 0.136225, loss_recon: 0.600192, loss_pred: 0.130618
iteration 1689: loss: 59.843967, loss_kl: 0.245375, loss_recon: 0.596558, loss_pred: 0.127443
iteration 1690: loss: 60.506142, loss_kl: 0.020976, loss_recon: 0.603617, loss_pred: 0.139267
 84%|████████████████████████▌    | 169/200 [2:19:22<25:50, 50.01s/it]iteration 1691: loss: 60.977123, loss_kl: 0.180034, loss_recon: 0.608277, loss_pred: 0.097716
iteration 1692: loss: 61.844196, loss_kl: 0.096622, loss_recon: 0.616899, loss_pred: 0.126572
iteration 1693: loss: 61.220894, loss_kl: 0.146898, loss_recon: 0.610807, loss_pred: 0.097997
iteration 1694: loss: 59.664207, loss_kl: 0.163608, loss_recon: 0.595003, loss_pred: 0.116906
iteration 1695: loss: 61.483093, loss_kl: -0.064827, loss_recon: 0.613980, loss_pred: 0.103665
iteration 1696: loss: 61.064541, loss_kl: 0.051450, loss_recon: 0.609546, loss_pred: 0.095194
iteration 1697: loss: 60.564751, loss_kl: 0.065308, loss_recon: 0.604215, loss_pred: 0.124486
iteration 1698: loss: 60.566868, loss_kl: 0.154557, loss_recon: 0.604188, loss_pred: 0.103636
iteration 1699: loss: 60.227360, loss_kl: 0.135958, loss_recon: 0.600821, loss_pred: 0.106214
iteration 1700: loss: 59.325493, loss_kl: 0.225136, loss_recon: 0.590725, loss_pred: 0.188292
 85%|████████████████████████▋    | 170/200 [2:20:11<24:53, 49.77s/it]iteration 1701: loss: 61.300175, loss_kl: 0.099110, loss_recon: 0.611719, loss_pred: 0.095917
iteration 1702: loss: 61.183483, loss_kl: 0.160615, loss_recon: 0.610015, loss_pred: 0.129483
iteration 1703: loss: 61.905975, loss_kl: 0.170687, loss_recon: 0.617553, loss_pred: 0.094851
iteration 1704: loss: 62.317924, loss_kl: 0.149984, loss_recon: 0.621199, loss_pred: 0.148967
iteration 1705: loss: 60.096233, loss_kl: 0.128020, loss_recon: 0.599447, loss_pred: 0.109717
iteration 1706: loss: 60.040970, loss_kl: 0.048776, loss_recon: 0.599289, loss_pred: 0.096104
iteration 1707: loss: 61.122704, loss_kl: 0.130219, loss_recon: 0.609952, loss_pred: 0.084982
iteration 1708: loss: 59.457466, loss_kl: 0.093638, loss_recon: 0.592963, loss_pred: 0.130532
iteration 1709: loss: 59.568661, loss_kl: 0.076408, loss_recon: 0.594409, loss_pred: 0.102801
iteration 1710: loss: 62.994228, loss_kl: 0.009997, loss_recon: 0.628838, loss_pred: 0.107140
 86%|████████████████████████▊    | 171/200 [2:21:01<24:01, 49.71s/it]iteration 1711: loss: 61.695789, loss_kl: 0.028275, loss_recon: 0.615513, loss_pred: 0.134090
iteration 1712: loss: 60.447456, loss_kl: 0.161933, loss_recon: 0.602926, loss_pred: 0.095474
iteration 1713: loss: 61.424374, loss_kl: 0.176103, loss_recon: 0.611797, loss_pred: 0.180136
iteration 1714: loss: 59.496910, loss_kl: 0.142490, loss_recon: 0.593577, loss_pred: 0.087037
iteration 1715: loss: 60.074997, loss_kl: 0.184591, loss_recon: 0.598306, loss_pred: 0.176743
iteration 1716: loss: 61.051735, loss_kl: 0.152261, loss_recon: 0.608881, loss_pred: 0.107799
iteration 1717: loss: 60.864376, loss_kl: 0.101762, loss_recon: 0.605921, loss_pred: 0.234970
iteration 1718: loss: 60.465023, loss_kl: 0.078318, loss_recon: 0.602620, loss_pred: 0.174324
iteration 1719: loss: 62.081158, loss_kl: 0.165085, loss_recon: 0.619424, loss_pred: 0.078317
iteration 1720: loss: 62.712341, loss_kl: 0.070925, loss_recon: 0.625149, loss_pred: 0.171431
 86%|████████████████████████▉    | 172/200 [2:21:50<23:06, 49.51s/it]iteration 1721: loss: 61.534252, loss_kl: 0.100139, loss_recon: 0.613431, loss_pred: 0.150477
iteration 1722: loss: 61.399689, loss_kl: 0.097356, loss_recon: 0.612678, loss_pred: 0.092361
iteration 1723: loss: 60.492390, loss_kl: 0.167697, loss_recon: 0.601849, loss_pred: 0.239444
iteration 1724: loss: 60.284622, loss_kl: 0.202669, loss_recon: 0.600468, loss_pred: 0.155581
iteration 1725: loss: 59.790211, loss_kl: 0.120366, loss_recon: 0.595259, loss_pred: 0.215435
iteration 1726: loss: 60.510696, loss_kl: 0.147655, loss_recon: 0.601929, loss_pred: 0.257805
iteration 1727: loss: 60.975445, loss_kl: 0.113823, loss_recon: 0.608085, loss_pred: 0.120714
iteration 1728: loss: 61.384029, loss_kl: 0.055366, loss_recon: 0.611376, loss_pred: 0.223976
iteration 1729: loss: 62.201935, loss_kl: 0.028199, loss_recon: 0.620858, loss_pred: 0.104730
iteration 1730: loss: 62.157104, loss_kl: 0.133621, loss_recon: 0.618707, loss_pred: 0.232203
 86%|█████████████████████████    | 173/200 [2:22:39<22:13, 49.37s/it]iteration 1731: loss: 59.632687, loss_kl: 0.142270, loss_recon: 0.594546, loss_pred: 0.114700
iteration 1732: loss: 61.273388, loss_kl: 0.142270, loss_recon: 0.610431, loss_pred: 0.166854
iteration 1733: loss: 62.029957, loss_kl: 0.086270, loss_recon: 0.618858, loss_pred: 0.105728
iteration 1734: loss: 60.251839, loss_kl: 0.050803, loss_recon: 0.599985, loss_pred: 0.230742
iteration 1735: loss: 59.759701, loss_kl: 0.164379, loss_recon: 0.594728, loss_pred: 0.213685
iteration 1736: loss: 61.085819, loss_kl: 0.097764, loss_recon: 0.609141, loss_pred: 0.128182
iteration 1737: loss: 61.457138, loss_kl: 0.124883, loss_recon: 0.611743, loss_pred: 0.227198
iteration 1738: loss: 61.171814, loss_kl: 0.075532, loss_recon: 0.610258, loss_pred: 0.112328
iteration 1739: loss: 61.228149, loss_kl: 0.048218, loss_recon: 0.610121, loss_pred: 0.194536
iteration 1740: loss: 63.439507, loss_kl: 0.104089, loss_recon: 0.632241, loss_pred: 0.169070
 87%|█████████████████████████▏   | 174/200 [2:23:28<21:21, 49.30s/it]iteration 1741: loss: 61.087692, loss_kl: 0.206261, loss_recon: 0.608214, loss_pred: 0.166239
iteration 1742: loss: 60.316254, loss_kl: 0.134557, loss_recon: 0.601117, loss_pred: 0.139273
iteration 1743: loss: 60.926991, loss_kl: 0.143344, loss_recon: 0.607617, loss_pred: 0.095780
iteration 1744: loss: 60.953518, loss_kl: 0.140075, loss_recon: 0.607213, loss_pred: 0.164209
iteration 1745: loss: 60.619411, loss_kl: 0.097639, loss_recon: 0.604617, loss_pred: 0.110290
iteration 1746: loss: 60.405766, loss_kl: 0.115602, loss_recon: 0.601062, loss_pred: 0.243483
iteration 1747: loss: 62.723633, loss_kl: 0.172159, loss_recon: 0.625492, loss_pred: 0.090947
iteration 1748: loss: 61.339714, loss_kl: 0.162045, loss_recon: 0.610416, loss_pred: 0.219477
iteration 1749: loss: 60.401752, loss_kl: 0.089590, loss_recon: 0.601482, loss_pred: 0.210065
iteration 1750: loss: 60.711807, loss_kl: -0.080159, loss_recon: 0.605927, loss_pred: 0.158016
 88%|█████████████████████████▍   | 175/200 [2:24:18<20:35, 49.41s/it]iteration 1751: loss: 60.700378, loss_kl: 0.142622, loss_recon: 0.603473, loss_pred: 0.278199
iteration 1752: loss: 60.998882, loss_kl: 0.061448, loss_recon: 0.608503, loss_pred: 0.116343
iteration 1753: loss: 62.705551, loss_kl: -0.061189, loss_recon: 0.625583, loss_pred: 0.179316
iteration 1754: loss: 60.283295, loss_kl: 0.089105, loss_recon: 0.601102, loss_pred: 0.126368
iteration 1755: loss: 60.349972, loss_kl: 0.097577, loss_recon: 0.601767, loss_pred: 0.122038
iteration 1756: loss: 60.122700, loss_kl: 0.146344, loss_recon: 0.598045, loss_pred: 0.241409
iteration 1757: loss: 60.392593, loss_kl: 0.089299, loss_recon: 0.602392, loss_pred: 0.106529
iteration 1758: loss: 60.174953, loss_kl: 0.121474, loss_recon: 0.599041, loss_pred: 0.207150
iteration 1759: loss: 61.555908, loss_kl: 0.099648, loss_recon: 0.613194, loss_pred: 0.184263
iteration 1760: loss: 63.494148, loss_kl: -0.069856, loss_recon: 0.633183, loss_pred: 0.212537
 88%|█████████████████████████▌   | 176/200 [2:25:08<19:48, 49.54s/it]iteration 1761: loss: 59.475063, loss_kl: 0.117691, loss_recon: 0.593008, loss_pred: 0.107812
iteration 1762: loss: 60.862080, loss_kl: 0.190657, loss_recon: 0.606493, loss_pred: 0.105167
iteration 1763: loss: 60.382351, loss_kl: 0.215420, loss_recon: 0.601306, loss_pred: 0.130162
iteration 1764: loss: 61.613831, loss_kl: 0.192249, loss_recon: 0.613714, loss_pred: 0.133895
iteration 1765: loss: 62.069225, loss_kl: 0.037548, loss_recon: 0.619255, loss_pred: 0.122540
iteration 1766: loss: 60.700344, loss_kl: 0.099411, loss_recon: 0.605503, loss_pred: 0.093931
iteration 1767: loss: 60.567699, loss_kl: 0.127450, loss_recon: 0.603574, loss_pred: 0.138333
iteration 1768: loss: 61.241177, loss_kl: 0.070514, loss_recon: 0.610101, loss_pred: 0.191243
iteration 1769: loss: 60.489094, loss_kl: 0.146042, loss_recon: 0.602851, loss_pred: 0.121585
iteration 1770: loss: 63.570068, loss_kl: 0.060805, loss_recon: 0.634382, loss_pred: 0.097503
 88%|█████████████████████████▋   | 177/200 [2:25:57<18:56, 49.42s/it]iteration 1771: loss: 61.224174, loss_kl: 0.179765, loss_recon: 0.609880, loss_pred: 0.127595
iteration 1772: loss: 59.764648, loss_kl: 0.133355, loss_recon: 0.595520, loss_pred: 0.132118
iteration 1773: loss: 61.922848, loss_kl: 0.107643, loss_recon: 0.617488, loss_pred: 0.108985
iteration 1774: loss: 61.352226, loss_kl: 0.085871, loss_recon: 0.611947, loss_pred: 0.105634
iteration 1775: loss: 61.597031, loss_kl: 0.137960, loss_recon: 0.613827, loss_pred: 0.131018
iteration 1776: loss: 61.522743, loss_kl: 0.125729, loss_recon: 0.613177, loss_pred: 0.129067
iteration 1777: loss: 59.671803, loss_kl: 0.086263, loss_recon: 0.594964, loss_pred: 0.123267
iteration 1778: loss: 60.425083, loss_kl: 0.112456, loss_recon: 0.602677, loss_pred: 0.089490
iteration 1779: loss: 61.123951, loss_kl: 0.097137, loss_recon: 0.609869, loss_pred: 0.078410
iteration 1780: loss: 59.353745, loss_kl: 0.048924, loss_recon: 0.591500, loss_pred: 0.174245
 89%|█████████████████████████▊   | 178/200 [2:26:46<18:04, 49.30s/it]iteration 1781: loss: 61.262249, loss_kl: 0.051066, loss_recon: 0.610728, loss_pred: 0.156549
iteration 1782: loss: 60.649635, loss_kl: -0.057759, loss_recon: 0.605859, loss_pred: 0.100910
iteration 1783: loss: 61.188671, loss_kl: -0.037466, loss_recon: 0.610937, loss_pred: 0.119060
iteration 1784: loss: 61.039337, loss_kl: 0.163458, loss_recon: 0.608202, loss_pred: 0.113961
iteration 1785: loss: 60.407925, loss_kl: 0.131374, loss_recon: 0.602060, loss_pred: 0.117417
iteration 1786: loss: 60.890198, loss_kl: 0.113860, loss_recon: 0.606721, loss_pred: 0.144772
iteration 1787: loss: 60.030453, loss_kl: 0.088243, loss_recon: 0.598725, loss_pred: 0.101148
iteration 1788: loss: 61.623653, loss_kl: 0.079512, loss_recon: 0.614548, loss_pred: 0.117683
iteration 1789: loss: 60.281349, loss_kl: 0.046935, loss_recon: 0.600373, loss_pred: 0.213843
iteration 1790: loss: 61.779926, loss_kl: 0.129681, loss_recon: 0.615709, loss_pred: 0.125552
 90%|█████████████████████████▉   | 179/200 [2:27:35<17:13, 49.23s/it]iteration 1791: loss: 62.545845, loss_kl: 0.147606, loss_recon: 0.621926, loss_pred: 0.252431
iteration 1792: loss: 60.159542, loss_kl: 0.007084, loss_recon: 0.600561, loss_pred: 0.098594
iteration 1793: loss: 59.759808, loss_kl: 0.119287, loss_recon: 0.595404, loss_pred: 0.137910
iteration 1794: loss: 59.676483, loss_kl: 0.102083, loss_recon: 0.594971, loss_pred: 0.109609
iteration 1795: loss: 61.564587, loss_kl: 0.035716, loss_recon: 0.613969, loss_pred: 0.143239
iteration 1796: loss: 62.317326, loss_kl: 0.154549, loss_recon: 0.621218, loss_pred: 0.089949
iteration 1797: loss: 61.192219, loss_kl: 0.147119, loss_recon: 0.609451, loss_pred: 0.146574
iteration 1798: loss: 59.330421, loss_kl: 0.086677, loss_recon: 0.591037, loss_pred: 0.167549
iteration 1799: loss: 61.904133, loss_kl: 0.110319, loss_recon: 0.616576, loss_pred: 0.171194
iteration 1800: loss: 60.816658, loss_kl: 0.140246, loss_recon: 0.605689, loss_pred: 0.151954
 90%|██████████████████████████   | 180/200 [2:28:24<16:23, 49.18s/it]iteration 1801: loss: 60.727043, loss_kl: 0.089090, loss_recon: 0.605308, loss_pred: 0.131802
iteration 1802: loss: 60.982845, loss_kl: 0.114978, loss_recon: 0.607942, loss_pred: 0.105552
iteration 1803: loss: 60.435490, loss_kl: 0.074461, loss_recon: 0.602319, loss_pred: 0.149782
iteration 1804: loss: 59.077473, loss_kl: 0.104821, loss_recon: 0.588870, loss_pred: 0.114695
iteration 1805: loss: 61.059048, loss_kl: 0.049795, loss_recon: 0.609032, loss_pred: 0.119829
iteration 1806: loss: 61.141113, loss_kl: 0.063462, loss_recon: 0.610074, loss_pred: 0.087828
iteration 1807: loss: 61.732960, loss_kl: 0.136838, loss_recon: 0.615438, loss_pred: 0.090250
iteration 1808: loss: 61.368454, loss_kl: 0.154544, loss_recon: 0.611276, loss_pred: 0.129109
iteration 1809: loss: 61.687908, loss_kl: 0.178517, loss_recon: 0.614713, loss_pred: 0.087551
iteration 1810: loss: 59.663689, loss_kl: -0.016993, loss_recon: 0.595344, loss_pred: 0.141600
 90%|██████████████████████████▏  | 181/200 [2:29:13<15:33, 49.11s/it]iteration 1811: loss: 59.643929, loss_kl: 0.166086, loss_recon: 0.593614, loss_pred: 0.155866
iteration 1812: loss: 61.012157, loss_kl: 0.132458, loss_recon: 0.607640, loss_pred: 0.147156
iteration 1813: loss: 60.064465, loss_kl: 0.015036, loss_recon: 0.599492, loss_pred: 0.103836
iteration 1814: loss: 60.267620, loss_kl: 0.089687, loss_recon: 0.600560, loss_pred: 0.143257
iteration 1815: loss: 62.386921, loss_kl: 0.040949, loss_recon: 0.622478, loss_pred: 0.107870
iteration 1816: loss: 61.855503, loss_kl: 0.033264, loss_recon: 0.617001, loss_pred: 0.130026
iteration 1817: loss: 59.560192, loss_kl: 0.160724, loss_recon: 0.592849, loss_pred: 0.152779
iteration 1818: loss: 62.931286, loss_kl: 0.054051, loss_recon: 0.627074, loss_pred: 0.182708
iteration 1819: loss: 60.962425, loss_kl: -0.022300, loss_recon: 0.608550, loss_pred: 0.124412
iteration 1820: loss: 59.228668, loss_kl: 0.061053, loss_recon: 0.586345, loss_pred: 0.547597
 91%|██████████████████████████▍  | 182/200 [2:30:02<14:44, 49.13s/it]iteration 1821: loss: 60.272434, loss_kl: -0.010962, loss_recon: 0.601590, loss_pred: 0.122210
iteration 1822: loss: 60.133255, loss_kl: 0.079571, loss_recon: 0.599679, loss_pred: 0.101492
iteration 1823: loss: 59.829697, loss_kl: 0.063703, loss_recon: 0.596374, loss_pred: 0.141178
iteration 1824: loss: 61.046799, loss_kl: 0.062616, loss_recon: 0.608620, loss_pred: 0.134595
iteration 1825: loss: 61.015621, loss_kl: 0.123201, loss_recon: 0.608038, loss_pred: 0.113007
iteration 1826: loss: 62.813847, loss_kl: 0.202056, loss_recon: 0.625636, loss_pred: 0.088182
iteration 1827: loss: 61.814613, loss_kl: 0.035506, loss_recon: 0.616814, loss_pred: 0.104752
iteration 1828: loss: 60.600842, loss_kl: 0.050172, loss_recon: 0.604146, loss_pred: 0.145993
iteration 1829: loss: 59.769295, loss_kl: 0.186676, loss_recon: 0.594775, loss_pred: 0.142103
iteration 1830: loss: 64.012825, loss_kl: 0.105032, loss_recon: 0.638119, loss_pred: 0.116691
 92%|██████████████████████████▌  | 183/200 [2:30:51<13:56, 49.20s/it]iteration 1831: loss: 61.456894, loss_kl: 0.014463, loss_recon: 0.613071, loss_pred: 0.137576
iteration 1832: loss: 59.021362, loss_kl: 0.105637, loss_recon: 0.588003, loss_pred: 0.132184
iteration 1833: loss: 60.253178, loss_kl: 0.076535, loss_recon: 0.600577, loss_pred: 0.131107
iteration 1834: loss: 60.685783, loss_kl: 0.009550, loss_recon: 0.605668, loss_pred: 0.110973
iteration 1835: loss: 60.821503, loss_kl: 0.124803, loss_recon: 0.605658, loss_pred: 0.150667
iteration 1836: loss: 61.968407, loss_kl: 0.014568, loss_recon: 0.618132, loss_pred: 0.142900
iteration 1837: loss: 61.571941, loss_kl: 0.044319, loss_recon: 0.614412, loss_pred: 0.093412
iteration 1838: loss: 61.031635, loss_kl: 0.085710, loss_recon: 0.608183, loss_pred: 0.141193
iteration 1839: loss: 60.256359, loss_kl: 0.067707, loss_recon: 0.601131, loss_pred: 0.086302
iteration 1840: loss: 62.441856, loss_kl: 0.214570, loss_recon: 0.621456, loss_pred: 0.115678
 92%|██████████████████████████▋  | 184/200 [2:31:41<13:10, 49.40s/it]iteration 1841: loss: 59.623634, loss_kl: 0.019896, loss_recon: 0.594843, loss_pred: 0.121828
iteration 1842: loss: 60.118771, loss_kl: 0.005337, loss_recon: 0.600037, loss_pred: 0.110363
iteration 1843: loss: 61.738197, loss_kl: 0.117719, loss_recon: 0.614884, loss_pred: 0.146046
iteration 1844: loss: 60.145729, loss_kl: 0.065754, loss_recon: 0.599345, loss_pred: 0.153259
iteration 1845: loss: 61.648232, loss_kl: 0.120890, loss_recon: 0.614154, loss_pred: 0.126273
iteration 1846: loss: 61.608780, loss_kl: 0.081329, loss_recon: 0.614223, loss_pred: 0.114801
iteration 1847: loss: 61.877178, loss_kl: 0.135821, loss_recon: 0.616579, loss_pred: 0.099544
iteration 1848: loss: 60.972069, loss_kl: 0.018620, loss_recon: 0.608530, loss_pred: 0.102711
iteration 1849: loss: 60.047413, loss_kl: 0.065347, loss_recon: 0.598704, loss_pred: 0.119388
iteration 1850: loss: 62.187004, loss_kl: 0.082265, loss_recon: 0.620038, loss_pred: 0.110692
 92%|██████████████████████████▊  | 185/200 [2:32:31<12:20, 49.37s/it]iteration 1851: loss: 60.157642, loss_kl: 0.107437, loss_recon: 0.599481, loss_pred: 0.110654
iteration 1852: loss: 60.873524, loss_kl: 0.098079, loss_recon: 0.606726, loss_pred: 0.110652
iteration 1853: loss: 60.634087, loss_kl: 0.091864, loss_recon: 0.603861, loss_pred: 0.163443
iteration 1854: loss: 60.324257, loss_kl: 0.114971, loss_recon: 0.601191, loss_pred: 0.099264
iteration 1855: loss: 61.130524, loss_kl: 0.124080, loss_recon: 0.608693, loss_pred: 0.146951
iteration 1856: loss: 61.307346, loss_kl: 0.137522, loss_recon: 0.610457, loss_pred: 0.134985
iteration 1857: loss: 61.647697, loss_kl: 0.094004, loss_recon: 0.614277, loss_pred: 0.133468
iteration 1858: loss: 61.195271, loss_kl: 0.049428, loss_recon: 0.610293, loss_pred: 0.120466
iteration 1859: loss: 60.873905, loss_kl: 0.047121, loss_recon: 0.606994, loss_pred: 0.131107
iteration 1860: loss: 60.611126, loss_kl: -0.011524, loss_recon: 0.604808, loss_pred: 0.140983
 93%|██████████████████████████▉  | 186/200 [2:33:22<11:39, 49.95s/it]iteration 1861: loss: 61.106197, loss_kl: 0.109538, loss_recon: 0.609184, loss_pred: 0.082602
iteration 1862: loss: 61.073112, loss_kl: 0.046005, loss_recon: 0.609437, loss_pred: 0.085242
iteration 1863: loss: 60.543159, loss_kl: 0.007840, loss_recon: 0.604368, loss_pred: 0.098860
iteration 1864: loss: 61.277893, loss_kl: 0.056945, loss_recon: 0.611236, loss_pred: 0.099620
iteration 1865: loss: 60.632339, loss_kl: 0.117167, loss_recon: 0.604054, loss_pred: 0.114405
iteration 1866: loss: 60.580379, loss_kl: 0.049552, loss_recon: 0.604226, loss_pred: 0.110172
iteration 1867: loss: 60.576500, loss_kl: 0.136399, loss_recon: 0.603340, loss_pred: 0.111520
iteration 1868: loss: 62.562054, loss_kl: 0.094468, loss_recon: 0.623410, loss_pred: 0.130303
iteration 1869: loss: 59.660278, loss_kl: 0.073739, loss_recon: 0.594870, loss_pred: 0.102485
iteration 1870: loss: 59.558006, loss_kl: 0.171728, loss_recon: 0.592246, loss_pred: 0.168495
 94%|███████████████████████████  | 187/200 [2:34:11<10:47, 49.83s/it]iteration 1871: loss: 60.328014, loss_kl: 0.062035, loss_recon: 0.600752, loss_pred: 0.190771
iteration 1872: loss: 60.905590, loss_kl: 0.105646, loss_recon: 0.606890, loss_pred: 0.110932
iteration 1873: loss: 62.136776, loss_kl: 0.170388, loss_recon: 0.617632, loss_pred: 0.203156
iteration 1874: loss: 61.539875, loss_kl: 0.126134, loss_recon: 0.612721, loss_pred: 0.141647
iteration 1875: loss: 60.263603, loss_kl: 0.047790, loss_recon: 0.600922, loss_pred: 0.123640
iteration 1876: loss: 60.792110, loss_kl: 0.062944, loss_recon: 0.605641, loss_pred: 0.165023
iteration 1877: loss: 60.508568, loss_kl: 0.164441, loss_recon: 0.602400, loss_pred: 0.104146
iteration 1878: loss: 61.253819, loss_kl: 0.080244, loss_recon: 0.610601, loss_pred: 0.113506
iteration 1879: loss: 60.291679, loss_kl: -0.021116, loss_recon: 0.601989, loss_pred: 0.113924
iteration 1880: loss: 60.879562, loss_kl: -0.033306, loss_recon: 0.607133, loss_pred: 0.199576
 94%|███████████████████████████▎ | 188/200 [2:35:00<09:54, 49.57s/it]iteration 1881: loss: 60.235104, loss_kl: 0.074804, loss_recon: 0.600295, loss_pred: 0.130847
iteration 1882: loss: 61.039658, loss_kl: 0.103394, loss_recon: 0.608178, loss_pred: 0.118449
iteration 1883: loss: 61.656399, loss_kl: 0.084813, loss_recon: 0.614642, loss_pred: 0.107401
iteration 1884: loss: 61.320564, loss_kl: 0.198761, loss_recon: 0.610060, loss_pred: 0.115830
iteration 1885: loss: 61.089153, loss_kl: 0.137481, loss_recon: 0.608401, loss_pred: 0.111596
iteration 1886: loss: 60.649109, loss_kl: 0.049182, loss_recon: 0.604340, loss_pred: 0.165972
iteration 1887: loss: 61.049820, loss_kl: 0.044216, loss_recon: 0.608813, loss_pred: 0.124298
iteration 1888: loss: 59.319241, loss_kl: 0.146266, loss_recon: 0.590077, loss_pred: 0.165240
iteration 1889: loss: 60.502186, loss_kl: 0.097341, loss_recon: 0.602548, loss_pred: 0.150074
iteration 1890: loss: 65.381210, loss_kl: 0.281502, loss_recon: 0.649408, loss_pred: 0.158867
 94%|███████████████████████████▍ | 189/200 [2:35:50<09:04, 49.50s/it]iteration 1891: loss: 60.483894, loss_kl: 0.127163, loss_recon: 0.602171, loss_pred: 0.139602
iteration 1892: loss: 61.794415, loss_kl: 0.106433, loss_recon: 0.615606, loss_pred: 0.127330
iteration 1893: loss: 60.507965, loss_kl: 0.043443, loss_recon: 0.603359, loss_pred: 0.128620
iteration 1894: loss: 60.329445, loss_kl: 0.022107, loss_recon: 0.602062, loss_pred: 0.101168
iteration 1895: loss: 60.999928, loss_kl: 0.102278, loss_recon: 0.607979, loss_pred: 0.099735
iteration 1896: loss: 61.573467, loss_kl: 0.020832, loss_recon: 0.614087, loss_pred: 0.143947
iteration 1897: loss: 59.996422, loss_kl: 0.114629, loss_recon: 0.597558, loss_pred: 0.125985
iteration 1898: loss: 61.423607, loss_kl: 0.220668, loss_recon: 0.610768, loss_pred: 0.126180
iteration 1899: loss: 61.187550, loss_kl: 0.131149, loss_recon: 0.608939, loss_pred: 0.162518
iteration 1900: loss: 60.516609, loss_kl: 0.077672, loss_recon: 0.602509, loss_pred: 0.188085
 95%|███████████████████████████▌ | 190/200 [2:36:39<08:13, 49.32s/it]iteration 1901: loss: 60.983135, loss_kl: 0.135138, loss_recon: 0.607312, loss_pred: 0.116807
iteration 1902: loss: 60.945930, loss_kl: 0.094513, loss_recon: 0.606820, loss_pred: 0.169371
iteration 1903: loss: 60.940392, loss_kl: 0.033990, loss_recon: 0.607536, loss_pred: 0.152764
iteration 1904: loss: 61.212730, loss_kl: 0.093537, loss_recon: 0.610078, loss_pred: 0.111360
iteration 1905: loss: 61.073738, loss_kl: 0.107390, loss_recon: 0.607554, loss_pred: 0.210934
iteration 1906: loss: 60.198898, loss_kl: 0.029737, loss_recon: 0.599556, loss_pred: 0.213536
iteration 1907: loss: 59.879978, loss_kl: 0.154995, loss_recon: 0.596079, loss_pred: 0.117034
iteration 1908: loss: 61.204273, loss_kl: -0.007670, loss_recon: 0.610877, loss_pred: 0.124252
iteration 1909: loss: 61.128456, loss_kl: 0.084796, loss_recon: 0.608878, loss_pred: 0.155810
iteration 1910: loss: 62.817146, loss_kl: 0.230949, loss_recon: 0.624270, loss_pred: 0.159226
 96%|███████████████████████████▋ | 191/200 [2:37:28<07:24, 49.41s/it]iteration 1911: loss: 61.945606, loss_kl: 0.111449, loss_recon: 0.616804, loss_pred: 0.153804
iteration 1912: loss: 60.257759, loss_kl: 0.092383, loss_recon: 0.600049, loss_pred: 0.160431
iteration 1913: loss: 60.865646, loss_kl: -0.062994, loss_recon: 0.607634, loss_pred: 0.165206
iteration 1914: loss: 61.439369, loss_kl: 0.086741, loss_recon: 0.612364, loss_pred: 0.116179
iteration 1915: loss: 60.568302, loss_kl: 0.015314, loss_recon: 0.603830, loss_pred: 0.169973
iteration 1916: loss: 60.481270, loss_kl: 0.101346, loss_recon: 0.602893, loss_pred: 0.090666
iteration 1917: loss: 60.390015, loss_kl: 0.064096, loss_recon: 0.602389, loss_pred: 0.087028
iteration 1918: loss: 60.232925, loss_kl: 0.061550, loss_recon: 0.600328, loss_pred: 0.138560
iteration 1919: loss: 60.996048, loss_kl: 0.109574, loss_recon: 0.607671, loss_pred: 0.119386
iteration 1920: loss: 62.431400, loss_kl: 0.040721, loss_recon: 0.622934, loss_pred: 0.097264
 96%|███████████████████████████▊ | 192/200 [2:38:18<06:34, 49.37s/it]iteration 1921: loss: 60.884968, loss_kl: 0.084833, loss_recon: 0.607034, loss_pred: 0.096722
iteration 1922: loss: 60.828400, loss_kl: 0.111211, loss_recon: 0.606331, loss_pred: 0.084085
iteration 1923: loss: 60.948154, loss_kl: 0.092489, loss_recon: 0.606915, loss_pred: 0.164215
iteration 1924: loss: 61.491272, loss_kl: 0.108187, loss_recon: 0.612856, loss_pred: 0.097456
iteration 1925: loss: 60.956219, loss_kl: 0.079886, loss_recon: 0.607692, loss_pred: 0.107172
iteration 1926: loss: 59.710640, loss_kl: 0.117882, loss_recon: 0.594719, loss_pred: 0.120905
iteration 1927: loss: 60.728779, loss_kl: 0.087322, loss_recon: 0.605466, loss_pred: 0.094858
iteration 1928: loss: 60.628880, loss_kl: -0.004362, loss_recon: 0.605210, loss_pred: 0.112238
iteration 1929: loss: 61.219837, loss_kl: 0.066213, loss_recon: 0.610773, loss_pred: 0.076294
iteration 1930: loss: 61.494934, loss_kl: 0.032868, loss_recon: 0.613808, loss_pred: 0.081275
 96%|███████████████████████████▉ | 193/200 [2:39:06<05:44, 49.23s/it]iteration 1931: loss: 59.792118, loss_kl: 0.067347, loss_recon: 0.595991, loss_pred: 0.125681
iteration 1932: loss: 62.120159, loss_kl: 0.143596, loss_recon: 0.618754, loss_pred: 0.101117
iteration 1933: loss: 61.648319, loss_kl: 0.097586, loss_recon: 0.614207, loss_pred: 0.129994
iteration 1934: loss: 59.956917, loss_kl: 0.087967, loss_recon: 0.597463, loss_pred: 0.122651
iteration 1935: loss: 61.130291, loss_kl: 0.025928, loss_recon: 0.609749, loss_pred: 0.129491
iteration 1936: loss: 62.125103, loss_kl: 0.155718, loss_recon: 0.618287, loss_pred: 0.140675
iteration 1937: loss: 60.342911, loss_kl: 0.029853, loss_recon: 0.601626, loss_pred: 0.150409
iteration 1938: loss: 60.720654, loss_kl: 0.050410, loss_recon: 0.605374, loss_pred: 0.132818
iteration 1939: loss: 60.511044, loss_kl: 0.079714, loss_recon: 0.603073, loss_pred: 0.123982
iteration 1940: loss: 60.589569, loss_kl: 0.052149, loss_recon: 0.603995, loss_pred: 0.137933
 97%|████████████████████████████▏| 194/200 [2:39:56<04:55, 49.22s/it]iteration 1941: loss: 60.445381, loss_kl: 0.071361, loss_recon: 0.602429, loss_pred: 0.131163
iteration 1942: loss: 61.057415, loss_kl: -0.049217, loss_recon: 0.610016, loss_pred: 0.105063
iteration 1943: loss: 59.063202, loss_kl: 0.076043, loss_recon: 0.588396, loss_pred: 0.147520
iteration 1944: loss: 61.596565, loss_kl: 0.088244, loss_recon: 0.614186, loss_pred: 0.089714
iteration 1945: loss: 59.385670, loss_kl: 0.069634, loss_recon: 0.592159, loss_pred: 0.100100
iteration 1946: loss: 60.514778, loss_kl: 0.047270, loss_recon: 0.603360, loss_pred: 0.131475
iteration 1947: loss: 62.709454, loss_kl: 0.044796, loss_recon: 0.625475, loss_pred: 0.117199
iteration 1948: loss: 60.937340, loss_kl: 0.158056, loss_recon: 0.606573, loss_pred: 0.122009
iteration 1949: loss: 61.648026, loss_kl: -0.020770, loss_recon: 0.615447, loss_pred: 0.124090
iteration 1950: loss: 61.348663, loss_kl: 0.094369, loss_recon: 0.611206, loss_pred: 0.133669
 98%|████████████████████████████▎| 195/200 [2:40:45<04:05, 49.11s/it]iteration 1951: loss: 60.736370, loss_kl: 0.082226, loss_recon: 0.605478, loss_pred: 0.106308
iteration 1952: loss: 61.360508, loss_kl: 0.123447, loss_recon: 0.611415, loss_pred: 0.095562
iteration 1953: loss: 61.103485, loss_kl: 0.066025, loss_recon: 0.609044, loss_pred: 0.133052
iteration 1954: loss: 60.225742, loss_kl: -0.000623, loss_recon: 0.601282, loss_pred: 0.098146
iteration 1955: loss: 60.762440, loss_kl: 0.042143, loss_recon: 0.605748, loss_pred: 0.145522
iteration 1956: loss: 60.727203, loss_kl: 0.056617, loss_recon: 0.605380, loss_pred: 0.132571
iteration 1957: loss: 60.856750, loss_kl: 0.107180, loss_recon: 0.606421, loss_pred: 0.107516
iteration 1958: loss: 60.145107, loss_kl: 0.115935, loss_recon: 0.599300, loss_pred: 0.099156
iteration 1959: loss: 61.946968, loss_kl: 0.019534, loss_recon: 0.618457, loss_pred: 0.081685
iteration 1960: loss: 59.582882, loss_kl: 0.187893, loss_recon: 0.591931, loss_pred: 0.201903
 98%|████████████████████████████▍| 196/200 [2:41:34<03:17, 49.27s/it]iteration 1961: loss: 59.855381, loss_kl: -0.015102, loss_recon: 0.597208, loss_pred: 0.149648
iteration 1962: loss: 60.961990, loss_kl: 0.102192, loss_recon: 0.607160, loss_pred: 0.143783
iteration 1963: loss: 60.255390, loss_kl: 0.021228, loss_recon: 0.600615, loss_pred: 0.172672
iteration 1964: loss: 60.665558, loss_kl: 0.184518, loss_recon: 0.603711, loss_pred: 0.109970
iteration 1965: loss: 61.625565, loss_kl: 0.072981, loss_recon: 0.614319, loss_pred: 0.120683
iteration 1966: loss: 60.783600, loss_kl: 0.033823, loss_recon: 0.605800, loss_pred: 0.169782
iteration 1967: loss: 61.085030, loss_kl: 0.084656, loss_recon: 0.608754, loss_pred: 0.125007
iteration 1968: loss: 61.624191, loss_kl: 0.049488, loss_recon: 0.614933, loss_pred: 0.081420
iteration 1969: loss: 60.492279, loss_kl: 0.114620, loss_recon: 0.602767, loss_pred: 0.100919
iteration 1970: loss: 63.154957, loss_kl: -0.039794, loss_recon: 0.630346, loss_pred: 0.160197
 98%|████████████████████████████▌| 197/200 [2:42:24<02:28, 49.38s/it]iteration 1971: loss: 61.467056, loss_kl: 0.029410, loss_recon: 0.612996, loss_pred: 0.138094
iteration 1972: loss: 59.884548, loss_kl: 0.175783, loss_recon: 0.595833, loss_pred: 0.125472
iteration 1973: loss: 61.509159, loss_kl: -0.007271, loss_recon: 0.613955, loss_pred: 0.120880
iteration 1974: loss: 61.612129, loss_kl: 0.078449, loss_recon: 0.614024, loss_pred: 0.131306
iteration 1975: loss: 60.200268, loss_kl: 0.127046, loss_recon: 0.598533, loss_pred: 0.219894
iteration 1976: loss: 61.689262, loss_kl: 0.011186, loss_recon: 0.615758, loss_pred: 0.102319
iteration 1977: loss: 60.834980, loss_kl: 0.108758, loss_recon: 0.604668, loss_pred: 0.259422
iteration 1978: loss: 60.541481, loss_kl: 0.104555, loss_recon: 0.603001, loss_pred: 0.136876
iteration 1979: loss: 60.683136, loss_kl: 0.118377, loss_recon: 0.603366, loss_pred: 0.228202
iteration 1980: loss: 59.788006, loss_kl: 0.031406, loss_recon: 0.596036, loss_pred: 0.152950
 99%|████████████████████████████▋| 198/200 [2:43:13<01:38, 49.47s/it]iteration 1981: loss: 61.107780, loss_kl: 0.106268, loss_recon: 0.607451, loss_pred: 0.256413
iteration 1982: loss: 60.785934, loss_kl: -0.024510, loss_recon: 0.607063, loss_pred: 0.104190
iteration 1983: loss: 61.473873, loss_kl: 0.038837, loss_recon: 0.612951, loss_pred: 0.139973
iteration 1984: loss: 62.414322, loss_kl: 0.050723, loss_recon: 0.622119, loss_pred: 0.151661
iteration 1985: loss: 60.149689, loss_kl: 0.091831, loss_recon: 0.599419, loss_pred: 0.115916
iteration 1986: loss: 59.985062, loss_kl: 0.037055, loss_recon: 0.598582, loss_pred: 0.089771
iteration 1987: loss: 59.669861, loss_kl: 0.104541, loss_recon: 0.594350, loss_pred: 0.130323
iteration 1988: loss: 61.068649, loss_kl: 0.074333, loss_recon: 0.608733, loss_pred: 0.121009
iteration 1989: loss: 61.198666, loss_kl: 0.129845, loss_recon: 0.609599, loss_pred: 0.108927
iteration 1990: loss: 61.245007, loss_kl: 0.163519, loss_recon: 0.609439, loss_pred: 0.137607
100%|████████████████████████████▊| 199/200 [2:44:03<00:49, 49.56s/it]iteration 1991: loss: 60.800388, loss_kl: 0.070671, loss_recon: 0.606410, loss_pred: 0.088674
iteration 1992: loss: 60.629784, loss_kl: 0.036375, loss_recon: 0.604784, loss_pred: 0.115027
iteration 1993: loss: 60.960033, loss_kl: 0.009845, loss_recon: 0.608445, loss_pred: 0.105736
iteration 1994: loss: 60.238903, loss_kl: 0.081643, loss_recon: 0.599891, loss_pred: 0.168178
iteration 1995: loss: 61.199314, loss_kl: 0.146490, loss_recon: 0.609482, loss_pred: 0.104599
iteration 1996: loss: 60.859741, loss_kl: 0.128688, loss_recon: 0.606192, loss_pred: 0.111877
iteration 1997: loss: 60.580959, loss_kl: 0.076271, loss_recon: 0.603194, loss_pred: 0.185283
iteration 1998: loss: 61.369495, loss_kl: 0.043996, loss_recon: 0.611964, loss_pred: 0.129071
iteration 1999: loss: 60.978485, loss_kl: 0.124206, loss_recon: 0.606208, loss_pred: 0.233445
iteration 2000: loss: 63.912209, loss_kl: 0.024482, loss_recon: 0.636135, loss_pred: 0.274218
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_199.pth
save model to ../model/TVG_Design[64, 64, 64]/TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234/epoch_199.pth
100%|████████████████████████████▊| 199/200 [2:44:52<00:49, 49.71s/it]
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design2', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], net_path=False, vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=False, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.001, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=64, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, number_of_samplings=6, num_classes=2, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[64, 64, 64]', distributed=False)
TVG_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.001_seed1234
4 test iterations per epoch
0it [00:00, ?it/s]0it [00:13, ?it/s]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 385, in <module>
    inferrer[dataset_name](args, net, test_save_path)
  File "/home/mhashemi/TransVNet/test.py", line 165, in inferrer_mat2
    name_batch, metric_batch = test_multiple_volumes_generative2(image_batch, label_batch, time_batch, model, name_batch, test_save_path, number_of_samplings)
  File "/home/mhashemi/TransVNet/utils.py", line 218, in test_multiple_volumes_generative2
    decoder_output = net.module.decoder(decoder_input)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mhashemi/TransVNet/networks/TransVNet_modeling.py", line 782, in forward
    x = self.decoder(x, features)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mhashemi/TransVNet/networks/TransVNet_modeling.py", line 630, in forward
    x = x.contiguous().view(B, hidden, h, w, d)
RuntimeError: shape '[1, 11, 2, 2, 2]' is invalid for input of size 121
