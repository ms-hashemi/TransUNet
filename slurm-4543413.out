/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=True, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.003, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[64, 64, 64]', distributed=False)
10 iterations per epoch. 2000 max iterations 
  0%|                                         | 0/200 [00:00<?, ?it/s]iteration 1: loss: 70.891281, loss_kl: 17.353088, loss_recon: 0.693148, loss_pred: 1.402912
iteration 2: loss: 72.747917, loss_kl: 33.333138, loss_recon: 0.703651, loss_pred: 2.049483
iteration 3: loss: 99.004257, loss_kl: 954.214111, loss_recon: 0.684983, loss_pred: 20.963856
iteration 4: loss: 74.442596, loss_kl: 251.073700, loss_recon: 0.688294, loss_pred: 3.102465
iteration 5: loss: 74.812874, loss_kl: 146.812042, loss_recon: 0.683565, loss_pred: 4.988303
iteration 6: loss: 74.965294, loss_kl: 137.902359, loss_recon: 0.683316, loss_pred: 5.254685
iteration 7: loss: 69.443642, loss_kl: 133.804489, loss_recon: 0.657560, loss_pred: 2.349583
iteration 8: loss: 71.895844, loss_kl: 143.139679, loss_recon: 0.681586, loss_pred: 2.305876
iteration 9: loss: 69.539093, loss_kl: 233.953049, loss_recon: 0.647634, loss_pred: 2.436148
iteration 10: loss: 68.489113, loss_kl: 288.630310, loss_recon: 0.641989, loss_pred: 1.403892
  0%|▏                              | 1/200 [00:55<3:02:39, 55.08s/it]iteration 11: loss: 68.453278, loss_kl: 286.414246, loss_recon: 0.648085, loss_pred: 0.780640
iteration 12: loss: 66.252274, loss_kl: 241.371185, loss_recon: 0.629767, loss_pred: 0.861810
iteration 13: loss: 65.505402, loss_kl: 199.991348, loss_recon: 0.625287, loss_pred: 0.976791
iteration 14: loss: 67.810493, loss_kl: 161.617950, loss_recon: 0.653022, loss_pred: 0.892129
iteration 15: loss: 68.521988, loss_kl: 263.383606, loss_recon: 0.644721, loss_pred: 1.416014
iteration 16: loss: 69.108589, loss_kl: 403.140137, loss_recon: 0.636571, loss_pred: 1.420132
iteration 17: loss: 70.425591, loss_kl: 476.698639, loss_recon: 0.646767, loss_pred: 0.981929
iteration 18: loss: 68.693359, loss_kl: 481.030701, loss_recon: 0.631462, loss_pred: 0.736843
iteration 19: loss: 68.742050, loss_kl: 402.595093, loss_recon: 0.638122, loss_pred: 0.903871
iteration 20: loss: 67.145676, loss_kl: 307.891602, loss_recon: 0.626908, loss_pred: 1.375951
  1%|▎                              | 2/200 [01:44<2:51:06, 51.85s/it]iteration 21: loss: 67.051666, loss_kl: 245.757553, loss_recon: 0.633401, loss_pred: 1.253964
iteration 22: loss: 66.034409, loss_kl: 180.619629, loss_recon: 0.631740, loss_pred: 1.054178
iteration 23: loss: 66.002495, loss_kl: 144.867905, loss_recon: 0.638095, loss_pred: 0.744341
iteration 24: loss: 65.174026, loss_kl: 135.444992, loss_recon: 0.628347, loss_pred: 0.984878
iteration 25: loss: 64.989410, loss_kl: 149.523880, loss_recon: 0.621324, loss_pred: 1.361735
iteration 26: loss: 64.146324, loss_kl: 144.792511, loss_recon: 0.615417, loss_pred: 1.156709
iteration 27: loss: 64.172424, loss_kl: 125.024147, loss_recon: 0.621580, loss_pred: 0.764212
iteration 28: loss: 64.786469, loss_kl: 107.669876, loss_recon: 0.631524, loss_pred: 0.557377
iteration 29: loss: 64.748100, loss_kl: 88.918663, loss_recon: 0.631855, loss_pred: 0.673371
iteration 30: loss: 64.580856, loss_kl: 77.479668, loss_recon: 0.631999, loss_pred: 0.606139
  2%|▍                              | 3/200 [02:35<2:48:01, 51.18s/it]iteration 31: loss: 63.435463, loss_kl: 74.746323, loss_recon: 0.621456, loss_pred: 0.542404
iteration 32: loss: 62.820614, loss_kl: 82.399536, loss_recon: 0.614399, loss_pred: 0.556737
iteration 33: loss: 63.917992, loss_kl: 84.107628, loss_recon: 0.626746, loss_pred: 0.402363
iteration 34: loss: 64.868469, loss_kl: 94.117371, loss_recon: 0.635960, loss_pred: 0.331329
iteration 35: loss: 64.714119, loss_kl: 86.458435, loss_recon: 0.635492, loss_pred: 0.300341
iteration 36: loss: 62.419773, loss_kl: 84.451828, loss_recon: 0.612146, loss_pred: 0.360702
iteration 37: loss: 64.385857, loss_kl: 81.655731, loss_recon: 0.630978, loss_pred: 0.471479
iteration 38: loss: 62.855526, loss_kl: 79.851730, loss_recon: 0.617193, loss_pred: 0.337715
iteration 39: loss: 63.365219, loss_kl: 76.671890, loss_recon: 0.623146, loss_pred: 0.283874
iteration 40: loss: 62.246235, loss_kl: 71.987335, loss_recon: 0.613059, loss_pred: 0.220424
  2%|▌                              | 4/200 [03:24<2:45:01, 50.52s/it]iteration 41: loss: 62.049721, loss_kl: 62.820377, loss_recon: 0.611924, loss_pred: 0.229152
iteration 42: loss: 62.887981, loss_kl: 56.046013, loss_recon: 0.620441, loss_pred: 0.283467
iteration 43: loss: 62.393867, loss_kl: 54.177258, loss_recon: 0.616003, loss_pred: 0.251783
iteration 44: loss: 63.344940, loss_kl: 49.648720, loss_recon: 0.626011, loss_pred: 0.247334
iteration 45: loss: 62.334370, loss_kl: 54.979240, loss_recon: 0.615462, loss_pred: 0.238356
iteration 46: loss: 63.904392, loss_kl: 54.079788, loss_recon: 0.631916, loss_pred: 0.171979
iteration 47: loss: 62.424450, loss_kl: 56.394142, loss_recon: 0.615865, loss_pred: 0.273983
iteration 48: loss: 61.919853, loss_kl: 53.353737, loss_recon: 0.611890, loss_pred: 0.197281
iteration 49: loss: 62.594143, loss_kl: 53.049408, loss_recon: 0.618925, loss_pred: 0.171138
iteration 50: loss: 63.205105, loss_kl: 50.710110, loss_recon: 0.624904, loss_pred: 0.207584
  2%|▊                              | 5/200 [04:14<2:43:03, 50.17s/it]iteration 51: loss: 62.593143, loss_kl: 55.057144, loss_recon: 0.618810, loss_pred: 0.161594
iteration 52: loss: 63.342754, loss_kl: 53.920547, loss_recon: 0.626163, loss_pred: 0.187202
iteration 53: loss: 62.721317, loss_kl: 55.194016, loss_recon: 0.620004, loss_pred: 0.168954
iteration 54: loss: 61.336994, loss_kl: 51.040176, loss_recon: 0.604503, loss_pred: 0.376299
iteration 55: loss: 63.553814, loss_kl: 44.587181, loss_recon: 0.629176, loss_pred: 0.190340
iteration 56: loss: 63.162380, loss_kl: 43.134766, loss_recon: 0.625198, loss_pred: 0.211197
iteration 57: loss: 61.841938, loss_kl: 46.599960, loss_recon: 0.612410, loss_pred: 0.134960
iteration 58: loss: 62.299664, loss_kl: 47.153954, loss_recon: 0.616832, loss_pred: 0.144948
iteration 59: loss: 61.670059, loss_kl: 46.896877, loss_recon: 0.610786, loss_pred: 0.122512
iteration 60: loss: 61.074947, loss_kl: 40.019505, loss_recon: 0.604288, loss_pred: 0.245954
  3%|▉                              | 6/200 [05:03<2:41:50, 50.06s/it]iteration 61: loss: 61.146118, loss_kl: 33.972511, loss_recon: 0.606639, loss_pred: 0.142531
iteration 62: loss: 61.700863, loss_kl: 30.588085, loss_recon: 0.612665, loss_pred: 0.128451
iteration 63: loss: 64.063057, loss_kl: 31.695044, loss_recon: 0.635896, loss_pred: 0.156522
iteration 64: loss: 62.667538, loss_kl: 35.159008, loss_recon: 0.621622, loss_pred: 0.153770
iteration 65: loss: 62.448322, loss_kl: 37.437855, loss_recon: 0.619383, loss_pred: 0.135668
iteration 66: loss: 62.359539, loss_kl: 35.632523, loss_recon: 0.618479, loss_pred: 0.155294
iteration 67: loss: 62.236057, loss_kl: 34.232555, loss_recon: 0.617775, loss_pred: 0.116212
iteration 68: loss: 62.183220, loss_kl: 29.785126, loss_recon: 0.617196, loss_pred: 0.165784
iteration 69: loss: 61.562737, loss_kl: 29.499832, loss_recon: 0.611541, loss_pred: 0.113616
iteration 70: loss: 62.062035, loss_kl: 29.447729, loss_recon: 0.616829, loss_pred: 0.084690
  4%|█                              | 7/200 [05:52<2:39:55, 49.72s/it]iteration 71: loss: 61.522106, loss_kl: 26.382874, loss_recon: 0.611486, loss_pred: 0.109644
iteration 72: loss: 61.725723, loss_kl: 27.030142, loss_recon: 0.612233, loss_pred: 0.232128
iteration 73: loss: 62.914387, loss_kl: 24.759394, loss_recon: 0.624797, loss_pred: 0.187129
iteration 74: loss: 62.255230, loss_kl: 25.424088, loss_recon: 0.618364, loss_pred: 0.164576
iteration 75: loss: 61.947842, loss_kl: 27.277344, loss_recon: 0.615629, loss_pred: 0.112189
iteration 76: loss: 62.185246, loss_kl: 26.653526, loss_recon: 0.618120, loss_pred: 0.106709
iteration 77: loss: 60.930328, loss_kl: 27.700922, loss_recon: 0.604786, loss_pred: 0.174737
iteration 78: loss: 61.767582, loss_kl: 27.198154, loss_recon: 0.613594, loss_pred: 0.136165
iteration 79: loss: 62.834049, loss_kl: 27.454563, loss_recon: 0.624095, loss_pred: 0.149998
iteration 80: loss: 61.984150, loss_kl: 26.467693, loss_recon: 0.616104, loss_pred: 0.109121
  4%|█▏                             | 8/200 [06:42<2:38:51, 49.64s/it]iteration 81: loss: 62.598785, loss_kl: 26.330036, loss_recon: 0.622421, loss_pred: 0.093339
iteration 82: loss: 61.428661, loss_kl: 26.677578, loss_recon: 0.610054, loss_pred: 0.156441
iteration 83: loss: 63.385395, loss_kl: 22.804180, loss_recon: 0.629765, loss_pred: 0.180835
iteration 84: loss: 61.505817, loss_kl: 23.087414, loss_recon: 0.611027, loss_pred: 0.172238
iteration 85: loss: 61.215576, loss_kl: 24.632393, loss_recon: 0.608540, loss_pred: 0.115284
iteration 86: loss: 62.951820, loss_kl: 24.531420, loss_recon: 0.624372, loss_pred: 0.269268
iteration 87: loss: 60.338634, loss_kl: 23.900158, loss_recon: 0.600139, loss_pred: 0.085716
iteration 88: loss: 61.881775, loss_kl: 22.019421, loss_recon: 0.615282, loss_pred: 0.133419
iteration 89: loss: 61.083397, loss_kl: 22.589916, loss_recon: 0.607272, loss_pred: 0.130344
iteration 90: loss: 61.097752, loss_kl: 19.455185, loss_recon: 0.606592, loss_pred: 0.243988
  4%|█▍                             | 9/200 [07:31<2:37:42, 49.54s/it]iteration 91: loss: 61.423264, loss_kl: 22.248943, loss_recon: 0.610717, loss_pred: 0.129042
iteration 92: loss: 61.593048, loss_kl: 21.312075, loss_recon: 0.612931, loss_pred: 0.086809
iteration 93: loss: 61.767838, loss_kl: 24.595428, loss_recon: 0.613497, loss_pred: 0.172217
iteration 94: loss: 62.549965, loss_kl: 27.080303, loss_recon: 0.621308, loss_pred: 0.148383
iteration 95: loss: 61.919205, loss_kl: 26.146456, loss_recon: 0.615397, loss_pred: 0.118020
iteration 96: loss: 61.835770, loss_kl: 23.940683, loss_recon: 0.614046, loss_pred: 0.191783
iteration 97: loss: 61.080803, loss_kl: 21.480030, loss_recon: 0.606827, loss_pred: 0.183288
iteration 98: loss: 62.074150, loss_kl: 21.038090, loss_recon: 0.617606, loss_pred: 0.103206
iteration 99: loss: 59.820633, loss_kl: 20.265984, loss_recon: 0.594161, loss_pred: 0.201862
iteration 100: loss: 61.053837, loss_kl: 20.125996, loss_recon: 0.606607, loss_pred: 0.191825
  5%|█▌                            | 10/200 [08:21<2:36:55, 49.56s/it]iteration 101: loss: 61.135120, loss_kl: 20.577112, loss_recon: 0.606730, loss_pred: 0.256365
iteration 102: loss: 61.420048, loss_kl: 23.524380, loss_recon: 0.610133, loss_pred: 0.171505
iteration 103: loss: 61.057915, loss_kl: 25.732138, loss_recon: 0.606719, loss_pred: 0.128669
iteration 104: loss: 61.808441, loss_kl: 26.452106, loss_recon: 0.613289, loss_pred: 0.215064
iteration 105: loss: 61.650219, loss_kl: 24.180202, loss_recon: 0.612715, loss_pred: 0.136922
iteration 106: loss: 61.473637, loss_kl: 22.740749, loss_recon: 0.611232, loss_pred: 0.123047
iteration 107: loss: 61.193268, loss_kl: 22.286751, loss_recon: 0.608539, loss_pred: 0.116547
iteration 108: loss: 61.492401, loss_kl: 19.905869, loss_recon: 0.611064, loss_pred: 0.186899
iteration 109: loss: 62.025703, loss_kl: 20.373175, loss_recon: 0.617073, loss_pred: 0.114658
iteration 110: loss: 62.889927, loss_kl: 20.042507, loss_recon: 0.625839, loss_pred: 0.105625
  6%|█▋                            | 11/200 [09:11<2:36:12, 49.59s/it]iteration 111: loss: 62.442692, loss_kl: 19.052128, loss_recon: 0.620865, loss_pred: 0.165638
iteration 112: loss: 62.112663, loss_kl: 16.655937, loss_recon: 0.617792, loss_pred: 0.166932
iteration 113: loss: 61.524467, loss_kl: 17.050678, loss_recon: 0.612193, loss_pred: 0.134661
iteration 114: loss: 60.474953, loss_kl: 17.593510, loss_recon: 0.602005, loss_pred: 0.098472
iteration 115: loss: 61.376972, loss_kl: 17.681545, loss_recon: 0.610753, loss_pred: 0.124880
iteration 116: loss: 61.524361, loss_kl: 19.382446, loss_recon: 0.611792, loss_pred: 0.151366
iteration 117: loss: 61.614292, loss_kl: 18.130821, loss_recon: 0.613317, loss_pred: 0.101233
iteration 118: loss: 61.009369, loss_kl: 17.533043, loss_recon: 0.607234, loss_pred: 0.110658
iteration 119: loss: 62.294407, loss_kl: 15.745333, loss_recon: 0.619376, loss_pred: 0.199391
iteration 120: loss: 61.805920, loss_kl: 17.086090, loss_recon: 0.615038, loss_pred: 0.131216
  6%|█▊                            | 12/200 [10:00<2:35:27, 49.61s/it]iteration 121: loss: 61.224392, loss_kl: 15.921478, loss_recon: 0.609515, loss_pred: 0.113680
iteration 122: loss: 60.075920, loss_kl: 16.580036, loss_recon: 0.597419, loss_pred: 0.168233
iteration 123: loss: 62.335686, loss_kl: 16.800880, loss_recon: 0.620346, loss_pred: 0.133080
iteration 124: loss: 61.957779, loss_kl: 18.804150, loss_recon: 0.615341, loss_pred: 0.235653
iteration 125: loss: 61.820641, loss_kl: 18.853037, loss_recon: 0.615495, loss_pred: 0.082631
iteration 126: loss: 61.440876, loss_kl: 18.694241, loss_recon: 0.611450, loss_pred: 0.108976
iteration 127: loss: 62.646080, loss_kl: 16.543282, loss_recon: 0.623951, loss_pred: 0.085573
iteration 128: loss: 61.361050, loss_kl: 17.733107, loss_recon: 0.610361, loss_pred: 0.147574
iteration 129: loss: 61.709545, loss_kl: 15.720677, loss_recon: 0.614581, loss_pred: 0.094280
iteration 130: loss: 61.151905, loss_kl: 15.323030, loss_recon: 0.608261, loss_pred: 0.172557
  6%|█▉                            | 13/200 [10:50<2:34:48, 49.67s/it]iteration 131: loss: 61.408997, loss_kl: 14.495438, loss_recon: 0.606081, loss_pred: 0.081874
iteration 132: loss: 60.516205, loss_kl: 11.330839, loss_recon: 0.598341, loss_pred: 0.120109
iteration 133: loss: 62.015747, loss_kl: 11.072122, loss_recon: 0.613405, loss_pred: 0.126117
iteration 134: loss: 63.277908, loss_kl: 9.365170, loss_recon: 0.626084, loss_pred: 0.204997
iteration 135: loss: 62.067883, loss_kl: 11.978651, loss_recon: 0.613506, loss_pred: 0.123125
iteration 136: loss: 61.360767, loss_kl: 11.761456, loss_recon: 0.605801, loss_pred: 0.197277
iteration 137: loss: 63.564934, loss_kl: 12.494711, loss_recon: 0.627382, loss_pred: 0.207000
iteration 138: loss: 62.963478, loss_kl: 13.053567, loss_recon: 0.621031, loss_pred: 0.212894
iteration 139: loss: 62.196800, loss_kl: 12.013200, loss_recon: 0.614133, loss_pred: 0.187643
iteration 140: loss: 61.740097, loss_kl: 12.304566, loss_recon: 0.610196, loss_pred: 0.110173
  7%|██                            | 14/200 [11:42<2:36:00, 50.33s/it]iteration 141: loss: 63.462753, loss_kl: 11.209765, loss_recon: 0.622493, loss_pred: 0.213502
iteration 142: loss: 61.230614, loss_kl: 9.112765, loss_recon: 0.602962, loss_pred: 0.121527
iteration 143: loss: 62.940735, loss_kl: 12.543501, loss_recon: 0.616535, loss_pred: 0.168356
iteration 144: loss: 61.917160, loss_kl: 12.716213, loss_recon: 0.606428, loss_pred: 0.140083
iteration 145: loss: 62.338745, loss_kl: 13.234670, loss_recon: 0.609970, loss_pred: 0.161246
iteration 146: loss: 62.739071, loss_kl: 11.360276, loss_recon: 0.615556, loss_pred: 0.170167
iteration 147: loss: 62.888237, loss_kl: 14.287110, loss_recon: 0.614266, loss_pred: 0.187264
iteration 148: loss: 63.408871, loss_kl: 14.441999, loss_recon: 0.619804, loss_pred: 0.140226
iteration 149: loss: 62.327991, loss_kl: 14.651434, loss_recon: 0.608874, loss_pred: 0.133645
iteration 150: loss: 63.584133, loss_kl: 14.870595, loss_recon: 0.620821, loss_pred: 0.175582
  8%|██▎                           | 15/200 [12:31<2:34:22, 50.07s/it]iteration 151: loss: 62.705555, loss_kl: 12.837275, loss_recon: 0.608642, loss_pred: 0.187889
iteration 152: loss: 63.674854, loss_kl: 14.712616, loss_recon: 0.615325, loss_pred: 0.247359
iteration 153: loss: 63.300251, loss_kl: 12.742127, loss_recon: 0.614943, loss_pred: 0.164765
iteration 154: loss: 62.337204, loss_kl: 16.302975, loss_recon: 0.600473, loss_pred: 0.190105
iteration 155: loss: 62.772724, loss_kl: 14.453062, loss_recon: 0.607696, loss_pred: 0.141619
iteration 156: loss: 62.295128, loss_kl: 14.981493, loss_recon: 0.601488, loss_pred: 0.216763
iteration 157: loss: 64.073151, loss_kl: 13.866772, loss_recon: 0.621070, loss_pred: 0.180079
iteration 158: loss: 66.867912, loss_kl: 16.251930, loss_recon: 0.644673, loss_pred: 0.307409
iteration 159: loss: 62.476234, loss_kl: 14.826718, loss_recon: 0.604233, loss_pred: 0.143229
iteration 160: loss: 63.177605, loss_kl: 15.311626, loss_recon: 0.611024, loss_pred: 0.103066
  8%|██▍                           | 16/200 [13:21<2:32:55, 49.87s/it]iteration 161: loss: 64.904808, loss_kl: 14.401265, loss_recon: 0.623499, loss_pred: 0.129772
iteration 162: loss: 64.122131, loss_kl: 13.687742, loss_recon: 0.616713, loss_pred: 0.145799
iteration 163: loss: 63.155651, loss_kl: 12.684790, loss_recon: 0.608494, loss_pred: 0.170180
iteration 164: loss: 62.953968, loss_kl: 12.239038, loss_recon: 0.607533, loss_pred: 0.139577
iteration 165: loss: 63.451717, loss_kl: 14.283915, loss_recon: 0.608397, loss_pred: 0.206629
iteration 166: loss: 64.060219, loss_kl: 11.238991, loss_recon: 0.620031, loss_pred: 0.164522
iteration 167: loss: 62.271671, loss_kl: 13.011769, loss_recon: 0.598750, loss_pred: 0.205503
iteration 168: loss: 63.577034, loss_kl: 15.443624, loss_recon: 0.608462, loss_pred: 0.130144
iteration 169: loss: 64.373268, loss_kl: 13.051627, loss_recon: 0.619788, loss_pred: 0.196595
iteration 170: loss: 63.324348, loss_kl: 10.391165, loss_recon: 0.613593, loss_pred: 0.215143
  8%|██▌                           | 17/200 [14:10<2:31:55, 49.81s/it]iteration 171: loss: 63.683273, loss_kl: 12.727633, loss_recon: 0.608505, loss_pred: 0.185434
iteration 172: loss: 63.226776, loss_kl: 11.186169, loss_recon: 0.607016, loss_pred: 0.198470
iteration 173: loss: 65.272964, loss_kl: 10.944757, loss_recon: 0.628880, loss_pred: 0.108416
iteration 174: loss: 63.343422, loss_kl: 9.543030, loss_recon: 0.612026, loss_pred: 0.155880
iteration 175: loss: 63.689373, loss_kl: 9.890315, loss_recon: 0.614752, loss_pred: 0.157035
iteration 176: loss: 63.081390, loss_kl: 9.378778, loss_recon: 0.609049, loss_pred: 0.225707
iteration 177: loss: 63.049686, loss_kl: 9.393942, loss_recon: 0.608463, loss_pred: 0.249453
iteration 178: loss: 63.005211, loss_kl: 8.358194, loss_recon: 0.610420, loss_pred: 0.224694
iteration 179: loss: 62.136345, loss_kl: 7.713957, loss_recon: 0.603604, loss_pred: 0.171442
iteration 180: loss: 64.037170, loss_kl: 8.601691, loss_recon: 0.620819, loss_pred: 0.166128
  9%|██▋                           | 18/200 [14:59<2:30:15, 49.54s/it]iteration 181: loss: 63.704723, loss_kl: 11.420115, loss_recon: 0.606882, loss_pred: 0.188861
iteration 182: loss: 63.448208, loss_kl: 7.566165, loss_recon: 0.614148, loss_pred: 0.160020
iteration 183: loss: 63.042522, loss_kl: 5.645879, loss_recon: 0.614968, loss_pred: 0.147802
iteration 184: loss: 61.746567, loss_kl: 6.801027, loss_recon: 0.598850, loss_pred: 0.177616
iteration 185: loss: 63.077892, loss_kl: 8.380676, loss_recon: 0.608521, loss_pred: 0.150725
iteration 186: loss: 61.803181, loss_kl: 6.865666, loss_recon: 0.599506, loss_pred: 0.152612
iteration 187: loss: 64.156441, loss_kl: 7.172184, loss_recon: 0.621895, loss_pred: 0.191134
iteration 188: loss: 64.136726, loss_kl: 9.384184, loss_recon: 0.616639, loss_pred: 0.149338
iteration 189: loss: 63.474541, loss_kl: 7.408154, loss_recon: 0.615237, loss_pred: 0.116539
iteration 190: loss: 63.091141, loss_kl: 8.129505, loss_recon: 0.609422, loss_pred: 0.136051
 10%|██▊                           | 19/200 [15:49<2:29:18, 49.49s/it]iteration 191: loss: 62.955406, loss_kl: 8.226961, loss_recon: 0.603719, loss_pred: 0.220727
iteration 192: loss: 63.839844, loss_kl: 7.183777, loss_recon: 0.615999, loss_pred: 0.176813
iteration 193: loss: 64.087135, loss_kl: 6.739990, loss_recon: 0.619692, loss_pred: 0.182175
iteration 194: loss: 65.031990, loss_kl: 8.949013, loss_recon: 0.622863, loss_pred: 0.175532
iteration 195: loss: 63.038525, loss_kl: 7.214283, loss_recon: 0.608348, loss_pred: 0.131802
iteration 196: loss: 61.372322, loss_kl: 6.727460, loss_recon: 0.592638, loss_pred: 0.176366
iteration 197: loss: 62.547165, loss_kl: 6.467730, loss_recon: 0.605417, loss_pred: 0.147980
iteration 198: loss: 63.957825, loss_kl: 6.052226, loss_recon: 0.620615, loss_pred: 0.158082
iteration 199: loss: 61.514549, loss_kl: 5.975023, loss_recon: 0.596496, loss_pred: 0.148926
iteration 200: loss: 64.616646, loss_kl: 5.860596, loss_recon: 0.627934, loss_pred: 0.140044
 10%|███                           | 20/200 [16:39<2:29:06, 49.70s/it]iteration 201: loss: 63.131447, loss_kl: 5.588674, loss_recon: 0.612047, loss_pred: 0.100415
iteration 202: loss: 62.220074, loss_kl: 6.008118, loss_recon: 0.601067, loss_pred: 0.149952
iteration 203: loss: 63.434357, loss_kl: 5.771260, loss_recon: 0.614330, loss_pred: 0.115356
iteration 204: loss: 61.890778, loss_kl: 5.488571, loss_recon: 0.599116, loss_pred: 0.185526
iteration 205: loss: 64.317917, loss_kl: 7.379752, loss_recon: 0.617592, loss_pred: 0.147057
iteration 206: loss: 64.404068, loss_kl: 6.465582, loss_recon: 0.621555, loss_pred: 0.135635
iteration 207: loss: 63.133495, loss_kl: 5.389022, loss_recon: 0.612465, loss_pred: 0.125852
iteration 208: loss: 63.343063, loss_kl: 5.302583, loss_recon: 0.614810, loss_pred: 0.129158
iteration 209: loss: 63.032494, loss_kl: 5.601192, loss_recon: 0.610651, loss_pred: 0.136966
iteration 210: loss: 62.422527, loss_kl: 5.474359, loss_recon: 0.603366, loss_pred: 0.296876
 10%|███▏                          | 21/200 [17:29<2:28:35, 49.81s/it]iteration 211: loss: 63.707100, loss_kl: 5.282971, loss_recon: 0.616307, loss_pred: 0.140766
iteration 212: loss: 62.997395, loss_kl: 4.824175, loss_recon: 0.611095, loss_pred: 0.120305
iteration 213: loss: 63.101982, loss_kl: 5.088413, loss_recon: 0.611226, loss_pred: 0.114936
iteration 214: loss: 62.906487, loss_kl: 6.094946, loss_recon: 0.605580, loss_pred: 0.115265
iteration 215: loss: 62.879082, loss_kl: 6.375604, loss_recon: 0.603771, loss_pred: 0.165975
iteration 216: loss: 63.458027, loss_kl: 5.138667, loss_recon: 0.614067, loss_pred: 0.168476
iteration 217: loss: 65.032684, loss_kl: 5.552311, loss_recon: 0.628142, loss_pred: 0.184080
iteration 218: loss: 62.437702, loss_kl: 5.965491, loss_recon: 0.600840, loss_pred: 0.167923
iteration 219: loss: 63.519951, loss_kl: 6.260562, loss_recon: 0.610085, loss_pred: 0.217565
iteration 220: loss: 62.628170, loss_kl: 4.069893, loss_recon: 0.608740, loss_pred: 0.262960
 11%|███▎                          | 22/200 [18:18<2:27:30, 49.72s/it]iteration 221: loss: 64.071007, loss_kl: 4.075035, loss_recon: 0.622770, loss_pred: 0.139530
iteration 222: loss: 62.669586, loss_kl: 4.170342, loss_recon: 0.608242, loss_pred: 0.152267
iteration 223: loss: 61.962288, loss_kl: 3.808419, loss_recon: 0.601653, loss_pred: 0.250759
iteration 224: loss: 62.267879, loss_kl: 4.074651, loss_recon: 0.604455, loss_pred: 0.168021
iteration 225: loss: 62.056110, loss_kl: 3.989930, loss_recon: 0.601747, loss_pred: 0.261508
iteration 226: loss: 62.741425, loss_kl: 3.577562, loss_recon: 0.609488, loss_pred: 0.340092
iteration 227: loss: 63.682812, loss_kl: 3.648456, loss_recon: 0.619404, loss_pred: 0.261121
iteration 228: loss: 63.874565, loss_kl: 3.895454, loss_recon: 0.621631, loss_pred: 0.129925
iteration 229: loss: 62.149441, loss_kl: 2.894831, loss_recon: 0.608504, loss_pred: 0.123732
iteration 230: loss: 62.366325, loss_kl: 3.851136, loss_recon: 0.606033, loss_pred: 0.199469
 12%|███▍                          | 23/200 [19:08<2:26:34, 49.69s/it]iteration 231: loss: 61.833893, loss_kl: 2.560348, loss_recon: 0.604411, loss_pred: 0.251936
iteration 232: loss: 62.955002, loss_kl: 4.122278, loss_recon: 0.607715, loss_pred: 0.346576
iteration 233: loss: 61.994591, loss_kl: 4.672509, loss_recon: 0.595246, loss_pred: 0.387879
iteration 234: loss: 63.816948, loss_kl: 3.684624, loss_recon: 0.620421, loss_pred: 0.132948
iteration 235: loss: 62.474216, loss_kl: 4.111441, loss_recon: 0.604966, loss_pred: 0.145585
iteration 236: loss: 62.670975, loss_kl: 3.952295, loss_recon: 0.607451, loss_pred: 0.164724
iteration 237: loss: 63.048447, loss_kl: 2.898913, loss_recon: 0.614985, loss_pred: 0.258165
iteration 238: loss: 63.751198, loss_kl: 3.592947, loss_recon: 0.619334, loss_pred: 0.216748
iteration 239: loss: 62.691875, loss_kl: 3.505436, loss_recon: 0.609747, loss_pred: 0.155203
iteration 240: loss: 62.706604, loss_kl: 2.591077, loss_recon: 0.613789, loss_pred: 0.173118
 12%|███▌                          | 24/200 [19:57<2:25:17, 49.53s/it]iteration 241: loss: 61.937050, loss_kl: 2.461077, loss_recon: 0.605034, loss_pred: 0.239486
iteration 242: loss: 63.349072, loss_kl: 3.716409, loss_recon: 0.613651, loss_pred: 0.180783
iteration 243: loss: 63.449478, loss_kl: 2.348857, loss_recon: 0.621934, loss_pred: 0.116429
iteration 244: loss: 64.107361, loss_kl: 3.701869, loss_recon: 0.621950, loss_pred: 0.116233
iteration 245: loss: 61.141338, loss_kl: 2.179518, loss_recon: 0.598539, loss_pred: 0.229966
iteration 246: loss: 63.185638, loss_kl: 2.904385, loss_recon: 0.616605, loss_pred: 0.115908
iteration 247: loss: 61.959282, loss_kl: 2.910917, loss_recon: 0.604235, loss_pred: 0.123454
iteration 248: loss: 61.190758, loss_kl: 2.164252, loss_recon: 0.600049, loss_pred: 0.135785
iteration 249: loss: 62.226894, loss_kl: 2.547688, loss_recon: 0.608822, loss_pred: 0.108539
iteration 250: loss: 62.881676, loss_kl: 3.021302, loss_recon: 0.612990, loss_pred: 0.116702
 12%|███▊                          | 25/200 [20:48<2:25:56, 50.04s/it]iteration 251: loss: 60.707458, loss_kl: 2.658492, loss_recon: 0.591776, loss_pred: 0.134673
iteration 252: loss: 61.836449, loss_kl: 2.269062, loss_recon: 0.604586, loss_pred: 0.187089
iteration 253: loss: 64.140953, loss_kl: 2.667183, loss_recon: 0.625828, loss_pred: 0.158393
iteration 254: loss: 62.484730, loss_kl: 1.734399, loss_recon: 0.614625, loss_pred: 0.112030
iteration 255: loss: 62.371441, loss_kl: 2.295207, loss_recon: 0.610485, loss_pred: 0.118456
iteration 256: loss: 62.297890, loss_kl: 2.232849, loss_recon: 0.609439, loss_pred: 0.182170
iteration 257: loss: 62.083649, loss_kl: 1.630819, loss_recon: 0.610263, loss_pred: 0.201515
iteration 258: loss: 61.984451, loss_kl: 1.907643, loss_recon: 0.608421, loss_pred: 0.141251
iteration 259: loss: 63.348103, loss_kl: 1.451038, loss_recon: 0.624662, loss_pred: 0.120388
iteration 260: loss: 61.164604, loss_kl: 2.192728, loss_recon: 0.598215, loss_pred: 0.192402
 13%|███▉                          | 26/200 [21:38<2:24:51, 49.95s/it]iteration 261: loss: 62.058399, loss_kl: 2.290967, loss_recon: 0.606392, loss_pred: 0.126210
iteration 262: loss: 62.648464, loss_kl: 1.644026, loss_recon: 0.615444, loss_pred: 0.176146
iteration 263: loss: 61.846489, loss_kl: 2.319273, loss_recon: 0.603287, loss_pred: 0.208776
iteration 264: loss: 62.435757, loss_kl: 2.423339, loss_recon: 0.609165, loss_pred: 0.151527
iteration 265: loss: 62.375031, loss_kl: 1.853549, loss_recon: 0.612025, loss_pred: 0.126380
iteration 266: loss: 61.743202, loss_kl: 1.340442, loss_recon: 0.608380, loss_pred: 0.148691
iteration 267: loss: 61.678844, loss_kl: 1.807783, loss_recon: 0.604571, loss_pred: 0.201409
iteration 268: loss: 60.320465, loss_kl: 1.236088, loss_recon: 0.593620, loss_pred: 0.260864
iteration 269: loss: 63.137886, loss_kl: 1.343454, loss_recon: 0.622646, loss_pred: 0.115013
iteration 270: loss: 63.022896, loss_kl: 1.361622, loss_recon: 0.621594, loss_pred: 0.094973
 14%|████                          | 27/200 [22:27<2:23:20, 49.71s/it]iteration 271: loss: 62.335907, loss_kl: 1.635237, loss_recon: 0.612489, loss_pred: 0.099319
iteration 272: loss: 61.775070, loss_kl: 1.569891, loss_recon: 0.606372, loss_pred: 0.189676
iteration 273: loss: 63.091816, loss_kl: 1.693451, loss_recon: 0.619007, loss_pred: 0.168254
iteration 274: loss: 61.336849, loss_kl: 1.340816, loss_recon: 0.604051, loss_pred: 0.121871
iteration 275: loss: 62.697773, loss_kl: 1.351435, loss_recon: 0.617561, loss_pred: 0.125404
iteration 276: loss: 61.939854, loss_kl: 1.918901, loss_recon: 0.606272, loss_pred: 0.153670
iteration 277: loss: 61.585136, loss_kl: 1.360244, loss_recon: 0.606332, loss_pred: 0.130399
iteration 278: loss: 61.156250, loss_kl: 1.924442, loss_recon: 0.598455, loss_pred: 0.148392
iteration 279: loss: 63.599678, loss_kl: 1.330968, loss_recon: 0.625703, loss_pred: 0.225511
iteration 280: loss: 62.540115, loss_kl: 1.467049, loss_recon: 0.615003, loss_pred: 0.153671
 14%|████▏                         | 28/200 [23:17<2:22:32, 49.72s/it]iteration 281: loss: 61.693695, loss_kl: 1.249809, loss_recon: 0.607074, loss_pred: 0.181906
iteration 282: loss: 62.551357, loss_kl: 1.480587, loss_recon: 0.614451, loss_pred: 0.153382
iteration 283: loss: 62.387550, loss_kl: 1.307575, loss_recon: 0.614128, loss_pred: 0.133226
iteration 284: loss: 61.082764, loss_kl: 1.292279, loss_recon: 0.600131, loss_pred: 0.237927
iteration 285: loss: 62.314457, loss_kl: 0.952202, loss_recon: 0.615267, loss_pred: 0.174886
iteration 286: loss: 62.674023, loss_kl: 1.771352, loss_recon: 0.613410, loss_pred: 0.193029
iteration 287: loss: 60.739262, loss_kl: 1.378725, loss_recon: 0.595758, loss_pred: 0.276088
iteration 288: loss: 61.598923, loss_kl: 1.396551, loss_recon: 0.605369, loss_pred: 0.163225
iteration 289: loss: 62.527412, loss_kl: 0.981778, loss_recon: 0.617650, loss_pred: 0.130505
iteration 290: loss: 62.625439, loss_kl: 1.225303, loss_recon: 0.616621, loss_pred: 0.174696
 14%|████▎                         | 29/200 [24:06<2:21:20, 49.60s/it]iteration 291: loss: 61.252110, loss_kl: 1.244653, loss_recon: 0.601270, loss_pred: 0.274802
iteration 292: loss: 62.263069, loss_kl: 0.756153, loss_recon: 0.616129, loss_pred: 0.133561
iteration 293: loss: 62.041294, loss_kl: 1.309657, loss_recon: 0.610086, loss_pred: 0.137914
iteration 294: loss: 60.945976, loss_kl: 1.002784, loss_recon: 0.600828, loss_pred: 0.178029
iteration 295: loss: 62.174915, loss_kl: 1.065978, loss_recon: 0.613284, loss_pred: 0.118247
iteration 296: loss: 61.596127, loss_kl: 1.008776, loss_recon: 0.607439, loss_pred: 0.162985
iteration 297: loss: 62.900440, loss_kl: 1.721230, loss_recon: 0.615782, loss_pred: 0.146295
iteration 298: loss: 61.613003, loss_kl: 1.148665, loss_recon: 0.606573, loss_pred: 0.170927
iteration 299: loss: 62.961163, loss_kl: 1.324576, loss_recon: 0.619329, loss_pred: 0.123267
iteration 300: loss: 61.366425, loss_kl: 1.151628, loss_recon: 0.603601, loss_pred: 0.219575
 15%|████▌                         | 30/200 [24:56<2:20:25, 49.56s/it]iteration 301: loss: 62.808380, loss_kl: 1.554105, loss_recon: 0.615558, loss_pred: 0.129257
iteration 302: loss: 62.171608, loss_kl: 1.272595, loss_recon: 0.610577, loss_pred: 0.194123
iteration 303: loss: 61.782001, loss_kl: 1.153795, loss_recon: 0.608030, loss_pred: 0.144989
iteration 304: loss: 61.697784, loss_kl: 0.855349, loss_recon: 0.609262, loss_pred: 0.153375
iteration 305: loss: 60.435810, loss_kl: 0.926552, loss_recon: 0.595689, loss_pred: 0.197181
iteration 306: loss: 62.107914, loss_kl: 0.671557, loss_recon: 0.614504, loss_pred: 0.172119
iteration 307: loss: 62.969124, loss_kl: 1.414994, loss_recon: 0.617786, loss_pred: 0.167760
iteration 308: loss: 60.803314, loss_kl: 0.671077, loss_recon: 0.600951, loss_pred: 0.223109
iteration 309: loss: 62.167217, loss_kl: 1.015739, loss_recon: 0.612045, loss_pred: 0.228543
iteration 310: loss: 61.529690, loss_kl: 1.119635, loss_recon: 0.605314, loss_pred: 0.188997
 16%|████▋                         | 31/200 [25:45<2:19:31, 49.54s/it]iteration 311: loss: 62.161724, loss_kl: 1.037057, loss_recon: 0.612339, loss_pred: 0.137123
iteration 312: loss: 63.350842, loss_kl: 0.860470, loss_recon: 0.625555, loss_pred: 0.139304
iteration 313: loss: 62.078972, loss_kl: 0.481988, loss_recon: 0.615460, loss_pred: 0.165479
iteration 314: loss: 62.460957, loss_kl: 0.946809, loss_recon: 0.616053, loss_pred: 0.133840
iteration 315: loss: 61.165638, loss_kl: 1.259550, loss_recon: 0.599876, loss_pred: 0.217728
iteration 316: loss: 60.284180, loss_kl: 0.742140, loss_recon: 0.594848, loss_pred: 0.233606
iteration 317: loss: 62.113071, loss_kl: 1.030299, loss_recon: 0.611731, loss_pred: 0.154482
iteration 318: loss: 61.778763, loss_kl: 0.651799, loss_recon: 0.611470, loss_pred: 0.134785
iteration 319: loss: 62.582680, loss_kl: 1.083875, loss_recon: 0.615886, loss_pred: 0.167709
iteration 320: loss: 61.856720, loss_kl: 0.787756, loss_recon: 0.610812, loss_pred: 0.174883
 16%|████▊                         | 32/200 [26:35<2:18:25, 49.44s/it]iteration 321: loss: 60.930412, loss_kl: 0.910601, loss_recon: 0.599843, loss_pred: 0.215858
iteration 322: loss: 63.348759, loss_kl: 0.969492, loss_recon: 0.624246, loss_pred: 0.146594
iteration 323: loss: 60.639870, loss_kl: 1.221302, loss_recon: 0.593963, loss_pred: 0.264094
iteration 324: loss: 61.815536, loss_kl: 0.711477, loss_recon: 0.610981, loss_pred: 0.146865
iteration 325: loss: 62.617508, loss_kl: 0.999783, loss_recon: 0.616750, loss_pred: 0.140670
iteration 326: loss: 61.338131, loss_kl: 0.546000, loss_recon: 0.607788, loss_pred: 0.121395
iteration 327: loss: 63.202332, loss_kl: 0.338448, loss_recon: 0.627645, loss_pred: 0.166421
iteration 328: loss: 61.869610, loss_kl: 1.339392, loss_recon: 0.606310, loss_pred: 0.164404
iteration 329: loss: 61.214985, loss_kl: 0.890319, loss_recon: 0.603327, loss_pred: 0.168237
iteration 330: loss: 62.432693, loss_kl: 1.014975, loss_recon: 0.614168, loss_pred: 0.201889
 16%|████▉                         | 33/200 [27:25<2:18:03, 49.60s/it]iteration 331: loss: 60.754997, loss_kl: 0.963027, loss_recon: 0.596910, loss_pred: 0.253478
iteration 332: loss: 61.670765, loss_kl: 0.548716, loss_recon: 0.609846, loss_pred: 0.224396
iteration 333: loss: 62.138344, loss_kl: 0.932883, loss_recon: 0.611683, loss_pred: 0.184910
iteration 334: loss: 61.831810, loss_kl: 0.865908, loss_recon: 0.609630, loss_pred: 0.140092
iteration 335: loss: 62.912945, loss_kl: 0.856210, loss_recon: 0.620439, loss_pred: 0.148510
iteration 336: loss: 62.052700, loss_kl: 0.928769, loss_recon: 0.611233, loss_pred: 0.147786
iteration 337: loss: 61.972046, loss_kl: 1.267998, loss_recon: 0.607156, loss_pred: 0.189284
iteration 338: loss: 63.320679, loss_kl: 1.059092, loss_recon: 0.623064, loss_pred: 0.122952
iteration 339: loss: 61.470821, loss_kl: 0.645066, loss_recon: 0.607518, loss_pred: 0.176178
iteration 340: loss: 62.274055, loss_kl: 0.251313, loss_recon: 0.619206, loss_pred: 0.141980
 17%|█████                         | 34/200 [28:14<2:17:25, 49.67s/it]iteration 341: loss: 62.295261, loss_kl: 0.725321, loss_recon: 0.615053, loss_pred: 0.150834
iteration 342: loss: 61.767792, loss_kl: 1.166126, loss_recon: 0.605655, loss_pred: 0.174720
iteration 343: loss: 61.982281, loss_kl: 0.711270, loss_recon: 0.612423, loss_pred: 0.113249
iteration 344: loss: 61.803951, loss_kl: 0.846071, loss_recon: 0.609047, loss_pred: 0.153691
iteration 345: loss: 62.479053, loss_kl: 0.974105, loss_recon: 0.614984, loss_pred: 0.122274
iteration 346: loss: 61.598576, loss_kl: 1.048324, loss_recon: 0.604051, loss_pred: 0.269708
iteration 347: loss: 62.218399, loss_kl: 0.609213, loss_recon: 0.615621, loss_pred: 0.119423
iteration 348: loss: 62.475113, loss_kl: 0.686544, loss_recon: 0.617371, loss_pred: 0.133051
iteration 349: loss: 61.296719, loss_kl: 1.050206, loss_recon: 0.601083, loss_pred: 0.263017
iteration 350: loss: 60.080070, loss_kl: 0.226034, loss_recon: 0.596787, loss_pred: 0.202204
 18%|█████▎                        | 35/200 [29:04<2:16:56, 49.80s/it]iteration 351: loss: 61.954758, loss_kl: 0.604805, loss_recon: 0.612413, loss_pred: 0.156515
iteration 352: loss: 59.915974, loss_kl: 0.881813, loss_recon: 0.588759, loss_pred: 0.228049
iteration 353: loss: 61.775154, loss_kl: 0.769519, loss_recon: 0.609044, loss_pred: 0.162209
iteration 354: loss: 61.910847, loss_kl: 0.639090, loss_recon: 0.611800, loss_pred: 0.142376
iteration 355: loss: 62.919029, loss_kl: 0.942084, loss_recon: 0.619259, loss_pred: 0.125696
iteration 356: loss: 60.920494, loss_kl: 0.541851, loss_recon: 0.602644, loss_pred: 0.157127
iteration 357: loss: 61.344879, loss_kl: 1.050190, loss_recon: 0.601887, loss_pred: 0.189137
iteration 358: loss: 61.638420, loss_kl: 0.718645, loss_recon: 0.608612, loss_pred: 0.115442
iteration 359: loss: 62.823410, loss_kl: 0.823862, loss_recon: 0.619248, loss_pred: 0.140040
iteration 360: loss: 63.139946, loss_kl: 0.539581, loss_recon: 0.624986, loss_pred: 0.144488
 18%|█████▍                        | 36/200 [29:54<2:16:02, 49.77s/it]iteration 361: loss: 62.405643, loss_kl: 1.049054, loss_recon: 0.612611, loss_pred: 0.137027
iteration 362: loss: 61.746857, loss_kl: 0.699684, loss_recon: 0.608503, loss_pred: 0.224585
iteration 363: loss: 62.071445, loss_kl: 0.443206, loss_recon: 0.614243, loss_pred: 0.221533
iteration 364: loss: 61.944370, loss_kl: 0.746529, loss_recon: 0.610311, loss_pred: 0.196320
iteration 365: loss: 60.711319, loss_kl: 0.927395, loss_recon: 0.595865, loss_pred: 0.234176
iteration 366: loss: 62.001640, loss_kl: 0.826801, loss_recon: 0.610233, loss_pred: 0.184251
iteration 367: loss: 63.068035, loss_kl: 1.527781, loss_recon: 0.614110, loss_pred: 0.189730
iteration 368: loss: 61.239464, loss_kl: 0.534682, loss_recon: 0.605483, loss_pred: 0.177694
iteration 369: loss: 62.452816, loss_kl: 0.723969, loss_recon: 0.615788, loss_pred: 0.178724
iteration 370: loss: 62.078873, loss_kl: 0.695906, loss_recon: 0.612135, loss_pred: 0.197023
 18%|█████▌                        | 37/200 [30:44<2:15:18, 49.80s/it]iteration 371: loss: 62.005386, loss_kl: 1.069392, loss_recon: 0.607800, loss_pred: 0.156016
iteration 372: loss: 61.911842, loss_kl: 0.929850, loss_recon: 0.608000, loss_pred: 0.181969
iteration 373: loss: 62.444557, loss_kl: 0.810160, loss_recon: 0.615046, loss_pred: 0.129759
iteration 374: loss: 62.388901, loss_kl: 1.218552, loss_recon: 0.609243, loss_pred: 0.246043
iteration 375: loss: 61.424252, loss_kl: 0.635838, loss_recon: 0.605139, loss_pred: 0.274476
iteration 376: loss: 62.981167, loss_kl: 0.690773, loss_recon: 0.621188, loss_pred: 0.171587
iteration 377: loss: 62.479946, loss_kl: 0.802963, loss_recon: 0.615553, loss_pred: 0.121647
iteration 378: loss: 60.881817, loss_kl: 0.629421, loss_recon: 0.600762, loss_pred: 0.176189
iteration 379: loss: 60.737526, loss_kl: 0.131721, loss_recon: 0.604539, loss_pred: 0.151905
iteration 380: loss: 61.640545, loss_kl: 0.690450, loss_recon: 0.607224, loss_pred: 0.227649
 19%|█████▋                        | 38/200 [31:34<2:14:16, 49.73s/it]iteration 381: loss: 62.853733, loss_kl: 0.657307, loss_recon: 0.619994, loss_pred: 0.197060
iteration 382: loss: 61.374653, loss_kl: 0.621158, loss_recon: 0.605475, loss_pred: 0.206011
iteration 383: loss: 61.986557, loss_kl: 0.693509, loss_recon: 0.611695, loss_pred: 0.123582
iteration 384: loss: 60.392952, loss_kl: 0.737195, loss_recon: 0.594196, loss_pred: 0.236127
iteration 385: loss: 62.456585, loss_kl: 1.062985, loss_recon: 0.612079, loss_pred: 0.185727
iteration 386: loss: 62.114895, loss_kl: 0.578051, loss_recon: 0.613650, loss_pred: 0.171883
iteration 387: loss: 61.662529, loss_kl: 0.684171, loss_recon: 0.607129, loss_pred: 0.265462
iteration 388: loss: 62.545151, loss_kl: 0.577395, loss_recon: 0.618004, loss_pred: 0.167335
iteration 389: loss: 62.407589, loss_kl: 0.517232, loss_recon: 0.617482, loss_pred: 0.142149
iteration 390: loss: 61.753624, loss_kl: 1.096762, loss_recon: 0.603487, loss_pred: 0.308150
 20%|█████▊                        | 39/200 [32:23<2:13:12, 49.65s/it]iteration 391: loss: 61.605347, loss_kl: 1.089152, loss_recon: 0.603304, loss_pred: 0.185762
iteration 392: loss: 61.586216, loss_kl: 0.463274, loss_recon: 0.609441, loss_pred: 0.178847
iteration 393: loss: 61.040455, loss_kl: 0.577354, loss_recon: 0.603198, loss_pred: 0.143263
iteration 394: loss: 62.301003, loss_kl: 0.157860, loss_recon: 0.619624, loss_pred: 0.180709
iteration 395: loss: 64.073257, loss_kl: 0.796761, loss_recon: 0.631491, loss_pred: 0.127418
iteration 396: loss: 61.102104, loss_kl: 0.258580, loss_recon: 0.606677, loss_pred: 0.175870
iteration 397: loss: 60.479130, loss_kl: 0.542149, loss_recon: 0.597648, loss_pred: 0.172182
iteration 398: loss: 61.267155, loss_kl: 0.403725, loss_recon: 0.607259, loss_pred: 0.137556
iteration 399: loss: 62.061527, loss_kl: 0.862439, loss_recon: 0.610467, loss_pred: 0.152370
iteration 400: loss: 61.787529, loss_kl: 0.566623, loss_recon: 0.610961, loss_pred: 0.124797
 20%|██████                        | 40/200 [33:12<2:12:00, 49.50s/it]iteration 401: loss: 60.843708, loss_kl: 0.449318, loss_recon: 0.601578, loss_pred: 0.236553
iteration 402: loss: 62.302979, loss_kl: 1.010837, loss_recon: 0.611778, loss_pred: 0.114306
iteration 403: loss: 61.053383, loss_kl: 0.523162, loss_recon: 0.603348, loss_pred: 0.195418
iteration 404: loss: 62.260117, loss_kl: 0.451695, loss_recon: 0.616601, loss_pred: 0.148272
iteration 405: loss: 61.418308, loss_kl: 0.473016, loss_recon: 0.607319, loss_pred: 0.213383
iteration 406: loss: 62.135929, loss_kl: 0.741307, loss_recon: 0.612035, loss_pred: 0.191122
iteration 407: loss: 61.532803, loss_kl: 1.002676, loss_recon: 0.603188, loss_pred: 0.211364
iteration 408: loss: 61.934681, loss_kl: 0.408110, loss_recon: 0.613912, loss_pred: 0.135394
iteration 409: loss: 61.349003, loss_kl: 0.014095, loss_recon: 0.611630, loss_pred: 0.171915
iteration 410: loss: 62.964340, loss_kl: 0.882906, loss_recon: 0.619671, loss_pred: 0.114286
 20%|██████▏                       | 41/200 [34:02<2:11:16, 49.54s/it]iteration 411: loss: 62.239960, loss_kl: 0.769143, loss_recon: 0.612795, loss_pred: 0.191296
iteration 412: loss: 60.353073, loss_kl: 0.928383, loss_recon: 0.591684, loss_pred: 0.256300
iteration 413: loss: 61.902725, loss_kl: 0.787640, loss_recon: 0.609809, loss_pred: 0.134162
iteration 414: loss: 63.153439, loss_kl: 0.163260, loss_recon: 0.628827, loss_pred: 0.107456
iteration 415: loss: 61.949028, loss_kl: 0.456929, loss_recon: 0.613819, loss_pred: 0.110186
iteration 416: loss: 61.944103, loss_kl: 0.469870, loss_recon: 0.613466, loss_pred: 0.127668
iteration 417: loss: 61.248611, loss_kl: 0.430906, loss_recon: 0.606192, loss_pred: 0.198464
iteration 418: loss: 60.570717, loss_kl: 0.298529, loss_recon: 0.600779, loss_pred: 0.194242
iteration 419: loss: 60.993042, loss_kl: 0.778572, loss_recon: 0.600645, loss_pred: 0.149995
iteration 420: loss: 63.441109, loss_kl: 0.892012, loss_recon: 0.623491, loss_pred: 0.199968
 21%|██████▎                       | 42/200 [34:51<2:10:14, 49.46s/it]iteration 421: loss: 61.606407, loss_kl: 0.477541, loss_recon: 0.610028, loss_pred: 0.126110
iteration 422: loss: 61.662659, loss_kl: 0.412787, loss_recon: 0.611334, loss_pred: 0.116438
iteration 423: loss: 62.720329, loss_kl: 0.635455, loss_recon: 0.619637, loss_pred: 0.121165
iteration 424: loss: 60.888725, loss_kl: 0.264930, loss_recon: 0.605215, loss_pred: 0.102309
iteration 425: loss: 61.164101, loss_kl: 0.443690, loss_recon: 0.605821, loss_pred: 0.138336
iteration 426: loss: 60.535381, loss_kl: 0.847130, loss_recon: 0.595123, loss_pred: 0.175994
iteration 427: loss: 61.923504, loss_kl: 0.854916, loss_recon: 0.609018, loss_pred: 0.166744
iteration 428: loss: 62.080223, loss_kl: 0.688527, loss_recon: 0.612983, loss_pred: 0.093358
iteration 429: loss: 62.541523, loss_kl: 0.512800, loss_recon: 0.618978, loss_pred: 0.130951
iteration 430: loss: 61.520065, loss_kl: 0.623547, loss_recon: 0.607224, loss_pred: 0.174082
 22%|██████▍                       | 43/200 [35:40<2:09:19, 49.42s/it]iteration 431: loss: 62.084591, loss_kl: 0.439188, loss_recon: 0.615053, loss_pred: 0.140104
iteration 432: loss: 62.710285, loss_kl: 0.501546, loss_recon: 0.620893, loss_pred: 0.119459
iteration 433: loss: 61.926266, loss_kl: 0.516715, loss_recon: 0.612489, loss_pred: 0.160633
iteration 434: loss: 60.372646, loss_kl: 0.620619, loss_recon: 0.595550, loss_pred: 0.196991
iteration 435: loss: 61.175953, loss_kl: 0.546041, loss_recon: 0.603568, loss_pred: 0.273086
iteration 436: loss: 60.369194, loss_kl: 0.687455, loss_recon: 0.594812, loss_pred: 0.200530
iteration 437: loss: 63.375397, loss_kl: 0.359316, loss_recon: 0.628110, loss_pred: 0.205044
iteration 438: loss: 62.597095, loss_kl: 0.751120, loss_recon: 0.617494, loss_pred: 0.096580
iteration 439: loss: 62.173878, loss_kl: 0.310339, loss_recon: 0.616716, loss_pred: 0.191956
iteration 440: loss: 61.364380, loss_kl: 0.760449, loss_recon: 0.604235, loss_pred: 0.180420
 22%|██████▌                       | 44/200 [36:30<2:08:31, 49.44s/it]iteration 441: loss: 62.762749, loss_kl: 0.735956, loss_recon: 0.618227, loss_pred: 0.204077
iteration 442: loss: 61.700947, loss_kl: 0.533698, loss_recon: 0.610151, loss_pred: 0.152126
iteration 443: loss: 61.473125, loss_kl: 0.459930, loss_recon: 0.608334, loss_pred: 0.179805
iteration 444: loss: 60.448071, loss_kl: 0.679299, loss_recon: 0.595704, loss_pred: 0.198405
iteration 445: loss: 60.975204, loss_kl: 0.378470, loss_recon: 0.604214, loss_pred: 0.175315
iteration 446: loss: 62.485645, loss_kl: 0.484506, loss_recon: 0.618698, loss_pred: 0.131382
iteration 447: loss: 61.510910, loss_kl: 0.654262, loss_recon: 0.606797, loss_pred: 0.176916
iteration 448: loss: 61.146229, loss_kl: 0.338322, loss_recon: 0.606789, loss_pred: 0.128979
iteration 449: loss: 62.129841, loss_kl: 0.663393, loss_recon: 0.612999, loss_pred: 0.166571
iteration 450: loss: 61.151997, loss_kl: 0.140216, loss_recon: 0.608212, loss_pred: 0.190563
 22%|██████▊                       | 45/200 [37:19<2:07:20, 49.30s/it]iteration 451: loss: 61.457188, loss_kl: 0.597263, loss_recon: 0.607085, loss_pred: 0.151400
iteration 452: loss: 62.181911, loss_kl: 0.620415, loss_recon: 0.614266, loss_pred: 0.134900
iteration 453: loss: 61.550034, loss_kl: 0.320560, loss_recon: 0.610800, loss_pred: 0.149443
iteration 454: loss: 61.187798, loss_kl: 0.140581, loss_recon: 0.608560, loss_pred: 0.191193
iteration 455: loss: 62.668785, loss_kl: 0.705592, loss_recon: 0.618017, loss_pred: 0.161442
iteration 456: loss: 61.168365, loss_kl: 0.650770, loss_recon: 0.602787, loss_pred: 0.238854
iteration 457: loss: 60.268173, loss_kl: 0.229007, loss_recon: 0.598100, loss_pred: 0.229175
iteration 458: loss: 61.002407, loss_kl: 0.455363, loss_recon: 0.603246, loss_pred: 0.222456
iteration 459: loss: 61.859093, loss_kl: 0.400952, loss_recon: 0.613130, loss_pred: 0.145174
iteration 460: loss: 62.893677, loss_kl: 0.375917, loss_recon: 0.622715, loss_pred: 0.246221
 23%|██████▉                       | 46/200 [38:08<2:06:42, 49.37s/it]iteration 461: loss: 61.164001, loss_kl: 0.287561, loss_recon: 0.607615, loss_pred: 0.114984
iteration 462: loss: 61.172836, loss_kl: 0.254052, loss_recon: 0.607746, loss_pred: 0.144163
iteration 463: loss: 61.659500, loss_kl: 0.368564, loss_recon: 0.611144, loss_pred: 0.176560
iteration 464: loss: 61.370708, loss_kl: 0.756308, loss_recon: 0.603984, loss_pred: 0.215992
iteration 465: loss: 62.518181, loss_kl: 0.678091, loss_recon: 0.616993, loss_pred: 0.140825
iteration 466: loss: 61.108112, loss_kl: 0.302665, loss_recon: 0.606460, loss_pred: 0.159466
iteration 467: loss: 61.801243, loss_kl: 0.510556, loss_recon: 0.611698, loss_pred: 0.120882
iteration 468: loss: 61.498684, loss_kl: 0.211453, loss_recon: 0.611735, loss_pred: 0.113717
iteration 469: loss: 60.022442, loss_kl: 0.578439, loss_recon: 0.592703, loss_pred: 0.173675
iteration 470: loss: 62.372047, loss_kl: 0.489652, loss_recon: 0.617657, loss_pred: 0.116707
 24%|███████                       | 47/200 [38:58<2:05:41, 49.29s/it]iteration 471: loss: 61.630962, loss_kl: 0.589224, loss_recon: 0.609078, loss_pred: 0.133973
iteration 472: loss: 60.712917, loss_kl: 0.241588, loss_recon: 0.603042, loss_pred: 0.167101
iteration 473: loss: 61.245304, loss_kl: 0.326099, loss_recon: 0.607849, loss_pred: 0.134334
iteration 474: loss: 61.981598, loss_kl: 0.666040, loss_recon: 0.612005, loss_pred: 0.115071
iteration 475: loss: 61.275864, loss_kl: 0.224617, loss_recon: 0.609101, loss_pred: 0.141195
iteration 476: loss: 62.051487, loss_kl: 0.553039, loss_recon: 0.613876, loss_pred: 0.110897
iteration 477: loss: 61.759228, loss_kl: 0.563645, loss_recon: 0.610276, loss_pred: 0.167997
iteration 478: loss: 61.699936, loss_kl: 0.617067, loss_recon: 0.608644, loss_pred: 0.218505
iteration 479: loss: 61.124916, loss_kl: 0.330409, loss_recon: 0.605996, loss_pred: 0.194953
iteration 480: loss: 61.982826, loss_kl: 0.647699, loss_recon: 0.611950, loss_pred: 0.140165
 24%|███████▏                      | 48/200 [39:47<2:05:20, 49.48s/it]iteration 481: loss: 59.967640, loss_kl: 0.320111, loss_recon: 0.594944, loss_pred: 0.153112
iteration 482: loss: 62.147110, loss_kl: 0.455126, loss_recon: 0.615465, loss_pred: 0.145487
iteration 483: loss: 60.713116, loss_kl: 0.491157, loss_recon: 0.600467, loss_pred: 0.175273
iteration 484: loss: 62.680931, loss_kl: 0.297881, loss_recon: 0.621791, loss_pred: 0.203908
iteration 485: loss: 61.427963, loss_kl: 0.444942, loss_recon: 0.608688, loss_pred: 0.114217
iteration 486: loss: 61.564171, loss_kl: 0.321025, loss_recon: 0.610982, loss_pred: 0.144932
iteration 487: loss: 61.948605, loss_kl: 0.505870, loss_recon: 0.612769, loss_pred: 0.165851
iteration 488: loss: 61.670662, loss_kl: 0.391451, loss_recon: 0.610828, loss_pred: 0.196460
iteration 489: loss: 61.917011, loss_kl: 0.459683, loss_recon: 0.613385, loss_pred: 0.118831
iteration 490: loss: 61.204517, loss_kl: 0.699930, loss_recon: 0.603320, loss_pred: 0.172584
 24%|███████▎                      | 49/200 [40:37<2:04:36, 49.51s/it]iteration 491: loss: 61.742611, loss_kl: 0.584358, loss_recon: 0.610098, loss_pred: 0.148463
iteration 492: loss: 62.751926, loss_kl: 0.389483, loss_recon: 0.622551, loss_pred: 0.107356
iteration 493: loss: 59.648651, loss_kl: 0.234711, loss_recon: 0.592429, loss_pred: 0.171078
iteration 494: loss: 61.672222, loss_kl: 0.310647, loss_recon: 0.612502, loss_pred: 0.111389
iteration 495: loss: 60.506718, loss_kl: 0.241868, loss_recon: 0.601237, loss_pred: 0.141141
iteration 496: loss: 61.988747, loss_kl: 0.704467, loss_recon: 0.611748, loss_pred: 0.109517
iteration 497: loss: 62.346951, loss_kl: 0.433392, loss_recon: 0.617701, loss_pred: 0.143424
iteration 498: loss: 61.069061, loss_kl: 0.654773, loss_recon: 0.602391, loss_pred: 0.175183
iteration 499: loss: 61.962818, loss_kl: 0.199169, loss_recon: 0.616092, loss_pred: 0.154434
iteration 500: loss: 60.080452, loss_kl: 0.358947, loss_recon: 0.595013, loss_pred: 0.220176
 25%|███████▌                      | 50/200 [41:27<2:04:11, 49.68s/it]iteration 501: loss: 60.523438, loss_kl: 0.257072, loss_recon: 0.603678, loss_pred: 0.153111
iteration 502: loss: 62.043320, loss_kl: 0.229928, loss_recon: 0.619056, loss_pred: 0.135381
iteration 503: loss: 61.102894, loss_kl: 0.584581, loss_recon: 0.609426, loss_pred: 0.154423
iteration 504: loss: 61.543118, loss_kl: 0.298631, loss_recon: 0.613950, loss_pred: 0.145124
iteration 505: loss: 59.700912, loss_kl: 0.427916, loss_recon: 0.595829, loss_pred: 0.113752
iteration 506: loss: 61.767971, loss_kl: 1.014316, loss_recon: 0.615799, loss_pred: 0.177952
iteration 507: loss: 61.385117, loss_kl: 0.842319, loss_recon: 0.612548, loss_pred: 0.121876
iteration 508: loss: 60.863613, loss_kl: 0.919508, loss_recon: 0.607030, loss_pred: 0.151462
iteration 509: loss: 60.676804, loss_kl: 1.089212, loss_recon: 0.605690, loss_pred: 0.096956
iteration 510: loss: 61.723026, loss_kl: 1.027261, loss_recon: 0.615914, loss_pred: 0.121331
 26%|███████▋                      | 51/200 [42:17<2:03:28, 49.72s/it]iteration 511: loss: 62.603443, loss_kl: 0.965259, loss_recon: 0.624703, loss_pred: 0.123480
iteration 512: loss: 60.387337, loss_kl: 1.168257, loss_recon: 0.602638, loss_pred: 0.111836
iteration 513: loss: 58.987671, loss_kl: 1.363465, loss_recon: 0.588415, loss_pred: 0.132533
iteration 514: loss: 60.670422, loss_kl: 1.053107, loss_recon: 0.605530, loss_pred: 0.106847
iteration 515: loss: 60.535675, loss_kl: 1.372053, loss_recon: 0.603947, loss_pred: 0.127241
iteration 516: loss: 61.173923, loss_kl: 1.324118, loss_recon: 0.610536, loss_pred: 0.107131
iteration 517: loss: 61.761223, loss_kl: 1.648794, loss_recon: 0.616132, loss_pred: 0.131576
iteration 518: loss: 60.848145, loss_kl: 1.873904, loss_recon: 0.607095, loss_pred: 0.119906
iteration 519: loss: 60.712036, loss_kl: 1.523796, loss_recon: 0.605887, loss_pred: 0.108087
iteration 520: loss: 62.335804, loss_kl: 1.118954, loss_recon: 0.622495, loss_pred: 0.075153
 26%|███████▊                      | 52/200 [43:07<2:02:34, 49.69s/it]iteration 521: loss: 61.331791, loss_kl: 1.126689, loss_recon: 0.612160, loss_pred: 0.104496
iteration 522: loss: 61.270283, loss_kl: 0.962296, loss_recon: 0.611548, loss_pred: 0.105847
iteration 523: loss: 61.576992, loss_kl: 1.022758, loss_recon: 0.614548, loss_pred: 0.112012
iteration 524: loss: 61.644352, loss_kl: 1.661329, loss_recon: 0.614896, loss_pred: 0.138103
iteration 525: loss: 60.636616, loss_kl: 1.096968, loss_recon: 0.604663, loss_pred: 0.159346
iteration 526: loss: 59.070782, loss_kl: 1.262440, loss_recon: 0.588866, loss_pred: 0.171542
iteration 527: loss: 61.228962, loss_kl: 0.973994, loss_recon: 0.610502, loss_pred: 0.168992
iteration 528: loss: 60.513569, loss_kl: 1.382005, loss_recon: 0.603663, loss_pred: 0.133411
iteration 529: loss: 61.421165, loss_kl: 1.376539, loss_recon: 0.612612, loss_pred: 0.146188
iteration 530: loss: 62.380779, loss_kl: 1.534384, loss_recon: 0.622209, loss_pred: 0.144492
 26%|███████▉                      | 53/200 [43:56<2:01:34, 49.62s/it]iteration 531: loss: 61.077587, loss_kl: 1.196238, loss_recon: 0.609905, loss_pred: 0.075171
iteration 532: loss: 60.026653, loss_kl: 0.664218, loss_recon: 0.599097, loss_pred: 0.110346
iteration 533: loss: 61.714310, loss_kl: 0.935570, loss_recon: 0.615775, loss_pred: 0.127485
iteration 534: loss: 61.972641, loss_kl: 0.709626, loss_recon: 0.618255, loss_pred: 0.140073
iteration 535: loss: 61.495636, loss_kl: 1.145957, loss_recon: 0.613163, loss_pred: 0.167898
iteration 536: loss: 60.698547, loss_kl: 0.830376, loss_recon: 0.605367, loss_pred: 0.153563
iteration 537: loss: 60.502903, loss_kl: 1.313328, loss_recon: 0.602685, loss_pred: 0.221235
iteration 538: loss: 62.045322, loss_kl: 0.620320, loss_recon: 0.619027, loss_pred: 0.136405
iteration 539: loss: 60.978256, loss_kl: 1.036533, loss_recon: 0.608474, loss_pred: 0.120479
iteration 540: loss: 61.078190, loss_kl: 0.675936, loss_recon: 0.609512, loss_pred: 0.120227
 27%|████████                      | 54/200 [44:46<2:01:13, 49.82s/it]iteration 541: loss: 62.370331, loss_kl: 1.256860, loss_recon: 0.622123, loss_pred: 0.145466
iteration 542: loss: 61.114773, loss_kl: 0.776504, loss_recon: 0.609811, loss_pred: 0.125943
iteration 543: loss: 61.088673, loss_kl: 0.907294, loss_recon: 0.609115, loss_pred: 0.168116
iteration 544: loss: 60.003269, loss_kl: 0.814905, loss_recon: 0.598462, loss_pred: 0.148919
iteration 545: loss: 61.354519, loss_kl: 0.619361, loss_recon: 0.612348, loss_pred: 0.113481
iteration 546: loss: 61.231140, loss_kl: 0.706586, loss_recon: 0.611358, loss_pred: 0.088265
iteration 547: loss: 61.417049, loss_kl: 0.306887, loss_recon: 0.613198, loss_pred: 0.094173
iteration 548: loss: 60.493843, loss_kl: 0.734484, loss_recon: 0.603920, loss_pred: 0.094530
iteration 549: loss: 60.239929, loss_kl: 0.664661, loss_recon: 0.601457, loss_pred: 0.087547
iteration 550: loss: 60.382713, loss_kl: 0.761769, loss_recon: 0.602929, loss_pred: 0.082165
 28%|████████▎                     | 55/200 [45:37<2:01:14, 50.17s/it]iteration 551: loss: 61.031940, loss_kl: 0.564499, loss_recon: 0.609207, loss_pred: 0.105627
iteration 552: loss: 60.449768, loss_kl: 0.872162, loss_recon: 0.603403, loss_pred: 0.100757
iteration 553: loss: 60.341686, loss_kl: 0.533525, loss_recon: 0.602208, loss_pred: 0.115501
iteration 554: loss: 59.602139, loss_kl: 0.903093, loss_recon: 0.595021, loss_pred: 0.090985
iteration 555: loss: 62.797016, loss_kl: 0.532185, loss_recon: 0.626908, loss_pred: 0.100895
iteration 556: loss: 61.643032, loss_kl: 0.525084, loss_recon: 0.615429, loss_pred: 0.094934
iteration 557: loss: 61.845795, loss_kl: 0.788609, loss_recon: 0.617255, loss_pred: 0.112420
iteration 558: loss: 61.342316, loss_kl: 0.680230, loss_recon: 0.611105, loss_pred: 0.225052
iteration 559: loss: 60.948483, loss_kl: 0.459320, loss_recon: 0.608582, loss_pred: 0.085681
iteration 560: loss: 60.496590, loss_kl: 0.599567, loss_recon: 0.603671, loss_pred: 0.123476
 28%|████████▍                     | 56/200 [46:26<1:59:32, 49.81s/it]iteration 561: loss: 61.575409, loss_kl: 0.604530, loss_recon: 0.614433, loss_pred: 0.126094
iteration 562: loss: 61.084633, loss_kl: 0.525899, loss_recon: 0.609126, loss_pred: 0.166787
iteration 563: loss: 61.550774, loss_kl: 0.741496, loss_recon: 0.614553, loss_pred: 0.088015
iteration 564: loss: 60.388863, loss_kl: 1.008983, loss_recon: 0.603114, loss_pred: 0.067421
iteration 565: loss: 60.516457, loss_kl: 0.588598, loss_recon: 0.604066, loss_pred: 0.104010
iteration 566: loss: 60.774231, loss_kl: 0.672648, loss_recon: 0.606631, loss_pred: 0.104421
iteration 567: loss: 61.582035, loss_kl: 0.506827, loss_recon: 0.614745, loss_pred: 0.102460
iteration 568: loss: 60.056046, loss_kl: 0.441061, loss_recon: 0.599097, loss_pred: 0.141931
iteration 569: loss: 60.994827, loss_kl: 0.544824, loss_recon: 0.608649, loss_pred: 0.124432
iteration 570: loss: 62.058090, loss_kl: 0.593823, loss_recon: 0.619087, loss_pred: 0.143404
 28%|████████▌                     | 57/200 [47:16<1:58:24, 49.68s/it]iteration 571: loss: 59.416931, loss_kl: 0.493840, loss_recon: 0.592984, loss_pred: 0.113560
iteration 572: loss: 60.136368, loss_kl: 0.087729, loss_recon: 0.600384, loss_pred: 0.097085
iteration 573: loss: 61.807404, loss_kl: 0.330531, loss_recon: 0.617211, loss_pred: 0.082996
iteration 574: loss: 61.707462, loss_kl: 0.403965, loss_recon: 0.616167, loss_pred: 0.086710
iteration 575: loss: 61.217045, loss_kl: 0.521915, loss_recon: 0.610952, loss_pred: 0.116640
iteration 576: loss: 61.239376, loss_kl: 0.412413, loss_recon: 0.611283, loss_pred: 0.106931
iteration 577: loss: 60.930065, loss_kl: 0.575352, loss_recon: 0.608543, loss_pred: 0.070003
iteration 578: loss: 60.817684, loss_kl: 0.457479, loss_recon: 0.607350, loss_pred: 0.078139
iteration 579: loss: 60.497356, loss_kl: 0.490974, loss_recon: 0.603617, loss_pred: 0.130754
iteration 580: loss: 61.324543, loss_kl: 0.649943, loss_recon: 0.612253, loss_pred: 0.092770
 29%|████████▋                     | 58/200 [48:06<1:58:10, 49.93s/it]iteration 581: loss: 60.581699, loss_kl: 0.452108, loss_recon: 0.604981, loss_pred: 0.079056
iteration 582: loss: 61.451530, loss_kl: 0.495433, loss_recon: 0.613437, loss_pred: 0.102833
iteration 583: loss: 61.243069, loss_kl: 0.506289, loss_recon: 0.611548, loss_pred: 0.083213
iteration 584: loss: 60.162292, loss_kl: 0.447176, loss_recon: 0.600095, loss_pred: 0.148293
iteration 585: loss: 60.999817, loss_kl: 0.503753, loss_recon: 0.608537, loss_pred: 0.141046
iteration 586: loss: 62.116676, loss_kl: 0.705416, loss_recon: 0.619633, loss_pred: 0.146298
iteration 587: loss: 61.425396, loss_kl: 0.627874, loss_recon: 0.613086, loss_pred: 0.110518
iteration 588: loss: 60.358871, loss_kl: 0.217415, loss_recon: 0.602657, loss_pred: 0.090974
iteration 589: loss: 61.270473, loss_kl: 0.224870, loss_recon: 0.611525, loss_pred: 0.115721
iteration 590: loss: 59.785423, loss_kl: 0.488704, loss_recon: 0.596189, loss_pred: 0.161632
 30%|████████▊                     | 59/200 [48:56<1:57:14, 49.89s/it]iteration 591: loss: 58.753544, loss_kl: 0.389351, loss_recon: 0.586233, loss_pred: 0.126353
iteration 592: loss: 62.169552, loss_kl: 0.507417, loss_recon: 0.620476, loss_pred: 0.116858
iteration 593: loss: 62.439766, loss_kl: 0.167642, loss_recon: 0.623494, loss_pred: 0.088718
iteration 594: loss: 61.145199, loss_kl: 0.404630, loss_recon: 0.610416, loss_pred: 0.099512
iteration 595: loss: 61.086800, loss_kl: 0.333705, loss_recon: 0.609680, loss_pred: 0.115496
iteration 596: loss: 60.957741, loss_kl: 0.265220, loss_recon: 0.608529, loss_pred: 0.102205
iteration 597: loss: 60.317062, loss_kl: 0.435015, loss_recon: 0.602110, loss_pred: 0.101679
iteration 598: loss: 61.540169, loss_kl: 0.473914, loss_recon: 0.614234, loss_pred: 0.112076
iteration 599: loss: 60.299339, loss_kl: 0.604684, loss_recon: 0.601878, loss_pred: 0.105529
iteration 600: loss: 60.986835, loss_kl: 0.521142, loss_recon: 0.608681, loss_pred: 0.113542
 30%|█████████                     | 60/200 [49:46<1:56:15, 49.83s/it]iteration 601: loss: 60.665310, loss_kl: 0.592820, loss_recon: 0.605872, loss_pred: 0.072218
iteration 602: loss: 61.723637, loss_kl: 0.476484, loss_recon: 0.616375, loss_pred: 0.081404
iteration 603: loss: 61.086517, loss_kl: 0.187948, loss_recon: 0.610048, loss_pred: 0.079819
iteration 604: loss: 59.835537, loss_kl: 0.232440, loss_recon: 0.596950, loss_pred: 0.138203
iteration 605: loss: 61.503323, loss_kl: 0.480700, loss_recon: 0.613901, loss_pred: 0.108379
iteration 606: loss: 62.103870, loss_kl: 0.297153, loss_recon: 0.619593, loss_pred: 0.141602
iteration 607: loss: 60.726974, loss_kl: 0.344563, loss_recon: 0.606131, loss_pred: 0.110437
iteration 608: loss: 60.347183, loss_kl: 0.451004, loss_recon: 0.602327, loss_pred: 0.110004
iteration 609: loss: 60.352524, loss_kl: 0.402006, loss_recon: 0.602442, loss_pred: 0.104304
iteration 610: loss: 61.341805, loss_kl: 0.556965, loss_recon: 0.612564, loss_pred: 0.079828
 30%|█████████▏                    | 61/200 [50:35<1:54:52, 49.59s/it]iteration 611: loss: 61.017246, loss_kl: 0.343251, loss_recon: 0.609087, loss_pred: 0.105071
iteration 612: loss: 59.908646, loss_kl: 0.507224, loss_recon: 0.597940, loss_pred: 0.109591
iteration 613: loss: 60.316166, loss_kl: 0.351055, loss_recon: 0.602274, loss_pred: 0.085239
iteration 614: loss: 59.735813, loss_kl: 0.364625, loss_recon: 0.595536, loss_pred: 0.178554
iteration 615: loss: 62.261688, loss_kl: 0.449268, loss_recon: 0.621790, loss_pred: 0.078173
iteration 616: loss: 60.569122, loss_kl: 0.250171, loss_recon: 0.604889, loss_pred: 0.077704
iteration 617: loss: 61.630177, loss_kl: 0.380868, loss_recon: 0.615522, loss_pred: 0.074204
iteration 618: loss: 60.912743, loss_kl: 0.149001, loss_recon: 0.608152, loss_pred: 0.096064
iteration 619: loss: 61.705856, loss_kl: 0.560603, loss_recon: 0.615987, loss_pred: 0.101553
iteration 620: loss: 60.889725, loss_kl: 0.494412, loss_recon: 0.608042, loss_pred: 0.080598
 31%|█████████▎                    | 62/200 [51:27<1:55:57, 50.42s/it]iteration 621: loss: 59.958096, loss_kl: 0.277911, loss_recon: 0.598563, loss_pred: 0.099043
iteration 622: loss: 61.518494, loss_kl: 0.262159, loss_recon: 0.614430, loss_pred: 0.072884
iteration 623: loss: 61.198345, loss_kl: 0.201586, loss_recon: 0.611114, loss_pred: 0.084917
iteration 624: loss: 60.483276, loss_kl: 0.205261, loss_recon: 0.603663, loss_pred: 0.114929
iteration 625: loss: 61.327126, loss_kl: 0.188137, loss_recon: 0.612419, loss_pred: 0.083375
iteration 626: loss: 61.191475, loss_kl: 0.508004, loss_recon: 0.611016, loss_pred: 0.084777
iteration 627: loss: 60.061680, loss_kl: 0.391305, loss_recon: 0.598305, loss_pred: 0.227237
iteration 628: loss: 60.496441, loss_kl: 0.370152, loss_recon: 0.604132, loss_pred: 0.079529
iteration 629: loss: 60.639439, loss_kl: 0.332340, loss_recon: 0.605493, loss_pred: 0.086770
iteration 630: loss: 62.083000, loss_kl: 0.524602, loss_recon: 0.619819, loss_pred: 0.095877
 32%|█████████▍                    | 63/200 [52:17<1:54:36, 50.19s/it]iteration 631: loss: 60.994663, loss_kl: 0.142897, loss_recon: 0.608963, loss_pred: 0.091294
iteration 632: loss: 61.603630, loss_kl: 0.514813, loss_recon: 0.614828, loss_pred: 0.095293
iteration 633: loss: 60.630619, loss_kl: 0.342750, loss_recon: 0.605324, loss_pred: 0.081260
iteration 634: loss: 60.596859, loss_kl: 0.140940, loss_recon: 0.604810, loss_pred: 0.108829
iteration 635: loss: 60.072968, loss_kl: 0.453847, loss_recon: 0.599455, loss_pred: 0.104987
iteration 636: loss: 61.527763, loss_kl: 0.479108, loss_recon: 0.614061, loss_pred: 0.097886
iteration 637: loss: 60.576717, loss_kl: 0.126807, loss_recon: 0.604579, loss_pred: 0.112494
iteration 638: loss: 61.695816, loss_kl: 0.246187, loss_recon: 0.615488, loss_pred: 0.134833
iteration 639: loss: 61.882820, loss_kl: 0.374075, loss_recon: 0.617998, loss_pred: 0.064489
iteration 640: loss: 60.129784, loss_kl: 0.324456, loss_recon: 0.599727, loss_pred: 0.141029
 32%|█████████▌                    | 64/200 [53:07<1:53:58, 50.28s/it]iteration 641: loss: 59.731636, loss_kl: 0.314163, loss_recon: 0.596073, loss_pred: 0.096299
iteration 642: loss: 60.972103, loss_kl: 0.504981, loss_recon: 0.608254, loss_pred: 0.101681
iteration 643: loss: 62.964737, loss_kl: 0.385305, loss_recon: 0.628070, loss_pred: 0.123409
iteration 644: loss: 60.060608, loss_kl: 0.417817, loss_recon: 0.599222, loss_pred: 0.101129
iteration 645: loss: 60.885323, loss_kl: 0.435975, loss_recon: 0.607238, loss_pred: 0.122671
iteration 646: loss: 60.873032, loss_kl: 0.313892, loss_recon: 0.607514, loss_pred: 0.093594
iteration 647: loss: 61.649391, loss_kl: 0.333545, loss_recon: 0.615119, loss_pred: 0.107700
iteration 648: loss: 61.762981, loss_kl: 0.400304, loss_recon: 0.616196, loss_pred: 0.107675
iteration 649: loss: 60.512650, loss_kl: 0.173249, loss_recon: 0.603253, loss_pred: 0.171923
iteration 650: loss: 60.603687, loss_kl: 0.173910, loss_recon: 0.604980, loss_pred: 0.090211
 32%|█████████▊                    | 65/200 [53:57<1:52:45, 50.12s/it]iteration 651: loss: 60.277363, loss_kl: 0.273832, loss_recon: 0.601471, loss_pred: 0.094967
iteration 652: loss: 60.359474, loss_kl: 0.275480, loss_recon: 0.601855, loss_pred: 0.138459
iteration 653: loss: 61.864201, loss_kl: 0.307603, loss_recon: 0.617556, loss_pred: 0.069007
iteration 654: loss: 60.795311, loss_kl: 0.350333, loss_recon: 0.606532, loss_pred: 0.097028
iteration 655: loss: 60.730850, loss_kl: 0.235281, loss_recon: 0.605440, loss_pred: 0.156541
iteration 656: loss: 60.870556, loss_kl: 0.312374, loss_recon: 0.607321, loss_pred: 0.098231
iteration 657: loss: 61.659035, loss_kl: 0.157129, loss_recon: 0.615418, loss_pred: 0.096955
iteration 658: loss: 60.985085, loss_kl: 0.139810, loss_recon: 0.608403, loss_pred: 0.126777
iteration 659: loss: 60.144100, loss_kl: 0.062169, loss_recon: 0.600496, loss_pred: 0.086482
iteration 660: loss: 61.688805, loss_kl: 0.164967, loss_recon: 0.615886, loss_pred: 0.078989
 33%|█████████▉                    | 66/200 [54:47<1:51:42, 50.02s/it]iteration 661: loss: 60.595009, loss_kl: 0.043025, loss_recon: 0.605132, loss_pred: 0.074555
iteration 662: loss: 60.766983, loss_kl: 0.144145, loss_recon: 0.606724, loss_pred: 0.070300
iteration 663: loss: 61.663437, loss_kl: 0.244102, loss_recon: 0.615689, loss_pred: 0.053437
iteration 664: loss: 59.766666, loss_kl: 0.230564, loss_recon: 0.596360, loss_pred: 0.091830
iteration 665: loss: 59.969532, loss_kl: 0.276194, loss_recon: 0.598108, loss_pred: 0.112231
iteration 666: loss: 62.273495, loss_kl: 0.319990, loss_recon: 0.620920, loss_pred: 0.127581
iteration 667: loss: 61.336102, loss_kl: 0.156246, loss_recon: 0.612109, loss_pred: 0.098862
iteration 668: loss: 60.522781, loss_kl: 0.152878, loss_recon: 0.603978, loss_pred: 0.099231
iteration 669: loss: 61.536831, loss_kl: 0.072496, loss_recon: 0.614589, loss_pred: 0.065742
iteration 670: loss: 60.731853, loss_kl: 0.040027, loss_recon: 0.605981, loss_pred: 0.126966
 34%|██████████                    | 67/200 [55:38<1:51:36, 50.35s/it]iteration 671: loss: 61.873260, loss_kl: 0.135282, loss_recon: 0.617680, loss_pred: 0.077131
iteration 672: loss: 61.191502, loss_kl: 0.152473, loss_recon: 0.610659, loss_pred: 0.093932
iteration 673: loss: 60.831654, loss_kl: 0.247022, loss_recon: 0.606702, loss_pred: 0.110059
iteration 674: loss: 60.408501, loss_kl: 0.213386, loss_recon: 0.602396, loss_pred: 0.124549
iteration 675: loss: 61.909927, loss_kl: 0.232322, loss_recon: 0.617734, loss_pred: 0.088174
iteration 676: loss: 60.838818, loss_kl: 0.207637, loss_recon: 0.607089, loss_pred: 0.086741
iteration 677: loss: 60.661625, loss_kl: 0.138008, loss_recon: 0.605546, loss_pred: 0.078368
iteration 678: loss: 59.578449, loss_kl: 0.128330, loss_recon: 0.594788, loss_pred: 0.072979
iteration 679: loss: 61.380791, loss_kl: 0.255046, loss_recon: 0.612297, loss_pred: 0.098041
iteration 680: loss: 60.467270, loss_kl: 0.226805, loss_recon: 0.603359, loss_pred: 0.084147
 34%|██████████▏                   | 68/200 [56:27<1:50:17, 50.14s/it]iteration 681: loss: 61.080929, loss_kl: 0.019363, loss_recon: 0.610098, loss_pred: 0.066375
iteration 682: loss: 61.271545, loss_kl: 0.048494, loss_recon: 0.611317, loss_pred: 0.127845
iteration 683: loss: 62.131145, loss_kl: 0.325326, loss_recon: 0.619492, loss_pred: 0.101359
iteration 684: loss: 60.720757, loss_kl: 0.278751, loss_recon: 0.605684, loss_pred: 0.083291
iteration 685: loss: 59.571720, loss_kl: 0.232578, loss_recon: 0.593508, loss_pred: 0.163340
iteration 686: loss: 62.467049, loss_kl: 0.301587, loss_recon: 0.623070, loss_pred: 0.085366
iteration 687: loss: 60.323578, loss_kl: 0.265664, loss_recon: 0.601273, loss_pred: 0.130472
iteration 688: loss: 62.029736, loss_kl: 0.233867, loss_recon: 0.618813, loss_pred: 0.090519
iteration 689: loss: 60.579433, loss_kl: 0.151250, loss_recon: 0.604158, loss_pred: 0.126138
iteration 690: loss: 60.419098, loss_kl: 0.070116, loss_recon: 0.603270, loss_pred: 0.074695
 34%|██████████▎                   | 69/200 [57:17<1:48:53, 49.88s/it]iteration 691: loss: 61.057732, loss_kl: 0.389455, loss_recon: 0.608445, loss_pred: 0.101333
iteration 692: loss: 61.732437, loss_kl: 0.028940, loss_recon: 0.616325, loss_pred: 0.091577
iteration 693: loss: 60.850304, loss_kl: 0.182989, loss_recon: 0.606987, loss_pred: 0.099071
iteration 694: loss: 62.176510, loss_kl: 0.274393, loss_recon: 0.620198, loss_pred: 0.077911
iteration 695: loss: 61.356834, loss_kl: 0.085344, loss_recon: 0.612394, loss_pred: 0.092913
iteration 696: loss: 59.894703, loss_kl: 0.282431, loss_recon: 0.597124, loss_pred: 0.101198
iteration 697: loss: 59.919106, loss_kl: 0.171054, loss_recon: 0.597215, loss_pred: 0.148518
iteration 698: loss: 60.479191, loss_kl: 0.194640, loss_recon: 0.603313, loss_pred: 0.092009
iteration 699: loss: 61.853130, loss_kl: 0.239137, loss_recon: 0.616843, loss_pred: 0.100148
iteration 700: loss: 60.259533, loss_kl: 0.321670, loss_recon: 0.600695, loss_pred: 0.097621
 35%|██████████▌                   | 70/200 [58:06<1:47:56, 49.82s/it]iteration 701: loss: 62.092403, loss_kl: 0.000671, loss_recon: 0.619824, loss_pred: 0.109752
iteration 702: loss: 59.309963, loss_kl: 0.496237, loss_recon: 0.590729, loss_pred: 0.074875
iteration 703: loss: 62.957100, loss_kl: 0.162748, loss_recon: 0.628090, loss_pred: 0.094906
iteration 704: loss: 60.449333, loss_kl: 0.159824, loss_recon: 0.603032, loss_pred: 0.093868
iteration 705: loss: 60.252186, loss_kl: 0.281434, loss_recon: 0.600801, loss_pred: 0.080111
iteration 706: loss: 61.282711, loss_kl: 0.387007, loss_recon: 0.610981, loss_pred: 0.058163
iteration 707: loss: 59.776264, loss_kl: 0.343676, loss_recon: 0.595662, loss_pred: 0.097757
iteration 708: loss: 60.634487, loss_kl: 0.154764, loss_recon: 0.605273, loss_pred: 0.056658
iteration 709: loss: 61.363056, loss_kl: 0.262648, loss_recon: 0.611850, loss_pred: 0.092210
iteration 710: loss: 61.896141, loss_kl: 0.322965, loss_recon: 0.616983, loss_pred: 0.092246
 36%|██████████▋                   | 71/200 [58:56<1:46:54, 49.72s/it]iteration 711: loss: 59.754360, loss_kl: 0.301252, loss_recon: 0.595337, loss_pred: 0.110266
iteration 712: loss: 60.154270, loss_kl: 0.160780, loss_recon: 0.599741, loss_pred: 0.121218
iteration 713: loss: 61.503365, loss_kl: 0.250004, loss_recon: 0.613194, loss_pred: 0.092404
iteration 714: loss: 60.503265, loss_kl: 0.060776, loss_recon: 0.603848, loss_pred: 0.096199
iteration 715: loss: 60.320534, loss_kl: 0.272726, loss_recon: 0.601561, loss_pred: 0.064541
iteration 716: loss: 61.773552, loss_kl: -0.179395, loss_recon: 0.617688, loss_pred: 0.070446
iteration 717: loss: 61.109138, loss_kl: 0.100567, loss_recon: 0.609729, loss_pred: 0.099401
iteration 718: loss: 61.729927, loss_kl: 0.170367, loss_recon: 0.615659, loss_pred: 0.101564
iteration 719: loss: 61.644356, loss_kl: 0.136547, loss_recon: 0.615020, loss_pred: 0.092375
iteration 720: loss: 60.616859, loss_kl: 0.257137, loss_recon: 0.604190, loss_pred: 0.103692
 36%|██████████▊                   | 72/200 [59:45<1:46:00, 49.69s/it]iteration 721: loss: 60.787090, loss_kl: 0.378482, loss_recon: 0.605004, loss_pred: 0.133005
iteration 722: loss: 60.690033, loss_kl: 0.442660, loss_recon: 0.604025, loss_pred: 0.107851
iteration 723: loss: 62.091042, loss_kl: 0.460571, loss_recon: 0.618151, loss_pred: 0.088955
iteration 724: loss: 60.513340, loss_kl: 0.151152, loss_recon: 0.603676, loss_pred: 0.084408
iteration 725: loss: 61.136219, loss_kl: 0.180022, loss_recon: 0.609870, loss_pred: 0.076079
iteration 726: loss: 60.389256, loss_kl: 0.157283, loss_recon: 0.602089, loss_pred: 0.116460
iteration 727: loss: 60.851471, loss_kl: 0.112924, loss_recon: 0.607130, loss_pred: 0.092624
iteration 728: loss: 60.210217, loss_kl: 0.345877, loss_recon: 0.599394, loss_pred: 0.130351
iteration 729: loss: 63.136185, loss_kl: 0.081852, loss_recon: 0.629890, loss_pred: 0.113985
iteration 730: loss: 60.249783, loss_kl: 0.299677, loss_recon: 0.600471, loss_pred: 0.081060
 36%|██████████▏                 | 73/200 [1:00:35<1:45:05, 49.65s/it]iteration 731: loss: 61.342873, loss_kl: 0.187840, loss_recon: 0.611818, loss_pred: 0.077340
iteration 732: loss: 61.115715, loss_kl: 0.189931, loss_recon: 0.609493, loss_pred: 0.081768
iteration 733: loss: 61.152252, loss_kl: 0.256114, loss_recon: 0.609671, loss_pred: 0.070993
iteration 734: loss: 61.490814, loss_kl: 0.181192, loss_recon: 0.613133, loss_pred: 0.096746
iteration 735: loss: 60.912514, loss_kl: 0.122467, loss_recon: 0.607433, loss_pred: 0.114608
iteration 736: loss: 60.233059, loss_kl: 0.305610, loss_recon: 0.599864, loss_pred: 0.110512
iteration 737: loss: 60.667229, loss_kl: 0.215679, loss_recon: 0.604614, loss_pred: 0.109741
iteration 738: loss: 61.591244, loss_kl: 0.332947, loss_recon: 0.613590, loss_pred: 0.083919
iteration 739: loss: 61.304550, loss_kl: 0.211601, loss_recon: 0.610071, loss_pred: 0.203176
iteration 740: loss: 61.032413, loss_kl: 0.323074, loss_recon: 0.607974, loss_pred: 0.091096
 37%|██████████▎                 | 74/200 [1:01:25<1:44:12, 49.62s/it]iteration 741: loss: 60.913143, loss_kl: 0.310956, loss_recon: 0.606636, loss_pred: 0.098662
iteration 742: loss: 61.015171, loss_kl: 0.397312, loss_recon: 0.607431, loss_pred: 0.079265
iteration 743: loss: 60.550674, loss_kl: 0.095564, loss_recon: 0.603910, loss_pred: 0.113333
iteration 744: loss: 61.194305, loss_kl: 0.467584, loss_recon: 0.608611, loss_pred: 0.106358
iteration 745: loss: 60.732719, loss_kl: 0.329214, loss_recon: 0.604801, loss_pred: 0.092846
iteration 746: loss: 60.479195, loss_kl: 0.189097, loss_recon: 0.602939, loss_pred: 0.093557
iteration 747: loss: 60.529060, loss_kl: 0.276579, loss_recon: 0.602930, loss_pred: 0.101855
iteration 748: loss: 60.933338, loss_kl: 0.444961, loss_recon: 0.606540, loss_pred: 0.063408
iteration 749: loss: 61.728836, loss_kl: 0.240743, loss_recon: 0.615250, loss_pred: 0.087043
iteration 750: loss: 62.015682, loss_kl: 0.227488, loss_recon: 0.618332, loss_pred: 0.072061
 38%|██████████▌                 | 75/200 [1:02:14<1:43:14, 49.56s/it]iteration 751: loss: 61.250141, loss_kl: 0.161911, loss_recon: 0.610834, loss_pred: 0.081812
iteration 752: loss: 62.289207, loss_kl: 0.213422, loss_recon: 0.620923, loss_pred: 0.084955
iteration 753: loss: 61.457253, loss_kl: 0.317091, loss_recon: 0.611938, loss_pred: 0.097084
iteration 754: loss: 61.609226, loss_kl: 0.207377, loss_recon: 0.613963, loss_pred: 0.104071
iteration 755: loss: 60.773113, loss_kl: 0.237442, loss_recon: 0.605699, loss_pred: 0.078632
iteration 756: loss: 60.583210, loss_kl: 0.288425, loss_recon: 0.603439, loss_pred: 0.087911
iteration 757: loss: 61.087944, loss_kl: 0.454825, loss_recon: 0.607733, loss_pred: 0.075900
iteration 758: loss: 61.022232, loss_kl: 0.193752, loss_recon: 0.608453, loss_pred: 0.075212
iteration 759: loss: 61.097809, loss_kl: 0.042800, loss_recon: 0.610000, loss_pred: 0.075381
iteration 760: loss: 59.290123, loss_kl: 0.398627, loss_recon: 0.589737, loss_pred: 0.107225
 38%|██████████▋                 | 76/200 [1:03:04<1:42:34, 49.64s/it]iteration 761: loss: 61.156754, loss_kl: 0.294658, loss_recon: 0.608927, loss_pred: 0.097762
iteration 762: loss: 61.253914, loss_kl: 0.193621, loss_recon: 0.610636, loss_pred: 0.080994
iteration 763: loss: 61.442444, loss_kl: 0.460351, loss_recon: 0.611111, loss_pred: 0.071552
iteration 764: loss: 61.666618, loss_kl: 0.327097, loss_recon: 0.614022, loss_pred: 0.079844
iteration 765: loss: 61.120358, loss_kl: 0.352154, loss_recon: 0.608374, loss_pred: 0.084203
iteration 766: loss: 61.137730, loss_kl: 0.362246, loss_recon: 0.608408, loss_pred: 0.092471
iteration 767: loss: 61.060833, loss_kl: 0.072228, loss_recon: 0.609053, loss_pred: 0.114719
iteration 768: loss: 60.450500, loss_kl: 0.260354, loss_recon: 0.601924, loss_pred: 0.111175
iteration 769: loss: 60.346554, loss_kl: 0.189292, loss_recon: 0.601577, loss_pred: 0.081982
iteration 770: loss: 59.844788, loss_kl: 0.164247, loss_recon: 0.596737, loss_pred: 0.078353
 38%|██████████▊                 | 77/200 [1:03:53<1:41:45, 49.64s/it]iteration 771: loss: 60.135479, loss_kl: 0.152472, loss_recon: 0.599713, loss_pred: 0.072046
iteration 772: loss: 61.315052, loss_kl: 0.484391, loss_recon: 0.609383, loss_pred: 0.084134
iteration 773: loss: 60.709511, loss_kl: 0.233053, loss_recon: 0.604845, loss_pred: 0.084226
iteration 774: loss: 60.250668, loss_kl: 0.329200, loss_recon: 0.599282, loss_pred: 0.123637
iteration 775: loss: 61.006554, loss_kl: 0.452144, loss_recon: 0.606169, loss_pred: 0.116573
iteration 776: loss: 62.558640, loss_kl: 0.671197, loss_recon: 0.620771, loss_pred: 0.076167
iteration 777: loss: 60.946968, loss_kl: 0.368740, loss_recon: 0.605969, loss_pred: 0.127383
iteration 778: loss: 61.910721, loss_kl: 0.372191, loss_recon: 0.616171, loss_pred: 0.068796
iteration 779: loss: 60.501377, loss_kl: 0.274122, loss_recon: 0.602611, loss_pred: 0.074664
iteration 780: loss: 60.838749, loss_kl: 0.196470, loss_recon: 0.606280, loss_pred: 0.092069
 39%|██████████▉                 | 78/200 [1:04:43<1:40:39, 49.51s/it]iteration 781: loss: 60.477112, loss_kl: 0.262218, loss_recon: 0.602159, loss_pred: 0.092442
iteration 782: loss: 61.221268, loss_kl: 0.324854, loss_recon: 0.609348, loss_pred: 0.077436
iteration 783: loss: 60.290703, loss_kl: 0.313078, loss_recon: 0.599934, loss_pred: 0.095811
iteration 784: loss: 60.811069, loss_kl: 0.232205, loss_recon: 0.605582, loss_pred: 0.103426
iteration 785: loss: 59.926178, loss_kl: 0.206694, loss_recon: 0.597099, loss_pred: 0.083262
iteration 786: loss: 61.664028, loss_kl: 0.342731, loss_recon: 0.613634, loss_pred: 0.080024
iteration 787: loss: 61.175053, loss_kl: 0.243346, loss_recon: 0.609335, loss_pred: 0.084924
iteration 788: loss: 61.819141, loss_kl: 0.334660, loss_recon: 0.615181, loss_pred: 0.085630
iteration 789: loss: 60.527573, loss_kl: 0.233116, loss_recon: 0.602474, loss_pred: 0.130128
iteration 790: loss: 61.916477, loss_kl: 0.558333, loss_recon: 0.614525, loss_pred: 0.104586
 40%|███████████                 | 79/200 [1:05:32<1:40:00, 49.59s/it]iteration 791: loss: 60.308208, loss_kl: 0.186164, loss_recon: 0.600796, loss_pred: 0.101383
iteration 792: loss: 61.816105, loss_kl: 0.207659, loss_recon: 0.615950, loss_pred: 0.079271
iteration 793: loss: 60.957325, loss_kl: 0.305994, loss_recon: 0.606825, loss_pred: 0.065815
iteration 794: loss: 61.749817, loss_kl: 0.495343, loss_recon: 0.613054, loss_pred: 0.106032
iteration 795: loss: 60.284927, loss_kl: 0.280217, loss_recon: 0.600049, loss_pred: 0.088612
iteration 796: loss: 60.973621, loss_kl: 0.164093, loss_recon: 0.607866, loss_pred: 0.074937
iteration 797: loss: 61.378029, loss_kl: 0.272017, loss_recon: 0.611127, loss_pred: 0.079478
iteration 798: loss: 60.224140, loss_kl: 0.508231, loss_recon: 0.597152, loss_pred: 0.161678
iteration 799: loss: 61.415630, loss_kl: 0.329595, loss_recon: 0.611016, loss_pred: 0.088832
iteration 800: loss: 61.098087, loss_kl: 0.231336, loss_recon: 0.608368, loss_pred: 0.103221
 40%|███████████▏                | 80/200 [1:06:23<1:39:34, 49.79s/it]iteration 801: loss: 60.915951, loss_kl: 0.355171, loss_recon: 0.605598, loss_pred: 0.099422
iteration 802: loss: 61.006821, loss_kl: 0.353376, loss_recon: 0.606112, loss_pred: 0.140165
iteration 803: loss: 60.820923, loss_kl: 0.536939, loss_recon: 0.603530, loss_pred: 0.079849
iteration 804: loss: 61.462093, loss_kl: 0.135011, loss_recon: 0.612726, loss_pred: 0.091929
iteration 805: loss: 61.662643, loss_kl: 0.441489, loss_recon: 0.612676, loss_pred: 0.075952
iteration 806: loss: 60.678379, loss_kl: 0.555286, loss_recon: 0.601808, loss_pred: 0.096195
iteration 807: loss: 61.978336, loss_kl: 0.252663, loss_recon: 0.617373, loss_pred: 0.058365
iteration 808: loss: 61.279194, loss_kl: 0.392648, loss_recon: 0.609303, loss_pred: 0.065054
iteration 809: loss: 59.807491, loss_kl: 0.317263, loss_recon: 0.594869, loss_pred: 0.091227
iteration 810: loss: 60.824322, loss_kl: 0.314725, loss_recon: 0.605169, loss_pred: 0.079917
 40%|███████████▎                | 81/200 [1:07:12<1:38:39, 49.74s/it]iteration 811: loss: 60.798878, loss_kl: 0.285113, loss_recon: 0.604961, loss_pred: 0.085404
iteration 812: loss: 61.547070, loss_kl: 0.429677, loss_recon: 0.611651, loss_pred: 0.054429
iteration 813: loss: 60.501842, loss_kl: 0.182204, loss_recon: 0.602686, loss_pred: 0.094318
iteration 814: loss: 62.267601, loss_kl: 0.135354, loss_recon: 0.620649, loss_pred: 0.099471
iteration 815: loss: 62.985485, loss_kl: 0.182041, loss_recon: 0.627731, loss_pred: 0.073613
iteration 816: loss: 60.230453, loss_kl: 0.673375, loss_recon: 0.595894, loss_pred: 0.127696
iteration 817: loss: 59.602444, loss_kl: 0.217843, loss_recon: 0.593199, loss_pred: 0.116408
iteration 818: loss: 61.147514, loss_kl: 0.122820, loss_recon: 0.609746, loss_pred: 0.079268
iteration 819: loss: 60.919617, loss_kl: 0.152876, loss_recon: 0.607057, loss_pred: 0.097346
iteration 820: loss: 59.783150, loss_kl: 0.070906, loss_recon: 0.596472, loss_pred: 0.081872
 41%|███████████▍                | 82/200 [1:08:02<1:37:39, 49.66s/it]iteration 821: loss: 60.990917, loss_kl: 0.413791, loss_recon: 0.605592, loss_pred: 0.099867
iteration 822: loss: 60.130711, loss_kl: 0.519881, loss_recon: 0.596312, loss_pred: 0.082566
iteration 823: loss: 61.114258, loss_kl: 0.209191, loss_recon: 0.608626, loss_pred: 0.083926
iteration 824: loss: 61.369926, loss_kl: 0.433118, loss_recon: 0.609584, loss_pred: 0.064200
iteration 825: loss: 60.027996, loss_kl: 0.190562, loss_recon: 0.597578, loss_pred: 0.117347
iteration 826: loss: 60.647705, loss_kl: 0.236414, loss_recon: 0.603720, loss_pred: 0.086146
iteration 827: loss: 62.315025, loss_kl: 0.195452, loss_recon: 0.620968, loss_pred: 0.061440
iteration 828: loss: 60.224789, loss_kl: 0.063906, loss_recon: 0.600848, loss_pred: 0.088750
iteration 829: loss: 62.531658, loss_kl: 0.415412, loss_recon: 0.621262, loss_pred: 0.072297
iteration 830: loss: 60.797054, loss_kl: 0.520303, loss_recon: 0.602976, loss_pred: 0.082179
 42%|███████████▌                | 83/200 [1:08:51<1:36:31, 49.50s/it]iteration 831: loss: 59.582951, loss_kl: 0.461922, loss_recon: 0.590917, loss_pred: 0.102480
iteration 832: loss: 61.839420, loss_kl: 0.309381, loss_recon: 0.614726, loss_pred: 0.106444
iteration 833: loss: 61.365406, loss_kl: 0.374445, loss_recon: 0.609648, loss_pred: 0.085456
iteration 834: loss: 61.053925, loss_kl: 0.384533, loss_recon: 0.606300, loss_pred: 0.100259
iteration 835: loss: 61.452103, loss_kl: 0.317106, loss_recon: 0.610763, loss_pred: 0.108947
iteration 836: loss: 60.838844, loss_kl: 0.077286, loss_recon: 0.606459, loss_pred: 0.127903
iteration 837: loss: 60.896652, loss_kl: 0.167198, loss_recon: 0.606505, loss_pred: 0.105455
iteration 838: loss: 60.429863, loss_kl: 0.213527, loss_recon: 0.601314, loss_pred: 0.118766
iteration 839: loss: 62.421307, loss_kl: 0.166690, loss_recon: 0.622071, loss_pred: 0.073918
iteration 840: loss: 60.523151, loss_kl: 0.039825, loss_recon: 0.603750, loss_pred: 0.114607
 42%|███████████▊                | 84/200 [1:09:41<1:36:13, 49.77s/it]iteration 841: loss: 59.948410, loss_kl: 0.326982, loss_recon: 0.595654, loss_pred: 0.094904
iteration 842: loss: 62.301559, loss_kl: 0.559837, loss_recon: 0.617062, loss_pred: 0.102026
iteration 843: loss: 59.781212, loss_kl: 0.524229, loss_recon: 0.591953, loss_pred: 0.124003
iteration 844: loss: 60.209930, loss_kl: 0.401008, loss_recon: 0.597703, loss_pred: 0.086234
iteration 845: loss: 61.403679, loss_kl: 0.890873, loss_recon: 0.605343, loss_pred: 0.084293
iteration 846: loss: 60.824913, loss_kl: 0.323253, loss_recon: 0.604717, loss_pred: 0.068347
iteration 847: loss: 60.911091, loss_kl: 0.154318, loss_recon: 0.606875, loss_pred: 0.087581
iteration 848: loss: 62.280788, loss_kl: 0.371507, loss_recon: 0.618782, loss_pred: 0.075169
iteration 849: loss: 62.003860, loss_kl: 0.315946, loss_recon: 0.616093, loss_pred: 0.116116
iteration 850: loss: 62.360085, loss_kl: 0.251704, loss_recon: 0.619960, loss_pred: 0.142235
 42%|███████████▉                | 85/200 [1:10:32<1:36:03, 50.11s/it]iteration 851: loss: 61.396683, loss_kl: 0.194272, loss_recon: 0.610709, loss_pred: 0.146860
iteration 852: loss: 60.830978, loss_kl: 0.531357, loss_recon: 0.601850, loss_pred: 0.156661
iteration 853: loss: 61.308826, loss_kl: 0.507700, loss_recon: 0.607487, loss_pred: 0.092603
iteration 854: loss: 61.671982, loss_kl: 0.217339, loss_recon: 0.613689, loss_pred: 0.102958
iteration 855: loss: 61.545010, loss_kl: 0.112786, loss_recon: 0.613690, loss_pred: 0.072153
iteration 856: loss: 60.446873, loss_kl: 0.187279, loss_recon: 0.601315, loss_pred: 0.142925
iteration 857: loss: 60.820515, loss_kl: 0.330023, loss_recon: 0.604416, loss_pred: 0.075007
iteration 858: loss: 61.426968, loss_kl: 0.460458, loss_recon: 0.609169, loss_pred: 0.086086
iteration 859: loss: 60.911320, loss_kl: 0.059520, loss_recon: 0.607895, loss_pred: 0.067001
iteration 860: loss: 60.747570, loss_kl: 0.254959, loss_recon: 0.604007, loss_pred: 0.112090
 43%|████████████                | 86/200 [1:11:21<1:34:34, 49.77s/it]iteration 861: loss: 61.615997, loss_kl: 0.434572, loss_recon: 0.611012, loss_pred: 0.097452
iteration 862: loss: 61.674603, loss_kl: 0.266113, loss_recon: 0.613460, loss_pred: 0.073053
iteration 863: loss: 61.810883, loss_kl: 0.357607, loss_recon: 0.613575, loss_pred: 0.109946
iteration 864: loss: 61.464417, loss_kl: 0.460800, loss_recon: 0.608767, loss_pred: 0.145169
iteration 865: loss: 60.552452, loss_kl: 0.285477, loss_recon: 0.601432, loss_pred: 0.135082
iteration 866: loss: 61.193695, loss_kl: 0.473212, loss_recon: 0.606172, loss_pred: 0.122028
iteration 867: loss: 59.937160, loss_kl: 0.310270, loss_recon: 0.595265, loss_pred: 0.112644
iteration 868: loss: 60.964249, loss_kl: 0.474459, loss_recon: 0.603867, loss_pred: 0.121874
iteration 869: loss: 60.885216, loss_kl: 0.269698, loss_recon: 0.605353, loss_pred: 0.090913
iteration 870: loss: 61.466850, loss_kl: 0.381095, loss_recon: 0.609890, loss_pred: 0.111837
 44%|████████████▏               | 87/200 [1:12:11<1:33:40, 49.74s/it]iteration 871: loss: 60.897003, loss_kl: 0.298919, loss_recon: 0.605080, loss_pred: 0.090084
iteration 872: loss: 61.435764, loss_kl: 0.190410, loss_recon: 0.611371, loss_pred: 0.108279
iteration 873: loss: 61.843395, loss_kl: 0.177679, loss_recon: 0.615835, loss_pred: 0.082173
iteration 874: loss: 61.578323, loss_kl: 0.361688, loss_recon: 0.610803, loss_pred: 0.136370
iteration 875: loss: 61.456585, loss_kl: 0.142213, loss_recon: 0.611856, loss_pred: 0.128737
iteration 876: loss: 61.523651, loss_kl: 0.371666, loss_recon: 0.610635, loss_pred: 0.088537
iteration 877: loss: 60.388741, loss_kl: 0.437460, loss_recon: 0.598951, loss_pred: 0.056226
iteration 878: loss: 60.841732, loss_kl: 0.265117, loss_recon: 0.604578, loss_pred: 0.118835
iteration 879: loss: 61.103180, loss_kl: 0.453676, loss_recon: 0.605620, loss_pred: 0.087519
iteration 880: loss: 60.324017, loss_kl: 0.534261, loss_recon: 0.596937, loss_pred: 0.096027
 44%|████████████▎               | 88/200 [1:13:00<1:32:31, 49.56s/it]iteration 881: loss: 60.783470, loss_kl: 0.235683, loss_recon: 0.604201, loss_pred: 0.127679
iteration 882: loss: 61.465721, loss_kl: 0.145782, loss_recon: 0.612448, loss_pred: 0.075133
iteration 883: loss: 61.050240, loss_kl: 0.333571, loss_recon: 0.605808, loss_pred: 0.135848
iteration 884: loss: 61.229351, loss_kl: 0.577042, loss_recon: 0.605613, loss_pred: 0.091041
iteration 885: loss: 59.979023, loss_kl: 0.355004, loss_recon: 0.594611, loss_pred: 0.162894
iteration 886: loss: 61.339741, loss_kl: 0.336426, loss_recon: 0.608970, loss_pred: 0.106296
iteration 887: loss: 61.251785, loss_kl: 0.333601, loss_recon: 0.607899, loss_pred: 0.128256
iteration 888: loss: 61.062729, loss_kl: 0.413424, loss_recon: 0.605526, loss_pred: 0.096714
iteration 889: loss: 61.229862, loss_kl: 0.267663, loss_recon: 0.608525, loss_pred: 0.109683
iteration 890: loss: 62.537090, loss_kl: 0.508427, loss_recon: 0.619400, loss_pred: 0.088680
 44%|████████████▍               | 89/200 [1:13:50<1:31:42, 49.57s/it]iteration 891: loss: 60.912518, loss_kl: 0.241173, loss_recon: 0.605503, loss_pred: 0.121036
iteration 892: loss: 60.954262, loss_kl: 0.349106, loss_recon: 0.604711, loss_pred: 0.134097
iteration 893: loss: 62.481564, loss_kl: 0.129235, loss_recon: 0.622829, loss_pred: 0.069469
iteration 894: loss: 61.518993, loss_kl: 0.227089, loss_recon: 0.612225, loss_pred: 0.069439
iteration 895: loss: 61.360687, loss_kl: 0.206288, loss_recon: 0.610804, loss_pred: 0.073998
iteration 896: loss: 60.777508, loss_kl: 0.145026, loss_recon: 0.605328, loss_pred: 0.099714
iteration 897: loss: 61.815155, loss_kl: 0.298633, loss_recon: 0.614239, loss_pred: 0.092639
iteration 898: loss: 60.999683, loss_kl: 0.371822, loss_recon: 0.605363, loss_pred: 0.091564
iteration 899: loss: 60.327255, loss_kl: 0.541126, loss_recon: 0.596968, loss_pred: 0.089295
iteration 900: loss: 59.992584, loss_kl: 0.209554, loss_recon: 0.596992, loss_pred: 0.083878
 45%|████████████▌               | 90/200 [1:14:39<1:30:45, 49.50s/it]iteration 901: loss: 61.066719, loss_kl: 0.185392, loss_recon: 0.607846, loss_pred: 0.096727
iteration 902: loss: 60.458397, loss_kl: 0.361087, loss_recon: 0.599993, loss_pred: 0.097969
iteration 903: loss: 60.994244, loss_kl: 0.391778, loss_recon: 0.605314, loss_pred: 0.071093
iteration 904: loss: 61.878628, loss_kl: 0.580656, loss_recon: 0.612109, loss_pred: 0.087055
iteration 905: loss: 61.987457, loss_kl: 0.516933, loss_recon: 0.613842, loss_pred: 0.086312
iteration 906: loss: 62.367683, loss_kl: 0.540818, loss_recon: 0.617294, loss_pred: 0.097464
iteration 907: loss: 60.925980, loss_kl: 0.445968, loss_recon: 0.603481, loss_pred: 0.131958
iteration 908: loss: 61.482059, loss_kl: 0.424061, loss_recon: 0.609411, loss_pred: 0.116949
iteration 909: loss: 60.086437, loss_kl: 0.107235, loss_recon: 0.598503, loss_pred: 0.128926
iteration 910: loss: 60.787773, loss_kl: 0.315544, loss_recon: 0.603866, loss_pred: 0.085633
 46%|████████████▋               | 91/200 [1:15:28<1:29:52, 49.47s/it]iteration 911: loss: 60.687996, loss_kl: 0.540781, loss_recon: 0.600435, loss_pred: 0.103668
iteration 912: loss: 60.502903, loss_kl: 0.249224, loss_recon: 0.601276, loss_pred: 0.126046
iteration 913: loss: 61.137718, loss_kl: 0.165562, loss_recon: 0.608410, loss_pred: 0.131157
iteration 914: loss: 61.397701, loss_kl: 0.263188, loss_recon: 0.610581, loss_pred: 0.076404
iteration 915: loss: 61.025028, loss_kl: 0.182602, loss_recon: 0.607153, loss_pred: 0.127108
iteration 916: loss: 61.514091, loss_kl: 0.371020, loss_recon: 0.610470, loss_pred: 0.096067
iteration 917: loss: 61.317432, loss_kl: 0.259617, loss_recon: 0.609445, loss_pred: 0.113320
iteration 918: loss: 62.694160, loss_kl: 0.310065, loss_recon: 0.623003, loss_pred: 0.083762
iteration 919: loss: 60.019863, loss_kl: 0.202931, loss_recon: 0.596984, loss_pred: 0.118534
iteration 920: loss: 60.885548, loss_kl: 0.354527, loss_recon: 0.604078, loss_pred: 0.123233
 46%|████████████▉               | 92/200 [1:16:18<1:28:59, 49.44s/it]iteration 921: loss: 61.466892, loss_kl: 0.178632, loss_recon: 0.611965, loss_pred: 0.091803
iteration 922: loss: 61.492218, loss_kl: 0.336437, loss_recon: 0.610854, loss_pred: 0.070415
iteration 923: loss: 62.301998, loss_kl: 0.465939, loss_recon: 0.617678, loss_pred: 0.068225
iteration 924: loss: 60.732838, loss_kl: 0.318797, loss_recon: 0.603013, loss_pred: 0.112735
iteration 925: loss: 61.384842, loss_kl: 0.343504, loss_recon: 0.609092, loss_pred: 0.132135
iteration 926: loss: 60.403507, loss_kl: 0.451039, loss_recon: 0.598183, loss_pred: 0.134208
iteration 927: loss: 61.088943, loss_kl: 0.298531, loss_recon: 0.606829, loss_pred: 0.107521
iteration 928: loss: 60.969250, loss_kl: 0.534550, loss_recon: 0.603420, loss_pred: 0.092696
iteration 929: loss: 60.654900, loss_kl: 0.259177, loss_recon: 0.603129, loss_pred: 0.082776
iteration 930: loss: 61.380581, loss_kl: 0.196699, loss_recon: 0.610525, loss_pred: 0.131377
 46%|█████████████               | 93/200 [1:17:07<1:28:13, 49.47s/it]iteration 931: loss: 61.758011, loss_kl: 0.310219, loss_recon: 0.613069, loss_pred: 0.140863
iteration 932: loss: 60.320347, loss_kl: 0.339688, loss_recon: 0.598479, loss_pred: 0.132712
iteration 933: loss: 60.639370, loss_kl: 0.161978, loss_recon: 0.603782, loss_pred: 0.099147
iteration 934: loss: 61.416321, loss_kl: 0.227357, loss_recon: 0.610941, loss_pred: 0.094861
iteration 935: loss: 61.495499, loss_kl: 0.253885, loss_recon: 0.611371, loss_pred: 0.104481
iteration 936: loss: 61.632183, loss_kl: 0.397076, loss_recon: 0.611510, loss_pred: 0.084098
iteration 937: loss: 60.782043, loss_kl: 0.457227, loss_recon: 0.601697, loss_pred: 0.155129
iteration 938: loss: 61.897121, loss_kl: 0.253690, loss_recon: 0.615580, loss_pred: 0.085443
iteration 939: loss: 60.517422, loss_kl: 0.222412, loss_recon: 0.602073, loss_pred: 0.087661
iteration 940: loss: 60.575703, loss_kl: 0.269622, loss_recon: 0.602165, loss_pred: 0.089539
 47%|█████████████▏              | 94/200 [1:17:57<1:27:33, 49.56s/it]iteration 941: loss: 61.321320, loss_kl: 0.438490, loss_recon: 0.607839, loss_pred: 0.098943
iteration 942: loss: 60.623112, loss_kl: 0.547748, loss_recon: 0.599423, loss_pred: 0.133096
iteration 943: loss: 61.751198, loss_kl: 0.513679, loss_recon: 0.611418, loss_pred: 0.095720
iteration 944: loss: 62.174255, loss_kl: 0.236272, loss_recon: 0.618742, loss_pred: 0.063743
iteration 945: loss: 59.853600, loss_kl: 0.193944, loss_recon: 0.595644, loss_pred: 0.095249
iteration 946: loss: 61.495789, loss_kl: 0.213218, loss_recon: 0.611832, loss_pred: 0.099380
iteration 947: loss: 60.968784, loss_kl: 0.464764, loss_recon: 0.604027, loss_pred: 0.101295
iteration 948: loss: 61.293144, loss_kl: 0.338816, loss_recon: 0.608825, loss_pred: 0.071873
iteration 949: loss: 60.640587, loss_kl: 0.302150, loss_recon: 0.602350, loss_pred: 0.103459
iteration 950: loss: 62.132870, loss_kl: 0.193944, loss_recon: 0.618565, loss_pred: 0.082432
 48%|█████████████▎              | 95/200 [1:18:47<1:26:45, 49.58s/it]iteration 951: loss: 60.919300, loss_kl: 0.235358, loss_recon: 0.606183, loss_pred: 0.065685
iteration 952: loss: 61.386688, loss_kl: 0.185162, loss_recon: 0.611328, loss_pred: 0.068715
iteration 953: loss: 62.571518, loss_kl: 0.674773, loss_recon: 0.617888, loss_pred: 0.107899
iteration 954: loss: 61.589287, loss_kl: 0.338020, loss_recon: 0.611275, loss_pred: 0.123722
iteration 955: loss: 60.446182, loss_kl: 0.189467, loss_recon: 0.600858, loss_pred: 0.170879
iteration 956: loss: 59.847473, loss_kl: 0.488843, loss_recon: 0.591280, loss_pred: 0.230623
iteration 957: loss: 61.785221, loss_kl: 0.333907, loss_recon: 0.613840, loss_pred: 0.067313
iteration 958: loss: 60.181278, loss_kl: 0.167584, loss_recon: 0.599043, loss_pred: 0.109384
iteration 959: loss: 61.439823, loss_kl: 0.475597, loss_recon: 0.608528, loss_pred: 0.111460
iteration 960: loss: 61.680302, loss_kl: 0.524906, loss_recon: 0.610569, loss_pred: 0.098474
 48%|█████████████▍              | 96/200 [1:19:36<1:25:46, 49.49s/it]iteration 961: loss: 60.370861, loss_kl: 0.431816, loss_recon: 0.598346, loss_pred: 0.104438
iteration 962: loss: 60.587215, loss_kl: 0.451050, loss_recon: 0.600173, loss_pred: 0.118890
iteration 963: loss: 61.453533, loss_kl: 0.330620, loss_recon: 0.610199, loss_pred: 0.103028
iteration 964: loss: 61.654175, loss_kl: 0.302934, loss_recon: 0.612655, loss_pred: 0.085765
iteration 965: loss: 60.852402, loss_kl: 0.323522, loss_recon: 0.604347, loss_pred: 0.094135
iteration 966: loss: 61.145634, loss_kl: 0.188138, loss_recon: 0.608830, loss_pred: 0.074452
iteration 967: loss: 61.414101, loss_kl: 0.198043, loss_recon: 0.611445, loss_pred: 0.071603
iteration 968: loss: 61.530579, loss_kl: 0.431165, loss_recon: 0.610294, loss_pred: 0.070059
iteration 969: loss: 60.538837, loss_kl: 0.460193, loss_recon: 0.599677, loss_pred: 0.110969
iteration 970: loss: 61.710815, loss_kl: 0.249671, loss_recon: 0.613239, loss_pred: 0.137246
 48%|█████████████▌              | 97/200 [1:20:26<1:25:01, 49.53s/it]iteration 971: loss: 59.903934, loss_kl: 0.051378, loss_recon: 0.597448, loss_pred: 0.107763
iteration 972: loss: 61.286934, loss_kl: 0.160050, loss_recon: 0.610520, loss_pred: 0.074871
iteration 973: loss: 60.991512, loss_kl: 0.617946, loss_recon: 0.602707, loss_pred: 0.102866
iteration 974: loss: 61.875595, loss_kl: 0.506073, loss_recon: 0.612643, loss_pred: 0.105252
iteration 975: loss: 61.267876, loss_kl: 0.245907, loss_recon: 0.609382, loss_pred: 0.083739
iteration 976: loss: 61.249908, loss_kl: 0.471340, loss_recon: 0.606447, loss_pred: 0.133853
iteration 977: loss: 61.003536, loss_kl: 0.309030, loss_recon: 0.605430, loss_pred: 0.151469
iteration 978: loss: 60.570225, loss_kl: 0.077380, loss_recon: 0.603201, loss_pred: 0.172730
iteration 979: loss: 61.235035, loss_kl: 0.160005, loss_recon: 0.609638, loss_pred: 0.111230
iteration 980: loss: 61.273964, loss_kl: 0.296419, loss_recon: 0.608741, loss_pred: 0.103442
 49%|█████████████▋              | 98/200 [1:21:14<1:23:51, 49.33s/it]iteration 981: loss: 61.784046, loss_kl: 0.233086, loss_recon: 0.614464, loss_pred: 0.104533
iteration 982: loss: 60.997231, loss_kl: 0.430220, loss_recon: 0.604720, loss_pred: 0.095048
iteration 983: loss: 60.788345, loss_kl: 0.387309, loss_recon: 0.603312, loss_pred: 0.069798
iteration 984: loss: 61.834919, loss_kl: 0.195294, loss_recon: 0.615610, loss_pred: 0.078645
iteration 985: loss: 59.883457, loss_kl: 0.423870, loss_recon: 0.593391, loss_pred: 0.120468
iteration 986: loss: 61.691582, loss_kl: 0.299712, loss_recon: 0.613295, loss_pred: 0.062367
iteration 987: loss: 62.002625, loss_kl: 0.666638, loss_recon: 0.612539, loss_pred: 0.082076
iteration 988: loss: 59.865612, loss_kl: 0.364244, loss_recon: 0.593524, loss_pred: 0.148952
iteration 989: loss: 60.948013, loss_kl: 0.256446, loss_recon: 0.605536, loss_pred: 0.137931
iteration 990: loss: 61.310078, loss_kl: 0.222849, loss_recon: 0.610197, loss_pred: 0.067546
 50%|█████████████▊              | 99/200 [1:22:04<1:23:19, 49.50s/it]iteration 991: loss: 60.402992, loss_kl: 0.189204, loss_recon: 0.601287, loss_pred: 0.085111
iteration 992: loss: 61.164188, loss_kl: 0.503520, loss_recon: 0.605828, loss_pred: 0.077863
iteration 993: loss: 61.187805, loss_kl: 0.318100, loss_recon: 0.607848, loss_pred: 0.084952
iteration 994: loss: 61.349350, loss_kl: 0.126739, loss_recon: 0.611480, loss_pred: 0.074600
iteration 995: loss: 60.761841, loss_kl: 0.389474, loss_recon: 0.602882, loss_pred: 0.084163
iteration 996: loss: 61.327389, loss_kl: 0.621840, loss_recon: 0.606262, loss_pred: 0.079338
iteration 997: loss: 61.826389, loss_kl: 0.704066, loss_recon: 0.610422, loss_pred: 0.080101
iteration 998: loss: 61.721970, loss_kl: 0.100363, loss_recon: 0.615372, loss_pred: 0.084415
iteration 999: loss: 60.253414, loss_kl: 0.487187, loss_recon: 0.596686, loss_pred: 0.097643
iteration 1000: loss: 61.238163, loss_kl: 0.572055, loss_recon: 0.605894, loss_pred: 0.076689
 50%|█████████████▌             | 100/200 [1:22:54<1:22:24, 49.45s/it]iteration 1001: loss: 60.975555, loss_kl: 0.265756, loss_recon: 0.608956, loss_pred: 0.077263
iteration 1002: loss: 62.389751, loss_kl: 0.232136, loss_recon: 0.623328, loss_pred: 0.054625
iteration 1003: loss: 59.634350, loss_kl: 0.286127, loss_recon: 0.595465, loss_pred: 0.084984
iteration 1004: loss: 59.314369, loss_kl: 0.338053, loss_recon: 0.592229, loss_pred: 0.088053
iteration 1005: loss: 60.674377, loss_kl: 0.644347, loss_recon: 0.605700, loss_pred: 0.097972
iteration 1006: loss: 61.352997, loss_kl: 0.504264, loss_recon: 0.612577, loss_pred: 0.090285
iteration 1007: loss: 59.953377, loss_kl: 0.450125, loss_recon: 0.598562, loss_pred: 0.092643
iteration 1008: loss: 60.042156, loss_kl: 0.803590, loss_recon: 0.599552, loss_pred: 0.078902
iteration 1009: loss: 61.175491, loss_kl: 0.775092, loss_recon: 0.610922, loss_pred: 0.075506
iteration 1010: loss: 62.191147, loss_kl: 1.218101, loss_recon: 0.621167, loss_pred: 0.062217
 50%|█████████████▋             | 101/200 [1:23:46<1:23:00, 50.31s/it]iteration 1011: loss: 60.919880, loss_kl: 0.777328, loss_recon: 0.608439, loss_pred: 0.068236
iteration 1012: loss: 60.687389, loss_kl: 0.585159, loss_recon: 0.606153, loss_pred: 0.066243
iteration 1013: loss: 60.300705, loss_kl: 0.873583, loss_recon: 0.602059, loss_pred: 0.086088
iteration 1014: loss: 61.714771, loss_kl: 0.906130, loss_recon: 0.616411, loss_pred: 0.064619
iteration 1015: loss: 60.869034, loss_kl: 1.130498, loss_recon: 0.607754, loss_pred: 0.082341
iteration 1016: loss: 61.256073, loss_kl: 1.019119, loss_recon: 0.611883, loss_pred: 0.057565
iteration 1017: loss: 58.519318, loss_kl: 1.581027, loss_recon: 0.583971, loss_pred: 0.106368
iteration 1018: loss: 60.880081, loss_kl: 0.853446, loss_recon: 0.608257, loss_pred: 0.045866
iteration 1019: loss: 61.441425, loss_kl: 1.484783, loss_recon: 0.613524, loss_pred: 0.074154
iteration 1020: loss: 60.988537, loss_kl: 0.921825, loss_recon: 0.608906, loss_pred: 0.088729
 51%|█████████████▊             | 102/200 [1:24:38<1:23:06, 50.88s/it]iteration 1021: loss: 60.641068, loss_kl: 1.180015, loss_recon: 0.605526, loss_pred: 0.076665
iteration 1022: loss: 61.328461, loss_kl: 0.857742, loss_recon: 0.612487, loss_pred: 0.071178
iteration 1023: loss: 61.274803, loss_kl: 1.247910, loss_recon: 0.611830, loss_pred: 0.079303
iteration 1024: loss: 60.377838, loss_kl: 0.584235, loss_recon: 0.602631, loss_pred: 0.108866
iteration 1025: loss: 61.511803, loss_kl: 0.796775, loss_recon: 0.614371, loss_pred: 0.066696
iteration 1026: loss: 59.424492, loss_kl: 1.346065, loss_recon: 0.593509, loss_pred: 0.060169
iteration 1027: loss: 62.531647, loss_kl: 1.024428, loss_recon: 0.624542, loss_pred: 0.067179
iteration 1028: loss: 59.599232, loss_kl: 0.915823, loss_recon: 0.595087, loss_pred: 0.081392
iteration 1029: loss: 60.246468, loss_kl: 1.082223, loss_recon: 0.601463, loss_pred: 0.089358
iteration 1030: loss: 60.828274, loss_kl: 1.196719, loss_recon: 0.607076, loss_pred: 0.108730
 52%|█████████████▉             | 103/200 [1:25:30<1:22:34, 51.08s/it]iteration 1031: loss: 62.336857, loss_kl: 0.847600, loss_recon: 0.622268, loss_pred: 0.101587
iteration 1032: loss: 59.428421, loss_kl: 1.051203, loss_recon: 0.593042, loss_pred: 0.113670
iteration 1033: loss: 61.062267, loss_kl: 1.457949, loss_recon: 0.609012, loss_pred: 0.146439
iteration 1034: loss: 61.733990, loss_kl: 0.783751, loss_recon: 0.616717, loss_pred: 0.054470
iteration 1035: loss: 59.493984, loss_kl: 0.744135, loss_recon: 0.593889, loss_pred: 0.097661
iteration 1036: loss: 60.683174, loss_kl: 1.138203, loss_recon: 0.605988, loss_pred: 0.072990
iteration 1037: loss: 61.894054, loss_kl: 1.128841, loss_recon: 0.617505, loss_pred: 0.132292
iteration 1038: loss: 60.024719, loss_kl: 0.656530, loss_recon: 0.599300, loss_pred: 0.088201
iteration 1039: loss: 59.894764, loss_kl: 0.587980, loss_recon: 0.597999, loss_pred: 0.088992
iteration 1040: loss: 61.453758, loss_kl: 0.696139, loss_recon: 0.613885, loss_pred: 0.058286
 52%|██████████████             | 104/200 [1:26:20<1:21:06, 50.69s/it]iteration 1041: loss: 60.695911, loss_kl: 0.948021, loss_recon: 0.606240, loss_pred: 0.062448
iteration 1042: loss: 60.148060, loss_kl: 1.128203, loss_recon: 0.600455, loss_pred: 0.091324
iteration 1043: loss: 61.649132, loss_kl: 0.879438, loss_recon: 0.615780, loss_pred: 0.062373
iteration 1044: loss: 60.768829, loss_kl: 0.905117, loss_recon: 0.606901, loss_pred: 0.069682
iteration 1045: loss: 60.541985, loss_kl: 0.344846, loss_recon: 0.604339, loss_pred: 0.104631
iteration 1046: loss: 61.227104, loss_kl: 0.750253, loss_recon: 0.611692, loss_pred: 0.050371
iteration 1047: loss: 61.040157, loss_kl: 1.374374, loss_recon: 0.609428, loss_pred: 0.083571
iteration 1048: loss: 59.770214, loss_kl: 0.505221, loss_recon: 0.597109, loss_pred: 0.054260
iteration 1049: loss: 60.033993, loss_kl: 0.677988, loss_recon: 0.599412, loss_pred: 0.086017
iteration 1050: loss: 61.678444, loss_kl: 0.627159, loss_recon: 0.616116, loss_pred: 0.060623
 52%|██████████████▏            | 105/200 [1:27:10<1:20:00, 50.53s/it]iteration 1051: loss: 60.377853, loss_kl: 0.790772, loss_recon: 0.602890, loss_pred: 0.080955
iteration 1052: loss: 61.085934, loss_kl: 0.847739, loss_recon: 0.610060, loss_pred: 0.071483
iteration 1053: loss: 61.225994, loss_kl: 0.724080, loss_recon: 0.611535, loss_pred: 0.065237
iteration 1054: loss: 60.926235, loss_kl: 1.061185, loss_recon: 0.608399, loss_pred: 0.075758
iteration 1055: loss: 60.344242, loss_kl: 0.928347, loss_recon: 0.602475, loss_pred: 0.087500
iteration 1056: loss: 61.386505, loss_kl: 0.549227, loss_recon: 0.613150, loss_pred: 0.066028
iteration 1057: loss: 60.790634, loss_kl: 0.597252, loss_recon: 0.607212, loss_pred: 0.063498
iteration 1058: loss: 60.151035, loss_kl: 0.762407, loss_recon: 0.600859, loss_pred: 0.057562
iteration 1059: loss: 61.797142, loss_kl: 0.475010, loss_recon: 0.617381, loss_pred: 0.054276
iteration 1060: loss: 59.023769, loss_kl: 0.616946, loss_recon: 0.589151, loss_pred: 0.102497
 53%|██████████████▎            | 106/200 [1:28:00<1:19:15, 50.59s/it]iteration 1061: loss: 60.883228, loss_kl: 0.446718, loss_recon: 0.608221, loss_pred: 0.056689
iteration 1062: loss: 60.409180, loss_kl: 0.323207, loss_recon: 0.603346, loss_pred: 0.071349
iteration 1063: loss: 61.319893, loss_kl: 0.755248, loss_recon: 0.612560, loss_pred: 0.056346
iteration 1064: loss: 59.970314, loss_kl: 0.374445, loss_recon: 0.598761, loss_pred: 0.090455
iteration 1065: loss: 61.667831, loss_kl: 0.547819, loss_recon: 0.615937, loss_pred: 0.068661
iteration 1066: loss: 61.387791, loss_kl: 0.527390, loss_recon: 0.613093, loss_pred: 0.073170
iteration 1067: loss: 61.310181, loss_kl: 0.258028, loss_recon: 0.612359, loss_pred: 0.071710
iteration 1068: loss: 60.158615, loss_kl: 0.549901, loss_recon: 0.600793, loss_pred: 0.073833
iteration 1069: loss: 60.218330, loss_kl: 0.406727, loss_recon: 0.601397, loss_pred: 0.074609
iteration 1070: loss: 59.811523, loss_kl: 0.549419, loss_recon: 0.597339, loss_pred: 0.072108
 54%|██████████████▍            | 107/200 [1:28:51<1:18:13, 50.46s/it]iteration 1071: loss: 60.416477, loss_kl: 0.583055, loss_recon: 0.603625, loss_pred: 0.048107
iteration 1072: loss: 61.378929, loss_kl: 0.842945, loss_recon: 0.612943, loss_pred: 0.076167
iteration 1073: loss: 60.406883, loss_kl: 0.412734, loss_recon: 0.603240, loss_pred: 0.078742
iteration 1074: loss: 61.110073, loss_kl: 0.463238, loss_recon: 0.610575, loss_pred: 0.047980
iteration 1075: loss: 61.670059, loss_kl: 0.270406, loss_recon: 0.616075, loss_pred: 0.059850
iteration 1076: loss: 61.600506, loss_kl: 0.575648, loss_recon: 0.615500, loss_pred: 0.044789
iteration 1077: loss: 59.735687, loss_kl: 0.303690, loss_recon: 0.596347, loss_pred: 0.097923
iteration 1078: loss: 61.618076, loss_kl: 0.168925, loss_recon: 0.615696, loss_pred: 0.046738
iteration 1079: loss: 59.736317, loss_kl: 0.289765, loss_recon: 0.596347, loss_pred: 0.098738
iteration 1080: loss: 59.575413, loss_kl: 0.377043, loss_recon: 0.595165, loss_pred: 0.055183
 54%|██████████████▌            | 108/200 [1:29:40<1:16:48, 50.09s/it]iteration 1081: loss: 60.869781, loss_kl: 0.239604, loss_recon: 0.607756, loss_pred: 0.091808
iteration 1082: loss: 60.836746, loss_kl: 0.495210, loss_recon: 0.607536, loss_pred: 0.078226
iteration 1083: loss: 60.572575, loss_kl: 0.221447, loss_recon: 0.605052, loss_pred: 0.065149
iteration 1084: loss: 60.287136, loss_kl: 0.554499, loss_recon: 0.602244, loss_pred: 0.057163
iteration 1085: loss: 60.219307, loss_kl: 0.299538, loss_recon: 0.601454, loss_pred: 0.070912
iteration 1086: loss: 61.498314, loss_kl: 0.307035, loss_recon: 0.614418, loss_pred: 0.053410
iteration 1087: loss: 61.245430, loss_kl: 0.480752, loss_recon: 0.611723, loss_pred: 0.068284
iteration 1088: loss: 59.919037, loss_kl: 0.432310, loss_recon: 0.598566, loss_pred: 0.058087
iteration 1089: loss: 60.625515, loss_kl: 0.216167, loss_recon: 0.605567, loss_pred: 0.066688
iteration 1090: loss: 60.965027, loss_kl: 0.427270, loss_recon: 0.609009, loss_pred: 0.059856
 55%|██████████████▋            | 109/200 [1:30:29<1:15:43, 49.93s/it]iteration 1091: loss: 61.584610, loss_kl: 0.544187, loss_recon: 0.614668, loss_pred: 0.112345
iteration 1092: loss: 60.932098, loss_kl: 0.170069, loss_recon: 0.608799, loss_pred: 0.050529
iteration 1093: loss: 61.297993, loss_kl: 0.409513, loss_recon: 0.612389, loss_pred: 0.055041
iteration 1094: loss: 59.706341, loss_kl: 0.569666, loss_recon: 0.596206, loss_pred: 0.080010
iteration 1095: loss: 61.077244, loss_kl: 0.226283, loss_recon: 0.610132, loss_pred: 0.061769
iteration 1096: loss: 60.764343, loss_kl: 0.138785, loss_recon: 0.607018, loss_pred: 0.061107
iteration 1097: loss: 60.535923, loss_kl: 0.255627, loss_recon: 0.604719, loss_pred: 0.061442
iteration 1098: loss: 60.216152, loss_kl: 0.226205, loss_recon: 0.601426, loss_pred: 0.071337
iteration 1099: loss: 60.994102, loss_kl: 0.223454, loss_recon: 0.609475, loss_pred: 0.044317
iteration 1100: loss: 60.344616, loss_kl: 0.157221, loss_recon: 0.602895, loss_pred: 0.053577
 55%|██████████████▊            | 110/200 [1:31:19<1:14:42, 49.81s/it]iteration 1101: loss: 61.419067, loss_kl: 0.492060, loss_recon: 0.613262, loss_pred: 0.087897
iteration 1102: loss: 61.335014, loss_kl: 0.310007, loss_recon: 0.612661, loss_pred: 0.065794
iteration 1103: loss: 61.237568, loss_kl: 0.206881, loss_recon: 0.611804, loss_pred: 0.055082
iteration 1104: loss: 60.756161, loss_kl: 0.377735, loss_recon: 0.606850, loss_pred: 0.067370
iteration 1105: loss: 61.139061, loss_kl: 0.348375, loss_recon: 0.610735, loss_pred: 0.062113
iteration 1106: loss: 61.247021, loss_kl: 0.416217, loss_recon: 0.611843, loss_pred: 0.058583
iteration 1107: loss: 59.332443, loss_kl: 0.407951, loss_recon: 0.592504, loss_pred: 0.077922
iteration 1108: loss: 60.342014, loss_kl: 0.172570, loss_recon: 0.602610, loss_pred: 0.079243
iteration 1109: loss: 60.674114, loss_kl: 0.499810, loss_recon: 0.606213, loss_pred: 0.047795
iteration 1110: loss: 59.565849, loss_kl: 0.319439, loss_recon: 0.594831, loss_pred: 0.079517
 56%|██████████████▉            | 111/200 [1:32:08<1:13:40, 49.67s/it]iteration 1111: loss: 60.470734, loss_kl: 0.227291, loss_recon: 0.604195, loss_pred: 0.048989
iteration 1112: loss: 59.739803, loss_kl: 0.333145, loss_recon: 0.596511, loss_pred: 0.085350
iteration 1113: loss: 61.112144, loss_kl: 0.305836, loss_recon: 0.610330, loss_pred: 0.076091
iteration 1114: loss: 60.864140, loss_kl: 0.440828, loss_recon: 0.607972, loss_pred: 0.062491
iteration 1115: loss: 60.995895, loss_kl: 0.259183, loss_recon: 0.609376, loss_pred: 0.055739
iteration 1116: loss: 60.308434, loss_kl: 0.336292, loss_recon: 0.602580, loss_pred: 0.047073
iteration 1117: loss: 60.338657, loss_kl: 0.374917, loss_recon: 0.602629, loss_pred: 0.072006
iteration 1118: loss: 62.161671, loss_kl: 0.259166, loss_recon: 0.620927, loss_pred: 0.066355
iteration 1119: loss: 60.191025, loss_kl: 0.208261, loss_recon: 0.601397, loss_pred: 0.049215
iteration 1120: loss: 60.991112, loss_kl: 0.271544, loss_recon: 0.609418, loss_pred: 0.046569
 56%|███████████████            | 112/200 [1:32:57<1:12:37, 49.52s/it]iteration 1121: loss: 59.348389, loss_kl: 0.243282, loss_recon: 0.592813, loss_pred: 0.064701
iteration 1122: loss: 60.951263, loss_kl: 0.247932, loss_recon: 0.608874, loss_pred: 0.061352
iteration 1123: loss: 60.539177, loss_kl: 0.300876, loss_recon: 0.604679, loss_pred: 0.068286
iteration 1124: loss: 59.922901, loss_kl: 0.226999, loss_recon: 0.598716, loss_pred: 0.049002
iteration 1125: loss: 62.555008, loss_kl: 0.121718, loss_recon: 0.625143, loss_pred: 0.039495
iteration 1126: loss: 62.189056, loss_kl: 0.387307, loss_recon: 0.621364, loss_pred: 0.048748
iteration 1127: loss: 61.092476, loss_kl: 0.257960, loss_recon: 0.610404, loss_pred: 0.049531
iteration 1128: loss: 59.918621, loss_kl: 0.174482, loss_recon: 0.598403, loss_pred: 0.076540
iteration 1129: loss: 60.004288, loss_kl: 0.129743, loss_recon: 0.599406, loss_pred: 0.062386
iteration 1130: loss: 61.387951, loss_kl: 0.310061, loss_recon: 0.613010, loss_pred: 0.083818
 56%|███████████████▎           | 113/200 [1:33:49<1:12:38, 50.09s/it]iteration 1131: loss: 59.986721, loss_kl: 0.240296, loss_recon: 0.599144, loss_pred: 0.060372
iteration 1132: loss: 60.689590, loss_kl: 0.301759, loss_recon: 0.606140, loss_pred: 0.060587
iteration 1133: loss: 61.760544, loss_kl: 0.156145, loss_recon: 0.616938, loss_pred: 0.058972
iteration 1134: loss: 60.917141, loss_kl: 0.289692, loss_recon: 0.608398, loss_pred: 0.062937
iteration 1135: loss: 61.550152, loss_kl: 0.195633, loss_recon: 0.614825, loss_pred: 0.057988
iteration 1136: loss: 61.114788, loss_kl: 0.181570, loss_recon: 0.610492, loss_pred: 0.056616
iteration 1137: loss: 60.548740, loss_kl: 0.189101, loss_recon: 0.604787, loss_pred: 0.060640
iteration 1138: loss: 60.441483, loss_kl: 0.187687, loss_recon: 0.603738, loss_pred: 0.058385
iteration 1139: loss: 61.713825, loss_kl: 0.106109, loss_recon: 0.616479, loss_pred: 0.060640
iteration 1140: loss: 59.162479, loss_kl: 0.116621, loss_recon: 0.591025, loss_pred: 0.054171
 57%|███████████████▍           | 114/200 [1:34:39<1:11:57, 50.20s/it]iteration 1141: loss: 60.789509, loss_kl: 0.307312, loss_recon: 0.607050, loss_pred: 0.057071
iteration 1142: loss: 60.881847, loss_kl: 0.331413, loss_recon: 0.608001, loss_pred: 0.052220
iteration 1143: loss: 60.140087, loss_kl: 0.207025, loss_recon: 0.600535, loss_pred: 0.068134
iteration 1144: loss: 61.290749, loss_kl: 0.179558, loss_recon: 0.611969, loss_pred: 0.077787
iteration 1145: loss: 61.099709, loss_kl: 0.072576, loss_recon: 0.610420, loss_pred: 0.051188
iteration 1146: loss: 60.488098, loss_kl: 0.106272, loss_recon: 0.604074, loss_pred: 0.071202
iteration 1147: loss: 61.201019, loss_kl: 0.052514, loss_recon: 0.611423, loss_pred: 0.054003
iteration 1148: loss: 59.601627, loss_kl: 0.157780, loss_recon: 0.595257, loss_pred: 0.061850
iteration 1149: loss: 60.935299, loss_kl: 0.304836, loss_recon: 0.608341, loss_pred: 0.074031
iteration 1150: loss: 60.767330, loss_kl: 0.235492, loss_recon: 0.606791, loss_pred: 0.067221
 57%|███████████████▌           | 115/200 [1:35:29<1:10:54, 50.05s/it]iteration 1151: loss: 61.561787, loss_kl: 0.173253, loss_recon: 0.614936, loss_pred: 0.045866
iteration 1152: loss: 60.392448, loss_kl: 0.301571, loss_recon: 0.603028, loss_pred: 0.050800
iteration 1153: loss: 60.345016, loss_kl: 0.176094, loss_recon: 0.602606, loss_pred: 0.061686
iteration 1154: loss: 59.992992, loss_kl: 0.372214, loss_recon: 0.598957, loss_pred: 0.049336
iteration 1155: loss: 61.207043, loss_kl: 0.125966, loss_recon: 0.611199, loss_pred: 0.070904
iteration 1156: loss: 61.048096, loss_kl: 0.090623, loss_recon: 0.609846, loss_pred: 0.051819
iteration 1157: loss: 61.128807, loss_kl: 0.175012, loss_recon: 0.610295, loss_pred: 0.076810
iteration 1158: loss: 60.779480, loss_kl: 0.152613, loss_recon: 0.606927, loss_pred: 0.067166
iteration 1159: loss: 60.742168, loss_kl: 0.054956, loss_recon: 0.606872, loss_pred: 0.047868
iteration 1160: loss: 60.343445, loss_kl: 0.040514, loss_recon: 0.602867, loss_pred: 0.051524
 58%|███████████████▋           | 116/200 [1:36:20<1:10:29, 50.35s/it]iteration 1161: loss: 60.931065, loss_kl: 0.127213, loss_recon: 0.608345, loss_pred: 0.075139
iteration 1162: loss: 60.444340, loss_kl: 0.155042, loss_recon: 0.603694, loss_pred: 0.048793
iteration 1163: loss: 61.824848, loss_kl: 0.119742, loss_recon: 0.617572, loss_pred: 0.047467
iteration 1164: loss: 60.023479, loss_kl: 0.107625, loss_recon: 0.599304, loss_pred: 0.074946
iteration 1165: loss: 60.620708, loss_kl: 0.109141, loss_recon: 0.605331, loss_pred: 0.069205
iteration 1166: loss: 61.168472, loss_kl: 0.119742, loss_recon: 0.610882, loss_pred: 0.060158
iteration 1167: loss: 61.733669, loss_kl: 0.170069, loss_recon: 0.616445, loss_pred: 0.060526
iteration 1168: loss: 59.490421, loss_kl: 0.153204, loss_recon: 0.594035, loss_pred: 0.061095
iteration 1169: loss: 60.785839, loss_kl: 0.217005, loss_recon: 0.606804, loss_pred: 0.068883
iteration 1170: loss: 60.659649, loss_kl: 0.257410, loss_recon: 0.605465, loss_pred: 0.069820
 58%|███████████████▊           | 117/200 [1:37:09<1:09:16, 50.08s/it]iteration 1171: loss: 59.321358, loss_kl: 0.233726, loss_recon: 0.591975, loss_pred: 0.075229
iteration 1172: loss: 62.130535, loss_kl: 0.151341, loss_recon: 0.620264, loss_pred: 0.072698
iteration 1173: loss: 62.028511, loss_kl: 0.012411, loss_recon: 0.619306, loss_pred: 0.095314
iteration 1174: loss: 61.470432, loss_kl: 0.057771, loss_recon: 0.613906, loss_pred: 0.067802
iteration 1175: loss: 59.394920, loss_kl: 0.002149, loss_recon: 0.592623, loss_pred: 0.132175
iteration 1176: loss: 62.073532, loss_kl: -0.035346, loss_recon: 0.620195, loss_pred: 0.061354
iteration 1177: loss: 60.948025, loss_kl: 0.203876, loss_recon: 0.608577, loss_pred: 0.047939
iteration 1178: loss: 59.710354, loss_kl: 0.226268, loss_recon: 0.595856, loss_pred: 0.077727
iteration 1179: loss: 60.945812, loss_kl: 0.141062, loss_recon: 0.608641, loss_pred: 0.052329
iteration 1180: loss: 60.125839, loss_kl: 0.131662, loss_recon: 0.600346, loss_pred: 0.063874
 59%|███████████████▉           | 118/200 [1:37:59<1:08:13, 49.93s/it]iteration 1181: loss: 60.466312, loss_kl: 0.046272, loss_recon: 0.603928, loss_pred: 0.062051
iteration 1182: loss: 61.050465, loss_kl: -0.019813, loss_recon: 0.610114, loss_pred: 0.043931
iteration 1183: loss: 61.384964, loss_kl: 0.383162, loss_recon: 0.612139, loss_pred: 0.076218
iteration 1184: loss: 60.662357, loss_kl: 0.182121, loss_recon: 0.605425, loss_pred: 0.074728
iteration 1185: loss: 59.432709, loss_kl: 0.017479, loss_recon: 0.593697, loss_pred: 0.058695
iteration 1186: loss: 60.770966, loss_kl: 0.148534, loss_recon: 0.606755, loss_pred: 0.058654
iteration 1187: loss: 61.750771, loss_kl: 0.172821, loss_recon: 0.616561, loss_pred: 0.051857
iteration 1188: loss: 60.584591, loss_kl: 0.191309, loss_recon: 0.604688, loss_pred: 0.068455
iteration 1189: loss: 59.945305, loss_kl: 0.266766, loss_recon: 0.598204, loss_pred: 0.058881
iteration 1190: loss: 61.418636, loss_kl: 0.037515, loss_recon: 0.613416, loss_pred: 0.067743
 60%|████████████████           | 119/200 [1:38:49<1:07:12, 49.79s/it]iteration 1191: loss: 61.993755, loss_kl: 0.291581, loss_recon: 0.618645, loss_pred: 0.045549
iteration 1192: loss: 59.913322, loss_kl: 0.159806, loss_recon: 0.598032, loss_pred: 0.064245
iteration 1193: loss: 61.163410, loss_kl: 0.215923, loss_recon: 0.610492, loss_pred: 0.052246
iteration 1194: loss: 61.935673, loss_kl: 0.121448, loss_recon: 0.618488, loss_pred: 0.052033
iteration 1195: loss: 59.976486, loss_kl: -0.146535, loss_recon: 0.599651, loss_pred: 0.053472
iteration 1196: loss: 60.569813, loss_kl: 0.205395, loss_recon: 0.604687, loss_pred: 0.042117
iteration 1197: loss: 61.787247, loss_kl: 0.189228, loss_recon: 0.616721, loss_pred: 0.060772
iteration 1198: loss: 59.922394, loss_kl: 0.192159, loss_recon: 0.597990, loss_pred: 0.068174
iteration 1199: loss: 59.792934, loss_kl: 0.156812, loss_recon: 0.596864, loss_pred: 0.061490
iteration 1200: loss: 59.958881, loss_kl: 0.028449, loss_recon: 0.598864, loss_pred: 0.064286
save model to ../model/TVG_Design[64, 64, 64]/TVG_encoderpretrained_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.003_seed1234/epoch_119.pth
 60%|████████████████▏          | 120/200 [1:39:38<1:06:23, 49.80s/it]iteration 1201: loss: 59.603394, loss_kl: 0.246163, loss_recon: 0.594506, loss_pred: 0.072376
iteration 1202: loss: 62.074886, loss_kl: 0.264806, loss_recon: 0.619102, loss_pred: 0.078127
iteration 1203: loss: 60.622349, loss_kl: 0.105174, loss_recon: 0.605289, loss_pred: 0.059120
iteration 1204: loss: 61.382298, loss_kl: 0.106501, loss_recon: 0.612862, loss_pred: 0.061282
iteration 1205: loss: 61.434135, loss_kl: 0.185067, loss_recon: 0.613236, loss_pred: 0.050024
iteration 1206: loss: 60.102921, loss_kl: 0.134929, loss_recon: 0.600018, loss_pred: 0.057029
iteration 1207: loss: 60.901276, loss_kl: 0.045619, loss_recon: 0.608177, loss_pred: 0.068693
iteration 1208: loss: 60.771873, loss_kl: 0.183148, loss_recon: 0.606609, loss_pred: 0.051159
iteration 1209: loss: 59.790058, loss_kl: 0.075728, loss_recon: 0.597065, loss_pred: 0.058816
iteration 1210: loss: 61.216255, loss_kl: 0.068096, loss_recon: 0.611352, loss_pred: 0.058755
 60%|████████████████▎          | 121/200 [1:40:28<1:05:40, 49.88s/it]iteration 1211: loss: 60.119568, loss_kl: 0.145608, loss_recon: 0.600116, loss_pred: 0.054660
iteration 1212: loss: 62.673130, loss_kl: 0.158488, loss_recon: 0.625652, loss_pred: 0.049861
iteration 1213: loss: 61.698521, loss_kl: -0.002166, loss_recon: 0.616558, loss_pred: 0.043531
iteration 1214: loss: 59.973454, loss_kl: 0.177921, loss_recon: 0.597862, loss_pred: 0.122048
iteration 1215: loss: 60.182877, loss_kl: 0.228895, loss_recon: 0.600375, loss_pred: 0.061543
iteration 1216: loss: 60.034061, loss_kl: 0.121159, loss_recon: 0.599219, loss_pred: 0.067757
iteration 1217: loss: 59.875393, loss_kl: 0.124991, loss_recon: 0.597318, loss_pred: 0.097763
iteration 1218: loss: 60.741192, loss_kl: 0.134333, loss_recon: 0.606355, loss_pred: 0.056508
iteration 1219: loss: 61.057232, loss_kl: 0.077158, loss_recon: 0.609872, loss_pred: 0.041744
iteration 1220: loss: 61.665623, loss_kl: 0.261799, loss_recon: 0.615164, loss_pred: 0.053316
 61%|████████████████▍          | 122/200 [1:41:18<1:04:35, 49.69s/it]iteration 1221: loss: 60.310032, loss_kl: 0.162483, loss_recon: 0.601863, loss_pred: 0.057719
iteration 1222: loss: 60.736725, loss_kl: 0.083536, loss_recon: 0.606514, loss_pred: 0.051366
iteration 1223: loss: 61.806698, loss_kl: 0.063498, loss_recon: 0.617028, loss_pred: 0.078093
iteration 1224: loss: 60.832188, loss_kl: 0.201344, loss_recon: 0.607079, loss_pred: 0.042552
iteration 1225: loss: 60.873924, loss_kl: 0.062848, loss_recon: 0.607937, loss_pred: 0.054752
iteration 1226: loss: 61.438309, loss_kl: 0.173256, loss_recon: 0.613208, loss_pred: 0.047161
iteration 1227: loss: 61.922764, loss_kl: 0.080423, loss_recon: 0.618231, loss_pred: 0.067014
iteration 1228: loss: 60.352097, loss_kl: -0.047414, loss_recon: 0.602995, loss_pred: 0.071841
iteration 1229: loss: 60.141525, loss_kl: -0.108911, loss_recon: 0.601266, loss_pred: 0.059187
iteration 1230: loss: 60.227867, loss_kl: 0.083794, loss_recon: 0.601009, loss_pred: 0.092941
 62%|████████████████▌          | 123/200 [1:42:07<1:03:43, 49.66s/it]iteration 1231: loss: 59.894588, loss_kl: 0.247069, loss_recon: 0.597274, loss_pred: 0.057123
iteration 1232: loss: 61.468075, loss_kl: 0.305229, loss_recon: 0.612813, loss_pred: 0.050774
iteration 1233: loss: 60.534302, loss_kl: 0.280859, loss_recon: 0.603149, loss_pred: 0.094255
iteration 1234: loss: 61.166096, loss_kl: 0.125674, loss_recon: 0.610137, loss_pred: 0.096418
iteration 1235: loss: 60.717968, loss_kl: 0.093934, loss_recon: 0.606191, loss_pred: 0.057011
iteration 1236: loss: 61.556541, loss_kl: 0.331807, loss_recon: 0.613015, loss_pred: 0.107177
iteration 1237: loss: 61.241043, loss_kl: 0.252436, loss_recon: 0.610242, loss_pred: 0.104388
iteration 1238: loss: 60.282719, loss_kl: 0.382116, loss_recon: 0.600347, loss_pred: 0.077788
iteration 1239: loss: 61.736271, loss_kl: 0.201036, loss_recon: 0.615976, loss_pred: 0.049102
iteration 1240: loss: 61.446487, loss_kl: 0.219412, loss_recon: 0.612493, loss_pred: 0.099408
 62%|████████████████▋          | 124/200 [1:43:00<1:04:01, 50.55s/it]iteration 1241: loss: 62.061607, loss_kl: -0.001125, loss_recon: 0.620007, loss_pred: 0.061462
iteration 1242: loss: 60.583210, loss_kl: 0.117838, loss_recon: 0.604494, loss_pred: 0.076649
iteration 1243: loss: 60.602489, loss_kl: 0.208064, loss_recon: 0.604561, loss_pred: 0.045445
iteration 1244: loss: 60.988575, loss_kl: 0.174048, loss_recon: 0.607968, loss_pred: 0.107366
iteration 1245: loss: 60.347305, loss_kl: 0.030310, loss_recon: 0.602280, loss_pred: 0.104633
iteration 1246: loss: 62.091732, loss_kl: 0.176694, loss_recon: 0.618831, loss_pred: 0.122902
iteration 1247: loss: 61.576599, loss_kl: 0.213516, loss_recon: 0.613777, loss_pred: 0.095307
iteration 1248: loss: 61.057560, loss_kl: 0.168541, loss_recon: 0.608787, loss_pred: 0.097104
iteration 1249: loss: 61.267193, loss_kl: 0.292828, loss_recon: 0.609997, loss_pred: 0.125394
iteration 1250: loss: 60.260002, loss_kl: 0.178122, loss_recon: 0.600432, loss_pred: 0.130358
 62%|████████████████▉          | 125/200 [1:43:50<1:02:58, 50.38s/it]iteration 1251: loss: 60.809917, loss_kl: 0.061579, loss_recon: 0.607188, loss_pred: 0.058764
iteration 1252: loss: 60.243469, loss_kl: 0.331600, loss_recon: 0.599969, loss_pred: 0.072591
iteration 1253: loss: 61.591984, loss_kl: 0.102463, loss_recon: 0.614136, loss_pred: 0.124662
iteration 1254: loss: 62.222191, loss_kl: 0.177681, loss_recon: 0.619928, loss_pred: 0.136117
iteration 1255: loss: 60.568874, loss_kl: 0.166576, loss_recon: 0.604116, loss_pred: 0.069837
iteration 1256: loss: 61.345078, loss_kl: 0.140827, loss_recon: 0.611983, loss_pred: 0.072833
iteration 1257: loss: 61.402607, loss_kl: 0.297563, loss_recon: 0.611567, loss_pred: 0.089790
iteration 1258: loss: 60.916306, loss_kl: 0.027863, loss_recon: 0.607380, loss_pred: 0.163650
iteration 1259: loss: 60.979206, loss_kl: 0.069528, loss_recon: 0.608356, loss_pred: 0.107093
iteration 1260: loss: 60.841099, loss_kl: 0.255155, loss_recon: 0.606041, loss_pred: 0.103125
 63%|█████████████████          | 126/200 [1:44:40<1:01:52, 50.17s/it]iteration 1261: loss: 60.636440, loss_kl: 0.245853, loss_recon: 0.604023, loss_pred: 0.095342
iteration 1262: loss: 61.703053, loss_kl: 0.348897, loss_recon: 0.613427, loss_pred: 0.163459
iteration 1263: loss: 61.027519, loss_kl: 0.201667, loss_recon: 0.608092, loss_pred: 0.104451
iteration 1264: loss: 61.048115, loss_kl: 0.258736, loss_recon: 0.608258, loss_pred: 0.076250
iteration 1265: loss: 61.557625, loss_kl: 0.167258, loss_recon: 0.613431, loss_pred: 0.120129
iteration 1266: loss: 60.303127, loss_kl: 0.028909, loss_recon: 0.601704, loss_pred: 0.116364
iteration 1267: loss: 61.041351, loss_kl: 0.103667, loss_recon: 0.609004, loss_pred: 0.082446
iteration 1268: loss: 61.043648, loss_kl: 0.047947, loss_recon: 0.609510, loss_pred: 0.065562
iteration 1269: loss: 61.118946, loss_kl: 0.315020, loss_recon: 0.608560, loss_pred: 0.085156
iteration 1270: loss: 60.952454, loss_kl: 0.372050, loss_recon: 0.606732, loss_pred: 0.069225
 64%|█████████████████▏         | 127/200 [1:45:29<1:00:51, 50.02s/it]iteration 1271: loss: 62.064068, loss_kl: 0.452587, loss_recon: 0.617095, loss_pred: 0.081232
iteration 1272: loss: 61.235710, loss_kl: 0.232675, loss_recon: 0.610287, loss_pred: 0.066435
iteration 1273: loss: 62.281315, loss_kl: 0.251463, loss_recon: 0.620746, loss_pred: 0.054806
iteration 1274: loss: 61.673389, loss_kl: 0.208547, loss_recon: 0.614751, loss_pred: 0.072291
iteration 1275: loss: 59.203785, loss_kl: 0.174037, loss_recon: 0.589755, loss_pred: 0.123204
iteration 1276: loss: 60.546207, loss_kl: 0.139908, loss_recon: 0.604005, loss_pred: 0.061177
iteration 1277: loss: 61.252422, loss_kl: 0.320210, loss_recon: 0.609929, loss_pred: 0.066098
iteration 1278: loss: 60.442532, loss_kl: 0.277011, loss_recon: 0.602064, loss_pred: 0.068845
iteration 1279: loss: 61.154778, loss_kl: 0.239587, loss_recon: 0.609225, loss_pred: 0.087546
iteration 1280: loss: 60.814747, loss_kl: 0.218971, loss_recon: 0.606278, loss_pred: 0.054676
 64%|██████████████████▌          | 128/200 [1:46:19<59:49, 49.86s/it]iteration 1281: loss: 61.500996, loss_kl: 0.454460, loss_recon: 0.611470, loss_pred: 0.061474
iteration 1282: loss: 61.619564, loss_kl: 0.372706, loss_recon: 0.613148, loss_pred: 0.064931
iteration 1283: loss: 59.189911, loss_kl: 0.229440, loss_recon: 0.589166, loss_pred: 0.125633
iteration 1284: loss: 59.507900, loss_kl: 0.449549, loss_recon: 0.591180, loss_pred: 0.100532
iteration 1285: loss: 61.213902, loss_kl: 0.326000, loss_recon: 0.609373, loss_pred: 0.066805
iteration 1286: loss: 61.189804, loss_kl: 0.362799, loss_recon: 0.609041, loss_pred: 0.052227
iteration 1287: loss: 61.512486, loss_kl: 0.271685, loss_recon: 0.612836, loss_pred: 0.054010
iteration 1288: loss: 61.263077, loss_kl: 0.088091, loss_recon: 0.611587, loss_pred: 0.047713
iteration 1289: loss: 61.672363, loss_kl: 0.184595, loss_recon: 0.615098, loss_pred: 0.043758
iteration 1290: loss: 60.915783, loss_kl: 0.129474, loss_recon: 0.607832, loss_pred: 0.049281
 64%|██████████████████▋          | 129/200 [1:47:08<58:51, 49.74s/it]iteration 1291: loss: 61.268463, loss_kl: 0.197127, loss_recon: 0.610847, loss_pred: 0.049103
iteration 1292: loss: 61.126724, loss_kl: 0.296171, loss_recon: 0.608779, loss_pred: 0.046528
iteration 1293: loss: 61.181297, loss_kl: 0.378637, loss_recon: 0.608627, loss_pred: 0.059902
iteration 1294: loss: 60.396965, loss_kl: 0.278253, loss_recon: 0.601533, loss_pred: 0.053547
iteration 1295: loss: 61.066673, loss_kl: 0.094289, loss_recon: 0.609522, loss_pred: 0.050079
iteration 1296: loss: 61.892788, loss_kl: 0.152943, loss_recon: 0.617381, loss_pred: 0.050154
iteration 1297: loss: 60.504604, loss_kl: 0.054334, loss_recon: 0.604076, loss_pred: 0.059879
iteration 1298: loss: 60.010830, loss_kl: 0.279965, loss_recon: 0.597471, loss_pred: 0.072426
iteration 1299: loss: 60.934406, loss_kl: 0.261088, loss_recon: 0.606730, loss_pred: 0.083004
iteration 1300: loss: 60.451302, loss_kl: 0.244567, loss_recon: 0.602127, loss_pred: 0.071541
 65%|██████████████████▊          | 130/200 [1:47:58<58:00, 49.72s/it]iteration 1301: loss: 61.822060, loss_kl: 0.422537, loss_recon: 0.614721, loss_pred: 0.044593
iteration 1302: loss: 61.057919, loss_kl: 0.231555, loss_recon: 0.608111, loss_pred: 0.079467
iteration 1303: loss: 60.783554, loss_kl: 0.106012, loss_recon: 0.606146, loss_pred: 0.092319
iteration 1304: loss: 61.779221, loss_kl: 0.100551, loss_recon: 0.616657, loss_pred: 0.040810
iteration 1305: loss: 61.103294, loss_kl: 0.148079, loss_recon: 0.609289, loss_pred: 0.067391
iteration 1306: loss: 60.707630, loss_kl: 0.129681, loss_recon: 0.605530, loss_pred: 0.060858
iteration 1307: loss: 59.657398, loss_kl: 0.249656, loss_recon: 0.594009, loss_pred: 0.076051
iteration 1308: loss: 60.684063, loss_kl: 0.245799, loss_recon: 0.604469, loss_pred: 0.059501
iteration 1309: loss: 61.200813, loss_kl: 0.316465, loss_recon: 0.608980, loss_pred: 0.074074
iteration 1310: loss: 59.647530, loss_kl: 0.150927, loss_recon: 0.594656, loss_pred: 0.072887
 66%|██████████████████▉          | 131/200 [1:48:50<58:01, 50.45s/it]iteration 1311: loss: 60.785706, loss_kl: 0.253236, loss_recon: 0.604989, loss_pred: 0.093766
iteration 1312: loss: 60.645454, loss_kl: 0.220318, loss_recon: 0.604060, loss_pred: 0.071447
iteration 1313: loss: 61.748714, loss_kl: 0.198724, loss_recon: 0.615462, loss_pred: 0.051030
iteration 1314: loss: 61.327415, loss_kl: 0.337968, loss_recon: 0.610171, loss_pred: 0.052694
iteration 1315: loss: 60.028473, loss_kl: 0.237860, loss_recon: 0.597956, loss_pred: 0.051559
iteration 1316: loss: 60.572765, loss_kl: 0.292846, loss_recon: 0.602883, loss_pred: 0.061154
iteration 1317: loss: 62.336620, loss_kl: 0.252484, loss_recon: 0.620986, loss_pred: 0.045485
iteration 1318: loss: 61.140366, loss_kl: 0.219594, loss_recon: 0.609269, loss_pred: 0.046020
iteration 1319: loss: 60.871567, loss_kl: 0.295572, loss_recon: 0.605780, loss_pred: 0.068250
iteration 1320: loss: 59.456184, loss_kl: 0.264463, loss_recon: 0.591840, loss_pred: 0.070579
 66%|███████████████████▏         | 132/200 [1:49:39<56:45, 50.08s/it]iteration 1321: loss: 61.468513, loss_kl: 0.193206, loss_recon: 0.612646, loss_pred: 0.048953
iteration 1322: loss: 60.605389, loss_kl: 0.316855, loss_recon: 0.602979, loss_pred: 0.053377
iteration 1323: loss: 60.336956, loss_kl: 0.186580, loss_recon: 0.601279, loss_pred: 0.059428
iteration 1324: loss: 62.066383, loss_kl: 0.340079, loss_recon: 0.617520, loss_pred: 0.041658
iteration 1325: loss: 60.657581, loss_kl: 0.267896, loss_recon: 0.603555, loss_pred: 0.087237
iteration 1326: loss: 60.512325, loss_kl: 0.139140, loss_recon: 0.603587, loss_pred: 0.041996
iteration 1327: loss: 61.699783, loss_kl: 0.244914, loss_recon: 0.614516, loss_pred: 0.051765
iteration 1328: loss: 60.939461, loss_kl: 0.026762, loss_recon: 0.608566, loss_pred: 0.061383
iteration 1329: loss: 59.845715, loss_kl: 0.167745, loss_recon: 0.596465, loss_pred: 0.064694
iteration 1330: loss: 60.437065, loss_kl: -0.051277, loss_recon: 0.603988, loss_pred: 0.079395
 66%|███████████████████▎         | 133/200 [1:50:29<55:44, 49.91s/it]iteration 1331: loss: 61.260925, loss_kl: 0.119595, loss_recon: 0.611120, loss_pred: 0.048314
iteration 1332: loss: 61.885590, loss_kl: 0.327493, loss_recon: 0.615416, loss_pred: 0.068348
iteration 1333: loss: 61.100857, loss_kl: 0.285200, loss_recon: 0.608121, loss_pred: 0.048708
iteration 1334: loss: 60.673252, loss_kl: 0.171401, loss_recon: 0.604828, loss_pred: 0.046155
iteration 1335: loss: 61.779819, loss_kl: 0.343389, loss_recon: 0.614325, loss_pred: 0.058299
iteration 1336: loss: 60.818825, loss_kl: 0.169710, loss_recon: 0.606319, loss_pred: 0.044066
iteration 1337: loss: 61.880829, loss_kl: 0.249991, loss_recon: 0.616170, loss_pred: 0.053412
iteration 1338: loss: 60.179707, loss_kl: 0.037604, loss_recon: 0.600858, loss_pred: 0.062235
iteration 1339: loss: 59.862614, loss_kl: 0.220369, loss_recon: 0.596042, loss_pred: 0.072981
iteration 1340: loss: 60.180286, loss_kl: 0.355804, loss_recon: 0.597972, loss_pred: 0.083673
 67%|███████████████████▍         | 134/200 [1:51:18<54:47, 49.81s/it]iteration 1341: loss: 60.991840, loss_kl: 0.105855, loss_recon: 0.608635, loss_pred: 0.035011
iteration 1342: loss: 62.873405, loss_kl: 0.297903, loss_recon: 0.625026, loss_pred: 0.108246
iteration 1343: loss: 61.297573, loss_kl: 0.265302, loss_recon: 0.609976, loss_pred: 0.066238
iteration 1344: loss: 60.754471, loss_kl: 0.251435, loss_recon: 0.604629, loss_pred: 0.070056
iteration 1345: loss: 60.591545, loss_kl: 0.074693, loss_recon: 0.604397, loss_pred: 0.086011
iteration 1346: loss: 59.979530, loss_kl: 0.077486, loss_recon: 0.598261, loss_pred: 0.085150
iteration 1347: loss: 60.254513, loss_kl: 0.112460, loss_recon: 0.600626, loss_pred: 0.092792
iteration 1348: loss: 60.779255, loss_kl: 0.134572, loss_recon: 0.605927, loss_pred: 0.067955
iteration 1349: loss: 61.477573, loss_kl: 0.288209, loss_recon: 0.611795, loss_pred: 0.044137
iteration 1350: loss: 60.343586, loss_kl: 0.458175, loss_recon: 0.598585, loss_pred: 0.081317
 68%|███████████████████▌         | 135/200 [1:52:08<53:56, 49.79s/it]iteration 1351: loss: 60.822891, loss_kl: 0.185755, loss_recon: 0.605956, loss_pred: 0.056290
iteration 1352: loss: 61.390579, loss_kl: 0.362063, loss_recon: 0.610034, loss_pred: 0.053751
iteration 1353: loss: 61.090958, loss_kl: 0.046878, loss_recon: 0.609798, loss_pred: 0.067971
iteration 1354: loss: 61.344540, loss_kl: 0.261377, loss_recon: 0.610499, loss_pred: 0.053957
iteration 1355: loss: 62.179150, loss_kl: 0.278717, loss_recon: 0.618854, loss_pred: 0.037117
iteration 1356: loss: 60.721123, loss_kl: 0.168992, loss_recon: 0.604760, loss_pred: 0.089515
iteration 1357: loss: 60.428562, loss_kl: 0.257587, loss_recon: 0.601002, loss_pred: 0.091167
iteration 1358: loss: 60.681076, loss_kl: 0.261179, loss_recon: 0.603916, loss_pred: 0.049032
iteration 1359: loss: 60.607464, loss_kl: 0.224991, loss_recon: 0.603426, loss_pred: 0.057706
iteration 1360: loss: 59.990837, loss_kl: 0.132485, loss_recon: 0.597827, loss_pred: 0.086105
 68%|███████████████████▋         | 136/200 [1:52:58<53:03, 49.74s/it]iteration 1361: loss: 60.222874, loss_kl: 0.312076, loss_recon: 0.598364, loss_pred: 0.086805
iteration 1362: loss: 60.891846, loss_kl: 0.252708, loss_recon: 0.606027, loss_pred: 0.046469
iteration 1363: loss: 59.883213, loss_kl: 0.220669, loss_recon: 0.595989, loss_pred: 0.072408
iteration 1364: loss: 60.049240, loss_kl: 0.207561, loss_recon: 0.597825, loss_pred: 0.067388
iteration 1365: loss: 61.650864, loss_kl: 0.154669, loss_recon: 0.614374, loss_pred: 0.064937
iteration 1366: loss: 61.399544, loss_kl: 0.174651, loss_recon: 0.611519, loss_pred: 0.079935
iteration 1367: loss: 59.823597, loss_kl: 0.300710, loss_recon: 0.594609, loss_pred: 0.073898
iteration 1368: loss: 60.976410, loss_kl: 0.154446, loss_recon: 0.607524, loss_pred: 0.075651
iteration 1369: loss: 62.170265, loss_kl: 0.202855, loss_recon: 0.619298, loss_pred: 0.045664
iteration 1370: loss: 62.348545, loss_kl: 0.292644, loss_recon: 0.620205, loss_pred: 0.046978
 68%|███████████████████▊         | 137/200 [1:53:49<52:45, 50.25s/it]iteration 1371: loss: 60.546246, loss_kl: 0.197018, loss_recon: 0.602771, loss_pred: 0.072086
iteration 1372: loss: 61.835022, loss_kl: 0.229406, loss_recon: 0.615564, loss_pred: 0.049220
iteration 1373: loss: 60.696571, loss_kl: 0.172506, loss_recon: 0.604713, loss_pred: 0.052790
iteration 1374: loss: 60.638950, loss_kl: 0.214844, loss_recon: 0.603797, loss_pred: 0.044418
iteration 1375: loss: 62.482998, loss_kl: 0.277934, loss_recon: 0.621289, loss_pred: 0.076181
iteration 1376: loss: 61.562050, loss_kl: 0.182635, loss_recon: 0.613405, loss_pred: 0.038932
iteration 1377: loss: 60.389538, loss_kl: 0.230034, loss_recon: 0.601095, loss_pred: 0.049953
iteration 1378: loss: 60.580841, loss_kl: 0.360978, loss_recon: 0.601473, loss_pred: 0.072585
iteration 1379: loss: 59.508595, loss_kl: 0.125363, loss_recon: 0.593158, loss_pred: 0.067433
iteration 1380: loss: 61.454605, loss_kl: 0.268925, loss_recon: 0.611265, loss_pred: 0.059193
 69%|████████████████████         | 138/200 [1:54:40<52:02, 50.36s/it]iteration 1381: loss: 61.920341, loss_kl: 0.325769, loss_recon: 0.615436, loss_pred: 0.050970
iteration 1382: loss: 59.789574, loss_kl: 0.328488, loss_recon: 0.593697, loss_pred: 0.091406
iteration 1383: loss: 61.638645, loss_kl: 0.128602, loss_recon: 0.614680, loss_pred: 0.042028
iteration 1384: loss: 59.853573, loss_kl: -0.014420, loss_recon: 0.597791, loss_pred: 0.088870
iteration 1385: loss: 60.798779, loss_kl: 0.343474, loss_recon: 0.603755, loss_pred: 0.079844
iteration 1386: loss: 61.244350, loss_kl: 0.231384, loss_recon: 0.609312, loss_pred: 0.081815
iteration 1387: loss: 61.098766, loss_kl: 0.224185, loss_recon: 0.608232, loss_pred: 0.051362
iteration 1388: loss: 60.636768, loss_kl: 0.478496, loss_recon: 0.601051, loss_pred: 0.053174
iteration 1389: loss: 61.726292, loss_kl: 0.471704, loss_recon: 0.611646, loss_pred: 0.090010
iteration 1390: loss: 60.762447, loss_kl: 0.213821, loss_recon: 0.604787, loss_pred: 0.069921
 70%|████████████████████▏        | 139/200 [1:55:29<50:56, 50.10s/it]iteration 1391: loss: 60.660339, loss_kl: 0.098864, loss_recon: 0.605094, loss_pred: 0.052038
iteration 1392: loss: 61.026951, loss_kl: 0.373168, loss_recon: 0.606026, loss_pred: 0.051211
iteration 1393: loss: 58.880302, loss_kl: 0.006390, loss_recon: 0.588032, loss_pred: 0.070693
iteration 1394: loss: 60.281479, loss_kl: 0.377277, loss_recon: 0.598287, loss_pred: 0.075520
iteration 1395: loss: 62.687931, loss_kl: 0.188224, loss_recon: 0.623978, loss_pred: 0.101928
iteration 1396: loss: 61.707012, loss_kl: 0.471190, loss_recon: 0.611567, loss_pred: 0.079156
iteration 1397: loss: 60.886673, loss_kl: 0.319612, loss_recon: 0.604605, loss_pred: 0.106529
iteration 1398: loss: 61.307720, loss_kl: 0.440950, loss_recon: 0.607863, loss_pred: 0.080502
iteration 1399: loss: 62.543358, loss_kl: 0.326254, loss_recon: 0.621410, loss_pred: 0.076072
iteration 1400: loss: 61.205486, loss_kl: 0.170029, loss_recon: 0.609260, loss_pred: 0.109455
 70%|████████████████████▎        | 140/200 [1:56:19<49:58, 49.98s/it]iteration 1401: loss: 60.942631, loss_kl: 0.142529, loss_recon: 0.607199, loss_pred: 0.080220
iteration 1402: loss: 61.956573, loss_kl: 0.222677, loss_recon: 0.616754, loss_pred: 0.058532
iteration 1403: loss: 60.392212, loss_kl: 0.441411, loss_recon: 0.598793, loss_pred: 0.071456
iteration 1404: loss: 61.548374, loss_kl: 0.243784, loss_recon: 0.612587, loss_pred: 0.045883
iteration 1405: loss: 61.720047, loss_kl: 0.114423, loss_recon: 0.615441, loss_pred: 0.061493
iteration 1406: loss: 59.663464, loss_kl: 0.469720, loss_recon: 0.590958, loss_pred: 0.097898
iteration 1407: loss: 61.690811, loss_kl: 0.251593, loss_recon: 0.613429, loss_pred: 0.096340
iteration 1408: loss: 61.328079, loss_kl: 0.072262, loss_recon: 0.611880, loss_pred: 0.067787
iteration 1409: loss: 60.800606, loss_kl: 0.294218, loss_recon: 0.604027, loss_pred: 0.103703
iteration 1410: loss: 60.865257, loss_kl: 0.281221, loss_recon: 0.604391, loss_pred: 0.144972
 70%|████████████████████▍        | 141/200 [1:57:10<49:25, 50.26s/it]iteration 1411: loss: 61.584286, loss_kl: 0.292134, loss_recon: 0.612200, loss_pred: 0.072191
iteration 1412: loss: 60.928131, loss_kl: 0.209970, loss_recon: 0.606297, loss_pred: 0.088491
iteration 1413: loss: 59.708839, loss_kl: 0.127968, loss_recon: 0.594960, loss_pred: 0.084874
iteration 1414: loss: 61.393940, loss_kl: 0.162745, loss_recon: 0.611605, loss_pred: 0.070737
iteration 1415: loss: 60.741440, loss_kl: 0.401869, loss_recon: 0.602637, loss_pred: 0.075850
iteration 1416: loss: 61.374676, loss_kl: 0.299758, loss_recon: 0.609970, loss_pred: 0.077878
iteration 1417: loss: 59.877354, loss_kl: 0.219601, loss_recon: 0.595796, loss_pred: 0.078135
iteration 1418: loss: 61.257965, loss_kl: 0.523435, loss_recon: 0.606861, loss_pred: 0.048411
iteration 1419: loss: 62.577126, loss_kl: 0.330191, loss_recon: 0.622000, loss_pred: 0.046891
iteration 1420: loss: 61.675766, loss_kl: 0.112432, loss_recon: 0.615069, loss_pred: 0.056398
 71%|████████████████████▌        | 142/200 [1:57:59<48:19, 50.00s/it]iteration 1421: loss: 61.550240, loss_kl: 0.188836, loss_recon: 0.612776, loss_pred: 0.083789
iteration 1422: loss: 61.094505, loss_kl: 0.139568, loss_recon: 0.608882, loss_pred: 0.066743
iteration 1423: loss: 59.459942, loss_kl: 0.168921, loss_recon: 0.591476, loss_pred: 0.143388
iteration 1424: loss: 60.131321, loss_kl: 0.228695, loss_recon: 0.598322, loss_pred: 0.070416
iteration 1425: loss: 61.802204, loss_kl: 0.278312, loss_recon: 0.614253, loss_pred: 0.098576
iteration 1426: loss: 61.015697, loss_kl: 0.243565, loss_recon: 0.606610, loss_pred: 0.111176
iteration 1427: loss: 62.595284, loss_kl: 0.334142, loss_recon: 0.621928, loss_pred: 0.068374
iteration 1428: loss: 60.640530, loss_kl: 0.296476, loss_recon: 0.602287, loss_pred: 0.115370
iteration 1429: loss: 60.791473, loss_kl: 0.140629, loss_recon: 0.605508, loss_pred: 0.100000
iteration 1430: loss: 60.733173, loss_kl: 0.255137, loss_recon: 0.603601, loss_pred: 0.117909
 72%|████████████████████▋        | 143/200 [1:58:49<47:25, 49.91s/it]iteration 1431: loss: 60.939632, loss_kl: 0.257947, loss_recon: 0.606128, loss_pred: 0.068880
iteration 1432: loss: 60.924484, loss_kl: 0.196172, loss_recon: 0.606689, loss_pred: 0.059430
iteration 1433: loss: 60.056698, loss_kl: 0.267323, loss_recon: 0.597277, loss_pred: 0.061651
iteration 1434: loss: 62.379845, loss_kl: 0.280926, loss_recon: 0.620221, loss_pred: 0.076784
iteration 1435: loss: 62.043159, loss_kl: 0.335584, loss_recon: 0.616386, loss_pred: 0.068995
iteration 1436: loss: 60.583378, loss_kl: 0.196018, loss_recon: 0.602617, loss_pred: 0.125664
iteration 1437: loss: 59.384224, loss_kl: 0.074706, loss_recon: 0.592034, loss_pred: 0.106149
iteration 1438: loss: 60.414425, loss_kl: 0.294434, loss_recon: 0.600566, loss_pred: 0.063430
iteration 1439: loss: 62.244614, loss_kl: 0.199072, loss_recon: 0.619854, loss_pred: 0.060120
iteration 1440: loss: 60.834061, loss_kl: 0.207025, loss_recon: 0.605665, loss_pred: 0.060538
 72%|████████████████████▉        | 144/200 [1:59:38<46:26, 49.76s/it]iteration 1441: loss: 60.900528, loss_kl: 0.200391, loss_recon: 0.606330, loss_pred: 0.067122
iteration 1442: loss: 62.768547, loss_kl: 0.385330, loss_recon: 0.623299, loss_pred: 0.053345
iteration 1443: loss: 59.997341, loss_kl: 0.138535, loss_recon: 0.597707, loss_pred: 0.088105
iteration 1444: loss: 60.710712, loss_kl: 0.198358, loss_recon: 0.604289, loss_pred: 0.083463
iteration 1445: loss: 59.986794, loss_kl: 0.311359, loss_recon: 0.596026, loss_pred: 0.072846
iteration 1446: loss: 61.178780, loss_kl: 0.458327, loss_recon: 0.606588, loss_pred: 0.061657
iteration 1447: loss: 60.305420, loss_kl: 0.147499, loss_recon: 0.600849, loss_pred: 0.072975
iteration 1448: loss: 61.211338, loss_kl: 0.198417, loss_recon: 0.609619, loss_pred: 0.051015
iteration 1449: loss: 61.540550, loss_kl: 0.364195, loss_recon: 0.611090, loss_pred: 0.067395
iteration 1450: loss: 60.938248, loss_kl: 0.228704, loss_recon: 0.606526, loss_pred: 0.056930
 72%|█████████████████████        | 145/200 [2:00:28<45:36, 49.76s/it]iteration 1451: loss: 61.607433, loss_kl: 0.149418, loss_recon: 0.614206, loss_pred: 0.037397
iteration 1452: loss: 61.240555, loss_kl: 0.272186, loss_recon: 0.608953, loss_pred: 0.073044
iteration 1453: loss: 60.841980, loss_kl: 0.056916, loss_recon: 0.607119, loss_pred: 0.073210
iteration 1454: loss: 61.229378, loss_kl: 0.398474, loss_recon: 0.607431, loss_pred: 0.087772
iteration 1455: loss: 61.407799, loss_kl: 0.358205, loss_recon: 0.609930, loss_pred: 0.056567
iteration 1456: loss: 61.348370, loss_kl: 0.048066, loss_recon: 0.612181, loss_pred: 0.082161
iteration 1457: loss: 59.593155, loss_kl: 0.073436, loss_recon: 0.594290, loss_pred: 0.090669
iteration 1458: loss: 60.779011, loss_kl: 0.160864, loss_recon: 0.605761, loss_pred: 0.042027
iteration 1459: loss: 60.595284, loss_kl: 0.498988, loss_recon: 0.600347, loss_pred: 0.061587
iteration 1460: loss: 61.053154, loss_kl: 0.142679, loss_recon: 0.608647, loss_pred: 0.045730
 73%|█████████████████████▏       | 146/200 [2:01:17<44:41, 49.66s/it]iteration 1461: loss: 61.709019, loss_kl: 0.172509, loss_recon: 0.614722, loss_pred: 0.064295
iteration 1462: loss: 61.240463, loss_kl: 0.320219, loss_recon: 0.608553, loss_pred: 0.064995
iteration 1463: loss: 60.896690, loss_kl: 0.086530, loss_recon: 0.607598, loss_pred: 0.050324
iteration 1464: loss: 60.425743, loss_kl: 0.402318, loss_recon: 0.599579, loss_pred: 0.065550
iteration 1465: loss: 61.133129, loss_kl: 0.114061, loss_recon: 0.609447, loss_pred: 0.074398
iteration 1466: loss: 61.989487, loss_kl: 0.514766, loss_recon: 0.614106, loss_pred: 0.064156
iteration 1467: loss: 61.264545, loss_kl: 0.285860, loss_recon: 0.609091, loss_pred: 0.069564
iteration 1468: loss: 59.600838, loss_kl: 0.398094, loss_recon: 0.591006, loss_pred: 0.102146
iteration 1469: loss: 60.610126, loss_kl: 0.342818, loss_recon: 0.602137, loss_pred: 0.053646
iteration 1470: loss: 60.984486, loss_kl: 0.187588, loss_recon: 0.607267, loss_pred: 0.070233
 74%|█████████████████████▎       | 147/200 [2:02:07<43:52, 49.67s/it]iteration 1471: loss: 60.015373, loss_kl: 0.189318, loss_recon: 0.597276, loss_pred: 0.098471
iteration 1472: loss: 60.117558, loss_kl: 0.388602, loss_recon: 0.596714, loss_pred: 0.057553
iteration 1473: loss: 60.758633, loss_kl: 0.338864, loss_recon: 0.603763, loss_pred: 0.043423
iteration 1474: loss: 61.910908, loss_kl: 0.267088, loss_recon: 0.615994, loss_pred: 0.044379
iteration 1475: loss: 61.655876, loss_kl: 0.255233, loss_recon: 0.613438, loss_pred: 0.056841
iteration 1476: loss: 61.402821, loss_kl: 0.090549, loss_recon: 0.612525, loss_pred: 0.059779
iteration 1477: loss: 61.203999, loss_kl: 0.148723, loss_recon: 0.609966, loss_pred: 0.058722
iteration 1478: loss: 60.783134, loss_kl: 0.358847, loss_recon: 0.603575, loss_pred: 0.066828
iteration 1479: loss: 60.114559, loss_kl: -0.019220, loss_recon: 0.600792, loss_pred: 0.054602
iteration 1480: loss: 61.256485, loss_kl: 0.372887, loss_recon: 0.608337, loss_pred: 0.049944
 74%|█████████████████████▍       | 148/200 [2:02:58<43:14, 49.90s/it]iteration 1481: loss: 61.067055, loss_kl: 0.168191, loss_recon: 0.608463, loss_pred: 0.052582
iteration 1482: loss: 61.321991, loss_kl: 0.316748, loss_recon: 0.609501, loss_pred: 0.055147
iteration 1483: loss: 61.944992, loss_kl: 0.194498, loss_recon: 0.616956, loss_pred: 0.054928
iteration 1484: loss: 61.144379, loss_kl: 0.629939, loss_recon: 0.604614, loss_pred: 0.053032
iteration 1485: loss: 61.735157, loss_kl: 0.209451, loss_recon: 0.614706, loss_pred: 0.055074
iteration 1486: loss: 60.989979, loss_kl: 0.512891, loss_recon: 0.603969, loss_pred: 0.080228
iteration 1487: loss: 61.054424, loss_kl: 0.363547, loss_recon: 0.606440, loss_pred: 0.046926
iteration 1488: loss: 61.366989, loss_kl: 0.472781, loss_recon: 0.608556, loss_pred: 0.038650
iteration 1489: loss: 59.052967, loss_kl: 0.169698, loss_recon: 0.588313, loss_pred: 0.051931
iteration 1490: loss: 59.977806, loss_kl: 0.137091, loss_recon: 0.597999, loss_pred: 0.040779
 74%|█████████████████████▌       | 149/200 [2:03:47<42:17, 49.76s/it]iteration 1491: loss: 59.895046, loss_kl: 0.323896, loss_recon: 0.595155, loss_pred: 0.055683
iteration 1492: loss: 61.467495, loss_kl: 0.206725, loss_recon: 0.612077, loss_pred: 0.053022
iteration 1493: loss: 60.879681, loss_kl: 0.018643, loss_recon: 0.608045, loss_pred: 0.056535
iteration 1494: loss: 61.798088, loss_kl: 0.270178, loss_recon: 0.614621, loss_pred: 0.065791
iteration 1495: loss: 61.681660, loss_kl: 0.219697, loss_recon: 0.613929, loss_pred: 0.069097
iteration 1496: loss: 60.332829, loss_kl: 0.357267, loss_recon: 0.598961, loss_pred: 0.079463
iteration 1497: loss: 61.796547, loss_kl: 0.215853, loss_recon: 0.615272, loss_pred: 0.053492
iteration 1498: loss: 59.729362, loss_kl: 0.298127, loss_recon: 0.593032, loss_pred: 0.128070
iteration 1499: loss: 60.881863, loss_kl: 0.123026, loss_recon: 0.606921, loss_pred: 0.066775
iteration 1500: loss: 61.089481, loss_kl: 0.172789, loss_recon: 0.608469, loss_pred: 0.069745
 75%|█████████████████████▊       | 150/200 [2:04:37<41:24, 49.68s/it]iteration 1501: loss: 59.269974, loss_kl: 0.250525, loss_recon: 0.591990, loss_pred: 0.068442
iteration 1502: loss: 60.633686, loss_kl: 0.220180, loss_recon: 0.605555, loss_pred: 0.075983
iteration 1503: loss: 60.411346, loss_kl: 0.181273, loss_recon: 0.603599, loss_pred: 0.049625
iteration 1504: loss: 61.107979, loss_kl: 0.103645, loss_recon: 0.610321, loss_pred: 0.074859
iteration 1505: loss: 60.878696, loss_kl: 0.123910, loss_recon: 0.608157, loss_pred: 0.061710
iteration 1506: loss: 61.318802, loss_kl: 0.381357, loss_recon: 0.612434, loss_pred: 0.071560
iteration 1507: loss: 60.449696, loss_kl: 0.396800, loss_recon: 0.603999, loss_pred: 0.045869
iteration 1508: loss: 61.373943, loss_kl: 0.396314, loss_recon: 0.613147, loss_pred: 0.055284
iteration 1509: loss: 60.892220, loss_kl: 0.586709, loss_recon: 0.608487, loss_pred: 0.037617
iteration 1510: loss: 60.624062, loss_kl: 0.318997, loss_recon: 0.605722, loss_pred: 0.048674
 76%|█████████████████████▉       | 151/200 [2:05:26<40:24, 49.48s/it]iteration 1511: loss: 60.076843, loss_kl: 0.638083, loss_recon: 0.600207, loss_pred: 0.049762
iteration 1512: loss: 60.192272, loss_kl: 0.313132, loss_recon: 0.601165, loss_pred: 0.072627
iteration 1513: loss: 60.383945, loss_kl: 0.528709, loss_recon: 0.603041, loss_pred: 0.074536
iteration 1514: loss: 59.881489, loss_kl: 0.780216, loss_recon: 0.598314, loss_pred: 0.042289
iteration 1515: loss: 59.969387, loss_kl: 0.564357, loss_recon: 0.599038, loss_pred: 0.059932
iteration 1516: loss: 61.436470, loss_kl: 0.957052, loss_recon: 0.613879, loss_pred: 0.039027
iteration 1517: loss: 61.689228, loss_kl: 0.791655, loss_recon: 0.616407, loss_pred: 0.040625
iteration 1518: loss: 61.162708, loss_kl: 0.679140, loss_recon: 0.610957, loss_pred: 0.060263
iteration 1519: loss: 60.743229, loss_kl: 1.295174, loss_recon: 0.606306, loss_pred: 0.099635
iteration 1520: loss: 61.554115, loss_kl: 0.815243, loss_recon: 0.615036, loss_pred: 0.042387
 76%|██████████████████████       | 152/200 [2:06:15<39:28, 49.35s/it]iteration 1521: loss: 61.482265, loss_kl: 0.894315, loss_recon: 0.614300, loss_pred: 0.043311
iteration 1522: loss: 61.051140, loss_kl: 1.115725, loss_recon: 0.609854, loss_pred: 0.054574
iteration 1523: loss: 61.192188, loss_kl: 0.661808, loss_recon: 0.611393, loss_pred: 0.046237
iteration 1524: loss: 60.449810, loss_kl: 0.877806, loss_recon: 0.603731, loss_pred: 0.067947
iteration 1525: loss: 60.691448, loss_kl: 0.925796, loss_recon: 0.606273, loss_pred: 0.054940
iteration 1526: loss: 61.254814, loss_kl: 0.792015, loss_recon: 0.611954, loss_pred: 0.051474
iteration 1527: loss: 61.293819, loss_kl: 1.154305, loss_recon: 0.612366, loss_pred: 0.045714
iteration 1528: loss: 60.650768, loss_kl: 0.370212, loss_recon: 0.605965, loss_pred: 0.050527
iteration 1529: loss: 58.884697, loss_kl: 0.744594, loss_recon: 0.587934, loss_pred: 0.083864
iteration 1530: loss: 59.947796, loss_kl: 0.804250, loss_recon: 0.598991, loss_pred: 0.040662
 76%|██████████████████████▏      | 153/200 [2:07:04<38:36, 49.29s/it]iteration 1531: loss: 59.136189, loss_kl: 1.042592, loss_recon: 0.590667, loss_pred: 0.059074
iteration 1532: loss: 60.774796, loss_kl: 0.930681, loss_recon: 0.607247, loss_pred: 0.040746
iteration 1533: loss: 60.385212, loss_kl: 0.983929, loss_recon: 0.603308, loss_pred: 0.044541
iteration 1534: loss: 61.491768, loss_kl: 0.958647, loss_recon: 0.614507, loss_pred: 0.031434
iteration 1535: loss: 60.738338, loss_kl: 0.839478, loss_recon: 0.606965, loss_pred: 0.033424
iteration 1536: loss: 61.671673, loss_kl: 0.619315, loss_recon: 0.616268, loss_pred: 0.038677
iteration 1537: loss: 61.725376, loss_kl: 1.036063, loss_recon: 0.616554, loss_pred: 0.059584
iteration 1538: loss: 60.544579, loss_kl: 0.509699, loss_recon: 0.605074, loss_pred: 0.032078
iteration 1539: loss: 59.194992, loss_kl: 0.698584, loss_recon: 0.591339, loss_pred: 0.054108
iteration 1540: loss: 60.901711, loss_kl: 0.618783, loss_recon: 0.608418, loss_pred: 0.053761
 77%|██████████████████████▎      | 154/200 [2:07:53<37:41, 49.15s/it]iteration 1541: loss: 62.401443, loss_kl: 0.623826, loss_recon: 0.623480, loss_pred: 0.047211
iteration 1542: loss: 60.306480, loss_kl: 0.681246, loss_recon: 0.602615, loss_pred: 0.038160
iteration 1543: loss: 59.174099, loss_kl: 0.413640, loss_recon: 0.591383, loss_pred: 0.031684
iteration 1544: loss: 61.292614, loss_kl: 0.560614, loss_recon: 0.612368, loss_pred: 0.050213
iteration 1545: loss: 60.490326, loss_kl: 0.392633, loss_recon: 0.604295, loss_pred: 0.056944
iteration 1546: loss: 59.892788, loss_kl: 0.493245, loss_recon: 0.598164, loss_pred: 0.071463
iteration 1547: loss: 60.488731, loss_kl: 0.539604, loss_recon: 0.604175, loss_pred: 0.065788
iteration 1548: loss: 61.304684, loss_kl: 0.677777, loss_recon: 0.612552, loss_pred: 0.042663
iteration 1549: loss: 60.474552, loss_kl: 0.588048, loss_recon: 0.604222, loss_pred: 0.046432
iteration 1550: loss: 61.010723, loss_kl: 0.568138, loss_recon: 0.609680, loss_pred: 0.037066
 78%|██████████████████████▍      | 155/200 [2:08:42<36:48, 49.08s/it]iteration 1551: loss: 60.790237, loss_kl: 0.580739, loss_recon: 0.607454, loss_pred: 0.039056
iteration 1552: loss: 60.113914, loss_kl: 0.356446, loss_recon: 0.600606, loss_pred: 0.049764
iteration 1553: loss: 60.552460, loss_kl: 0.431235, loss_recon: 0.605052, loss_pred: 0.042994
iteration 1554: loss: 60.894741, loss_kl: 0.486577, loss_recon: 0.608407, loss_pred: 0.049158
iteration 1555: loss: 60.207397, loss_kl: 0.547991, loss_recon: 0.601668, loss_pred: 0.035130
iteration 1556: loss: 60.624809, loss_kl: 0.605354, loss_recon: 0.605665, loss_pred: 0.052277
iteration 1557: loss: 61.834133, loss_kl: 0.604012, loss_recon: 0.617916, loss_pred: 0.036517
iteration 1558: loss: 60.534359, loss_kl: 0.605335, loss_recon: 0.604723, loss_pred: 0.056030
iteration 1559: loss: 60.751564, loss_kl: 0.505098, loss_recon: 0.606686, loss_pred: 0.077888
iteration 1560: loss: 60.615421, loss_kl: 0.488947, loss_recon: 0.605731, loss_pred: 0.037479
 78%|██████████████████████▌      | 156/200 [2:09:31<36:01, 49.13s/it]iteration 1561: loss: 61.387016, loss_kl: 0.192312, loss_recon: 0.613108, loss_pred: 0.074262
iteration 1562: loss: 61.129276, loss_kl: 0.477742, loss_recon: 0.610420, loss_pred: 0.082521
iteration 1563: loss: 60.284595, loss_kl: 0.071667, loss_recon: 0.602491, loss_pred: 0.034793
iteration 1564: loss: 60.323017, loss_kl: 0.331085, loss_recon: 0.602692, loss_pred: 0.050552
iteration 1565: loss: 60.900448, loss_kl: 0.574379, loss_recon: 0.608494, loss_pred: 0.045294
iteration 1566: loss: 61.348293, loss_kl: 0.296213, loss_recon: 0.612987, loss_pred: 0.046631
iteration 1567: loss: 59.401550, loss_kl: 0.494666, loss_recon: 0.593492, loss_pred: 0.047439
iteration 1568: loss: 58.957920, loss_kl: 0.464317, loss_recon: 0.589072, loss_pred: 0.046091
iteration 1569: loss: 61.297298, loss_kl: 0.570486, loss_recon: 0.612122, loss_pred: 0.079350
iteration 1570: loss: 61.984470, loss_kl: 0.527042, loss_recon: 0.619273, loss_pred: 0.051935
 78%|██████████████████████▊      | 157/200 [2:10:21<35:22, 49.36s/it]iteration 1571: loss: 60.919609, loss_kl: 0.440098, loss_recon: 0.608746, loss_pred: 0.040657
iteration 1572: loss: 61.405121, loss_kl: 0.305614, loss_recon: 0.613454, loss_pred: 0.056704
iteration 1573: loss: 60.359993, loss_kl: 0.432745, loss_recon: 0.603086, loss_pred: 0.047040
iteration 1574: loss: 60.584705, loss_kl: 0.381116, loss_recon: 0.605459, loss_pred: 0.034983
iteration 1575: loss: 60.421093, loss_kl: 0.361171, loss_recon: 0.603609, loss_pred: 0.056566
iteration 1576: loss: 59.514828, loss_kl: 0.454895, loss_recon: 0.594652, loss_pred: 0.045057
iteration 1577: loss: 61.808544, loss_kl: 0.191185, loss_recon: 0.617708, loss_pred: 0.035793
iteration 1578: loss: 60.086224, loss_kl: 0.149704, loss_recon: 0.600443, loss_pred: 0.040449
iteration 1579: loss: 61.268978, loss_kl: 0.177709, loss_recon: 0.612369, loss_pred: 0.030342
iteration 1580: loss: 60.734135, loss_kl: 0.244748, loss_recon: 0.606842, loss_pred: 0.047504
 79%|██████████████████████▉      | 158/200 [2:11:10<34:31, 49.33s/it]iteration 1581: loss: 61.867043, loss_kl: 0.420950, loss_recon: 0.618145, loss_pred: 0.048349
iteration 1582: loss: 59.745686, loss_kl: 0.314144, loss_recon: 0.597001, loss_pred: 0.042404
iteration 1583: loss: 61.193951, loss_kl: 0.411695, loss_recon: 0.611467, loss_pred: 0.043132
iteration 1584: loss: 61.430981, loss_kl: -0.045768, loss_recon: 0.613697, loss_pred: 0.061703
iteration 1585: loss: 60.969849, loss_kl: 0.231787, loss_recon: 0.609312, loss_pred: 0.036371
iteration 1586: loss: 59.636982, loss_kl: 0.408583, loss_recon: 0.595908, loss_pred: 0.042068
iteration 1587: loss: 59.851116, loss_kl: 0.243864, loss_recon: 0.598180, loss_pred: 0.030663
iteration 1588: loss: 60.081436, loss_kl: 0.192431, loss_recon: 0.600224, loss_pred: 0.057112
iteration 1589: loss: 60.019901, loss_kl: 0.115490, loss_recon: 0.599743, loss_pred: 0.044447
iteration 1590: loss: 62.188927, loss_kl: 0.148645, loss_recon: 0.621531, loss_pred: 0.034332
 80%|███████████████████████      | 159/200 [2:12:00<33:47, 49.45s/it]iteration 1591: loss: 60.640408, loss_kl: 0.282297, loss_recon: 0.605911, loss_pred: 0.046460
iteration 1592: loss: 60.411381, loss_kl: 0.264485, loss_recon: 0.603684, loss_pred: 0.040359
iteration 1593: loss: 58.449593, loss_kl: 0.489689, loss_recon: 0.584030, loss_pred: 0.041733
iteration 1594: loss: 60.826012, loss_kl: 0.248567, loss_recon: 0.607824, loss_pred: 0.041085
iteration 1595: loss: 61.123775, loss_kl: 0.152300, loss_recon: 0.610777, loss_pred: 0.044503
iteration 1596: loss: 61.493706, loss_kl: 0.241076, loss_recon: 0.614533, loss_pred: 0.037944
iteration 1597: loss: 60.780777, loss_kl: 0.315917, loss_recon: 0.607443, loss_pred: 0.033342
iteration 1598: loss: 59.749710, loss_kl: 0.100235, loss_recon: 0.597186, loss_pred: 0.030069
iteration 1599: loss: 61.237560, loss_kl: 0.422804, loss_recon: 0.611901, loss_pred: 0.043245
iteration 1600: loss: 61.762383, loss_kl: 0.247103, loss_recon: 0.616893, loss_pred: 0.070652
save model to ../model/TVG_Design[64, 64, 64]/TVG_encoderpretrained_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.003_seed1234/epoch_159.pth
 80%|███████████████████████▏     | 160/200 [2:12:49<32:56, 49.41s/it]iteration 1601: loss: 61.050896, loss_kl: 0.192395, loss_recon: 0.610115, loss_pred: 0.037438
iteration 1602: loss: 61.069054, loss_kl: 0.290984, loss_recon: 0.610129, loss_pred: 0.053267
iteration 1603: loss: 60.321342, loss_kl: 0.321141, loss_recon: 0.602339, loss_pred: 0.084222
iteration 1604: loss: 61.184063, loss_kl: 0.318572, loss_recon: 0.611424, loss_pred: 0.038498
iteration 1605: loss: 59.946846, loss_kl: 0.261399, loss_recon: 0.599005, loss_pred: 0.043699
iteration 1606: loss: 61.173054, loss_kl: 0.206275, loss_recon: 0.610945, loss_pred: 0.076495
iteration 1607: loss: 60.253399, loss_kl: 0.307504, loss_recon: 0.602046, loss_pred: 0.045744
iteration 1608: loss: 61.436543, loss_kl: 0.179487, loss_recon: 0.613808, loss_pred: 0.053985
iteration 1609: loss: 60.371414, loss_kl: 0.366357, loss_recon: 0.603186, loss_pred: 0.049177
iteration 1610: loss: 60.203533, loss_kl: 0.184807, loss_recon: 0.601599, loss_pred: 0.041752
 80%|███████████████████████▎     | 161/200 [2:13:39<32:10, 49.50s/it]iteration 1611: loss: 60.053905, loss_kl: 0.186414, loss_recon: 0.599881, loss_pred: 0.063903
iteration 1612: loss: 60.899498, loss_kl: 0.035159, loss_recon: 0.608504, loss_pred: 0.048794
iteration 1613: loss: 62.626438, loss_kl: 0.136312, loss_recon: 0.625773, loss_pred: 0.047785
iteration 1614: loss: 60.711311, loss_kl: 0.269300, loss_recon: 0.606543, loss_pred: 0.054327
iteration 1615: loss: 61.578045, loss_kl: 0.266965, loss_recon: 0.615288, loss_pred: 0.046551
iteration 1616: loss: 59.990437, loss_kl: 0.269528, loss_recon: 0.599088, loss_pred: 0.078934
iteration 1617: loss: 59.945461, loss_kl: 0.147268, loss_recon: 0.598961, loss_pred: 0.047909
iteration 1618: loss: 58.949978, loss_kl: 0.157318, loss_recon: 0.588819, loss_pred: 0.066464
iteration 1619: loss: 62.469173, loss_kl: 0.185809, loss_recon: 0.623961, loss_pred: 0.071171
iteration 1620: loss: 60.764149, loss_kl: 0.303565, loss_recon: 0.607279, loss_pred: 0.033262
 81%|███████████████████████▍     | 162/200 [2:14:28<31:21, 49.51s/it]iteration 1621: loss: 60.156670, loss_kl: 0.197508, loss_recon: 0.600852, loss_pred: 0.069474
iteration 1622: loss: 62.427814, loss_kl: -0.032574, loss_recon: 0.623879, loss_pred: 0.040280
iteration 1623: loss: 60.955772, loss_kl: 0.229651, loss_recon: 0.609010, loss_pred: 0.052437
iteration 1624: loss: 60.475235, loss_kl: 0.328476, loss_recon: 0.604208, loss_pred: 0.051105
iteration 1625: loss: 60.706795, loss_kl: 0.200344, loss_recon: 0.606564, loss_pred: 0.048360
iteration 1626: loss: 60.621296, loss_kl: 0.227385, loss_recon: 0.605753, loss_pred: 0.043757
iteration 1627: loss: 59.509609, loss_kl: 0.179085, loss_recon: 0.594511, loss_pred: 0.056710
iteration 1628: loss: 60.106838, loss_kl: 0.229990, loss_recon: 0.600650, loss_pred: 0.039523
iteration 1629: loss: 60.023300, loss_kl: 0.157553, loss_recon: 0.599783, loss_pred: 0.043442
iteration 1630: loss: 61.932053, loss_kl: 0.151795, loss_recon: 0.618927, loss_pred: 0.037804
 82%|███████████████████████▋     | 163/200 [2:15:18<30:30, 49.47s/it]iteration 1631: loss: 60.470158, loss_kl: 0.234628, loss_recon: 0.604125, loss_pred: 0.046023
iteration 1632: loss: 60.102852, loss_kl: 0.175432, loss_recon: 0.600479, loss_pred: 0.046259
iteration 1633: loss: 61.094795, loss_kl: 0.114514, loss_recon: 0.610486, loss_pred: 0.040541
iteration 1634: loss: 59.736904, loss_kl: 0.228977, loss_recon: 0.596594, loss_pred: 0.066110
iteration 1635: loss: 61.334473, loss_kl: 0.071385, loss_recon: 0.612885, loss_pred: 0.042384
iteration 1636: loss: 62.016354, loss_kl: 0.242704, loss_recon: 0.619628, loss_pred: 0.041554
iteration 1637: loss: 58.668564, loss_kl: 0.153975, loss_recon: 0.585868, loss_pred: 0.074137
iteration 1638: loss: 61.537804, loss_kl: 0.054107, loss_recon: 0.614781, loss_pred: 0.056997
iteration 1639: loss: 60.086445, loss_kl: 0.248888, loss_recon: 0.600062, loss_pred: 0.067927
iteration 1640: loss: 61.897770, loss_kl: 0.217190, loss_recon: 0.618432, loss_pred: 0.043842
 82%|███████████████████████▊     | 164/200 [2:16:07<29:40, 49.46s/it]iteration 1641: loss: 59.267742, loss_kl: 0.175313, loss_recon: 0.591986, loss_pred: 0.053517
iteration 1642: loss: 60.563892, loss_kl: 0.132656, loss_recon: 0.605192, loss_pred: 0.032829
iteration 1643: loss: 59.574375, loss_kl: 0.154784, loss_recon: 0.595237, loss_pred: 0.036912
iteration 1644: loss: 60.060501, loss_kl: 0.054766, loss_recon: 0.600230, loss_pred: 0.032652
iteration 1645: loss: 61.450706, loss_kl: 0.175196, loss_recon: 0.613890, loss_pred: 0.046091
iteration 1646: loss: 60.541203, loss_kl: 0.143673, loss_recon: 0.604882, loss_pred: 0.040138
iteration 1647: loss: 61.283459, loss_kl: 0.167535, loss_recon: 0.612256, loss_pred: 0.042909
iteration 1648: loss: 61.294025, loss_kl: 0.076845, loss_recon: 0.612532, loss_pred: 0.033927
iteration 1649: loss: 61.606030, loss_kl: 0.013317, loss_recon: 0.615653, loss_pred: 0.039506
iteration 1650: loss: 60.894257, loss_kl: 0.161012, loss_recon: 0.608423, loss_pred: 0.037626
 82%|███████████████████████▉     | 165/200 [2:16:56<28:48, 49.38s/it]iteration 1651: loss: 60.353436, loss_kl: 0.167030, loss_recon: 0.602848, loss_pred: 0.047121
iteration 1652: loss: 60.818832, loss_kl: 0.099490, loss_recon: 0.607667, loss_pred: 0.039316
iteration 1653: loss: 61.571014, loss_kl: 0.182912, loss_recon: 0.615049, loss_pred: 0.042582
iteration 1654: loss: 61.348282, loss_kl: 0.130124, loss_recon: 0.612985, loss_pred: 0.033066
iteration 1655: loss: 59.636669, loss_kl: 0.118678, loss_recon: 0.595555, loss_pred: 0.065915
iteration 1656: loss: 60.013000, loss_kl: 0.098328, loss_recon: 0.599554, loss_pred: 0.044975
iteration 1657: loss: 61.260258, loss_kl: 0.093618, loss_recon: 0.612108, loss_pred: 0.037406
iteration 1658: loss: 60.450932, loss_kl: 0.078783, loss_recon: 0.604002, loss_pred: 0.040630
iteration 1659: loss: 61.240276, loss_kl: 0.144414, loss_recon: 0.611800, loss_pred: 0.041690
iteration 1660: loss: 59.808445, loss_kl: 0.177430, loss_recon: 0.597595, loss_pred: 0.026122
 83%|████████████████████████     | 166/200 [2:17:48<28:25, 50.15s/it]iteration 1661: loss: 59.231300, loss_kl: 0.203951, loss_recon: 0.591527, loss_pred: 0.044288
iteration 1662: loss: 60.566650, loss_kl: 0.202344, loss_recon: 0.604896, loss_pred: 0.042986
iteration 1663: loss: 60.850704, loss_kl: 0.020620, loss_recon: 0.607991, loss_pred: 0.048140
iteration 1664: loss: 60.937477, loss_kl: 0.146424, loss_recon: 0.608542, loss_pred: 0.058616
iteration 1665: loss: 60.457142, loss_kl: 0.128389, loss_recon: 0.603739, loss_pred: 0.061665
iteration 1666: loss: 61.638813, loss_kl: 0.057387, loss_recon: 0.615793, loss_pred: 0.049815
iteration 1667: loss: 61.258938, loss_kl: 0.038694, loss_recon: 0.612195, loss_pred: 0.032962
iteration 1668: loss: 61.457111, loss_kl: 0.044111, loss_recon: 0.614139, loss_pred: 0.035803
iteration 1669: loss: 60.119137, loss_kl: 0.151217, loss_recon: 0.600463, loss_pred: 0.047361
iteration 1670: loss: 60.338215, loss_kl: 0.124201, loss_recon: 0.602821, loss_pred: 0.035153
 84%|████████████████████████▏    | 167/200 [2:18:38<27:31, 50.05s/it]iteration 1671: loss: 58.900345, loss_kl: -0.058808, loss_recon: 0.588812, loss_pred: 0.031372
iteration 1672: loss: 60.795532, loss_kl: 0.126578, loss_recon: 0.607300, loss_pred: 0.039162
iteration 1673: loss: 59.425098, loss_kl: -0.031307, loss_recon: 0.593810, loss_pred: 0.050592
iteration 1674: loss: 61.640549, loss_kl: -0.030498, loss_recon: 0.616114, loss_pred: 0.035467
iteration 1675: loss: 59.005348, loss_kl: -0.002111, loss_recon: 0.589650, loss_pred: 0.040825
iteration 1676: loss: 61.723988, loss_kl: 0.097420, loss_recon: 0.616613, loss_pred: 0.042459
iteration 1677: loss: 62.394634, loss_kl: 0.069773, loss_recon: 0.623511, loss_pred: 0.028973
iteration 1678: loss: 60.165348, loss_kl: 0.078028, loss_recon: 0.600943, loss_pred: 0.054857
iteration 1679: loss: 61.845749, loss_kl: 0.006777, loss_recon: 0.618064, loss_pred: 0.037892
iteration 1680: loss: 61.055134, loss_kl: 0.097060, loss_recon: 0.609943, loss_pred: 0.040673
 84%|████████████████████████▎    | 168/200 [2:19:29<26:46, 50.20s/it]iteration 1681: loss: 60.701122, loss_kl: 0.024876, loss_recon: 0.606429, loss_pred: 0.052111
iteration 1682: loss: 60.901867, loss_kl: 0.002933, loss_recon: 0.608425, loss_pred: 0.058691
iteration 1683: loss: 62.259129, loss_kl: 0.191773, loss_recon: 0.621806, loss_pred: 0.031056
iteration 1684: loss: 60.504230, loss_kl: 0.148823, loss_recon: 0.604029, loss_pred: 0.064437
iteration 1685: loss: 60.141140, loss_kl: -0.008711, loss_recon: 0.600561, loss_pred: 0.087158
iteration 1686: loss: 60.647972, loss_kl: 0.192741, loss_recon: 0.605635, loss_pred: 0.036720
iteration 1687: loss: 60.314251, loss_kl: 0.149332, loss_recon: 0.602237, loss_pred: 0.053540
iteration 1688: loss: 60.545078, loss_kl: 0.133265, loss_recon: 0.604348, loss_pred: 0.077276
iteration 1689: loss: 60.496296, loss_kl: 0.094131, loss_recon: 0.604294, loss_pred: 0.043551
iteration 1690: loss: 60.961227, loss_kl: 0.128164, loss_recon: 0.608865, loss_pred: 0.042993
 84%|████████████████████████▌    | 169/200 [2:20:19<25:57, 50.24s/it]iteration 1691: loss: 62.006481, loss_kl: 0.138532, loss_recon: 0.619171, loss_pred: 0.049626
iteration 1692: loss: 60.392982, loss_kl: 0.071340, loss_recon: 0.603272, loss_pred: 0.045322
iteration 1693: loss: 60.516815, loss_kl: 0.176691, loss_recon: 0.603986, loss_pred: 0.067514
iteration 1694: loss: 59.773556, loss_kl: 0.031996, loss_recon: 0.597015, loss_pred: 0.062861
iteration 1695: loss: 60.833374, loss_kl: 0.101746, loss_recon: 0.607581, loss_pred: 0.046092
iteration 1696: loss: 61.035454, loss_kl: 0.131774, loss_recon: 0.609599, loss_pred: 0.037676
iteration 1697: loss: 59.946686, loss_kl: 0.015220, loss_recon: 0.598974, loss_pred: 0.044878
iteration 1698: loss: 61.592419, loss_kl: 0.203770, loss_recon: 0.614958, loss_pred: 0.038073
iteration 1699: loss: 60.199593, loss_kl: 0.190052, loss_recon: 0.601018, loss_pred: 0.043242
iteration 1700: loss: 60.906864, loss_kl: 0.085821, loss_recon: 0.608487, loss_pred: 0.033475
 85%|████████████████████████▋    | 170/200 [2:21:08<25:01, 50.04s/it]iteration 1701: loss: 61.304699, loss_kl: 0.277502, loss_recon: 0.611689, loss_pred: 0.045096
iteration 1702: loss: 59.557262, loss_kl: 0.114621, loss_recon: 0.594397, loss_pred: 0.080140
iteration 1703: loss: 61.043552, loss_kl: -0.074756, loss_recon: 0.610185, loss_pred: 0.049494
iteration 1704: loss: 60.823967, loss_kl: 0.098501, loss_recon: 0.607561, loss_pred: 0.035722
iteration 1705: loss: 60.597004, loss_kl: 0.075178, loss_recon: 0.605371, loss_pred: 0.035333
iteration 1706: loss: 59.659786, loss_kl: 0.091591, loss_recon: 0.595953, loss_pred: 0.034576
iteration 1707: loss: 61.666214, loss_kl: 0.072893, loss_recon: 0.615892, loss_pred: 0.053185
iteration 1708: loss: 60.897442, loss_kl: 0.106855, loss_recon: 0.608270, loss_pred: 0.035563
iteration 1709: loss: 60.429306, loss_kl: 0.029415, loss_recon: 0.603842, loss_pred: 0.035468
iteration 1710: loss: 60.653431, loss_kl: 0.081780, loss_recon: 0.605894, loss_pred: 0.037326
 86%|████████████████████████▊    | 171/200 [2:21:58<24:04, 49.81s/it]iteration 1711: loss: 60.639572, loss_kl: 0.082013, loss_recon: 0.605677, loss_pred: 0.041859
iteration 1712: loss: 60.055206, loss_kl: 0.073188, loss_recon: 0.599808, loss_pred: 0.047624
iteration 1713: loss: 61.710644, loss_kl: 0.153417, loss_recon: 0.616165, loss_pred: 0.037913
iteration 1714: loss: 60.669857, loss_kl: 0.130011, loss_recon: 0.605850, loss_pred: 0.037212
iteration 1715: loss: 60.201046, loss_kl: 0.091872, loss_recon: 0.601312, loss_pred: 0.036196
iteration 1716: loss: 60.658657, loss_kl: 0.134635, loss_recon: 0.605649, loss_pred: 0.044377
iteration 1717: loss: 60.228100, loss_kl: 0.170242, loss_recon: 0.601314, loss_pred: 0.034301
iteration 1718: loss: 60.044949, loss_kl: -0.042799, loss_recon: 0.600208, loss_pred: 0.039820
iteration 1719: loss: 61.165062, loss_kl: 0.052140, loss_recon: 0.611082, loss_pred: 0.037769
iteration 1720: loss: 60.988853, loss_kl: 0.028550, loss_recon: 0.609406, loss_pred: 0.037769
 86%|████████████████████████▉    | 172/200 [2:22:47<23:10, 49.67s/it]iteration 1721: loss: 60.146088, loss_kl: 0.185470, loss_recon: 0.600326, loss_pred: 0.038143
iteration 1722: loss: 59.842667, loss_kl: 0.012575, loss_recon: 0.597955, loss_pred: 0.042032
iteration 1723: loss: 61.395653, loss_kl: -0.005124, loss_recon: 0.613666, loss_pred: 0.031113
iteration 1724: loss: 60.475121, loss_kl: 0.188883, loss_recon: 0.603612, loss_pred: 0.037239
iteration 1725: loss: 60.729893, loss_kl: 0.139164, loss_recon: 0.606418, loss_pred: 0.031636
iteration 1726: loss: 61.380810, loss_kl: 0.088405, loss_recon: 0.612961, loss_pred: 0.048849
iteration 1727: loss: 59.788136, loss_kl: 0.151796, loss_recon: 0.596800, loss_pred: 0.046518
iteration 1728: loss: 60.067768, loss_kl: 0.158245, loss_recon: 0.599573, loss_pred: 0.046263
iteration 1729: loss: 61.368839, loss_kl: 0.110628, loss_recon: 0.612903, loss_pred: 0.033636
iteration 1730: loss: 61.332481, loss_kl: 0.150960, loss_recon: 0.612357, loss_pred: 0.035528
 86%|█████████████████████████    | 173/200 [2:23:37<22:21, 49.68s/it]iteration 1731: loss: 61.140842, loss_kl: 0.220040, loss_recon: 0.610066, loss_pred: 0.036234
iteration 1732: loss: 60.816723, loss_kl: 0.102228, loss_recon: 0.607424, loss_pred: 0.028804
iteration 1733: loss: 60.010788, loss_kl: 0.085288, loss_recon: 0.599315, loss_pred: 0.041284
iteration 1734: loss: 60.174282, loss_kl: 0.187719, loss_recon: 0.600468, loss_pred: 0.043875
iteration 1735: loss: 59.686832, loss_kl: 0.230790, loss_recon: 0.595284, loss_pred: 0.055606
iteration 1736: loss: 60.280304, loss_kl: 0.204996, loss_recon: 0.601488, loss_pred: 0.040116
iteration 1737: loss: 60.808983, loss_kl: 0.217109, loss_recon: 0.606740, loss_pred: 0.038229
iteration 1738: loss: 61.944187, loss_kl: 0.196641, loss_recon: 0.618169, loss_pred: 0.039693
iteration 1739: loss: 61.424000, loss_kl: 0.076522, loss_recon: 0.613402, loss_pred: 0.049682
iteration 1740: loss: 60.476288, loss_kl: 0.117895, loss_recon: 0.603807, loss_pred: 0.043084
 87%|█████████████████████████▏   | 174/200 [2:24:26<21:31, 49.67s/it]iteration 1741: loss: 60.033134, loss_kl: 0.102380, loss_recon: 0.599438, loss_pred: 0.039680
iteration 1742: loss: 59.471493, loss_kl: 0.068907, loss_recon: 0.593915, loss_pred: 0.046539
iteration 1743: loss: 60.363541, loss_kl: 0.118083, loss_recon: 0.602525, loss_pred: 0.053751
iteration 1744: loss: 61.704781, loss_kl: 0.231414, loss_recon: 0.615409, loss_pred: 0.051621
iteration 1745: loss: 60.054211, loss_kl: 0.119988, loss_recon: 0.599343, loss_pred: 0.061654
iteration 1746: loss: 61.650417, loss_kl: 0.042817, loss_recon: 0.615725, loss_pred: 0.057180
iteration 1747: loss: 62.080952, loss_kl: 0.147912, loss_recon: 0.619725, loss_pred: 0.036723
iteration 1748: loss: 60.033054, loss_kl: 0.060383, loss_recon: 0.599522, loss_pred: 0.051553
iteration 1749: loss: 61.136379, loss_kl: 0.022745, loss_recon: 0.610755, loss_pred: 0.049856
iteration 1750: loss: 60.499691, loss_kl: 0.116821, loss_recon: 0.604019, loss_pred: 0.041115
 88%|█████████████████████████▍   | 175/200 [2:25:16<20:41, 49.66s/it]iteration 1751: loss: 60.332672, loss_kl: 0.151201, loss_recon: 0.602249, loss_pred: 0.028454
iteration 1752: loss: 60.163883, loss_kl: 0.141993, loss_recon: 0.600509, loss_pred: 0.038480
iteration 1753: loss: 60.101734, loss_kl: -0.030125, loss_recon: 0.600923, loss_pred: 0.025241
iteration 1754: loss: 60.853031, loss_kl: 0.241735, loss_recon: 0.606938, loss_pred: 0.032388
iteration 1755: loss: 60.163586, loss_kl: -0.024852, loss_recon: 0.601485, loss_pred: 0.028090
iteration 1756: loss: 61.672676, loss_kl: 0.138543, loss_recon: 0.615629, loss_pred: 0.037073
iteration 1757: loss: 60.220482, loss_kl: 0.111182, loss_recon: 0.601130, loss_pred: 0.049115
iteration 1758: loss: 60.249920, loss_kl: 0.045310, loss_recon: 0.601750, loss_pred: 0.051104
iteration 1759: loss: 61.851799, loss_kl: 0.263512, loss_recon: 0.616647, loss_pred: 0.048780
iteration 1760: loss: 61.150368, loss_kl: -0.014655, loss_recon: 0.611103, loss_pred: 0.047746
 88%|█████████████████████████▌   | 176/200 [2:26:07<20:01, 50.06s/it]iteration 1761: loss: 59.212856, loss_kl: 0.035451, loss_recon: 0.591563, loss_pred: 0.036576
iteration 1762: loss: 60.475719, loss_kl: 0.173675, loss_recon: 0.603284, loss_pred: 0.049327
iteration 1763: loss: 61.312305, loss_kl: 0.077233, loss_recon: 0.612207, loss_pred: 0.047965
iteration 1764: loss: 58.616669, loss_kl: 0.358917, loss_recon: 0.583656, loss_pred: 0.048487
iteration 1765: loss: 60.983433, loss_kl: 0.115309, loss_recon: 0.608826, loss_pred: 0.035743
iteration 1766: loss: 61.440845, loss_kl: 0.096368, loss_recon: 0.613423, loss_pred: 0.044186
iteration 1767: loss: 60.297585, loss_kl: 0.158858, loss_recon: 0.601514, loss_pred: 0.056489
iteration 1768: loss: 62.320824, loss_kl: -0.058118, loss_recon: 0.623210, loss_pred: 0.032583
iteration 1769: loss: 62.276299, loss_kl: 0.220030, loss_recon: 0.621111, loss_pred: 0.041005
iteration 1770: loss: 60.008598, loss_kl: 0.121349, loss_recon: 0.598902, loss_pred: 0.049889
 88%|█████████████████████████▋   | 177/200 [2:26:57<19:09, 49.98s/it]iteration 1771: loss: 61.673210, loss_kl: -0.006623, loss_recon: 0.616282, loss_pred: 0.049052
iteration 1772: loss: 61.204735, loss_kl: 0.204579, loss_recon: 0.610541, loss_pred: 0.027083
iteration 1773: loss: 60.094410, loss_kl: 0.136238, loss_recon: 0.599744, loss_pred: 0.037751
iteration 1774: loss: 60.251713, loss_kl: 0.213161, loss_recon: 0.600795, loss_pred: 0.043508
iteration 1775: loss: 59.424889, loss_kl: 0.284631, loss_recon: 0.592096, loss_pred: 0.043384
iteration 1776: loss: 61.016743, loss_kl: 0.320094, loss_recon: 0.607830, loss_pred: 0.040396
iteration 1777: loss: 61.460861, loss_kl: 0.218712, loss_recon: 0.612927, loss_pred: 0.036089
iteration 1778: loss: 60.493343, loss_kl: 0.107475, loss_recon: 0.603971, loss_pred: 0.031344
iteration 1779: loss: 60.089184, loss_kl: 0.071754, loss_recon: 0.599989, loss_pred: 0.046916
iteration 1780: loss: 61.865704, loss_kl: 0.165951, loss_recon: 0.617233, loss_pred: 0.042188
 89%|█████████████████████████▊   | 178/200 [2:27:46<18:16, 49.82s/it]iteration 1781: loss: 59.854515, loss_kl: 0.065301, loss_recon: 0.597660, loss_pred: 0.046528
iteration 1782: loss: 61.270412, loss_kl: 0.147612, loss_recon: 0.611329, loss_pred: 0.042480
iteration 1783: loss: 60.705730, loss_kl: 0.191472, loss_recon: 0.605467, loss_pred: 0.035783
iteration 1784: loss: 61.111286, loss_kl: 0.282601, loss_recon: 0.608787, loss_pred: 0.050716
iteration 1785: loss: 60.471577, loss_kl: 0.239439, loss_recon: 0.602800, loss_pred: 0.037514
iteration 1786: loss: 61.596733, loss_kl: 0.125963, loss_recon: 0.614752, loss_pred: 0.040500
iteration 1787: loss: 60.216724, loss_kl: 0.174123, loss_recon: 0.600643, loss_pred: 0.040353
iteration 1788: loss: 61.099422, loss_kl: 0.241639, loss_recon: 0.609105, loss_pred: 0.033419
iteration 1789: loss: 60.085571, loss_kl: 0.184177, loss_recon: 0.599320, loss_pred: 0.035055
iteration 1790: loss: 61.001320, loss_kl: 0.282433, loss_recon: 0.607811, loss_pred: 0.038441
 90%|█████████████████████████▉   | 179/200 [2:28:36<17:27, 49.87s/it]iteration 1791: loss: 60.484436, loss_kl: 0.089520, loss_recon: 0.603810, loss_pred: 0.042319
iteration 1792: loss: 59.901985, loss_kl: 0.094588, loss_recon: 0.597953, loss_pred: 0.042101
iteration 1793: loss: 60.364590, loss_kl: 0.103538, loss_recon: 0.602539, loss_pred: 0.039970
iteration 1794: loss: 60.167358, loss_kl: 0.198429, loss_recon: 0.599847, loss_pred: 0.047054
iteration 1795: loss: 60.746738, loss_kl: 0.048183, loss_recon: 0.606684, loss_pred: 0.045449
iteration 1796: loss: 60.532673, loss_kl: 0.346595, loss_recon: 0.602490, loss_pred: 0.046848
iteration 1797: loss: 61.748711, loss_kl: 0.331217, loss_recon: 0.614819, loss_pred: 0.040553
iteration 1798: loss: 61.444466, loss_kl: 0.064400, loss_recon: 0.613557, loss_pred: 0.044740
iteration 1799: loss: 61.131763, loss_kl: 0.195027, loss_recon: 0.609540, loss_pred: 0.044490
iteration 1800: loss: 60.964630, loss_kl: 0.044165, loss_recon: 0.608885, loss_pred: 0.045956
 90%|██████████████████████████   | 180/200 [2:29:26<16:35, 49.76s/it]iteration 1801: loss: 61.810333, loss_kl: 0.036456, loss_recon: 0.617387, loss_pred: 0.045322
iteration 1802: loss: 60.609562, loss_kl: 0.247268, loss_recon: 0.603977, loss_pred: 0.033092
iteration 1803: loss: 59.493141, loss_kl: 0.191528, loss_recon: 0.592973, loss_pred: 0.057364
iteration 1804: loss: 60.454777, loss_kl: 0.184753, loss_recon: 0.602930, loss_pred: 0.028223
iteration 1805: loss: 62.336597, loss_kl: 0.092618, loss_recon: 0.622243, loss_pred: 0.045349
iteration 1806: loss: 60.921711, loss_kl: 0.106254, loss_recon: 0.608108, loss_pred: 0.034159
iteration 1807: loss: 60.112137, loss_kl: 0.305930, loss_recon: 0.598547, loss_pred: 0.036320
iteration 1808: loss: 60.366203, loss_kl: 0.254589, loss_recon: 0.601468, loss_pred: 0.035395
iteration 1809: loss: 61.235485, loss_kl: 0.163523, loss_recon: 0.610849, loss_pred: 0.032413
iteration 1810: loss: 60.164902, loss_kl: 0.215225, loss_recon: 0.599798, loss_pred: 0.029520
 90%|██████████████████████████▏  | 181/200 [2:30:16<15:46, 49.84s/it]iteration 1811: loss: 60.437031, loss_kl: 0.098674, loss_recon: 0.603204, loss_pred: 0.041411
iteration 1812: loss: 60.108444, loss_kl: 0.124742, loss_recon: 0.599683, loss_pred: 0.045036
iteration 1813: loss: 60.992893, loss_kl: 0.234822, loss_recon: 0.607739, loss_pred: 0.039922
iteration 1814: loss: 61.071369, loss_kl: 0.109689, loss_recon: 0.609568, loss_pred: 0.030960
iteration 1815: loss: 61.419201, loss_kl: 0.306705, loss_recon: 0.611482, loss_pred: 0.037137
iteration 1816: loss: 62.567265, loss_kl: 0.192182, loss_recon: 0.623889, loss_pred: 0.031830
iteration 1817: loss: 60.587360, loss_kl: 0.139361, loss_recon: 0.604311, loss_pred: 0.050042
iteration 1818: loss: 60.224617, loss_kl: 0.115504, loss_recon: 0.600963, loss_pred: 0.040240
iteration 1819: loss: 60.281052, loss_kl: 0.095525, loss_recon: 0.601693, loss_pred: 0.038930
iteration 1820: loss: 60.037788, loss_kl: 0.146826, loss_recon: 0.598819, loss_pred: 0.043992
 91%|██████████████████████████▍  | 182/200 [2:31:05<14:53, 49.65s/it]iteration 1821: loss: 61.474030, loss_kl: 0.124073, loss_recon: 0.613280, loss_pred: 0.046537
iteration 1822: loss: 60.531464, loss_kl: 0.100210, loss_recon: 0.604234, loss_pred: 0.027732
iteration 1823: loss: 60.595329, loss_kl: 0.095271, loss_recon: 0.604864, loss_pred: 0.032556
iteration 1824: loss: 60.435165, loss_kl: 0.091552, loss_recon: 0.603042, loss_pred: 0.057520
iteration 1825: loss: 61.614635, loss_kl: 0.262078, loss_recon: 0.613670, loss_pred: 0.037445
iteration 1826: loss: 61.022022, loss_kl: 0.173673, loss_recon: 0.608425, loss_pred: 0.040282
iteration 1827: loss: 61.199757, loss_kl: -0.123022, loss_recon: 0.612535, loss_pred: 0.044908
iteration 1828: loss: 60.550713, loss_kl: 0.284571, loss_recon: 0.602946, loss_pred: 0.027848
iteration 1829: loss: 58.971405, loss_kl: 0.205709, loss_recon: 0.587415, loss_pred: 0.064904
iteration 1830: loss: 61.278049, loss_kl: 0.090804, loss_recon: 0.611639, loss_pred: 0.041301
 92%|██████████████████████████▌  | 183/200 [2:31:54<14:03, 49.59s/it]iteration 1831: loss: 60.254551, loss_kl: 0.028278, loss_recon: 0.601856, loss_pred: 0.045193
iteration 1832: loss: 59.899075, loss_kl: 0.133880, loss_recon: 0.597426, loss_pred: 0.043768
iteration 1833: loss: 59.797039, loss_kl: -0.010241, loss_recon: 0.597641, loss_pred: 0.041563
iteration 1834: loss: 60.540005, loss_kl: 0.343970, loss_recon: 0.602033, loss_pred: 0.047178
iteration 1835: loss: 61.471275, loss_kl: 0.321316, loss_recon: 0.611595, loss_pred: 0.041352
iteration 1836: loss: 60.530624, loss_kl: 0.162959, loss_recon: 0.603573, loss_pred: 0.036173
iteration 1837: loss: 61.918568, loss_kl: 0.339735, loss_recon: 0.615961, loss_pred: 0.036581
iteration 1838: loss: 61.045994, loss_kl: 0.107096, loss_recon: 0.609162, loss_pred: 0.039702
iteration 1839: loss: 60.716343, loss_kl: -0.010863, loss_recon: 0.606824, loss_pred: 0.043065
iteration 1840: loss: 61.292988, loss_kl: 0.273466, loss_recon: 0.610297, loss_pred: 0.033152
 92%|██████████████████████████▋  | 184/200 [2:32:43<13:09, 49.37s/it]iteration 1841: loss: 59.376640, loss_kl: 0.197060, loss_recon: 0.591414, loss_pred: 0.061598
iteration 1842: loss: 60.568756, loss_kl: -0.038518, loss_recon: 0.605535, loss_pred: 0.049180
iteration 1843: loss: 60.272449, loss_kl: 0.297735, loss_recon: 0.599726, loss_pred: 0.037442
iteration 1844: loss: 60.332737, loss_kl: 0.139096, loss_recon: 0.601731, loss_pred: 0.037046
iteration 1845: loss: 61.440224, loss_kl: 0.126244, loss_recon: 0.612904, loss_pred: 0.038558
iteration 1846: loss: 60.571987, loss_kl: -0.096747, loss_recon: 0.606066, loss_pred: 0.050605
iteration 1847: loss: 60.988964, loss_kl: 0.178417, loss_recon: 0.607862, loss_pred: 0.045588
iteration 1848: loss: 60.338913, loss_kl: 0.365747, loss_recon: 0.599717, loss_pred: 0.044945
iteration 1849: loss: 62.070187, loss_kl: 0.257006, loss_recon: 0.617965, loss_pred: 0.047201
iteration 1850: loss: 61.638653, loss_kl: 0.408862, loss_recon: 0.612450, loss_pred: 0.033344
 92%|██████████████████████████▊  | 185/200 [2:33:33<12:20, 49.38s/it]iteration 1851: loss: 58.996449, loss_kl: 0.046280, loss_recon: 0.588864, loss_pred: 0.067391
iteration 1852: loss: 60.965557, loss_kl: 0.134976, loss_recon: 0.608037, loss_pred: 0.037593
iteration 1853: loss: 61.334839, loss_kl: 0.290913, loss_recon: 0.610053, loss_pred: 0.061661
iteration 1854: loss: 61.862026, loss_kl: 0.413032, loss_recon: 0.614305, loss_pred: 0.051233
iteration 1855: loss: 62.611500, loss_kl: 0.462619, loss_recon: 0.621471, loss_pred: 0.038414
iteration 1856: loss: 61.198795, loss_kl: 0.168683, loss_recon: 0.609909, loss_pred: 0.052565
iteration 1857: loss: 60.748306, loss_kl: 0.109968, loss_recon: 0.606124, loss_pred: 0.034608
iteration 1858: loss: 60.663334, loss_kl: 0.043777, loss_recon: 0.605950, loss_pred: 0.028019
iteration 1859: loss: 59.366253, loss_kl: 0.132773, loss_recon: 0.591908, loss_pred: 0.053175
iteration 1860: loss: 60.593334, loss_kl: 0.301701, loss_recon: 0.602814, loss_pred: 0.034147
 93%|██████████████████████████▉  | 186/200 [2:34:22<11:31, 49.41s/it]iteration 1861: loss: 59.875179, loss_kl: 0.147080, loss_recon: 0.596989, loss_pred: 0.035048
iteration 1862: loss: 61.026829, loss_kl: 0.414690, loss_recon: 0.605900, loss_pred: 0.038596
iteration 1863: loss: 60.564064, loss_kl: 0.285798, loss_recon: 0.602480, loss_pred: 0.041584
iteration 1864: loss: 61.750103, loss_kl: 0.008878, loss_recon: 0.617026, loss_pred: 0.039017
iteration 1865: loss: 60.862736, loss_kl: 0.095191, loss_recon: 0.607229, loss_pred: 0.048436
iteration 1866: loss: 60.806740, loss_kl: 0.246969, loss_recon: 0.604979, loss_pred: 0.071676
iteration 1867: loss: 61.459023, loss_kl: 0.458030, loss_recon: 0.609903, loss_pred: 0.028819
iteration 1868: loss: 61.228329, loss_kl: 0.193524, loss_recon: 0.610108, loss_pred: 0.031667
iteration 1869: loss: 59.911129, loss_kl: 0.384631, loss_recon: 0.594952, loss_pred: 0.046573
iteration 1870: loss: 61.154354, loss_kl: 0.062684, loss_recon: 0.610540, loss_pred: 0.040188
 94%|███████████████████████████  | 187/200 [2:35:13<10:45, 49.69s/it]iteration 1871: loss: 60.889576, loss_kl: 0.293822, loss_recon: 0.605556, loss_pred: 0.040176
iteration 1872: loss: 61.112999, loss_kl: 0.165827, loss_recon: 0.608986, loss_pred: 0.048536
iteration 1873: loss: 59.584419, loss_kl: 0.321918, loss_recon: 0.592013, loss_pred: 0.061219
iteration 1874: loss: 60.712128, loss_kl: -0.016701, loss_recon: 0.606847, loss_pred: 0.044112
iteration 1875: loss: 60.848240, loss_kl: 0.290184, loss_recon: 0.605021, loss_pred: 0.055928
iteration 1876: loss: 60.272533, loss_kl: -0.011901, loss_recon: 0.602294, loss_pred: 0.055046
iteration 1877: loss: 61.299675, loss_kl: 0.135986, loss_recon: 0.611341, loss_pred: 0.029605
iteration 1878: loss: 60.798615, loss_kl: 0.389091, loss_recon: 0.603488, loss_pred: 0.060748
iteration 1879: loss: 61.727413, loss_kl: 0.446378, loss_recon: 0.612480, loss_pred: 0.033055
iteration 1880: loss: 61.197590, loss_kl: 0.118083, loss_recon: 0.610432, loss_pred: 0.036263
 94%|███████████████████████████▎ | 188/200 [2:36:02<09:56, 49.72s/it]iteration 1881: loss: 61.229725, loss_kl: 0.283949, loss_recon: 0.609065, loss_pred: 0.039280
iteration 1882: loss: 60.880920, loss_kl: 0.052434, loss_recon: 0.607914, loss_pred: 0.037047
iteration 1883: loss: 60.895996, loss_kl: 0.246829, loss_recon: 0.606086, loss_pred: 0.040554
iteration 1884: loss: 60.054989, loss_kl: 0.253163, loss_recon: 0.597526, loss_pred: 0.049215
iteration 1885: loss: 59.878201, loss_kl: 0.215499, loss_recon: 0.596146, loss_pred: 0.048139
iteration 1886: loss: 60.864883, loss_kl: 0.135590, loss_recon: 0.606857, loss_pred: 0.043635
iteration 1887: loss: 61.830929, loss_kl: 0.179597, loss_recon: 0.616132, loss_pred: 0.038131
iteration 1888: loss: 59.311569, loss_kl: 0.474307, loss_recon: 0.587526, loss_pred: 0.084676
iteration 1889: loss: 61.062851, loss_kl: -0.071924, loss_recon: 0.611021, loss_pred: 0.032703
iteration 1890: loss: 62.463428, loss_kl: 0.283953, loss_recon: 0.621455, loss_pred: 0.033992
 94%|███████████████████████████▍ | 189/200 [2:36:52<09:07, 49.81s/it]iteration 1891: loss: 60.476387, loss_kl: 0.309388, loss_recon: 0.601359, loss_pred: 0.031140
iteration 1892: loss: 61.470589, loss_kl: 0.347665, loss_recon: 0.610875, loss_pred: 0.035404
iteration 1893: loss: 61.249931, loss_kl: 0.105505, loss_recon: 0.611083, loss_pred: 0.036133
iteration 1894: loss: 60.012272, loss_kl: 0.233520, loss_recon: 0.597393, loss_pred: 0.039488
iteration 1895: loss: 60.193951, loss_kl: 0.120280, loss_recon: 0.600226, loss_pred: 0.051063
iteration 1896: loss: 60.331474, loss_kl: 0.308884, loss_recon: 0.599762, loss_pred: 0.046426
iteration 1897: loss: 59.567432, loss_kl: 0.341243, loss_recon: 0.591845, loss_pred: 0.041679
iteration 1898: loss: 62.011948, loss_kl: 0.320715, loss_recon: 0.616597, loss_pred: 0.031537
iteration 1899: loss: 62.488941, loss_kl: 0.311235, loss_recon: 0.621455, loss_pred: 0.032178
iteration 1900: loss: 61.076454, loss_kl: 0.259086, loss_recon: 0.607691, loss_pred: 0.048244
 95%|███████████████████████████▌ | 190/200 [2:37:42<08:16, 49.65s/it]iteration 1901: loss: 60.411690, loss_kl: 0.485732, loss_recon: 0.598759, loss_pred: 0.050108
iteration 1902: loss: 61.237965, loss_kl: 0.262291, loss_recon: 0.609106, loss_pred: 0.065079
iteration 1903: loss: 61.131332, loss_kl: 0.099045, loss_recon: 0.609756, loss_pred: 0.056672
iteration 1904: loss: 59.731953, loss_kl: 0.280118, loss_recon: 0.594034, loss_pred: 0.048434
iteration 1905: loss: 61.059418, loss_kl: 0.358501, loss_recon: 0.606611, loss_pred: 0.039804
iteration 1906: loss: 61.548241, loss_kl: 0.220261, loss_recon: 0.612920, loss_pred: 0.035958
iteration 1907: loss: 61.569492, loss_kl: 0.447448, loss_recon: 0.610861, loss_pred: 0.035972
iteration 1908: loss: 61.490704, loss_kl: 0.253500, loss_recon: 0.612012, loss_pred: 0.036049
iteration 1909: loss: 59.899227, loss_kl: 0.209179, loss_recon: 0.596250, loss_pred: 0.065039
iteration 1910: loss: 61.177353, loss_kl: 0.281937, loss_recon: 0.608467, loss_pred: 0.048741
 96%|███████████████████████████▋ | 191/200 [2:38:31<07:27, 49.69s/it]iteration 1911: loss: 61.093662, loss_kl: 0.105279, loss_recon: 0.609582, loss_pred: 0.030168
iteration 1912: loss: 60.730343, loss_kl: 0.474669, loss_recon: 0.602066, loss_pred: 0.049093
iteration 1913: loss: 61.699436, loss_kl: 0.225343, loss_recon: 0.614392, loss_pred: 0.034944
iteration 1914: loss: 62.470203, loss_kl: 0.315594, loss_recon: 0.621182, loss_pred: 0.036442
iteration 1915: loss: 60.161968, loss_kl: 0.152288, loss_recon: 0.599493, loss_pred: 0.060409
iteration 1916: loss: 59.996616, loss_kl: 0.198662, loss_recon: 0.597572, loss_pred: 0.040766
iteration 1917: loss: 61.471878, loss_kl: 0.346227, loss_recon: 0.610788, loss_pred: 0.046872
iteration 1918: loss: 61.197205, loss_kl: 0.024759, loss_recon: 0.611395, loss_pred: 0.032949
iteration 1919: loss: 60.013031, loss_kl: 0.544081, loss_recon: 0.594118, loss_pred: 0.057101
iteration 1920: loss: 59.938793, loss_kl: 0.250175, loss_recon: 0.596343, loss_pred: 0.054336
 96%|███████████████████████████▊ | 192/200 [2:39:20<06:36, 49.51s/it]iteration 1921: loss: 60.057789, loss_kl: 0.054641, loss_recon: 0.599648, loss_pred: 0.038342
iteration 1922: loss: 59.667259, loss_kl: 0.237372, loss_recon: 0.593907, loss_pred: 0.039204
iteration 1923: loss: 59.787422, loss_kl: 0.156612, loss_recon: 0.595753, loss_pred: 0.055467
iteration 1924: loss: 60.648811, loss_kl: 0.328027, loss_recon: 0.602711, loss_pred: 0.049734
iteration 1925: loss: 63.104061, loss_kl: 0.567407, loss_recon: 0.625009, loss_pred: 0.035731
iteration 1926: loss: 61.355606, loss_kl: 0.242163, loss_recon: 0.610711, loss_pred: 0.042365
iteration 1927: loss: 62.152081, loss_kl: 0.626242, loss_recon: 0.614917, loss_pred: 0.034124
iteration 1928: loss: 60.632282, loss_kl: 0.037081, loss_recon: 0.605566, loss_pred: 0.038579
iteration 1929: loss: 59.517193, loss_kl: 0.191995, loss_recon: 0.592675, loss_pred: 0.057733
iteration 1930: loss: 62.158169, loss_kl: 0.181005, loss_recon: 0.619346, loss_pred: 0.042524
 96%|███████████████████████████▉ | 193/200 [2:40:10<05:46, 49.44s/it]iteration 1931: loss: 61.547138, loss_kl: 0.204414, loss_recon: 0.612770, loss_pred: 0.065706
iteration 1932: loss: 61.220917, loss_kl: 0.450984, loss_recon: 0.606967, loss_pred: 0.073210
iteration 1933: loss: 61.326019, loss_kl: 0.212388, loss_recon: 0.610697, loss_pred: 0.043929
iteration 1934: loss: 61.858150, loss_kl: 0.596271, loss_recon: 0.612028, loss_pred: 0.059068
iteration 1935: loss: 59.944553, loss_kl: 0.188085, loss_recon: 0.596728, loss_pred: 0.083646
iteration 1936: loss: 61.619267, loss_kl: 0.295717, loss_recon: 0.612774, loss_pred: 0.046140
iteration 1937: loss: 59.502831, loss_kl: 0.397531, loss_recon: 0.590652, loss_pred: 0.040072
iteration 1938: loss: 60.129539, loss_kl: 0.455389, loss_recon: 0.596260, loss_pred: 0.048151
iteration 1939: loss: 61.753819, loss_kl: 0.333787, loss_recon: 0.613682, loss_pred: 0.051875
iteration 1940: loss: 60.701813, loss_kl: 0.118504, loss_recon: 0.605341, loss_pred: 0.049249
 97%|████████████████████████████▏| 194/200 [2:41:00<04:58, 49.83s/it]iteration 1941: loss: 60.438492, loss_kl: 0.276741, loss_recon: 0.601194, loss_pred: 0.042378
iteration 1942: loss: 59.595387, loss_kl: 0.442503, loss_recon: 0.590916, loss_pred: 0.061284
iteration 1943: loss: 60.950684, loss_kl: 0.319858, loss_recon: 0.605893, loss_pred: 0.041533
iteration 1944: loss: 61.823708, loss_kl: 0.272045, loss_recon: 0.614954, loss_pred: 0.056308
iteration 1945: loss: 60.462662, loss_kl: 0.127025, loss_recon: 0.602942, loss_pred: 0.041402
iteration 1946: loss: 60.418087, loss_kl: 0.206356, loss_recon: 0.601694, loss_pred: 0.042312
iteration 1947: loss: 61.314095, loss_kl: 0.113106, loss_recon: 0.611586, loss_pred: 0.042372
iteration 1948: loss: 61.181953, loss_kl: 0.067238, loss_recon: 0.610743, loss_pred: 0.040425
iteration 1949: loss: 61.376522, loss_kl: 0.370045, loss_recon: 0.609694, loss_pred: 0.037083
iteration 1950: loss: 61.718052, loss_kl: 0.447980, loss_recon: 0.612163, loss_pred: 0.053761
 98%|████████████████████████████▎| 195/200 [2:41:50<04:08, 49.62s/it]iteration 1951: loss: 62.841717, loss_kl: 0.331931, loss_recon: 0.624720, loss_pred: 0.037816
iteration 1952: loss: 61.077484, loss_kl: 0.107990, loss_recon: 0.609005, loss_pred: 0.068985
iteration 1953: loss: 59.690784, loss_kl: 0.180169, loss_recon: 0.594306, loss_pred: 0.080036
iteration 1954: loss: 60.253109, loss_kl: 0.224432, loss_recon: 0.599846, loss_pred: 0.044078
iteration 1955: loss: 60.655602, loss_kl: 0.230708, loss_recon: 0.603711, loss_pred: 0.053815
iteration 1956: loss: 60.098419, loss_kl: 0.035554, loss_recon: 0.600184, loss_pred: 0.044497
iteration 1957: loss: 60.944710, loss_kl: 0.172769, loss_recon: 0.607271, loss_pred: 0.044877
iteration 1958: loss: 60.679188, loss_kl: 0.153248, loss_recon: 0.604862, loss_pred: 0.039739
iteration 1959: loss: 60.990517, loss_kl: 0.149397, loss_recon: 0.608041, loss_pred: 0.037028
iteration 1960: loss: 61.487377, loss_kl: 0.273662, loss_recon: 0.611846, loss_pred: 0.029085
 98%|████████████████████████████▍| 196/200 [2:42:39<03:18, 49.54s/it]iteration 1961: loss: 60.034317, loss_kl: 0.141744, loss_recon: 0.598473, loss_pred: 0.045238
iteration 1962: loss: 60.385807, loss_kl: 0.390553, loss_recon: 0.599366, loss_pred: 0.058613
iteration 1963: loss: 61.354557, loss_kl: 0.392938, loss_recon: 0.609196, loss_pred: 0.041988
iteration 1964: loss: 58.827629, loss_kl: 0.219188, loss_recon: 0.585538, loss_pred: 0.054597
iteration 1965: loss: 61.692329, loss_kl: 0.336748, loss_recon: 0.613026, loss_pred: 0.052998
iteration 1966: loss: 61.295662, loss_kl: 0.112159, loss_recon: 0.611260, loss_pred: 0.057478
iteration 1967: loss: 61.533611, loss_kl: 0.242698, loss_recon: 0.612571, loss_pred: 0.033816
iteration 1968: loss: 60.742359, loss_kl: 0.088739, loss_recon: 0.605871, loss_pred: 0.066533
iteration 1969: loss: 61.057552, loss_kl: 0.147684, loss_recon: 0.608265, loss_pred: 0.083371
iteration 1970: loss: 62.043457, loss_kl: 0.169583, loss_recon: 0.618395, loss_pred: 0.034409
 98%|████████████████████████████▌| 197/200 [2:43:29<02:28, 49.57s/it]iteration 1971: loss: 60.565193, loss_kl: 0.307213, loss_recon: 0.602126, loss_pred: 0.045402
iteration 1972: loss: 60.913506, loss_kl: 0.260877, loss_recon: 0.606043, loss_pred: 0.048332
iteration 1973: loss: 61.162048, loss_kl: -0.035898, loss_recon: 0.611593, loss_pred: 0.038672
iteration 1974: loss: 60.259289, loss_kl: 0.310390, loss_recon: 0.598968, loss_pred: 0.052143
iteration 1975: loss: 61.446960, loss_kl: 0.402459, loss_recon: 0.610000, loss_pred: 0.044502
iteration 1976: loss: 62.471699, loss_kl: 0.260340, loss_recon: 0.621626, loss_pred: 0.048803
iteration 1977: loss: 60.123112, loss_kl: 0.183066, loss_recon: 0.598794, loss_pred: 0.060617
iteration 1978: loss: 60.955513, loss_kl: 0.331995, loss_recon: 0.605763, loss_pred: 0.047192
iteration 1979: loss: 60.857952, loss_kl: 0.356303, loss_recon: 0.604574, loss_pred: 0.044292
iteration 1980: loss: 60.079945, loss_kl: 0.149920, loss_recon: 0.598840, loss_pred: 0.045978
 99%|████████████████████████████▋| 198/200 [2:44:18<01:39, 49.50s/it]iteration 1981: loss: 60.792946, loss_kl: 0.236657, loss_recon: 0.605152, loss_pred: 0.041087
iteration 1982: loss: 61.008030, loss_kl: 0.216602, loss_recon: 0.607536, loss_pred: 0.037816
iteration 1983: loss: 59.963932, loss_kl: -0.054913, loss_recon: 0.599722, loss_pred: 0.046662
iteration 1984: loss: 61.712490, loss_kl: 0.080493, loss_recon: 0.615788, loss_pred: 0.053170
iteration 1985: loss: 62.055763, loss_kl: 0.161881, loss_recon: 0.618629, loss_pred: 0.031010
iteration 1986: loss: 61.333481, loss_kl: 0.251880, loss_recon: 0.610286, loss_pred: 0.053018
iteration 1987: loss: 60.208824, loss_kl: 0.263998, loss_recon: 0.598991, loss_pred: 0.045736
iteration 1988: loss: 60.372913, loss_kl: 0.174961, loss_recon: 0.601545, loss_pred: 0.043460
iteration 1989: loss: 60.407555, loss_kl: 0.194137, loss_recon: 0.601643, loss_pred: 0.049163
iteration 1990: loss: 60.237930, loss_kl: 0.133419, loss_recon: 0.600411, loss_pred: 0.063391
100%|████████████████████████████▊| 199/200 [2:45:08<00:49, 49.60s/it]iteration 1991: loss: 60.875813, loss_kl: 0.123034, loss_recon: 0.606964, loss_pred: 0.056378
iteration 1992: loss: 61.176296, loss_kl: 0.127670, loss_recon: 0.609886, loss_pred: 0.060021
iteration 1993: loss: 60.615597, loss_kl: 0.071065, loss_recon: 0.604842, loss_pred: 0.060304
iteration 1994: loss: 61.625446, loss_kl: 0.297205, loss_recon: 0.612943, loss_pred: 0.033898
iteration 1995: loss: 61.728573, loss_kl: 0.338253, loss_recon: 0.613462, loss_pred: 0.044106
iteration 1996: loss: 59.658478, loss_kl: 0.267208, loss_recon: 0.593448, loss_pred: 0.046439
iteration 1997: loss: 60.323807, loss_kl: 0.227447, loss_recon: 0.600558, loss_pred: 0.040552
iteration 1998: loss: 60.231575, loss_kl: 0.067711, loss_recon: 0.601253, loss_pred: 0.038520
iteration 1999: loss: 60.716194, loss_kl: 0.202676, loss_recon: 0.604821, loss_pred: 0.031465
iteration 2000: loss: 61.415871, loss_kl: 0.315915, loss_recon: 0.610703, loss_pred: 0.029608
save model to ../model/TVG_Design[64, 64, 64]/TVG_encoderpretrained_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.003_seed1234/epoch_199.pth
save model to ../model/TVG_Design[64, 64, 64]/TVG_encoderpretrained_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.003_seed1234/epoch_199.pth
100%|████████████████████████████▊| 199/200 [2:46:01<00:50, 50.06s/it]
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design2', img_size=[64, 64, 64], vit_patches_size=[4, 4, 4], net_path=False, vit_name='Conv-ViT-Gen2-B_16', pretrained_net_path=False, is_encoder_pretrained=True, deterministic=1, max_epochs=200, batch_size=32, base_lr=0.003, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=64, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, number_of_samplings=6, num_classes=2, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[64, 64, 64]', distributed=False)
TVG_encoderpretrained_Conv-ViT-Gen2-B_16_vitpatch[4, 4, 4]_epo200_bs32_lr0.003_seed1234
4 test iterations per epoch
0it [00:00, ?it/s]0it [00:14, ?it/s]
Traceback (most recent call last):
  File "/home/mhashemi/TransVNet/test.py", line 376, in <module>
    inferrer[dataset_name](args, net, test_save_path)
  File "/home/mhashemi/TransVNet/test.py", line 156, in inferrer_mat2
    name_batch, metric_batch = test_multiple_volumes_generative2(image_batch, label_batch, time_batch, model, name_batch, test_save_path, number_of_samplings)
  File "/home/mhashemi/TransVNet/utils.py", line 205, in test_multiple_volumes_generative2
    decoder_output = net.module.decoder(decoder_input)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/mhashemi/TransVNet/networks/TransVNet_modeling.py", line 727, in forward
    x = torch.unsqueeze(self.fc(x), -1)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x11 and 75x64)
