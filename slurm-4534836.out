/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[160, 160, 160], vit_name='Conv-ViT-Gen-B_16', pretrained_net_path='../model/TVG_Design[160, 160, 160]/TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_99.pth', is_encoder_pretrained=True, vit_patches_size=[8, 8, 8], deterministic=1, max_epochs=100, batch_size=32, base_lr=0.01, seed=1234, gpu=4, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, num_classes=2, root_path='/work/sheidaei/mhashemi/data/mat', list_dir='./lists/lists_Design', exp='TVG_Design[160, 160, 160]', distributed=False)
10 iterations per epoch. 1000 max iterations 
  0%|                                         | 0/100 [00:00<?, ?it/s]iteration 1: loss: 0.612930, loss_kl: 0.756578, loss_recon: 0.611004, loss_pred: 0.022598
iteration 2: loss: 0.680185, loss_kl: 2.053891, loss_recon: 0.660426, loss_pred: 5.937315
iteration 3: loss: 0.659056, loss_kl: 2.057352, loss_recon: 0.647389, loss_pred: 2.661113
iteration 4: loss: 0.643783, loss_kl: 2.235435, loss_recon: 0.636473, loss_pred: 0.720927
iteration 5: loss: 0.664359, loss_kl: 1.291747, loss_recon: 0.657315, loss_pred: 1.557183
iteration 6: loss: 0.624953, loss_kl: 0.492390, loss_recon: 0.620869, loss_pred: 1.159250
iteration 7: loss: 0.620235, loss_kl: 0.085566, loss_recon: 0.617919, loss_pred: 0.851234
iteration 8: loss: 0.622163, loss_kl: 0.630937, loss_recon: 0.618340, loss_pred: 0.915209
iteration 9: loss: 0.620060, loss_kl: 0.437728, loss_recon: 0.616416, loss_pred: 1.036110
iteration 10: loss: 0.615805, loss_kl: 0.732305, loss_recon: 0.611033, loss_pred: 1.197711
iteration 11: loss: 0.630303, loss_kl: 0.705542, loss_recon: 0.616823, loss_pred: 1.390118
iteration 12: loss: 0.626391, loss_kl: 0.635935, loss_recon: 0.613738, loss_pred: 1.331267
iteration 13: loss: 0.626339, loss_kl: 1.234737, loss_recon: 0.613388, loss_pred: 0.778664
iteration 14: loss: 0.625675, loss_kl: 1.498137, loss_recon: 0.613190, loss_pred: 0.442948
iteration 15: loss: 0.618835, loss_kl: 0.712245, loss_recon: 0.611951, loss_pred: 0.357883
iteration 16: loss: 0.619270, loss_kl: 0.594128, loss_recon: 0.613735, loss_pred: 0.266443
iteration 17: loss: 0.615693, loss_kl: 0.231997, loss_recon: 0.612503, loss_pred: 0.263982
iteration 18: loss: 0.617914, loss_kl: 0.579741, loss_recon: 0.612571, loss_pred: 0.250877
iteration 19: loss: 0.618420, loss_kl: 0.873785, loss_recon: 0.611021, loss_pred: 0.276551
iteration 20: loss: 0.613823, loss_kl: 0.820304, loss_recon: 0.606717, loss_pred: 0.284565
iteration 21: loss: 0.632329, loss_kl: 0.958570, loss_recon: 0.612829, loss_pred: 0.214274
iteration 22: loss: 0.627405, loss_kl: 0.886542, loss_recon: 0.610164, loss_pred: 0.150445
iteration 23: loss: 0.620458, loss_kl: 0.369032, loss_recon: 0.611098, loss_pred: 0.193957
iteration 24: loss: 0.631489, loss_kl: 1.134338, loss_recon: 0.610522, loss_pred: 0.126690
iteration 25: loss: 0.612770, loss_kl: 0.042296, loss_recon: 0.609207, loss_pred: 0.171978
iteration 26: loss: 0.634407, loss_kl: 1.317062, loss_recon: 0.610416, loss_pred: 0.125902
iteration 27: loss: 0.626531, loss_kl: 0.944338, loss_recon: 0.608956, loss_pred: 0.112728
iteration 28: loss: 0.623241, loss_kl: 0.772759, loss_recon: 0.609039, loss_pred: 0.081445
iteration 29: loss: 0.628874, loss_kl: 1.189059, loss_recon: 0.608088, loss_pred: 0.061117
iteration 30: loss: 0.626677, loss_kl: 1.216730, loss_recon: 0.604771, loss_pred: 0.100808
iteration 31: loss: 0.656904, loss_kl: 0.972511, loss_recon: 0.611699, loss_pred: 0.096438
iteration 32: loss: 0.655814, loss_kl: 0.969746, loss_recon: 0.608415, loss_pred: 0.151051
iteration 33: loss: 0.647818, loss_kl: 0.785861, loss_recon: 0.610421, loss_pred: 0.098439
iteration 34: loss: 0.654116, loss_kl: 0.961387, loss_recon: 0.609770, loss_pred: 0.087245
iteration 35: loss: 0.646831, loss_kl: 0.745891, loss_recon: 0.609887, loss_pred: 0.127694
iteration 36: loss: 0.666709, loss_kl: 1.286266, loss_recon: 0.609692, loss_pred: 0.061985
iteration 37: loss: 0.693614, loss_kl: 1.892434, loss_recon: 0.608933, loss_pred: 0.109957
iteration 38: loss: 0.670258, loss_kl: 1.389221, loss_recon: 0.608816, loss_pred: 0.063672
iteration 39: loss: 0.651980, loss_kl: 0.958157, loss_recon: 0.607515, loss_pred: 0.093270
iteration 40: loss: 0.642661, loss_kl: 0.786849, loss_recon: 0.604220, loss_pred: 0.122154
iteration 41: loss: 0.755181, loss_kl: 1.362754, loss_recon: 0.610366, loss_pred: 0.037773
iteration 42: loss: 0.681980, loss_kl: 0.676639, loss_recon: 0.607922, loss_pred: 0.039590
iteration 43: loss: 0.765405, loss_kl: 1.421703, loss_recon: 0.609648, loss_pred: 0.084643
iteration 44: loss: 0.757262, loss_kl: 1.355961, loss_recon: 0.609631, loss_pred: 0.071793
iteration 45: loss: 0.734679, loss_kl: 1.160264, loss_recon: 0.608184, loss_pred: 0.063085
iteration 46: loss: 0.764378, loss_kl: 1.446717, loss_recon: 0.609425, loss_pred: 0.051856
iteration 47: loss: 0.742266, loss_kl: 1.235625, loss_recon: 0.608590, loss_pred: 0.057175
iteration 48: loss: 0.745801, loss_kl: 1.282715, loss_recon: 0.608380, loss_pred: 0.046306
iteration 49: loss: 0.801920, loss_kl: 1.828184, loss_recon: 0.607795, loss_pred: 0.049228
iteration 50: loss: 0.842051, loss_kl: 2.167740, loss_recon: 0.604196, loss_pred: 0.132592
iteration 51: loss: 0.928231, loss_kl: 1.315743, loss_recon: 0.610346, loss_pred: 0.057559
iteration 52: loss: 0.930749, loss_kl: 1.216716, loss_recon: 0.608518, loss_pred: 0.175361
iteration 53: loss: 1.052676, loss_kl: 1.665403, loss_recon: 0.611454, loss_pred: 0.240727
iteration 54: loss: 0.988302, loss_kl: 1.504414, loss_recon: 0.609513, loss_pred: 0.131997
iteration 55: loss: 0.915494, loss_kl: 1.116473, loss_recon: 0.609536, loss_pred: 0.205301
iteration 56: loss: 1.040761, loss_kl: 1.679808, loss_recon: 0.610218, loss_pred: 0.180185
iteration 57: loss: 1.028229, loss_kl: 1.734757, loss_recon: 0.609238, loss_pred: 0.075335
iteration 58: loss: 0.922599, loss_kl: 1.243741, loss_recon: 0.609630, loss_pred: 0.108324
iteration 59: loss: 0.901860, loss_kl: 1.164754, loss_recon: 0.608927, loss_pred: 0.100751
iteration 60: loss: 1.034284, loss_kl: 1.727316, loss_recon: 0.605183, loss_pred: 0.126452
iteration 61: loss: 1.341610, loss_kl: 1.602946, loss_recon: 0.610837, loss_pred: 0.056822
iteration 62: loss: 1.209995, loss_kl: 1.250943, loss_recon: 0.609389, loss_pred: 0.113182
iteration 63: loss: 1.413192, loss_kl: 1.656212, loss_recon: 0.611681, loss_pred: 0.164221
iteration 64: loss: 1.229782, loss_kl: 1.285665, loss_recon: 0.612158, loss_pred: 0.117114
iteration 65: loss: 1.429084, loss_kl: 1.739392, loss_recon: 0.611126, loss_pred: 0.118394
iteration 66: loss: 1.131453, loss_kl: 1.074306, loss_recon: 0.614486, loss_pred: 0.099855
iteration 67: loss: 1.352362, loss_kl: 1.618047, loss_recon: 0.613804, loss_pred: 0.059401
iteration 68: loss: 1.379158, loss_kl: 1.606410, loss_recon: 0.614758, loss_pred: 0.129733
iteration 69: loss: 1.330882, loss_kl: 1.424147, loss_recon: 0.611265, loss_pred: 0.210282
iteration 70: loss: 1.616495, loss_kl: 1.860214, loss_recon: 0.610010, loss_pred: 0.425764
iteration 71: loss: 2.410024, loss_kl: 2.386437, loss_recon: 0.612683, loss_pred: 0.285765
iteration 72: loss: 1.924689, loss_kl: 1.829925, loss_recon: 0.612434, loss_pred: 0.121073
iteration 73: loss: 1.853770, loss_kl: 1.723053, loss_recon: 0.613700, loss_pred: 0.120622
iteration 74: loss: 1.851904, loss_kl: 1.757298, loss_recon: 0.610804, loss_pred: 0.087910
iteration 75: loss: 1.694641, loss_kl: 1.487759, loss_recon: 0.609548, loss_pred: 0.125505
iteration 76: loss: 1.699566, loss_kl: 1.532934, loss_recon: 0.610551, loss_pred: 0.086162
iteration 77: loss: 2.176577, loss_kl: 2.167199, loss_recon: 0.609008, loss_pred: 0.163387
iteration 78: loss: 1.794583, loss_kl: 1.679495, loss_recon: 0.609774, loss_pred: 0.082023
iteration 79: loss: 1.569270, loss_kl: 1.300541, loss_recon: 0.608527, loss_pred: 0.127846
iteration 80: loss: 1.211785, loss_kl: 0.834816, loss_recon: 0.604850, loss_pred: 0.067546
iteration 81: loss: 1.554857, loss_kl: 1.035757, loss_recon: 0.610879, loss_pred: 0.084154
iteration 82: loss: 1.659638, loss_kl: 1.184073, loss_recon: 0.607749, loss_pred: 0.063862
iteration 83: loss: 2.001109, loss_kl: 1.545998, loss_recon: 0.609981, loss_pred: 0.104400
iteration 84: loss: 2.032255, loss_kl: 1.591279, loss_recon: 0.609420, loss_pred: 0.096736
iteration 85: loss: 2.388873, loss_kl: 2.027044, loss_recon: 0.608639, loss_pred: 0.084980
iteration 86: loss: 2.527037, loss_kl: 2.101938, loss_recon: 0.610248, loss_pred: 0.172090
iteration 87: loss: 1.663248, loss_kl: 1.211553, loss_recon: 0.608574, loss_pred: 0.039685
iteration 88: loss: 1.946038, loss_kl: 1.413370, loss_recon: 0.609620, loss_pred: 0.172122
iteration 89: loss: 2.441202, loss_kl: 2.109959, loss_recon: 0.608745, loss_pred: 0.064020
iteration 90: loss: 2.433983, loss_kl: 1.968018, loss_recon: 0.605784, loss_pred: 0.200910
iteration 91: loss: 2.463767, loss_kl: 1.921618, loss_recon: 0.611405, loss_pred: 0.062930
iteration 92: loss: 2.045710, loss_kl: 1.415618, loss_recon: 0.609694, loss_pred: 0.122874
iteration 93: loss: 1.861163, loss_kl: 1.270372, loss_recon: 0.611673, loss_pred: 0.068283
iteration 94: loss: 2.155502, loss_kl: 1.509053, loss_recon: 0.610083, loss_pred: 0.146649
iteration 95: loss: 1.532593, loss_kl: 0.929841, loss_recon: 0.608896, loss_pred: 0.059772
iteration 96: loss: 2.160713, loss_kl: 1.532835, loss_recon: 0.609541, loss_pred: 0.129029
iteration 97: loss: 2.565881, loss_kl: 2.043147, loss_recon: 0.608292, loss_pred: 0.054139
iteration 98: loss: 2.207951, loss_kl: 1.548058, loss_recon: 0.608651, loss_pred: 0.165371
iteration 99: loss: 1.738768, loss_kl: 1.125720, loss_recon: 0.607477, loss_pred: 0.086301
iteration 100: loss: 1.999437, loss_kl: 1.382890, loss_recon: 0.605226, loss_pred: 0.110814
iteration 101: loss: 2.003519, loss_kl: 1.304880, loss_recon: 0.610167, loss_pred: 0.126542
iteration 102: loss: 1.998396, loss_kl: 1.318591, loss_recon: 0.607354, loss_pred: 0.110460
iteration 103: loss: 2.038958, loss_kl: 1.390461, loss_recon: 0.608937, loss_pred: 0.078633
iteration 104: loss: 2.570789, loss_kl: 1.925776, loss_recon: 0.612383, loss_pred: 0.086141
iteration 105: loss: 2.291646, loss_kl: 1.670910, loss_recon: 0.608775, loss_pred: 0.057943
iteration 106: loss: 1.858155, loss_kl: 1.235035, loss_recon: 0.609949, loss_pred: 0.047277
iteration 107: loss: 1.814798, loss_kl: 1.153480, loss_recon: 0.609188, loss_pred: 0.085072
iteration 108: loss: 2.005214, loss_kl: 1.380573, loss_recon: 0.608682, loss_pred: 0.054118
iteration 109: loss: 2.761859, loss_kl: 2.061312, loss_recon: 0.608061, loss_pred: 0.151336
iteration 110: loss: 2.563212, loss_kl: 1.925911, loss_recon: 0.605916, loss_pred: 0.084866
iteration 111: loss: 1.864420, loss_kl: 1.091344, loss_recon: 0.611483, loss_pred: 0.174701
iteration 112: loss: 1.802019, loss_kl: 1.121844, loss_recon: 0.608499, loss_pred: 0.084163
iteration 113: loss: 2.566498, loss_kl: 1.916835, loss_recon: 0.611026, loss_pred: 0.059095
iteration 114: loss: 2.380192, loss_kl: 1.688398, loss_recon: 0.610535, loss_pred: 0.099773
iteration 115: loss: 1.750125, loss_kl: 1.005353, loss_recon: 0.610147, loss_pred: 0.146552
iteration 116: loss: 2.000543, loss_kl: 1.293912, loss_recon: 0.612023, loss_pred: 0.109134
iteration 117: loss: 1.216642, loss_kl: 0.565819, loss_recon: 0.609855, loss_pred: 0.047316
iteration 118: loss: 1.972583, loss_kl: 1.311675, loss_recon: 0.610065, loss_pred: 0.065098
iteration 119: loss: 1.881357, loss_kl: 1.226487, loss_recon: 0.609123, loss_pred: 0.059057
iteration 120: loss: 1.455836, loss_kl: 0.785352, loss_recon: 0.606875, loss_pred: 0.072491
iteration 121: loss: 1.744672, loss_kl: 1.099561, loss_recon: 0.612245, loss_pred: 0.037403
iteration 122: loss: 2.341024, loss_kl: 1.678898, loss_recon: 0.608914, loss_pred: 0.060150
iteration 123: loss: 1.483087, loss_kl: 0.792322, loss_recon: 0.610469, loss_pred: 0.083792
iteration 124: loss: 1.804769, loss_kl: 1.118099, loss_recon: 0.610261, loss_pred: 0.081194
iteration 125: loss: 1.512475, loss_kl: 0.846756, loss_recon: 0.610446, loss_pred: 0.058886
iteration 126: loss: 1.095957, loss_kl: 0.414266, loss_recon: 0.611716, loss_pred: 0.071915
iteration 127: loss: 1.952485, loss_kl: 1.298095, loss_recon: 0.610670, loss_pred: 0.049094
iteration 128: loss: 1.370194, loss_kl: 0.722850, loss_recon: 0.610174, loss_pred: 0.040215
iteration 129: loss: 1.783547, loss_kl: 1.071659, loss_recon: 0.610589, loss_pred: 0.105997
iteration 130: loss: 1.637318, loss_kl: 0.914361, loss_recon: 0.605657, loss_pred: 0.121433
iteration 131: loss: 1.752485, loss_kl: 1.093871, loss_recon: 0.612325, loss_pred: 0.046289
iteration 132: loss: 1.984931, loss_kl: 1.314157, loss_recon: 0.609574, loss_pred: 0.061199
iteration 133: loss: 2.771438, loss_kl: 2.065289, loss_recon: 0.611869, loss_pred: 0.094280
iteration 134: loss: 1.677863, loss_kl: 0.886509, loss_recon: 0.611478, loss_pred: 0.179876
iteration 135: loss: 1.648099, loss_kl: 0.966847, loss_recon: 0.610252, loss_pred: 0.071000
iteration 136: loss: 2.259154, loss_kl: 1.440449, loss_recon: 0.611211, loss_pred: 0.207494
iteration 137: loss: 2.193732, loss_kl: 1.472534, loss_recon: 0.609958, loss_pred: 0.111240
iteration 138: loss: 1.983144, loss_kl: 1.274943, loss_recon: 0.610005, loss_pred: 0.098195
iteration 139: loss: 1.492999, loss_kl: 0.820750, loss_recon: 0.610030, loss_pred: 0.062219
iteration 140: loss: 1.794711, loss_kl: 1.054591, loss_recon: 0.606088, loss_pred: 0.134032
iteration 141: loss: 2.121795, loss_kl: 1.414583, loss_recon: 0.612353, loss_pred: 0.094859
iteration 142: loss: 2.523806, loss_kl: 1.726187, loss_recon: 0.611189, loss_pred: 0.186430
iteration 143: loss: 1.533145, loss_kl: 0.862978, loss_recon: 0.612255, loss_pred: 0.057912
iteration 144: loss: 1.936061, loss_kl: 1.245534, loss_recon: 0.611446, loss_pred: 0.079082
iteration 145: loss: 2.181134, loss_kl: 1.471587, loss_recon: 0.609379, loss_pred: 0.100168
iteration 146: loss: 1.719455, loss_kl: 1.051376, loss_recon: 0.610859, loss_pred: 0.057220
iteration 147: loss: 2.122929, loss_kl: 1.463008, loss_recon: 0.609886, loss_pred: 0.050035
iteration 148: loss: 1.606463, loss_kl: 0.927424, loss_recon: 0.610509, loss_pred: 0.068529
iteration 149: loss: 1.764545, loss_kl: 1.090920, loss_recon: 0.608857, loss_pred: 0.064768
iteration 150: loss: 2.173523, loss_kl: 1.501756, loss_recon: 0.604929, loss_pred: 0.066838
iteration 151: loss: 2.238451, loss_kl: 1.578419, loss_recon: 0.610711, loss_pred: 0.049320
iteration 152: loss: 1.848838, loss_kl: 1.185419, loss_recon: 0.608065, loss_pred: 0.055354
iteration 153: loss: 1.793147, loss_kl: 1.120010, loss_recon: 0.610880, loss_pred: 0.062257
iteration 154: loss: 1.931830, loss_kl: 1.240521, loss_recon: 0.609392, loss_pred: 0.081918
iteration 155: loss: 1.802786, loss_kl: 1.104264, loss_recon: 0.608560, loss_pred: 0.089962
iteration 156: loss: 1.695325, loss_kl: 1.011093, loss_recon: 0.609807, loss_pred: 0.074425
iteration 157: loss: 1.773248, loss_kl: 1.095821, loss_recon: 0.607808, loss_pred: 0.069618
iteration 158: loss: 1.524368, loss_kl: 0.860246, loss_recon: 0.608477, loss_pred: 0.055645
iteration 159: loss: 1.856478, loss_kl: 1.175884, loss_recon: 0.607923, loss_pred: 0.072672
iteration 160: loss: 2.513342, loss_kl: 1.832014, loss_recon: 0.605383, loss_pred: 0.075945
iteration 161: loss: 2.038746, loss_kl: 1.353758, loss_recon: 0.611219, loss_pred: 0.073769
iteration 162: loss: 1.960254, loss_kl: 1.259042, loss_recon: 0.608148, loss_pred: 0.093064
iteration 163: loss: 1.525910, loss_kl: 0.823525, loss_recon: 0.609602, loss_pred: 0.092783
iteration 164: loss: 1.648822, loss_kl: 0.955088, loss_recon: 0.609843, loss_pred: 0.083890
iteration 165: loss: 1.472120, loss_kl: 0.796158, loss_recon: 0.609875, loss_pred: 0.066088
iteration 166: loss: 1.365988, loss_kl: 0.707045, loss_recon: 0.610610, loss_pred: 0.048333
iteration 167: loss: 1.151509, loss_kl: 0.507354, loss_recon: 0.608700, loss_pred: 0.035455
iteration 168: loss: 1.358853, loss_kl: 0.681589, loss_recon: 0.610151, loss_pred: 0.067113
iteration 169: loss: 1.867357, loss_kl: 1.188816, loss_recon: 0.609227, loss_pred: 0.069313
iteration 170: loss: 1.784669, loss_kl: 1.123174, loss_recon: 0.604881, loss_pred: 0.056614
iteration 171: loss: 2.244686, loss_kl: 1.600012, loss_recon: 0.611135, loss_pred: 0.033539
iteration 172: loss: 1.576871, loss_kl: 0.925838, loss_recon: 0.608881, loss_pred: 0.042152
iteration 173: loss: 1.623583, loss_kl: 0.952036, loss_recon: 0.611640, loss_pred: 0.059907
iteration 174: loss: 1.377682, loss_kl: 0.681508, loss_recon: 0.610619, loss_pred: 0.085555
iteration 175: loss: 2.105380, loss_kl: 1.381521, loss_recon: 0.610562, loss_pred: 0.113296
iteration 176: loss: 2.017448, loss_kl: 1.340829, loss_recon: 0.611307, loss_pred: 0.065313
iteration 177: loss: 1.853209, loss_kl: 1.189552, loss_recon: 0.609896, loss_pred: 0.053761
iteration 178: loss: 1.842374, loss_kl: 1.162767, loss_recon: 0.610762, loss_pred: 0.068844
iteration 179: loss: 2.144648, loss_kl: 1.450823, loss_recon: 0.608631, loss_pred: 0.085194
iteration 180: loss: 2.055533, loss_kl: 1.403524, loss_recon: 0.605073, loss_pred: 0.046936
iteration 181: loss: 1.851990, loss_kl: 1.194059, loss_recon: 0.612146, loss_pred: 0.045784
iteration 182: loss: 1.769807, loss_kl: 1.111677, loss_recon: 0.609978, loss_pred: 0.048152
iteration 183: loss: 2.068789, loss_kl: 1.375374, loss_recon: 0.612257, loss_pred: 0.081159
iteration 184: loss: 1.941640, loss_kl: 1.249544, loss_recon: 0.609767, loss_pred: 0.082330
iteration 185: loss: 1.906599, loss_kl: 1.218772, loss_recon: 0.609115, loss_pred: 0.078712
iteration 186: loss: 1.667803, loss_kl: 1.012669, loss_recon: 0.610968, loss_pred: 0.044166
iteration 187: loss: 1.990709, loss_kl: 1.321584, loss_recon: 0.609625, loss_pred: 0.059500
iteration 188: loss: 2.005515, loss_kl: 1.343159, loss_recon: 0.609566, loss_pred: 0.052790
iteration 189: loss: 1.558549, loss_kl: 0.872653, loss_recon: 0.609676, loss_pred: 0.076219
iteration 190: loss: 2.252666, loss_kl: 1.524618, loss_recon: 0.609758, loss_pred: 0.118290
iteration 191: loss: 2.214074, loss_kl: 1.545137, loss_recon: 0.610419, loss_pred: 0.058519
iteration 192: loss: 1.358587, loss_kl: 0.702340, loss_recon: 0.608113, loss_pred: 0.048135
iteration 193: loss: 1.992767, loss_kl: 1.313695, loss_recon: 0.613120, loss_pred: 0.065952
iteration 194: loss: 1.213011, loss_kl: 0.519060, loss_recon: 0.612242, loss_pred: 0.081709
iteration 195: loss: 1.473282, loss_kl: 0.805415, loss_recon: 0.610528, loss_pred: 0.057339
iteration 196: loss: 1.573198, loss_kl: 0.923634, loss_recon: 0.610637, loss_pred: 0.038927
iteration 197: loss: 2.150382, loss_kl: 1.504107, loss_recon: 0.609225, loss_pred: 0.037049
iteration 198: loss: 1.284230, loss_kl: 0.618449, loss_recon: 0.609954, loss_pred: 0.055826
iteration 199: loss: 1.973934, loss_kl: 1.286037, loss_recon: 0.609521, loss_pred: 0.078377
iteration 200: loss: 1.780222, loss_kl: 1.087270, loss_recon: 0.606577, loss_pred: 0.086375
iteration 201: loss: 1.873585, loss_kl: 1.195495, loss_recon: 0.612857, loss_pred: 0.065233
iteration 202: loss: 2.334654, loss_kl: 1.690462, loss_recon: 0.609474, loss_pred: 0.034718
iteration 203: loss: 1.751274, loss_kl: 1.097246, loss_recon: 0.611124, loss_pred: 0.042904
iteration 204: loss: 1.512206, loss_kl: 0.833485, loss_recon: 0.609940, loss_pred: 0.068782
iteration 205: loss: 1.915330, loss_kl: 1.209887, loss_recon: 0.609378, loss_pred: 0.096065
iteration 206: loss: 1.392613, loss_kl: 0.728168, loss_recon: 0.610530, loss_pred: 0.053915
iteration 207: loss: 1.294759, loss_kl: 0.634349, loss_recon: 0.609325, loss_pred: 0.051085
iteration 208: loss: 1.581133, loss_kl: 0.924137, loss_recon: 0.609259, loss_pred: 0.047738
iteration 209: loss: 1.839816, loss_kl: 1.186078, loss_recon: 0.608434, loss_pred: 0.045304
iteration 210: loss: 1.104343, loss_kl: 0.439727, loss_recon: 0.605339, loss_pred: 0.059277
iteration 211: loss: 1.477348, loss_kl: 0.777759, loss_recon: 0.611536, loss_pred: 0.088053
iteration 212: loss: 1.260313, loss_kl: 0.580971, loss_recon: 0.608182, loss_pred: 0.071161
iteration 213: loss: 1.431261, loss_kl: 0.784419, loss_recon: 0.609998, loss_pred: 0.036843
iteration 214: loss: 1.812356, loss_kl: 1.145332, loss_recon: 0.609680, loss_pred: 0.057344
iteration 215: loss: 1.641064, loss_kl: 0.959049, loss_recon: 0.609826, loss_pred: 0.072189
iteration 216: loss: 1.726768, loss_kl: 1.063350, loss_recon: 0.610573, loss_pred: 0.052845
iteration 217: loss: 1.554726, loss_kl: 0.889645, loss_recon: 0.608567, loss_pred: 0.056514
iteration 218: loss: 2.075393, loss_kl: 1.440033, loss_recon: 0.608729, loss_pred: 0.026631
iteration 219: loss: 2.132071, loss_kl: 1.490479, loss_recon: 0.608123, loss_pred: 0.033468
iteration 220: loss: 2.080789, loss_kl: 1.429504, loss_recon: 0.604986, loss_pred: 0.046299
iteration 221: loss: 1.973337, loss_kl: 1.296227, loss_recon: 0.611032, loss_pred: 0.066077
iteration 222: loss: 1.625025, loss_kl: 0.933937, loss_recon: 0.608825, loss_pred: 0.082264
iteration 223: loss: 2.068251, loss_kl: 1.426235, loss_recon: 0.609512, loss_pred: 0.032504
iteration 224: loss: 2.295609, loss_kl: 1.636935, loss_recon: 0.609869, loss_pred: 0.048806
iteration 225: loss: 1.714648, loss_kl: 1.054018, loss_recon: 0.609676, loss_pred: 0.050954
iteration 226: loss: 2.131767, loss_kl: 1.463168, loss_recon: 0.610927, loss_pred: 0.057671
iteration 227: loss: 1.809400, loss_kl: 1.129537, loss_recon: 0.608952, loss_pred: 0.070911
iteration 228: loss: 1.514709, loss_kl: 0.879832, loss_recon: 0.609030, loss_pred: 0.025847
iteration 229: loss: 1.979969, loss_kl: 1.319948, loss_recon: 0.608558, loss_pred: 0.051464
iteration 230: loss: 1.673980, loss_kl: 1.014147, loss_recon: 0.604790, loss_pred: 0.055043
iteration 231: loss: 1.985036, loss_kl: 1.327596, loss_recon: 0.610874, loss_pred: 0.046566
iteration 232: loss: 2.009490, loss_kl: 1.291771, loss_recon: 0.608742, loss_pred: 0.108977
iteration 233: loss: 1.637418, loss_kl: 0.974567, loss_recon: 0.610321, loss_pred: 0.052530
iteration 234: loss: 2.020987, loss_kl: 1.377103, loss_recon: 0.609505, loss_pred: 0.034379
iteration 235: loss: 1.597323, loss_kl: 0.911603, loss_recon: 0.609029, loss_pred: 0.076691
iteration 236: loss: 0.951659, loss_kl: 0.309387, loss_recon: 0.609731, loss_pred: 0.032540
iteration 237: loss: 1.538664, loss_kl: 0.894463, loss_recon: 0.608858, loss_pred: 0.035343
iteration 238: loss: 1.463208, loss_kl: 0.815561, loss_recon: 0.608843, loss_pred: 0.038804
iteration 239: loss: 1.086169, loss_kl: 0.449746, loss_recon: 0.608415, loss_pred: 0.028007
iteration 240: loss: 1.838061, loss_kl: 1.197684, loss_recon: 0.604873, loss_pred: 0.035505
iteration 241: loss: 1.633658, loss_kl: 0.964700, loss_recon: 0.611079, loss_pred: 0.057880
iteration 242: loss: 1.902776, loss_kl: 1.262420, loss_recon: 0.608154, loss_pred: 0.032203
iteration 243: loss: 1.876093, loss_kl: 1.188488, loss_recon: 0.609773, loss_pred: 0.077831
iteration 244: loss: 1.846639, loss_kl: 1.187037, loss_recon: 0.609386, loss_pred: 0.050216
iteration 245: loss: 1.875201, loss_kl: 1.224390, loss_recon: 0.608662, loss_pred: 0.042149
iteration 246: loss: 1.385253, loss_kl: 0.740392, loss_recon: 0.609252, loss_pred: 0.035609
iteration 247: loss: 1.594229, loss_kl: 0.947276, loss_recon: 0.609022, loss_pred: 0.037931
iteration 248: loss: 1.232446, loss_kl: 0.587657, loss_recon: 0.608551, loss_pred: 0.036237
iteration 249: loss: 1.128035, loss_kl: 0.492791, loss_recon: 0.607482, loss_pred: 0.027761
iteration 250: loss: 1.377442, loss_kl: 0.730374, loss_recon: 0.604083, loss_pred: 0.042985
iteration 251: loss: 0.612331, loss_kl: 0.772185, loss_recon: 0.610306, loss_pred: 0.046634
iteration 252: loss: 0.611708, loss_kl: 1.128504, loss_recon: 0.608716, loss_pred: 0.081433
iteration 253: loss: 0.615750, loss_kl: 1.982803, loss_recon: 0.610488, loss_pred: 0.145540
iteration 254: loss: 0.617126, loss_kl: 1.927398, loss_recon: 0.611860, loss_pred: 0.202188
iteration 255: loss: 0.621183, loss_kl: 3.525239, loss_recon: 0.611873, loss_pred: 0.239824
iteration 256: loss: 0.618978, loss_kl: 2.862197, loss_recon: 0.611304, loss_pred: 0.241589
iteration 257: loss: 0.620197, loss_kl: 3.903619, loss_recon: 0.609909, loss_pred: 0.257243
iteration 258: loss: 0.623806, loss_kl: 5.503580, loss_recon: 0.609590, loss_pred: 0.246027
iteration 259: loss: 0.619622, loss_kl: 4.367335, loss_recon: 0.608206, loss_pred: 0.249765
iteration 260: loss: 0.618840, loss_kl: 5.438704, loss_recon: 0.604681, loss_pred: 0.287300
iteration 261: loss: 0.659160, loss_kl: 7.151457, loss_recon: 0.611422, loss_pred: 0.270307
iteration 262: loss: 0.659586, loss_kl: 7.544299, loss_recon: 0.609282, loss_pred: 0.276447
iteration 263: loss: 0.653984, loss_kl: 6.414478, loss_recon: 0.610838, loss_pred: 0.293401
iteration 264: loss: 0.662872, loss_kl: 7.951576, loss_recon: 0.609757, loss_pred: 0.306261
iteration 265: loss: 0.654309, loss_kl: 6.755988, loss_recon: 0.609123, loss_pred: 0.269073
iteration 266: loss: 0.657616, loss_kl: 7.116309, loss_recon: 0.610256, loss_pred: 0.246713
iteration 267: loss: 0.657784, loss_kl: 7.373358, loss_recon: 0.608894, loss_pred: 0.227505
iteration 268: loss: 0.664197, loss_kl: 8.381954, loss_recon: 0.608893, loss_pred: 0.216097
iteration 269: loss: 0.664921, loss_kl: 8.690289, loss_recon: 0.607866, loss_pred: 0.179994
iteration 270: loss: 0.662534, loss_kl: 8.790198, loss_recon: 0.604826, loss_pred: 0.181631
iteration 271: loss: 0.761682, loss_kl: 8.938723, loss_recon: 0.610731, loss_pred: 0.140270
iteration 272: loss: 0.754689, loss_kl: 8.712190, loss_recon: 0.607840, loss_pred: 0.120113
iteration 273: loss: 0.754860, loss_kl: 8.584580, loss_recon: 0.610306, loss_pred: 0.109666
iteration 274: loss: 0.732076, loss_kl: 7.275678, loss_recon: 0.609494, loss_pred: 0.097065
iteration 275: loss: 0.747675, loss_kl: 8.248415, loss_recon: 0.608790, loss_pred: 0.104872
iteration 276: loss: 0.739983, loss_kl: 7.746655, loss_recon: 0.609801, loss_pred: 0.083209
iteration 277: loss: 0.722070, loss_kl: 6.756249, loss_recon: 0.608241, loss_pred: 0.090014
iteration 278: loss: 0.712114, loss_kl: 6.174596, loss_recon: 0.608122, loss_pred: 0.080062
iteration 279: loss: 0.705031, loss_kl: 5.808924, loss_recon: 0.607053, loss_pred: 0.083959
iteration 280: loss: 0.698900, loss_kl: 5.589046, loss_recon: 0.604563, loss_pred: 0.084923
iteration 281: loss: 0.852875, loss_kl: 5.687299, loss_recon: 0.609655, loss_pred: 0.063987
iteration 282: loss: 0.819532, loss_kl: 4.935616, loss_recon: 0.607352, loss_pred: 0.081681
iteration 283: loss: 0.816121, loss_kl: 4.809024, loss_recon: 0.609553, loss_pred: 0.075547
iteration 284: loss: 0.768332, loss_kl: 3.708968, loss_recon: 0.608972, loss_pred: 0.059312
iteration 285: loss: 0.717815, loss_kl: 2.520205, loss_recon: 0.608587, loss_pred: 0.062657
iteration 286: loss: 0.740941, loss_kl: 3.076384, loss_recon: 0.609112, loss_pred: 0.040897
iteration 287: loss: 0.714169, loss_kl: 2.469642, loss_recon: 0.607748, loss_pred: 0.046841
iteration 288: loss: 0.745777, loss_kl: 3.226798, loss_recon: 0.607821, loss_pred: 0.035359
iteration 289: loss: 0.684752, loss_kl: 1.805375, loss_recon: 0.606756, loss_pred: 0.038952
iteration 290: loss: 0.678631, loss_kl: 1.732605, loss_recon: 0.603457, loss_pred: 0.044986
iteration 291: loss: 0.759432, loss_kl: 1.405531, loss_recon: 0.609787, loss_pred: 0.041714
iteration 292: loss: 0.708028, loss_kl: 0.938901, loss_recon: 0.607450, loss_pred: 0.033801
iteration 293: loss: 0.683588, loss_kl: 0.682744, loss_recon: 0.609058, loss_pred: 0.038046
iteration 294: loss: 0.695965, loss_kl: 0.816386, loss_recon: 0.608172, loss_pred: 0.032672
iteration 295: loss: 0.678773, loss_kl: 0.646791, loss_recon: 0.607877, loss_pred: 0.038851
iteration 296: loss: 0.677622, loss_kl: 0.634243, loss_recon: 0.609019, loss_pred: 0.029226
iteration 297: loss: 0.678452, loss_kl: 0.651397, loss_recon: 0.607648, loss_pred: 0.033357
iteration 298: loss: 0.683546, loss_kl: 0.704075, loss_recon: 0.607912, loss_pred: 0.027386
iteration 299: loss: 0.637727, loss_kl: 0.273613, loss_recon: 0.606657, loss_pred: 0.026873
iteration 300: loss: 0.669156, loss_kl: 0.606124, loss_recon: 0.603463, loss_pred: 0.029211
iteration 301: loss: 0.725388, loss_kl: 0.471764, loss_recon: 0.609587, loss_pred: 0.028509
iteration 302: loss: 0.782005, loss_kl: 0.730739, loss_recon: 0.607045, loss_pred: 0.025112
iteration 303: loss: 0.682714, loss_kl: 0.290574, loss_recon: 0.609337, loss_pred: 0.026425
iteration 304: loss: 0.699029, loss_kl: 0.362048, loss_recon: 0.608484, loss_pred: 0.029116
iteration 305: loss: 0.839445, loss_kl: 0.964015, loss_recon: 0.607592, loss_pred: 0.037617
iteration 306: loss: 0.732194, loss_kl: 0.504391, loss_recon: 0.608840, loss_pred: 0.028513
iteration 307: loss: 0.743606, loss_kl: 0.557477, loss_recon: 0.607425, loss_pred: 0.030841
iteration 308: loss: 0.754352, loss_kl: 0.607883, loss_recon: 0.607915, loss_pred: 0.024740
iteration 309: loss: 0.784898, loss_kl: 0.749649, loss_recon: 0.606450, loss_pred: 0.021267
iteration 310: loss: 0.678559, loss_kl: 0.294931, loss_recon: 0.603466, loss_pred: 0.029481
iteration 311: loss: 1.110871, loss_kl: 1.112080, loss_recon: 0.609847, loss_pred: 0.025869
iteration 312: loss: 0.884505, loss_kl: 0.604598, loss_recon: 0.607231, loss_pred: 0.025159
iteration 313: loss: 0.956310, loss_kl: 0.766190, loss_recon: 0.609196, loss_pred: 0.022193
iteration 314: loss: 0.903099, loss_kl: 0.635856, loss_recon: 0.608083, loss_pred: 0.034201
iteration 315: loss: 1.136359, loss_kl: 1.173059, loss_recon: 0.607741, loss_pred: 0.027565
iteration 316: loss: 0.952474, loss_kl: 0.757433, loss_recon: 0.608848, loss_pred: 0.023029
iteration 317: loss: 0.973139, loss_kl: 0.799435, loss_recon: 0.608025, loss_pred: 0.029830
iteration 318: loss: 1.012349, loss_kl: 0.894088, loss_recon: 0.608049, loss_pred: 0.024177
iteration 319: loss: 0.767353, loss_kl: 0.345027, loss_recon: 0.606333, loss_pred: 0.020690
iteration 320: loss: 0.969932, loss_kl: 0.802079, loss_recon: 0.603844, loss_pred: 0.029397
save model to ../model/TVG_Design[160, 160, 160]/TVG_pretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_131.pth
iteration 321: loss: 0.971287, loss_kl: 0.513522, loss_recon: 0.610239, loss_pred: 0.023267
iteration 322: loss: 0.923085, loss_kl: 0.435649, loss_recon: 0.607262, loss_pred: 0.033901
iteration 323: loss: 1.017149, loss_kl: 0.575821, loss_recon: 0.608898, loss_pred: 0.031148
iteration 324: loss: 1.012782, loss_kl: 0.553730, loss_recon: 0.608155, loss_pred: 0.047850
iteration 325: loss: 1.137317, loss_kl: 0.753558, loss_recon: 0.607681, loss_pred: 0.033879
iteration 326: loss: 0.960843, loss_kl: 0.501803, loss_recon: 0.609348, loss_pred: 0.020784
iteration 327: loss: 1.065284, loss_kl: 0.645907, loss_recon: 0.607592, loss_pred: 0.034568
iteration 328: loss: 1.019209, loss_kl: 0.583034, loss_recon: 0.608019, loss_pred: 0.028304
iteration 329: loss: 0.996497, loss_kl: 0.545816, loss_recon: 0.607354, loss_pred: 0.032744
iteration 330: loss: 0.953546, loss_kl: 0.489571, loss_recon: 0.603923, loss_pred: 0.030232
iteration 331: loss: 0.995316, loss_kl: 0.425543, loss_recon: 0.610211, loss_pred: 0.031336
iteration 332: loss: 1.013227, loss_kl: 0.446148, loss_recon: 0.607401, loss_pred: 0.035313
iteration 333: loss: 1.389744, loss_kl: 0.834166, loss_recon: 0.609415, loss_pred: 0.091596
iteration 334: loss: 1.264209, loss_kl: 0.733583, loss_recon: 0.608442, loss_pred: 0.044402
iteration 335: loss: 1.736194, loss_kl: 1.293023, loss_recon: 0.608139, loss_pred: 0.045271
iteration 336: loss: 1.918952, loss_kl: 1.503368, loss_recon: 0.608982, loss_pred: 0.050747
iteration 337: loss: 1.152256, loss_kl: 0.622270, loss_recon: 0.607896, loss_pred: 0.023544
iteration 338: loss: 1.022300, loss_kl: 0.460124, loss_recon: 0.608585, loss_pred: 0.030698
iteration 339: loss: 1.401932, loss_kl: 0.902226, loss_recon: 0.607212, loss_pred: 0.040610
iteration 340: loss: 1.136649, loss_kl: 0.593598, loss_recon: 0.603877, loss_pred: 0.038469
iteration 341: loss: 1.291762, loss_kl: 0.668197, loss_recon: 0.610123, loss_pred: 0.062084
iteration 342: loss: 1.406409, loss_kl: 0.824518, loss_recon: 0.607500, loss_pred: 0.031402
iteration 343: loss: 1.204085, loss_kl: 0.596922, loss_recon: 0.609949, loss_pred: 0.039613
iteration 344: loss: 1.276426, loss_kl: 0.658691, loss_recon: 0.608887, loss_pred: 0.056484
iteration 345: loss: 1.358945, loss_kl: 0.751651, loss_recon: 0.607962, loss_pred: 0.052923
iteration 346: loss: 2.391507, loss_kl: 1.877719, loss_recon: 0.609565, loss_pred: 0.031384
iteration 347: loss: 1.336117, loss_kl: 0.744081, loss_recon: 0.608451, loss_pred: 0.035513
iteration 348: loss: 1.896860, loss_kl: 1.332438, loss_recon: 0.608374, loss_pred: 0.047996
iteration 349: loss: 1.799032, loss_kl: 1.217019, loss_recon: 0.606840, loss_pred: 0.060250
iteration 350: loss: 1.725598, loss_kl: 1.141056, loss_recon: 0.603722, loss_pred: 0.060877
iteration 351: loss: 1.499997, loss_kl: 0.886759, loss_recon: 0.610880, loss_pred: 0.026651
iteration 352: loss: 1.518732, loss_kl: 0.904087, loss_recon: 0.608448, loss_pred: 0.031069
iteration 353: loss: 1.252525, loss_kl: 0.625662, loss_recon: 0.610710, loss_pred: 0.033689
iteration 354: loss: 1.603268, loss_kl: 0.973146, loss_recon: 0.609387, loss_pred: 0.047891
iteration 355: loss: 1.739429, loss_kl: 1.102346, loss_recon: 0.609165, loss_pred: 0.058801
iteration 356: loss: 1.871203, loss_kl: 1.266361, loss_recon: 0.609799, loss_pred: 0.029510
iteration 357: loss: 1.766167, loss_kl: 1.162540, loss_recon: 0.608234, loss_pred: 0.027032
iteration 358: loss: 1.921068, loss_kl: 1.303941, loss_recon: 0.608809, loss_pred: 0.044174
iteration 359: loss: 2.181360, loss_kl: 1.572911, loss_recon: 0.607729, loss_pred: 0.043717
iteration 360: loss: 1.800622, loss_kl: 1.185564, loss_recon: 0.605318, loss_pred: 0.042400
iteration 361: loss: 1.480118, loss_kl: 0.850781, loss_recon: 0.609871, loss_pred: 0.028570
iteration 362: loss: 1.456056, loss_kl: 0.822728, loss_recon: 0.607328, loss_pred: 0.034880
iteration 363: loss: 1.671427, loss_kl: 1.007277, loss_recon: 0.610446, loss_pred: 0.064803
iteration 364: loss: 1.342769, loss_kl: 0.701922, loss_recon: 0.608786, loss_pred: 0.039740
iteration 365: loss: 1.657066, loss_kl: 0.965781, loss_recon: 0.608530, loss_pred: 0.093725
iteration 366: loss: 1.947006, loss_kl: 1.281067, loss_recon: 0.609659, loss_pred: 0.070271
iteration 367: loss: 1.631605, loss_kl: 0.991815, loss_recon: 0.607709, loss_pred: 0.042793
iteration 368: loss: 2.329052, loss_kl: 1.696802, loss_recon: 0.608664, loss_pred: 0.041586
iteration 369: loss: 2.509365, loss_kl: 1.882770, loss_recon: 0.607836, loss_pred: 0.038653
iteration 370: loss: 1.871024, loss_kl: 1.233856, loss_recon: 0.605045, loss_pred: 0.045368
iteration 371: loss: 1.689899, loss_kl: 1.054411, loss_recon: 0.610762, loss_pred: 0.029048
iteration 372: loss: 1.365312, loss_kl: 0.713749, loss_recon: 0.608110, loss_pred: 0.046486
iteration 373: loss: 1.827641, loss_kl: 1.166777, loss_recon: 0.610940, loss_pred: 0.054799
iteration 374: loss: 1.618494, loss_kl: 0.979436, loss_recon: 0.609593, loss_pred: 0.033507
iteration 375: loss: 2.183821, loss_kl: 1.522554, loss_recon: 0.610422, loss_pred: 0.057148
iteration 376: loss: 1.546963, loss_kl: 0.880304, loss_recon: 0.610003, loss_pred: 0.060410
iteration 377: loss: 1.799016, loss_kl: 1.171989, loss_recon: 0.608596, loss_pred: 0.023199
iteration 378: loss: 1.685854, loss_kl: 1.042988, loss_recon: 0.609538, loss_pred: 0.037640
iteration 379: loss: 1.792293, loss_kl: 1.132476, loss_recon: 0.608256, loss_pred: 0.056305
iteration 380: loss: 1.551280, loss_kl: 0.897229, loss_recon: 0.604474, loss_pred: 0.053370
iteration 381: loss: 1.892193, loss_kl: 1.241548, loss_recon: 0.610416, loss_pred: 0.040230
iteration 382: loss: 1.932560, loss_kl: 1.283160, loss_recon: 0.608656, loss_pred: 0.040745
iteration 383: loss: 1.360867, loss_kl: 0.702511, loss_recon: 0.611242, loss_pred: 0.047115
iteration 384: loss: 1.568447, loss_kl: 0.909412, loss_recon: 0.610743, loss_pred: 0.048292
iteration 385: loss: 1.191932, loss_kl: 0.545576, loss_recon: 0.609099, loss_pred: 0.037257
iteration 386: loss: 1.318097, loss_kl: 0.664784, loss_recon: 0.610103, loss_pred: 0.043210
iteration 387: loss: 1.892373, loss_kl: 1.236063, loss_recon: 0.608954, loss_pred: 0.047355
iteration 388: loss: 2.066260, loss_kl: 1.430189, loss_recon: 0.608968, loss_pred: 0.027104
iteration 389: loss: 1.813764, loss_kl: 1.172143, loss_recon: 0.608189, loss_pred: 0.033432
iteration 390: loss: 1.552601, loss_kl: 0.920598, loss_recon: 0.604327, loss_pred: 0.027676
iteration 391: loss: 1.846028, loss_kl: 1.200088, loss_recon: 0.610839, loss_pred: 0.035101
iteration 392: loss: 1.869759, loss_kl: 1.236377, loss_recon: 0.608199, loss_pred: 0.025183
iteration 393: loss: 1.953412, loss_kl: 1.316611, loss_recon: 0.611258, loss_pred: 0.025544
iteration 394: loss: 1.510491, loss_kl: 0.869321, loss_recon: 0.609749, loss_pred: 0.031421
iteration 395: loss: 1.412689, loss_kl: 0.759102, loss_recon: 0.608458, loss_pred: 0.045129
iteration 396: loss: 1.742670, loss_kl: 1.098671, loss_recon: 0.609400, loss_pred: 0.034600
iteration 397: loss: 1.665484, loss_kl: 1.026897, loss_recon: 0.608478, loss_pred: 0.030109
iteration 398: loss: 1.844850, loss_kl: 1.214719, loss_recon: 0.609471, loss_pred: 0.020660
iteration 399: loss: 2.074836, loss_kl: 1.431877, loss_recon: 0.608233, loss_pred: 0.034726
iteration 400: loss: 1.866641, loss_kl: 1.222383, loss_recon: 0.605485, loss_pred: 0.038773
iteration 401: loss: 2.262097, loss_kl: 1.608479, loss_recon: 0.610868, loss_pred: 0.042750
iteration 402: loss: 2.204738, loss_kl: 1.539500, loss_recon: 0.608335, loss_pred: 0.056903
iteration 403: loss: 1.853654, loss_kl: 1.208285, loss_recon: 0.610648, loss_pred: 0.034721
iteration 404: loss: 1.781039, loss_kl: 1.148703, loss_recon: 0.610278, loss_pred: 0.022057
iteration 405: loss: 1.582804, loss_kl: 0.923132, loss_recon: 0.609928, loss_pred: 0.049744
iteration 406: loss: 1.107742, loss_kl: 0.464889, loss_recon: 0.610416, loss_pred: 0.032438
iteration 407: loss: 1.713341, loss_kl: 1.041956, loss_recon: 0.607927, loss_pred: 0.063458
iteration 408: loss: 1.609190, loss_kl: 0.958788, loss_recon: 0.608667, loss_pred: 0.041735
iteration 409: loss: 1.086485, loss_kl: 0.447310, loss_recon: 0.608589, loss_pred: 0.030585
iteration 410: loss: 1.462556, loss_kl: 0.822833, loss_recon: 0.605032, loss_pred: 0.034692
iteration 411: loss: 1.642772, loss_kl: 0.999851, loss_recon: 0.611634, loss_pred: 0.031288
iteration 412: loss: 1.773901, loss_kl: 1.114967, loss_recon: 0.609626, loss_pred: 0.049308
iteration 413: loss: 1.806531, loss_kl: 1.171747, loss_recon: 0.610421, loss_pred: 0.024363
iteration 414: loss: 1.735401, loss_kl: 1.063121, loss_recon: 0.610123, loss_pred: 0.062157
iteration 415: loss: 1.992085, loss_kl: 1.346876, loss_recon: 0.610442, loss_pred: 0.034768
iteration 416: loss: 1.744085, loss_kl: 1.084947, loss_recon: 0.610749, loss_pred: 0.048389
iteration 417: loss: 1.461311, loss_kl: 0.812361, loss_recon: 0.608461, loss_pred: 0.040490
iteration 418: loss: 1.395695, loss_kl: 0.760138, loss_recon: 0.608665, loss_pred: 0.026893
iteration 419: loss: 1.904174, loss_kl: 1.266044, loss_recon: 0.608570, loss_pred: 0.029560
iteration 420: loss: 1.463777, loss_kl: 0.831979, loss_recon: 0.604496, loss_pred: 0.027302
iteration 421: loss: 1.541079, loss_kl: 0.904919, loss_recon: 0.611203, loss_pred: 0.024958
iteration 422: loss: 1.802769, loss_kl: 1.157356, loss_recon: 0.607419, loss_pred: 0.037994
iteration 423: loss: 1.874178, loss_kl: 1.239606, loss_recon: 0.609824, loss_pred: 0.024748
iteration 424: loss: 1.777985, loss_kl: 1.132111, loss_recon: 0.609997, loss_pred: 0.035878
iteration 425: loss: 1.974341, loss_kl: 1.318372, loss_recon: 0.609365, loss_pred: 0.046604
iteration 426: loss: 1.533576, loss_kl: 0.891931, loss_recon: 0.610345, loss_pred: 0.031300
iteration 427: loss: 1.763921, loss_kl: 1.110822, loss_recon: 0.608865, loss_pred: 0.044234
iteration 428: loss: 1.749527, loss_kl: 1.120360, loss_recon: 0.608230, loss_pred: 0.020937
iteration 429: loss: 1.482050, loss_kl: 0.841435, loss_recon: 0.608342, loss_pred: 0.032274
iteration 430: loss: 0.846311, loss_kl: 0.213472, loss_recon: 0.604606, loss_pred: 0.028234
iteration 431: loss: 1.761128, loss_kl: 1.108057, loss_recon: 0.610944, loss_pred: 0.042126
iteration 432: loss: 1.928981, loss_kl: 1.271762, loss_recon: 0.608118, loss_pred: 0.049101
iteration 433: loss: 1.786419, loss_kl: 1.136541, loss_recon: 0.609626, loss_pred: 0.040253
iteration 434: loss: 1.686148, loss_kl: 1.057577, loss_recon: 0.609348, loss_pred: 0.019222
iteration 435: loss: 1.959783, loss_kl: 1.308910, loss_recon: 0.609217, loss_pred: 0.041655
iteration 436: loss: 1.991369, loss_kl: 1.350929, loss_recon: 0.609898, loss_pred: 0.030542
iteration 437: loss: 1.250838, loss_kl: 0.600978, loss_recon: 0.608358, loss_pred: 0.041502
iteration 438: loss: 1.722916, loss_kl: 1.082660, loss_recon: 0.608361, loss_pred: 0.031895
iteration 439: loss: 1.800423, loss_kl: 1.162642, loss_recon: 0.608066, loss_pred: 0.029715
iteration 440: loss: 1.347678, loss_kl: 0.717719, loss_recon: 0.603736, loss_pred: 0.026224
iteration 441: loss: 1.249633, loss_kl: 0.611061, loss_recon: 0.610751, loss_pred: 0.027820
iteration 442: loss: 1.384776, loss_kl: 0.730589, loss_recon: 0.608323, loss_pred: 0.045864
iteration 443: loss: 1.721918, loss_kl: 1.077610, loss_recon: 0.609738, loss_pred: 0.034570
iteration 444: loss: 1.770177, loss_kl: 1.100929, loss_recon: 0.609033, loss_pred: 0.060215
iteration 445: loss: 1.599592, loss_kl: 0.955449, loss_recon: 0.608741, loss_pred: 0.035402
iteration 446: loss: 1.478993, loss_kl: 0.842905, loss_recon: 0.609783, loss_pred: 0.026305
iteration 447: loss: 1.702463, loss_kl: 1.057706, loss_recon: 0.608727, loss_pred: 0.036029
iteration 448: loss: 1.421558, loss_kl: 0.787913, loss_recon: 0.608445, loss_pred: 0.025200
iteration 449: loss: 0.981715, loss_kl: 0.354228, loss_recon: 0.607476, loss_pred: 0.020011
iteration 450: loss: 1.537090, loss_kl: 0.890519, loss_recon: 0.605396, loss_pred: 0.041174
iteration 451: loss: 1.384757, loss_kl: 0.723937, loss_recon: 0.610649, loss_pred: 0.050172
iteration 452: loss: 1.505357, loss_kl: 0.874475, loss_recon: 0.607581, loss_pred: 0.023302
iteration 453: loss: 1.774444, loss_kl: 1.124724, loss_recon: 0.609586, loss_pred: 0.040134
iteration 454: loss: 1.724207, loss_kl: 1.093863, loss_recon: 0.609604, loss_pred: 0.020741
iteration 455: loss: 1.630371, loss_kl: 0.978005, loss_recon: 0.608913, loss_pred: 0.043453
iteration 456: loss: 1.591257, loss_kl: 0.942630, loss_recon: 0.609678, loss_pred: 0.038949
iteration 457: loss: 1.973792, loss_kl: 1.334066, loss_recon: 0.608550, loss_pred: 0.031176
iteration 458: loss: 1.053215, loss_kl: 0.387724, loss_recon: 0.608540, loss_pred: 0.056951
iteration 459: loss: 1.297031, loss_kl: 0.665440, loss_recon: 0.607783, loss_pred: 0.023809
iteration 460: loss: 1.502560, loss_kl: 0.869483, loss_recon: 0.604786, loss_pred: 0.028290
iteration 461: loss: 1.412080, loss_kl: 0.742218, loss_recon: 0.610608, loss_pred: 0.059254
iteration 462: loss: 1.864407, loss_kl: 1.227558, loss_recon: 0.607939, loss_pred: 0.028909
iteration 463: loss: 1.372360, loss_kl: 0.724226, loss_recon: 0.610241, loss_pred: 0.037893
iteration 464: loss: 1.277817, loss_kl: 0.625684, loss_recon: 0.609091, loss_pred: 0.043042
iteration 465: loss: 1.528376, loss_kl: 0.882017, loss_recon: 0.609017, loss_pred: 0.037342
iteration 466: loss: 1.640994, loss_kl: 0.988824, loss_recon: 0.609850, loss_pred: 0.042321
iteration 467: loss: 2.227666, loss_kl: 1.584306, loss_recon: 0.608490, loss_pred: 0.034870
iteration 468: loss: 1.566561, loss_kl: 0.921883, loss_recon: 0.608570, loss_pred: 0.036109
iteration 469: loss: 1.779107, loss_kl: 1.135763, loss_recon: 0.607534, loss_pred: 0.035809
iteration 470: loss: 1.535392, loss_kl: 0.906477, loss_recon: 0.604365, loss_pred: 0.024550
iteration 471: loss: 1.495815, loss_kl: 0.850195, loss_recon: 0.610774, loss_pred: 0.034846
iteration 472: loss: 1.344923, loss_kl: 0.697796, loss_recon: 0.607604, loss_pred: 0.039523
iteration 473: loss: 1.616496, loss_kl: 0.984378, loss_recon: 0.609914, loss_pred: 0.022204
iteration 474: loss: 1.737819, loss_kl: 1.103539, loss_recon: 0.608676, loss_pred: 0.025604
iteration 475: loss: 1.700859, loss_kl: 1.058025, loss_recon: 0.608730, loss_pred: 0.034103
iteration 476: loss: 1.872544, loss_kl: 1.243401, loss_recon: 0.609556, loss_pred: 0.019587
iteration 477: loss: 1.543622, loss_kl: 0.911000, loss_recon: 0.608072, loss_pred: 0.024550
iteration 478: loss: 1.446581, loss_kl: 0.810745, loss_recon: 0.608439, loss_pred: 0.027397
iteration 479: loss: 1.087085, loss_kl: 0.451919, loss_recon: 0.607565, loss_pred: 0.027601
iteration 480: loss: 1.648821, loss_kl: 1.024443, loss_recon: 0.604386, loss_pred: 0.019991
iteration 481: loss: 2.143145, loss_kl: 1.514996, loss_recon: 0.610700, loss_pred: 0.017449
iteration 482: loss: 1.145266, loss_kl: 0.509866, loss_recon: 0.607286, loss_pred: 0.028114
iteration 483: loss: 2.051265, loss_kl: 1.415400, loss_recon: 0.609827, loss_pred: 0.026039
iteration 484: loss: 1.966281, loss_kl: 1.328961, loss_recon: 0.609159, loss_pred: 0.028161
iteration 485: loss: 1.233575, loss_kl: 0.597227, loss_recon: 0.608205, loss_pred: 0.028143
iteration 486: loss: 1.638201, loss_kl: 0.989751, loss_recon: 0.609339, loss_pred: 0.039111
iteration 487: loss: 0.918822, loss_kl: 0.280557, loss_recon: 0.608410, loss_pred: 0.029854
iteration 488: loss: 2.156440, loss_kl: 1.511486, loss_recon: 0.608192, loss_pred: 0.036762
iteration 489: loss: 1.612139, loss_kl: 0.977793, loss_recon: 0.607289, loss_pred: 0.027057
iteration 490: loss: 1.461646, loss_kl: 0.814979, loss_recon: 0.604236, loss_pred: 0.042431
iteration 491: loss: 1.699162, loss_kl: 1.071064, loss_recon: 0.609862, loss_pred: 0.018236
iteration 492: loss: 1.625454, loss_kl: 0.998858, loss_recon: 0.607576, loss_pred: 0.019020
iteration 493: loss: 1.961940, loss_kl: 1.316998, loss_recon: 0.610018, loss_pred: 0.034925
iteration 494: loss: 1.513470, loss_kl: 0.878533, loss_recon: 0.609158, loss_pred: 0.025779
iteration 495: loss: 2.182292, loss_kl: 1.548097, loss_recon: 0.608443, loss_pred: 0.025753
iteration 496: loss: 1.780951, loss_kl: 1.140601, loss_recon: 0.609666, loss_pred: 0.030683
iteration 497: loss: 1.628773, loss_kl: 0.988427, loss_recon: 0.608285, loss_pred: 0.032061
iteration 498: loss: 1.344687, loss_kl: 0.715513, loss_recon: 0.608871, loss_pred: 0.020302
iteration 499: loss: 1.318388, loss_kl: 0.677612, loss_recon: 0.607897, loss_pred: 0.032879
iteration 500: loss: 1.904414, loss_kl: 1.258383, loss_recon: 0.604745, loss_pred: 0.041285
iteration 501: loss: 0.613004, loss_kl: 1.237130, loss_recon: 0.609902, loss_pred: 0.017500
iteration 502: loss: 0.610501, loss_kl: 1.077855, loss_recon: 0.607786, loss_pred: 0.019965
iteration 503: loss: 0.613870, loss_kl: 1.006726, loss_recon: 0.611261, loss_pred: 0.048426
iteration 504: loss: 0.615294, loss_kl: 1.939831, loss_recon: 0.610301, loss_pred: 0.079429
iteration 505: loss: 0.615462, loss_kl: 2.602179, loss_recon: 0.608720, loss_pred: 0.124160
iteration 506: loss: 0.616971, loss_kl: 2.306503, loss_recon: 0.610815, loss_pred: 0.183243
iteration 507: loss: 0.617515, loss_kl: 3.283939, loss_recon: 0.608876, loss_pred: 0.210057
iteration 508: loss: 0.616996, loss_kl: 2.825433, loss_recon: 0.609411, loss_pred: 0.242167
iteration 509: loss: 0.619879, loss_kl: 4.605465, loss_recon: 0.607755, loss_pred: 0.297949
iteration 510: loss: 0.615182, loss_kl: 3.654026, loss_recon: 0.605278, loss_pred: 0.351444
iteration 511: loss: 0.640093, loss_kl: 4.309114, loss_recon: 0.610051, loss_pred: 0.361615
iteration 512: loss: 0.648143, loss_kl: 5.933005, loss_recon: 0.607605, loss_pred: 0.369419
iteration 513: loss: 0.644709, loss_kl: 4.968916, loss_recon: 0.610188, loss_pred: 0.398074
iteration 514: loss: 0.640234, loss_kl: 4.442612, loss_recon: 0.609249, loss_pred: 0.374624
iteration 515: loss: 0.651127, loss_kl: 6.201549, loss_recon: 0.608796, loss_pred: 0.379625
iteration 516: loss: 0.647810, loss_kl: 5.596640, loss_recon: 0.609495, loss_pred: 0.360282
iteration 517: loss: 0.648645, loss_kl: 6.026810, loss_recon: 0.607954, loss_pred: 0.299486
iteration 518: loss: 0.656442, loss_kl: 7.163312, loss_recon: 0.608664, loss_pred: 0.264812
iteration 519: loss: 0.635766, loss_kl: 4.230623, loss_recon: 0.607051, loss_pred: 0.233680
iteration 520: loss: 0.642324, loss_kl: 5.779759, loss_recon: 0.603805, loss_pred: 0.208731
iteration 521: loss: 0.702472, loss_kl: 5.427536, loss_recon: 0.609689, loss_pred: 0.152907
iteration 522: loss: 0.717778, loss_kl: 6.542090, loss_recon: 0.606901, loss_pred: 0.126659
iteration 523: loss: 0.696868, loss_kl: 5.172505, loss_recon: 0.609155, loss_pred: 0.103004
iteration 524: loss: 0.694085, loss_kl: 5.086210, loss_recon: 0.608231, loss_pred: 0.077513
iteration 525: loss: 0.688147, loss_kl: 4.767282, loss_recon: 0.607632, loss_pred: 0.075338
iteration 526: loss: 0.699087, loss_kl: 5.388516, loss_recon: 0.608701, loss_pred: 0.047835
iteration 527: loss: 0.665870, loss_kl: 3.444801, loss_recon: 0.607383, loss_pred: 0.072874
iteration 528: loss: 0.669419, loss_kl: 3.647159, loss_recon: 0.607665, loss_pred: 0.067044
iteration 529: loss: 0.664095, loss_kl: 3.385252, loss_recon: 0.606561, loss_pred: 0.075119
iteration 530: loss: 0.650752, loss_kl: 2.789266, loss_recon: 0.603285, loss_pred: 0.065610
iteration 531: loss: 0.704273, loss_kl: 2.157892, loss_recon: 0.609755, loss_pred: 0.077112
iteration 532: loss: 0.690617, loss_kl: 1.899843, loss_recon: 0.607084, loss_pred: 0.075424
iteration 533: loss: 0.689869, loss_kl: 1.850351, loss_recon: 0.608868, loss_pred: 0.065018
iteration 534: loss: 0.692052, loss_kl: 1.939585, loss_recon: 0.608257, loss_pred: 0.041865
iteration 535: loss: 0.654617, loss_kl: 1.057631, loss_recon: 0.607719, loss_pred: 0.051338
iteration 536: loss: 0.666071, loss_kl: 1.320940, loss_recon: 0.608428, loss_pred: 0.042103
iteration 537: loss: 0.667407, loss_kl: 1.375808, loss_recon: 0.607436, loss_pred: 0.042286
iteration 538: loss: 0.628939, loss_kl: 0.461411, loss_recon: 0.607908, loss_pred: 0.035889
iteration 539: loss: 0.631792, loss_kl: 0.557202, loss_recon: 0.606690, loss_pred: 0.036353
iteration 540: loss: 0.630355, loss_kl: 0.591103, loss_recon: 0.603039, loss_pred: 0.054811
iteration 541: loss: 0.679746, loss_kl: 0.642083, loss_recon: 0.609436, loss_pred: 0.037893
iteration 542: loss: 0.648023, loss_kl: 0.360338, loss_recon: 0.606848, loss_pred: 0.037871
iteration 543: loss: 0.629846, loss_kl: 0.167120, loss_recon: 0.608511, loss_pred: 0.039212
iteration 544: loss: 0.654785, loss_kl: 0.422591, loss_recon: 0.608155, loss_pred: 0.028370
iteration 545: loss: 0.663715, loss_kl: 0.512911, loss_recon: 0.607725, loss_pred: 0.028570
iteration 546: loss: 0.617857, loss_kl: 0.068355, loss_recon: 0.608478, loss_pred: 0.022349
iteration 547: loss: 0.647390, loss_kl: 0.367335, loss_recon: 0.607157, loss_pred: 0.021768
iteration 548: loss: 0.672662, loss_kl: 0.605303, loss_recon: 0.607799, loss_pred: 0.021999
iteration 549: loss: 0.650313, loss_kl: 0.397377, loss_recon: 0.606036, loss_pred: 0.030835
iteration 550: loss: 0.666856, loss_kl: 0.573347, loss_recon: 0.603484, loss_pred: 0.039529
iteration 551: loss: 0.744599, loss_kl: 0.562501, loss_recon: 0.609583, loss_pred: 0.020781
iteration 552: loss: 0.676897, loss_kl: 0.281275, loss_recon: 0.606933, loss_pred: 0.020976
iteration 553: loss: 0.744943, loss_kl: 0.569257, loss_recon: 0.608855, loss_pred: 0.018660
iteration 554: loss: 0.725183, loss_kl: 0.485923, loss_recon: 0.608029, loss_pred: 0.020197
iteration 555: loss: 0.741628, loss_kl: 0.553771, loss_recon: 0.608065, loss_pred: 0.023236
iteration 556: loss: 0.712713, loss_kl: 0.429077, loss_recon: 0.608589, loss_pred: 0.020751
iteration 557: loss: 0.805773, loss_kl: 0.835499, loss_recon: 0.607415, loss_pred: 0.021432
iteration 558: loss: 0.807367, loss_kl: 0.843761, loss_recon: 0.607713, loss_pred: 0.018768
iteration 559: loss: 0.696188, loss_kl: 0.363721, loss_recon: 0.606596, loss_pred: 0.023326
iteration 560: loss: 0.755067, loss_kl: 0.630430, loss_recon: 0.603528, loss_pred: 0.024236
iteration 561: loss: 0.947509, loss_kl: 0.749135, loss_recon: 0.609691, loss_pred: 0.018134
iteration 562: loss: 1.006368, loss_kl: 0.888184, loss_recon: 0.606945, loss_pred: 0.019004
iteration 563: loss: 1.077359, loss_kl: 1.047402, loss_recon: 0.608762, loss_pred: 0.016897
iteration 564: loss: 0.946382, loss_kl: 0.752850, loss_recon: 0.607914, loss_pred: 0.015896
iteration 565: loss: 0.948577, loss_kl: 0.748866, loss_recon: 0.607923, loss_pred: 0.024845
iteration 566: loss: 0.872284, loss_kl: 0.575800, loss_recon: 0.608736, loss_pred: 0.022784
iteration 567: loss: 0.944591, loss_kl: 0.746322, loss_recon: 0.607277, loss_pred: 0.019802
iteration 568: loss: 0.852419, loss_kl: 0.538915, loss_recon: 0.608191, loss_pred: 0.015789
iteration 569: loss: 1.033344, loss_kl: 0.954392, loss_recon: 0.607138, loss_pred: 0.013628
iteration 570: loss: 0.753511, loss_kl: 0.324576, loss_recon: 0.603263, loss_pred: 0.016673
iteration 571: loss: 0.969394, loss_kl: 0.515351, loss_recon: 0.609535, loss_pred: 0.019671
iteration 572: loss: 0.901340, loss_kl: 0.413244, loss_recon: 0.607391, loss_pred: 0.023786
iteration 573: loss: 1.042405, loss_kl: 0.621974, loss_recon: 0.609563, loss_pred: 0.021554
iteration 574: loss: 0.980877, loss_kl: 0.533847, loss_recon: 0.608795, loss_pred: 0.019347
iteration 575: loss: 0.991687, loss_kl: 0.526885, loss_recon: 0.608177, loss_pred: 0.043299
iteration 576: loss: 1.136413, loss_kl: 0.746347, loss_recon: 0.609264, loss_pred: 0.037394
iteration 577: loss: 1.030735, loss_kl: 0.600399, loss_recon: 0.607899, loss_pred: 0.028253
iteration 578: loss: 1.221273, loss_kl: 0.888014, loss_recon: 0.609537, loss_pred: 0.021485
iteration 579: loss: 1.002333, loss_kl: 0.545682, loss_recon: 0.608925, loss_pred: 0.039218
iteration 580: loss: 1.313478, loss_kl: 1.016297, loss_recon: 0.604805, loss_pred: 0.037325
iteration 581: loss: 1.266413, loss_kl: 0.721946, loss_recon: 0.609804, loss_pred: 0.057038
iteration 582: loss: 1.705381, loss_kl: 1.272041, loss_recon: 0.607973, loss_pred: 0.029895
iteration 583: loss: 1.413290, loss_kl: 0.902556, loss_recon: 0.610986, loss_pred: 0.049277
iteration 584: loss: 1.085414, loss_kl: 0.528441, loss_recon: 0.610094, loss_pred: 0.035467
iteration 585: loss: 1.619226, loss_kl: 1.154151, loss_recon: 0.609677, loss_pred: 0.043551
iteration 586: loss: 1.670121, loss_kl: 1.200471, loss_recon: 0.609614, loss_pred: 0.057687
iteration 587: loss: 1.514837, loss_kl: 1.035417, loss_recon: 0.608454, loss_pred: 0.039892
iteration 588: loss: 1.488564, loss_kl: 1.015588, loss_recon: 0.609328, loss_pred: 0.027514
iteration 589: loss: 1.074823, loss_kl: 0.523176, loss_recon: 0.608255, loss_pred: 0.030348
iteration 590: loss: 1.097328, loss_kl: 0.563962, loss_recon: 0.604684, loss_pred: 0.020498
iteration 591: loss: 1.138563, loss_kl: 0.527009, loss_recon: 0.610842, loss_pred: 0.038371
iteration 592: loss: 1.733347, loss_kl: 1.175781, loss_recon: 0.607896, loss_pred: 0.029984
iteration 593: loss: 1.383538, loss_kl: 0.792826, loss_recon: 0.609789, loss_pred: 0.036139
iteration 594: loss: 1.522817, loss_kl: 0.944854, loss_recon: 0.609994, loss_pred: 0.033109
iteration 595: loss: 1.743786, loss_kl: 1.174453, loss_recon: 0.609872, loss_pred: 0.040378
iteration 596: loss: 1.422114, loss_kl: 0.834562, loss_recon: 0.610325, loss_pred: 0.035157
iteration 597: loss: 1.274730, loss_kl: 0.661603, loss_recon: 0.608911, loss_pred: 0.051730
iteration 598: loss: 1.819266, loss_kl: 1.267173, loss_recon: 0.608847, loss_pred: 0.029623
iteration 599: loss: 1.262440, loss_kl: 0.683225, loss_recon: 0.608065, loss_pred: 0.017846
iteration 600: loss: 1.297066, loss_kl: 0.716990, loss_recon: 0.606278, loss_pred: 0.023094
iteration 601: loss: 1.307834, loss_kl: 0.674232, loss_recon: 0.612348, loss_pred: 0.040257
iteration 602: loss: 1.170283, loss_kl: 0.532298, loss_recon: 0.608135, loss_pred: 0.045210
iteration 603: loss: 2.146159, loss_kl: 1.544459, loss_recon: 0.609591, loss_pred: 0.034094
iteration 604: loss: 1.520848, loss_kl: 0.906455, loss_recon: 0.610251, loss_pred: 0.029024
iteration 605: loss: 1.267496, loss_kl: 0.633546, loss_recon: 0.609970, loss_pred: 0.041946
iteration 606: loss: 1.575690, loss_kl: 0.965987, loss_recon: 0.610753, loss_pred: 0.025316
iteration 607: loss: 1.999962, loss_kl: 1.392705, loss_recon: 0.608830, loss_pred: 0.036438
iteration 608: loss: 1.790955, loss_kl: 1.191449, loss_recon: 0.608333, loss_pred: 0.023487
iteration 609: loss: 1.309348, loss_kl: 0.683156, loss_recon: 0.607711, loss_pred: 0.037652
iteration 610: loss: 1.649243, loss_kl: 1.034911, loss_recon: 0.605254, loss_pred: 0.037604
iteration 611: loss: 1.462300, loss_kl: 0.829564, loss_recon: 0.611790, loss_pred: 0.029845
iteration 612: loss: 1.364973, loss_kl: 0.718144, loss_recon: 0.608147, loss_pred: 0.046600
iteration 613: loss: 1.884126, loss_kl: 1.258235, loss_recon: 0.609936, loss_pred: 0.029286
iteration 614: loss: 1.597386, loss_kl: 0.971906, loss_recon: 0.609407, loss_pred: 0.026410
iteration 615: loss: 1.317409, loss_kl: 0.688695, loss_recon: 0.609508, loss_pred: 0.026613
iteration 616: loss: 1.546923, loss_kl: 0.907481, loss_recon: 0.610497, loss_pred: 0.038741
iteration 617: loss: 1.728700, loss_kl: 1.099570, loss_recon: 0.608899, loss_pred: 0.031947
iteration 618: loss: 1.756009, loss_kl: 1.130040, loss_recon: 0.609113, loss_pred: 0.028855
iteration 619: loss: 1.517976, loss_kl: 0.889268, loss_recon: 0.608330, loss_pred: 0.029895
iteration 620: loss: 1.734663, loss_kl: 1.121596, loss_recon: 0.604598, loss_pred: 0.020292
iteration 621: loss: 1.499513, loss_kl: 0.869559, loss_recon: 0.610902, loss_pred: 0.022612
iteration 622: loss: 1.693161, loss_kl: 1.049183, loss_recon: 0.607818, loss_pred: 0.040508
iteration 623: loss: 1.392912, loss_kl: 0.762108, loss_recon: 0.609677, loss_pred: 0.024264
iteration 624: loss: 1.410755, loss_kl: 0.787789, loss_recon: 0.609344, loss_pred: 0.016833
iteration 625: loss: 1.426712, loss_kl: 0.789674, loss_recon: 0.608835, loss_pred: 0.031479
iteration 626: loss: 1.898543, loss_kl: 1.267109, loss_recon: 0.609685, loss_pred: 0.026913
iteration 627: loss: 1.789312, loss_kl: 1.157325, loss_recon: 0.608273, loss_pred: 0.028445
iteration 628: loss: 1.719726, loss_kl: 1.057461, loss_recon: 0.607626, loss_pred: 0.059093
iteration 629: loss: 1.461432, loss_kl: 0.823753, loss_recon: 0.607148, loss_pred: 0.033953
iteration 630: loss: 1.938048, loss_kl: 1.277548, loss_recon: 0.604361, loss_pred: 0.061482
iteration 631: loss: 1.582743, loss_kl: 0.949508, loss_recon: 0.610236, loss_pred: 0.022998
iteration 632: loss: 1.471887, loss_kl: 0.835088, loss_recon: 0.608038, loss_pred: 0.028760
iteration 633: loss: 1.480305, loss_kl: 0.826792, loss_recon: 0.610180, loss_pred: 0.043333
iteration 634: loss: 1.461833, loss_kl: 0.831064, loss_recon: 0.608645, loss_pred: 0.022124
iteration 635: loss: 1.764384, loss_kl: 1.103292, loss_recon: 0.608203, loss_pred: 0.052889
iteration 636: loss: 1.545993, loss_kl: 0.916506, loss_recon: 0.609344, loss_pred: 0.020143
iteration 637: loss: 1.533558, loss_kl: 0.890350, loss_recon: 0.608659, loss_pred: 0.034548
iteration 638: loss: 1.391812, loss_kl: 0.750246, loss_recon: 0.608391, loss_pred: 0.033175
iteration 639: loss: 1.584215, loss_kl: 0.944024, loss_recon: 0.607909, loss_pred: 0.032282
iteration 640: loss: 1.487753, loss_kl: 0.860128, loss_recon: 0.604550, loss_pred: 0.023076
iteration 641: loss: 1.158944, loss_kl: 0.520702, loss_recon: 0.610546, loss_pred: 0.027696
iteration 642: loss: 1.493517, loss_kl: 0.857335, loss_recon: 0.607883, loss_pred: 0.028299
iteration 643: loss: 2.002185, loss_kl: 1.360273, loss_recon: 0.609378, loss_pred: 0.032534
iteration 644: loss: 1.672224, loss_kl: 1.019919, loss_recon: 0.608955, loss_pred: 0.043349
iteration 645: loss: 1.529164, loss_kl: 0.859690, loss_recon: 0.609257, loss_pred: 0.060217
iteration 646: loss: 1.690674, loss_kl: 1.055940, loss_recon: 0.610129, loss_pred: 0.024606
iteration 647: loss: 1.788650, loss_kl: 1.164893, loss_recon: 0.608087, loss_pred: 0.015670
iteration 648: loss: 1.594882, loss_kl: 0.955524, loss_recon: 0.607839, loss_pred: 0.031520
iteration 649: loss: 1.758557, loss_kl: 1.122729, loss_recon: 0.607033, loss_pred: 0.028795
iteration 650: loss: 1.668904, loss_kl: 1.032538, loss_recon: 0.604123, loss_pred: 0.032243
save model to ../model/TVG_Design[160, 160, 160]/TVG_pretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_164.pth
iteration 651: loss: 1.377883, loss_kl: 0.730035, loss_recon: 0.609965, loss_pred: 0.037883
iteration 652: loss: 1.750917, loss_kl: 1.119190, loss_recon: 0.606790, loss_pred: 0.024937
iteration 653: loss: 1.386304, loss_kl: 0.733966, loss_recon: 0.610572, loss_pred: 0.041766
iteration 654: loss: 2.018072, loss_kl: 1.389586, loss_recon: 0.609645, loss_pred: 0.018841
iteration 655: loss: 1.771853, loss_kl: 1.124758, loss_recon: 0.609113, loss_pred: 0.037982
iteration 656: loss: 1.574429, loss_kl: 0.928136, loss_recon: 0.609645, loss_pred: 0.036649
iteration 657: loss: 2.060722, loss_kl: 1.410187, loss_recon: 0.607963, loss_pred: 0.042572
iteration 658: loss: 2.168844, loss_kl: 1.527588, loss_recon: 0.608107, loss_pred: 0.033149
iteration 659: loss: 1.783810, loss_kl: 1.159098, loss_recon: 0.607613, loss_pred: 0.017099
iteration 660: loss: 1.548128, loss_kl: 0.922682, loss_recon: 0.604764, loss_pred: 0.020682
iteration 661: loss: 1.392976, loss_kl: 0.762593, loss_recon: 0.610267, loss_pred: 0.020116
iteration 662: loss: 1.163699, loss_kl: 0.540283, loss_recon: 0.607005, loss_pred: 0.016411
iteration 663: loss: 1.638899, loss_kl: 1.003627, loss_recon: 0.609276, loss_pred: 0.025996
iteration 664: loss: 1.977554, loss_kl: 1.331715, loss_recon: 0.608443, loss_pred: 0.037396
iteration 665: loss: 1.699650, loss_kl: 1.052952, loss_recon: 0.608383, loss_pred: 0.038315
iteration 666: loss: 2.252385, loss_kl: 1.614398, loss_recon: 0.609448, loss_pred: 0.028538
iteration 667: loss: 1.610905, loss_kl: 0.963845, loss_recon: 0.608258, loss_pred: 0.038802
iteration 668: loss: 1.468536, loss_kl: 0.842470, loss_recon: 0.608065, loss_pred: 0.018000
iteration 669: loss: 1.004010, loss_kl: 0.355413, loss_recon: 0.607079, loss_pred: 0.041518
iteration 670: loss: 1.702549, loss_kl: 1.066289, loss_recon: 0.604366, loss_pred: 0.031894
iteration 671: loss: 1.784437, loss_kl: 1.130942, loss_recon: 0.610400, loss_pred: 0.043095
iteration 672: loss: 1.596589, loss_kl: 0.967565, loss_recon: 0.608327, loss_pred: 0.020697
iteration 673: loss: 1.648174, loss_kl: 0.999292, loss_recon: 0.610067, loss_pred: 0.038815
iteration 674: loss: 1.523269, loss_kl: 0.898197, loss_recon: 0.608341, loss_pred: 0.016731
iteration 675: loss: 1.780951, loss_kl: 1.143387, loss_recon: 0.608311, loss_pred: 0.029253
iteration 676: loss: 1.456198, loss_kl: 0.822815, loss_recon: 0.609360, loss_pred: 0.024023
iteration 677: loss: 1.882819, loss_kl: 1.244770, loss_recon: 0.608193, loss_pred: 0.029856
iteration 678: loss: 1.592811, loss_kl: 0.966613, loss_recon: 0.608452, loss_pred: 0.017746
iteration 679: loss: 1.372479, loss_kl: 0.736609, loss_recon: 0.607312, loss_pred: 0.028557
iteration 680: loss: 1.329829, loss_kl: 0.687390, loss_recon: 0.603779, loss_pred: 0.038659
iteration 681: loss: 1.258694, loss_kl: 0.617855, loss_recon: 0.610178, loss_pred: 0.030660
iteration 682: loss: 1.348543, loss_kl: 0.715983, loss_recon: 0.607330, loss_pred: 0.025230
iteration 683: loss: 1.387199, loss_kl: 0.746551, loss_recon: 0.609949, loss_pred: 0.030699
iteration 684: loss: 0.952151, loss_kl: 0.323913, loss_recon: 0.609334, loss_pred: 0.018904
iteration 685: loss: 1.395263, loss_kl: 0.755104, loss_recon: 0.608996, loss_pred: 0.031163
iteration 686: loss: 1.693833, loss_kl: 1.059687, loss_recon: 0.609250, loss_pred: 0.024897
iteration 687: loss: 1.664447, loss_kl: 1.038925, loss_recon: 0.608001, loss_pred: 0.017521
iteration 688: loss: 1.802030, loss_kl: 1.165639, loss_recon: 0.608384, loss_pred: 0.028008
iteration 689: loss: 1.635656, loss_kl: 1.009771, loss_recon: 0.607098, loss_pred: 0.018786
iteration 690: loss: 1.363641, loss_kl: 0.733626, loss_recon: 0.603290, loss_pred: 0.026726
iteration 691: loss: 1.833466, loss_kl: 1.202628, loss_recon: 0.609863, loss_pred: 0.020975
iteration 692: loss: 1.452181, loss_kl: 0.830339, loss_recon: 0.607218, loss_pred: 0.014624
iteration 693: loss: 1.515734, loss_kl: 0.875356, loss_recon: 0.609371, loss_pred: 0.031008
iteration 694: loss: 1.825252, loss_kl: 1.192806, loss_recon: 0.608714, loss_pred: 0.023732
iteration 695: loss: 1.536893, loss_kl: 0.902232, loss_recon: 0.608007, loss_pred: 0.026654
iteration 696: loss: 1.761095, loss_kl: 1.135472, loss_recon: 0.608825, loss_pred: 0.016798
iteration 697: loss: 1.373442, loss_kl: 0.741190, loss_recon: 0.607900, loss_pred: 0.024353
iteration 698: loss: 1.562701, loss_kl: 0.934661, loss_recon: 0.608150, loss_pred: 0.019890
iteration 699: loss: 1.442143, loss_kl: 0.817539, loss_recon: 0.607183, loss_pred: 0.017421
iteration 700: loss: 1.228745, loss_kl: 0.602577, loss_recon: 0.604519, loss_pred: 0.021649
iteration 701: loss: 1.644795, loss_kl: 1.012153, loss_recon: 0.610601, loss_pred: 0.022040
iteration 702: loss: 1.309086, loss_kl: 0.667716, loss_recon: 0.607235, loss_pred: 0.034135
iteration 703: loss: 1.964706, loss_kl: 1.338557, loss_recon: 0.610008, loss_pred: 0.016141
iteration 704: loss: 1.693334, loss_kl: 1.055063, loss_recon: 0.608450, loss_pred: 0.029820
iteration 705: loss: 1.362094, loss_kl: 0.721825, loss_recon: 0.608194, loss_pred: 0.032075
iteration 706: loss: 1.163785, loss_kl: 0.537998, loss_recon: 0.608747, loss_pred: 0.017040
iteration 707: loss: 1.487743, loss_kl: 0.856019, loss_recon: 0.607810, loss_pred: 0.023914
iteration 708: loss: 1.959938, loss_kl: 1.332007, loss_recon: 0.608283, loss_pred: 0.019648
iteration 709: loss: 1.704952, loss_kl: 1.065829, loss_recon: 0.607290, loss_pred: 0.031834
iteration 710: loss: 1.112098, loss_kl: 0.484901, loss_recon: 0.603766, loss_pred: 0.023430
iteration 711: loss: 1.036203, loss_kl: 0.400663, loss_recon: 0.610146, loss_pred: 0.025394
iteration 712: loss: 1.422177, loss_kl: 0.791340, loss_recon: 0.607208, loss_pred: 0.023630
iteration 713: loss: 1.617578, loss_kl: 0.989851, loss_recon: 0.609443, loss_pred: 0.018284
iteration 714: loss: 1.795579, loss_kl: 1.168934, loss_recon: 0.608503, loss_pred: 0.018142
iteration 715: loss: 1.388699, loss_kl: 0.754629, loss_recon: 0.608421, loss_pred: 0.025649
iteration 716: loss: 1.261607, loss_kl: 0.623773, loss_recon: 0.609462, loss_pred: 0.028372
iteration 717: loss: 1.617577, loss_kl: 0.987199, loss_recon: 0.607825, loss_pred: 0.022553
iteration 718: loss: 1.648326, loss_kl: 1.025299, loss_recon: 0.608085, loss_pred: 0.014942
iteration 719: loss: 1.199283, loss_kl: 0.554158, loss_recon: 0.607056, loss_pred: 0.038069
iteration 720: loss: 1.324782, loss_kl: 0.679301, loss_recon: 0.603961, loss_pred: 0.041521
iteration 721: loss: 1.561874, loss_kl: 0.937060, loss_recon: 0.610317, loss_pred: 0.014497
iteration 722: loss: 2.186398, loss_kl: 1.553956, loss_recon: 0.607280, loss_pred: 0.025162
iteration 723: loss: 1.889452, loss_kl: 1.246302, loss_recon: 0.609557, loss_pred: 0.033594
iteration 724: loss: 1.403498, loss_kl: 0.771891, loss_recon: 0.609031, loss_pred: 0.022575
iteration 725: loss: 2.168403, loss_kl: 1.536282, loss_recon: 0.608500, loss_pred: 0.023622
iteration 726: loss: 1.945884, loss_kl: 1.308756, loss_recon: 0.609216, loss_pred: 0.027912
iteration 727: loss: 1.439295, loss_kl: 0.798315, loss_recon: 0.607961, loss_pred: 0.033020
iteration 728: loss: 1.606733, loss_kl: 0.980860, loss_recon: 0.608341, loss_pred: 0.017531
iteration 729: loss: 1.621260, loss_kl: 0.990617, loss_recon: 0.607901, loss_pred: 0.022742
iteration 730: loss: 1.497037, loss_kl: 0.857143, loss_recon: 0.603957, loss_pred: 0.035936
iteration 731: loss: 1.602677, loss_kl: 0.973032, loss_recon: 0.610036, loss_pred: 0.019610
iteration 732: loss: 1.649426, loss_kl: 1.022573, loss_recon: 0.607242, loss_pred: 0.019612
iteration 733: loss: 1.482027, loss_kl: 0.845773, loss_recon: 0.609140, loss_pred: 0.027114
iteration 734: loss: 2.096747, loss_kl: 1.468092, loss_recon: 0.608792, loss_pred: 0.019862
iteration 735: loss: 1.516668, loss_kl: 0.884102, loss_recon: 0.609133, loss_pred: 0.023432
iteration 736: loss: 1.388068, loss_kl: 0.750543, loss_recon: 0.609218, loss_pred: 0.028308
iteration 737: loss: 1.526511, loss_kl: 0.897958, loss_recon: 0.608169, loss_pred: 0.020383
iteration 738: loss: 1.565699, loss_kl: 0.937358, loss_recon: 0.608921, loss_pred: 0.019420
iteration 739: loss: 1.991695, loss_kl: 1.357381, loss_recon: 0.607933, loss_pred: 0.026381
iteration 740: loss: 1.832327, loss_kl: 1.210134, loss_recon: 0.605010, loss_pred: 0.017183
iteration 741: loss: 1.344796, loss_kl: 0.706404, loss_recon: 0.610838, loss_pred: 0.027554
iteration 742: loss: 1.335227, loss_kl: 0.705852, loss_recon: 0.607275, loss_pred: 0.022100
iteration 743: loss: 1.648370, loss_kl: 1.017207, loss_recon: 0.609528, loss_pred: 0.021635
iteration 744: loss: 1.658525, loss_kl: 1.032053, loss_recon: 0.609329, loss_pred: 0.017144
iteration 745: loss: 2.219845, loss_kl: 1.592492, loss_recon: 0.609563, loss_pred: 0.017790
iteration 746: loss: 1.609216, loss_kl: 0.980307, loss_recon: 0.609352, loss_pred: 0.019558
iteration 747: loss: 1.612564, loss_kl: 0.972795, loss_recon: 0.608226, loss_pred: 0.031544
iteration 748: loss: 1.542969, loss_kl: 0.906424, loss_recon: 0.607879, loss_pred: 0.028666
iteration 749: loss: 1.270923, loss_kl: 0.636563, loss_recon: 0.606958, loss_pred: 0.027403
iteration 750: loss: 1.743626, loss_kl: 1.117201, loss_recon: 0.604585, loss_pred: 0.021840
iteration 751: loss: 0.613369, loss_kl: 1.065859, loss_recon: 0.610682, loss_pred: 0.020847
iteration 752: loss: 0.610077, loss_kl: 1.039088, loss_recon: 0.607391, loss_pred: 0.047042
iteration 753: loss: 0.615296, loss_kl: 2.106929, loss_recon: 0.609819, loss_pred: 0.108234
iteration 754: loss: 0.616673, loss_kl: 2.895570, loss_recon: 0.609194, loss_pred: 0.129139
iteration 755: loss: 0.619792, loss_kl: 3.811426, loss_recon: 0.609863, loss_pred: 0.204184
iteration 756: loss: 0.619929, loss_kl: 3.465064, loss_recon: 0.610619, loss_pred: 0.299973
iteration 757: loss: 0.620859, loss_kl: 4.598259, loss_recon: 0.608678, loss_pred: 0.328051
iteration 758: loss: 0.624379, loss_kl: 5.872013, loss_recon: 0.608870, loss_pred: 0.400392
iteration 759: loss: 0.622944, loss_kl: 5.038332, loss_recon: 0.609370, loss_pred: 0.451359
iteration 760: loss: 0.624798, loss_kl: 7.213888, loss_recon: 0.605742, loss_pred: 0.492892
iteration 761: loss: 0.663286, loss_kl: 7.595236, loss_recon: 0.611118, loss_pred: 0.515221
iteration 762: loss: 0.660249, loss_kl: 7.674281, loss_recon: 0.607587, loss_pred: 0.513043
iteration 763: loss: 0.667346, loss_kl: 8.345003, loss_recon: 0.610048, loss_pred: 0.563214
iteration 764: loss: 0.660373, loss_kl: 7.428154, loss_recon: 0.609212, loss_pred: 0.525956
iteration 765: loss: 0.674308, loss_kl: 9.625746, loss_recon: 0.609249, loss_pred: 0.488902
iteration 766: loss: 0.664031, loss_kl: 7.820186, loss_recon: 0.610646, loss_pred: 0.479465
iteration 767: loss: 0.668809, loss_kl: 8.976705, loss_recon: 0.608414, loss_pred: 0.412924
iteration 768: loss: 0.664286, loss_kl: 8.323601, loss_recon: 0.608494, loss_pred: 0.350329
iteration 769: loss: 0.658463, loss_kl: 7.621750, loss_recon: 0.607503, loss_pred: 0.300952
iteration 770: loss: 0.663825, loss_kl: 8.948534, loss_recon: 0.604531, loss_pred: 0.269998
iteration 771: loss: 0.761838, loss_kl: 8.874767, loss_recon: 0.610671, loss_pred: 0.217217
iteration 772: loss: 0.778929, loss_kl: 10.138010, loss_recon: 0.607292, loss_pred: 0.185176
iteration 773: loss: 0.743322, loss_kl: 7.934587, loss_recon: 0.608742, loss_pred: 0.159832
iteration 774: loss: 0.760846, loss_kl: 9.020962, loss_recon: 0.608534, loss_pred: 0.139885
iteration 775: loss: 0.750292, loss_kl: 8.403316, loss_recon: 0.608619, loss_pred: 0.117674
iteration 776: loss: 0.755166, loss_kl: 8.637540, loss_recon: 0.609893, loss_pred: 0.099983
iteration 777: loss: 0.706946, loss_kl: 5.840429, loss_recon: 0.608292, loss_pred: 0.093175
iteration 778: loss: 0.729489, loss_kl: 7.249671, loss_recon: 0.607594, loss_pred: 0.081780
iteration 779: loss: 0.740390, loss_kl: 7.956336, loss_recon: 0.606975, loss_pred: 0.068000
iteration 780: loss: 0.732546, loss_kl: 7.639589, loss_recon: 0.604359, loss_pred: 0.070289
iteration 781: loss: 0.858550, loss_kl: 5.805698, loss_recon: 0.610593, loss_pred: 0.057574
iteration 782: loss: 0.821792, loss_kl: 5.017487, loss_recon: 0.607314, loss_pred: 0.054137
iteration 783: loss: 0.815507, loss_kl: 4.823649, loss_recon: 0.609169, loss_pred: 0.055495
iteration 784: loss: 0.790731, loss_kl: 4.254955, loss_recon: 0.608956, loss_pred: 0.043368
iteration 785: loss: 0.763879, loss_kl: 3.649046, loss_recon: 0.608061, loss_pred: 0.035492
iteration 786: loss: 0.759057, loss_kl: 3.513310, loss_recon: 0.609281, loss_pred: 0.028334
iteration 787: loss: 0.719098, loss_kl: 2.591646, loss_recon: 0.608439, loss_pred: 0.025042
iteration 788: loss: 0.677902, loss_kl: 1.633815, loss_recon: 0.607838, loss_pred: 0.022935
iteration 789: loss: 0.695348, loss_kl: 2.068661, loss_recon: 0.606847, loss_pred: 0.024060
iteration 790: loss: 0.677018, loss_kl: 1.709458, loss_recon: 0.603851, loss_pred: 0.020687
iteration 791: loss: 0.750001, loss_kl: 1.334512, loss_recon: 0.609611, loss_pred: 0.023219
iteration 792: loss: 0.700676, loss_kl: 0.881255, loss_recon: 0.607432, loss_pred: 0.020520
iteration 793: loss: 0.714478, loss_kl: 0.996347, loss_recon: 0.609161, loss_pred: 0.022188
iteration 794: loss: 0.696782, loss_kl: 0.829158, loss_recon: 0.608311, loss_pred: 0.026450
iteration 795: loss: 0.663642, loss_kl: 0.520755, loss_recon: 0.607524, loss_pred: 0.021966
iteration 796: loss: 0.662747, loss_kl: 0.508019, loss_recon: 0.608406, loss_pred: 0.017521
iteration 797: loss: 0.699270, loss_kl: 0.870481, loss_recon: 0.607657, loss_pred: 0.015521
iteration 798: loss: 0.681790, loss_kl: 0.702909, loss_recon: 0.607246, loss_pred: 0.018016
iteration 799: loss: 0.683187, loss_kl: 0.725072, loss_recon: 0.606406, loss_pred: 0.017490
iteration 800: loss: 0.629057, loss_kl: 0.229371, loss_recon: 0.603024, loss_pred: 0.022396
iteration 801: loss: 0.777964, loss_kl: 0.713237, loss_recon: 0.609391, loss_pred: 0.015015
iteration 802: loss: 0.726564, loss_kl: 0.501837, loss_recon: 0.606656, loss_pred: 0.016178
iteration 803: loss: 0.771698, loss_kl: 0.691447, loss_recon: 0.608469, loss_pred: 0.013719
iteration 804: loss: 0.723937, loss_kl: 0.483944, loss_recon: 0.608099, loss_pred: 0.016492
iteration 805: loss: 0.790183, loss_kl: 0.769140, loss_recon: 0.607372, loss_pred: 0.020624
iteration 806: loss: 0.839502, loss_kl: 0.982899, loss_recon: 0.608293, loss_pred: 0.015950
iteration 807: loss: 0.791424, loss_kl: 0.780874, loss_recon: 0.607028, loss_pred: 0.015736
iteration 808: loss: 0.803773, loss_kl: 0.833344, loss_recon: 0.607411, loss_pred: 0.014961
iteration 809: loss: 0.710904, loss_kl: 0.437249, loss_recon: 0.606288, loss_pred: 0.014707
iteration 810: loss: 0.819824, loss_kl: 0.922444, loss_recon: 0.602898, loss_pred: 0.014700
iteration 811: loss: 1.099769, loss_kl: 1.102215, loss_recon: 0.609250, loss_pred: 0.011876
iteration 812: loss: 1.100808, loss_kl: 1.110953, loss_recon: 0.606473, loss_pred: 0.011805
iteration 813: loss: 0.909237, loss_kl: 0.668372, loss_recon: 0.608485, loss_pred: 0.014712
iteration 814: loss: 0.920732, loss_kl: 0.694992, loss_recon: 0.608262, loss_pred: 0.014706
iteration 815: loss: 0.898462, loss_kl: 0.642315, loss_recon: 0.607574, loss_pred: 0.018362
iteration 816: loss: 1.175227, loss_kl: 1.270160, loss_recon: 0.608640, loss_pred: 0.016701
iteration 817: loss: 1.096168, loss_kl: 1.095085, loss_recon: 0.606978, loss_pred: 0.015987
iteration 818: loss: 1.006090, loss_kl: 0.889814, loss_recon: 0.607842, loss_pred: 0.014707
iteration 819: loss: 0.903361, loss_kl: 0.653082, loss_recon: 0.606446, loss_pred: 0.021285
iteration 820: loss: 0.828610, loss_kl: 0.493996, loss_recon: 0.603471, loss_pred: 0.017352
iteration 821: loss: 1.248558, loss_kl: 0.930113, loss_recon: 0.609314, loss_pred: 0.020284
iteration 822: loss: 0.975105, loss_kl: 0.524149, loss_recon: 0.606517, loss_pred: 0.023850
iteration 823: loss: 1.154638, loss_kl: 0.795003, loss_recon: 0.608481, loss_pred: 0.016997
iteration 824: loss: 1.020201, loss_kl: 0.586925, loss_recon: 0.608295, loss_pred: 0.025477
iteration 825: loss: 0.903409, loss_kl: 0.411893, loss_recon: 0.607942, loss_pred: 0.027393
iteration 826: loss: 0.971478, loss_kl: 0.513031, loss_recon: 0.608873, loss_pred: 0.026072
iteration 827: loss: 1.075777, loss_kl: 0.670968, loss_recon: 0.607217, loss_pred: 0.025664
iteration 828: loss: 1.181160, loss_kl: 0.831579, loss_recon: 0.607584, loss_pred: 0.021187
iteration 829: loss: 0.926802, loss_kl: 0.458006, loss_recon: 0.606920, loss_pred: 0.017580
iteration 830: loss: 1.043284, loss_kl: 0.638736, loss_recon: 0.603991, loss_pred: 0.014383
iteration 831: loss: 1.374203, loss_kl: 0.875537, loss_recon: 0.609873, loss_pred: 0.031244
iteration 832: loss: 0.965816, loss_kl: 0.402160, loss_recon: 0.606751, loss_pred: 0.023825
iteration 833: loss: 1.084015, loss_kl: 0.530450, loss_recon: 0.609077, loss_pred: 0.033005
iteration 834: loss: 1.612240, loss_kl: 1.161945, loss_recon: 0.608852, loss_pred: 0.028448
iteration 835: loss: 0.893983, loss_kl: 0.311051, loss_recon: 0.608060, loss_pred: 0.028161
iteration 836: loss: 1.437902, loss_kl: 0.959505, loss_recon: 0.609359, loss_pred: 0.023457
iteration 837: loss: 1.286688, loss_kl: 0.773899, loss_recon: 0.607897, loss_pred: 0.031402
iteration 838: loss: 1.063191, loss_kl: 0.515930, loss_recon: 0.607569, loss_pred: 0.024608
iteration 839: loss: 0.848233, loss_kl: 0.266286, loss_recon: 0.606474, loss_pred: 0.020530
iteration 840: loss: 0.930710, loss_kl: 0.370567, loss_recon: 0.604596, loss_pred: 0.016327
iteration 841: loss: 1.047053, loss_kl: 0.445488, loss_recon: 0.610117, loss_pred: 0.022629
iteration 842: loss: 1.705451, loss_kl: 1.160826, loss_recon: 0.607222, loss_pred: 0.015773
iteration 843: loss: 1.317125, loss_kl: 0.742289, loss_recon: 0.609087, loss_pred: 0.016276
iteration 844: loss: 1.057334, loss_kl: 0.464882, loss_recon: 0.608374, loss_pred: 0.016117
iteration 845: loss: 1.297680, loss_kl: 0.722047, loss_recon: 0.608145, loss_pred: 0.016694
iteration 846: loss: 1.255857, loss_kl: 0.677078, loss_recon: 0.609094, loss_pred: 0.015838
iteration 847: loss: 1.578365, loss_kl: 1.026036, loss_recon: 0.607475, loss_pred: 0.014139
iteration 848: loss: 1.275355, loss_kl: 0.697874, loss_recon: 0.608186, loss_pred: 0.016905
iteration 849: loss: 1.512295, loss_kl: 0.951896, loss_recon: 0.606920, loss_pred: 0.018088
iteration 850: loss: 1.290833, loss_kl: 0.719038, loss_recon: 0.603640, loss_pred: 0.017194
iteration 851: loss: 1.824685, loss_kl: 1.226738, loss_recon: 0.609140, loss_pred: 0.022021
iteration 852: loss: 1.632543, loss_kl: 1.031275, loss_recon: 0.606511, loss_pred: 0.022792
iteration 853: loss: 1.564258, loss_kl: 0.951139, loss_recon: 0.608612, loss_pred: 0.030618
iteration 854: loss: 1.586497, loss_kl: 0.978120, loss_recon: 0.609024, loss_pred: 0.026061
iteration 855: loss: 1.390900, loss_kl: 0.766437, loss_recon: 0.608613, loss_pred: 0.037226
iteration 856: loss: 1.380896, loss_kl: 0.773501, loss_recon: 0.608480, loss_pred: 0.020021
iteration 857: loss: 1.406298, loss_kl: 0.793066, loss_recon: 0.607187, loss_pred: 0.027880
iteration 858: loss: 1.719290, loss_kl: 1.105491, loss_recon: 0.608752, loss_pred: 0.035390
iteration 859: loss: 1.413558, loss_kl: 0.795109, loss_recon: 0.607332, loss_pred: 0.033146
iteration 860: loss: 1.493609, loss_kl: 0.887274, loss_recon: 0.603464, loss_pred: 0.027193
iteration 861: loss: 1.894155, loss_kl: 1.286479, loss_recon: 0.609917, loss_pred: 0.011195
iteration 862: loss: 1.926424, loss_kl: 1.312392, loss_recon: 0.607629, loss_pred: 0.020200
iteration 863: loss: 1.551316, loss_kl: 0.925910, loss_recon: 0.610074, loss_pred: 0.025179
iteration 864: loss: 1.562220, loss_kl: 0.933200, loss_recon: 0.608994, loss_pred: 0.029998
iteration 865: loss: 1.160537, loss_kl: 0.534863, loss_recon: 0.608571, loss_pred: 0.022878
iteration 866: loss: 1.745221, loss_kl: 1.129304, loss_recon: 0.609241, loss_pred: 0.018560
iteration 867: loss: 1.308748, loss_kl: 0.690166, loss_recon: 0.608029, loss_pred: 0.017884
iteration 868: loss: 1.559777, loss_kl: 0.933500, loss_recon: 0.608347, loss_pred: 0.027885
iteration 869: loss: 1.517026, loss_kl: 0.881397, loss_recon: 0.607704, loss_pred: 0.037438
iteration 870: loss: 1.647121, loss_kl: 1.022078, loss_recon: 0.604342, loss_pred: 0.031611
iteration 871: loss: 1.885211, loss_kl: 1.247346, loss_recon: 0.609735, loss_pred: 0.033240
iteration 872: loss: 1.338733, loss_kl: 0.707410, loss_recon: 0.607632, loss_pred: 0.026620
iteration 873: loss: 1.410322, loss_kl: 0.767325, loss_recon: 0.609200, loss_pred: 0.037006
iteration 874: loss: 1.882653, loss_kl: 1.247014, loss_recon: 0.609001, loss_pred: 0.031741
iteration 875: loss: 1.346389, loss_kl: 0.713187, loss_recon: 0.608083, loss_pred: 0.028077
iteration 876: loss: 1.940706, loss_kl: 1.313458, loss_recon: 0.609455, loss_pred: 0.023127
iteration 877: loss: 1.527842, loss_kl: 0.898702, loss_recon: 0.608746, loss_pred: 0.024075
iteration 878: loss: 1.601847, loss_kl: 0.971437, loss_recon: 0.608926, loss_pred: 0.025462
iteration 879: loss: 1.032729, loss_kl: 0.401455, loss_recon: 0.607477, loss_pred: 0.025501
iteration 880: loss: 1.185220, loss_kl: 0.559541, loss_recon: 0.604212, loss_pred: 0.023795
iteration 881: loss: 1.368566, loss_kl: 0.723637, loss_recon: 0.610171, loss_pred: 0.034758
iteration 882: loss: 1.416438, loss_kl: 0.784806, loss_recon: 0.607922, loss_pred: 0.023711
iteration 883: loss: 1.707700, loss_kl: 1.052036, loss_recon: 0.610631, loss_pred: 0.045034
iteration 884: loss: 1.604520, loss_kl: 0.973985, loss_recon: 0.609897, loss_pred: 0.020638
iteration 885: loss: 1.967300, loss_kl: 1.325349, loss_recon: 0.609690, loss_pred: 0.032261
iteration 886: loss: 1.852114, loss_kl: 1.226797, loss_recon: 0.609205, loss_pred: 0.016112
iteration 887: loss: 1.543208, loss_kl: 0.886520, loss_recon: 0.608288, loss_pred: 0.048400
iteration 888: loss: 1.499239, loss_kl: 0.867692, loss_recon: 0.609042, loss_pred: 0.022505
iteration 889: loss: 2.068770, loss_kl: 1.415463, loss_recon: 0.609204, loss_pred: 0.044104
iteration 890: loss: 1.599708, loss_kl: 0.974395, loss_recon: 0.605114, loss_pred: 0.020199
iteration 891: loss: 1.658748, loss_kl: 1.017829, loss_recon: 0.611713, loss_pred: 0.029206
iteration 892: loss: 1.516118, loss_kl: 0.886262, loss_recon: 0.607624, loss_pred: 0.022232
iteration 893: loss: 1.240083, loss_kl: 0.607137, loss_recon: 0.609836, loss_pred: 0.023109
iteration 894: loss: 1.838860, loss_kl: 1.196437, loss_recon: 0.608762, loss_pred: 0.033661
iteration 895: loss: 1.511507, loss_kl: 0.864924, loss_recon: 0.608482, loss_pred: 0.038100
iteration 896: loss: 1.598845, loss_kl: 0.963482, loss_recon: 0.609363, loss_pred: 0.025999
iteration 897: loss: 1.858194, loss_kl: 1.234079, loss_recon: 0.607860, loss_pred: 0.016256
iteration 898: loss: 1.590726, loss_kl: 0.963302, loss_recon: 0.607817, loss_pred: 0.019607
iteration 899: loss: 1.381178, loss_kl: 0.756530, loss_recon: 0.606841, loss_pred: 0.017807
iteration 900: loss: 1.610940, loss_kl: 0.970288, loss_recon: 0.604304, loss_pred: 0.036348
iteration 901: loss: 1.582865, loss_kl: 0.940526, loss_recon: 0.610685, loss_pred: 0.031654
iteration 902: loss: 1.467789, loss_kl: 0.837350, loss_recon: 0.607594, loss_pred: 0.022845
iteration 903: loss: 2.121860, loss_kl: 1.484883, loss_recon: 0.609683, loss_pred: 0.027294
iteration 904: loss: 1.315956, loss_kl: 0.670624, loss_recon: 0.609204, loss_pred: 0.036128
iteration 905: loss: 1.517197, loss_kl: 0.841229, loss_recon: 0.609546, loss_pred: 0.066421
iteration 906: loss: 1.698635, loss_kl: 1.061887, loss_recon: 0.608894, loss_pred: 0.027853
iteration 907: loss: 1.091604, loss_kl: 0.439890, loss_recon: 0.608521, loss_pred: 0.043192
iteration 908: loss: 1.641142, loss_kl: 0.982560, loss_recon: 0.609235, loss_pred: 0.049347
iteration 909: loss: 1.376139, loss_kl: 0.731698, loss_recon: 0.607456, loss_pred: 0.036985
iteration 910: loss: 1.461165, loss_kl: 0.830867, loss_recon: 0.604303, loss_pred: 0.025995
iteration 911: loss: 1.392684, loss_kl: 0.755829, loss_recon: 0.609917, loss_pred: 0.026938
iteration 912: loss: 1.759501, loss_kl: 1.125182, loss_recon: 0.607311, loss_pred: 0.027008
iteration 913: loss: 1.551031, loss_kl: 0.914722, loss_recon: 0.610629, loss_pred: 0.025680
iteration 914: loss: 1.313015, loss_kl: 0.666926, loss_recon: 0.608864, loss_pred: 0.037225
iteration 915: loss: 1.153713, loss_kl: 0.486699, loss_recon: 0.608297, loss_pred: 0.058717
iteration 916: loss: 1.613511, loss_kl: 0.982340, loss_recon: 0.608591, loss_pred: 0.022580
iteration 917: loss: 1.244643, loss_kl: 0.623467, loss_recon: 0.608233, loss_pred: 0.012944
iteration 918: loss: 1.455641, loss_kl: 0.820688, loss_recon: 0.608670, loss_pred: 0.026284
iteration 919: loss: 1.752645, loss_kl: 1.113707, loss_recon: 0.607976, loss_pred: 0.030962
iteration 920: loss: 1.484017, loss_kl: 0.861569, loss_recon: 0.603874, loss_pred: 0.018574
iteration 921: loss: 1.529984, loss_kl: 0.884385, loss_recon: 0.609910, loss_pred: 0.035689
iteration 922: loss: 1.548258, loss_kl: 0.889542, loss_recon: 0.607072, loss_pred: 0.051644
iteration 923: loss: 1.106340, loss_kl: 0.458666, loss_recon: 0.609396, loss_pred: 0.038278
iteration 924: loss: 1.996651, loss_kl: 1.373940, loss_recon: 0.608914, loss_pred: 0.013797
iteration 925: loss: 1.160266, loss_kl: 0.454615, loss_recon: 0.609071, loss_pred: 0.096579
iteration 926: loss: 1.390184, loss_kl: 0.735132, loss_recon: 0.609909, loss_pred: 0.045143
iteration 927: loss: 1.327962, loss_kl: 0.664573, loss_recon: 0.607672, loss_pred: 0.055717
iteration 928: loss: 1.173250, loss_kl: 0.529924, loss_recon: 0.607637, loss_pred: 0.035689
iteration 929: loss: 1.171608, loss_kl: 0.506712, loss_recon: 0.606969, loss_pred: 0.057927
iteration 930: loss: 1.075757, loss_kl: 0.444526, loss_recon: 0.603676, loss_pred: 0.027555
iteration 931: loss: 1.279332, loss_kl: 0.646148, loss_recon: 0.609411, loss_pred: 0.023773
iteration 932: loss: 1.324453, loss_kl: 0.679100, loss_recon: 0.606821, loss_pred: 0.038532
iteration 933: loss: 1.731804, loss_kl: 1.092795, loss_recon: 0.609112, loss_pred: 0.029897
iteration 934: loss: 1.503732, loss_kl: 0.870101, loss_recon: 0.608494, loss_pred: 0.025136
iteration 935: loss: 1.642044, loss_kl: 0.990425, loss_recon: 0.608570, loss_pred: 0.043049
iteration 936: loss: 1.580895, loss_kl: 0.944253, loss_recon: 0.608854, loss_pred: 0.027789
iteration 937: loss: 1.563365, loss_kl: 0.931630, loss_recon: 0.607819, loss_pred: 0.023916
iteration 938: loss: 1.160251, loss_kl: 0.528697, loss_recon: 0.608203, loss_pred: 0.023351
iteration 939: loss: 1.120155, loss_kl: 0.488824, loss_recon: 0.607031, loss_pred: 0.024299
iteration 940: loss: 1.605668, loss_kl: 0.975194, loss_recon: 0.604118, loss_pred: 0.026356
iteration 941: loss: 1.768220, loss_kl: 1.140242, loss_recon: 0.610083, loss_pred: 0.017894
iteration 942: loss: 1.328844, loss_kl: 0.701254, loss_recon: 0.607439, loss_pred: 0.020151
iteration 943: loss: 1.395697, loss_kl: 0.765728, loss_recon: 0.609854, loss_pred: 0.020115
iteration 944: loss: 1.272276, loss_kl: 0.642070, loss_recon: 0.608694, loss_pred: 0.021512
iteration 945: loss: 1.551991, loss_kl: 0.921505, loss_recon: 0.608742, loss_pred: 0.021744
iteration 946: loss: 1.374820, loss_kl: 0.741369, loss_recon: 0.609549, loss_pred: 0.023902
iteration 947: loss: 1.290136, loss_kl: 0.667223, loss_recon: 0.608345, loss_pred: 0.014568
iteration 948: loss: 1.885748, loss_kl: 1.257294, loss_recon: 0.608354, loss_pred: 0.020099
iteration 949: loss: 1.971334, loss_kl: 1.336896, loss_recon: 0.607172, loss_pred: 0.027265
iteration 950: loss: 1.967118, loss_kl: 1.340802, loss_recon: 0.604744, loss_pred: 0.021571
iteration 951: loss: 1.459964, loss_kl: 0.782738, loss_recon: 0.610874, loss_pred: 0.066352
iteration 952: loss: 1.305434, loss_kl: 0.678974, loss_recon: 0.607544, loss_pred: 0.018916
iteration 953: loss: 1.157135, loss_kl: 0.508870, loss_recon: 0.609292, loss_pred: 0.038973
iteration 954: loss: 2.054637, loss_kl: 1.428992, loss_recon: 0.608945, loss_pred: 0.016700
iteration 955: loss: 1.485782, loss_kl: 0.845630, loss_recon: 0.608503, loss_pred: 0.031649
iteration 956: loss: 1.535734, loss_kl: 0.890713, loss_recon: 0.609267, loss_pred: 0.035755
iteration 957: loss: 1.623945, loss_kl: 0.978381, loss_recon: 0.607869, loss_pred: 0.037695
iteration 958: loss: 1.283000, loss_kl: 0.641042, loss_recon: 0.607206, loss_pred: 0.034753
iteration 959: loss: 1.693356, loss_kl: 1.054831, loss_recon: 0.606384, loss_pred: 0.032141
iteration 960: loss: 1.495947, loss_kl: 0.861736, loss_recon: 0.603686, loss_pred: 0.030525
iteration 961: loss: 1.242886, loss_kl: 0.613215, loss_recon: 0.609889, loss_pred: 0.019782
iteration 962: loss: 1.324648, loss_kl: 0.694138, loss_recon: 0.607015, loss_pred: 0.023496
iteration 963: loss: 1.411619, loss_kl: 0.787398, loss_recon: 0.609314, loss_pred: 0.014907
iteration 964: loss: 1.319531, loss_kl: 0.689561, loss_recon: 0.608375, loss_pred: 0.021595
iteration 965: loss: 1.430803, loss_kl: 0.791579, loss_recon: 0.607694, loss_pred: 0.031529
iteration 966: loss: 1.531674, loss_kl: 0.900787, loss_recon: 0.608971, loss_pred: 0.021916
iteration 967: loss: 1.336731, loss_kl: 0.709442, loss_recon: 0.607615, loss_pred: 0.019674
iteration 968: loss: 1.705068, loss_kl: 1.074068, loss_recon: 0.608038, loss_pred: 0.022961
iteration 969: loss: 1.517883, loss_kl: 0.871044, loss_recon: 0.607708, loss_pred: 0.039132
iteration 970: loss: 1.637132, loss_kl: 1.005704, loss_recon: 0.604089, loss_pred: 0.027339
iteration 971: loss: 1.504865, loss_kl: 0.872089, loss_recon: 0.609575, loss_pred: 0.023201
iteration 972: loss: 1.757000, loss_kl: 1.119462, loss_recon: 0.607318, loss_pred: 0.030220
iteration 973: loss: 1.152751, loss_kl: 0.520517, loss_recon: 0.609138, loss_pred: 0.023096
iteration 974: loss: 1.619733, loss_kl: 0.980184, loss_recon: 0.608787, loss_pred: 0.030762
iteration 975: loss: 1.638948, loss_kl: 1.008484, loss_recon: 0.608510, loss_pred: 0.021955
iteration 976: loss: 2.110348, loss_kl: 1.461458, loss_recon: 0.609235, loss_pred: 0.039654
iteration 977: loss: 1.603691, loss_kl: 0.973336, loss_recon: 0.607340, loss_pred: 0.023015
iteration 978: loss: 0.790781, loss_kl: 0.149505, loss_recon: 0.607876, loss_pred: 0.033400
iteration 979: loss: 1.564320, loss_kl: 0.936246, loss_recon: 0.606583, loss_pred: 0.021490
iteration 980: loss: 1.635745, loss_kl: 1.004516, loss_recon: 0.604739, loss_pred: 0.026491
save model to ../model/TVG_Design[160, 160, 160]/TVG_pretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_197.pth
iteration 981: loss: 1.664146, loss_kl: 1.028321, loss_recon: 0.609637, loss_pred: 0.026187
iteration 982: loss: 1.335388, loss_kl: 0.713097, loss_recon: 0.607027, loss_pred: 0.015265
iteration 983: loss: 1.558603, loss_kl: 0.908114, loss_recon: 0.609591, loss_pred: 0.040898
iteration 984: loss: 1.505538, loss_kl: 0.879246, loss_recon: 0.608608, loss_pred: 0.017684
iteration 985: loss: 1.426690, loss_kl: 0.773697, loss_recon: 0.608565, loss_pred: 0.044429
iteration 986: loss: 1.320750, loss_kl: 0.675744, loss_recon: 0.609182, loss_pred: 0.035824
iteration 987: loss: 1.285388, loss_kl: 0.659843, loss_recon: 0.607540, loss_pred: 0.018004
iteration 988: loss: 1.561876, loss_kl: 0.914404, loss_recon: 0.608060, loss_pred: 0.039412
iteration 989: loss: 1.189259, loss_kl: 0.566319, loss_recon: 0.607271, loss_pred: 0.015668
iteration 990: loss: 1.367730, loss_kl: 0.737929, loss_recon: 0.604042, loss_pred: 0.025760
iteration 991: loss: 1.431051, loss_kl: 0.796017, loss_recon: 0.609991, loss_pred: 0.025043
iteration 992: loss: 1.823547, loss_kl: 1.198596, loss_recon: 0.607198, loss_pred: 0.017754
iteration 993: loss: 1.741009, loss_kl: 1.101345, loss_recon: 0.608914, loss_pred: 0.030750
iteration 994: loss: 1.416089, loss_kl: 0.781694, loss_recon: 0.608471, loss_pred: 0.025925
iteration 995: loss: 1.242183, loss_kl: 0.601365, loss_recon: 0.608806, loss_pred: 0.032012
iteration 996: loss: 1.732727, loss_kl: 1.095835, loss_recon: 0.609349, loss_pred: 0.027543
iteration 997: loss: 1.024346, loss_kl: 0.394152, loss_recon: 0.607499, loss_pred: 0.022695
iteration 998: loss: 1.213864, loss_kl: 0.579666, loss_recon: 0.607722, loss_pred: 0.026476
iteration 999: loss: 1.033394, loss_kl: 0.407319, loss_recon: 0.607874, loss_pred: 0.018202
iteration 1000: loss: 1.284991, loss_kl: 0.657504, loss_recon: 0.604440, loss_pred: 0.023047
save model to ../model/TVG_Design[160, 160, 160]/TVG_pretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234/epoch_199.pth
  0%|                                      | 0/100 [10:31:59<?, ?it/s]
/work/sheidaei/conda/envs/mytorch/lib/python3.9/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343997789/work/aten/src/ATen/native/TensorShape.cpp:3483.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Namespace(dataset='Design', img_size=[160, 160, 160], vit_name='Conv-ViT-Gen-B_16', pretrained_net_path=False, is_encoder_pretrained=True, vit_patches_size=[8, 8, 8], deterministic=1, max_epochs=100, batch_size=32, base_lr=0.01, seed=1234, is_savenii=False, test_save_dir='../predictions', gpu=4, batch_size_test=32, world_size=-1, rank=-1, dist_url='env://', dist_backend='nccl', local_rank=-1, index=None, num_classes=2, volume_path='/work/sheidaei/mhashemi/data/mat', Dataset=<class 'datasets.dataset_3D.Design_dataset'>, list_dir='./lists/lists_Design', z_spacing=1, exp='TVG_Design[160, 160, 160]', distributed=False)
TVG_encoderpretrained_Conv-ViT-Gen-B_16_vitpatch[8, 8, 8]_epo100_bs32_lr0.01_seed1234
7 test iterations per epoch
0it [00:00, ?it/s]name  280784 surrogate_model_error 0.009286 generative_error 0.035217 reconstruction_loss 0.609309
name  280610 surrogate_model_error 0.016496 generative_error 0.043947 reconstruction_loss 0.651817
name  280181 surrogate_model_error 0.014441 generative_error 0.072421 reconstruction_loss 0.666095
name  219192 surrogate_model_error 0.013652 generative_error 0.036467 reconstruction_loss 0.685639
name  280992 surrogate_model_error 0.036175 generative_error 3.581256 reconstruction_loss 0.693575
name  280076 surrogate_model_error 0.044715 generative_error 19.380045 reconstruction_loss 0.693962
name _164463 surrogate_model_error 0.033428 generative_error 18.651493 reconstruction_loss 0.699679
name _171426 surrogate_model_error 0.057496 generative_error 20.154896 reconstruction_loss 0.681109
name _128790 surrogate_model_error 0.027141 generative_error 17.855793 reconstruction_loss 0.642669
name _039692 surrogate_model_error 0.010810 generative_error 7.802137 reconstruction_loss 0.614525
name _174992 surrogate_model_error 0.119449 generative_error 11.588111 reconstruction_loss 0.528712
name  118151 surrogate_model_error 0.018482 generative_error 3.725792 reconstruction_loss 0.420844
name _047610 surrogate_model_error 0.038214 generative_error 0.694012 reconstruction_loss 0.392144
name  280263 surrogate_model_error 0.004626 generative_error 0.026986 reconstruction_loss 0.529161
name  280822 surrogate_model_error 0.008384 generative_error 0.033356 reconstruction_loss 0.596763
name _280614 surrogate_model_error 0.018813 generative_error 0.070334 reconstruction_loss 0.670129
name  280190 surrogate_model_error 0.017768 generative_error 0.070241 reconstruction_loss 0.678335
name  220399 surrogate_model_error 0.020724 generative_error 0.270517 reconstruction_loss 0.690292
name  281042 surrogate_model_error 0.041215 generative_error 10.869484 reconstruction_loss 0.694597
name  202023 surrogate_model_error 0.051512 generative_error 18.639961 reconstruction_loss 0.694640
name _131312 surrogate_model_error 0.044224 generative_error 20.144461 reconstruction_loss 0.689226
name  135831 surrogate_model_error 0.015405 generative_error 14.020157 reconstruction_loss 0.649601
name  152913 surrogate_model_error 0.168861 generative_error 12.617745 reconstruction_loss 0.606386
name _044927 surrogate_model_error 0.014061 generative_error 7.891529 reconstruction_loss 0.589710
name _150512 surrogate_model_error 0.017864 generative_error 11.578046 reconstruction_loss 0.534492
name  122554 surrogate_model_error 0.058438 generative_error 4.477610 reconstruction_loss 0.420008
name _066216 surrogate_model_error 0.064290 generative_error 0.135443 reconstruction_loss 0.327672
name  280369 surrogate_model_error 0.005176 generative_error 0.028464 reconstruction_loss 0.542857
name  280914 surrogate_model_error 0.006485 generative_error 0.039292 reconstruction_loss 0.591853
name  280657 surrogate_model_error 0.015328 generative_error 0.043827 reconstruction_loss 0.649402
name _280240 surrogate_model_error 0.018472 generative_error 0.074851 reconstruction_loss 0.685892
name  280051 surrogate_model_error 0.013778 generative_error 0.361277 reconstruction_loss 0.689337
1it [00:39, 39.47s/it]name  281045 surrogate_model_error 0.025054 generative_error 0.021140 reconstruction_loss 0.686639
name  205047 surrogate_model_error 0.046536 generative_error 18.945175 reconstruction_loss 0.704363
name _169599 surrogate_model_error 0.034522 generative_error 16.748383 reconstruction_loss 0.685403
name  123537 surrogate_model_error 0.059731 generative_error 18.642113 reconstruction_loss 0.656344
name _128910 surrogate_model_error 0.052521 generative_error 17.610668 reconstruction_loss 0.644118
name  045830 surrogate_model_error 0.015095 generative_error 8.324089 reconstruction_loss 0.609634
name  130077 surrogate_model_error 0.095439 generative_error 12.780731 reconstruction_loss 0.548177
name _130356 surrogate_model_error 0.016642 generative_error 4.814519 reconstruction_loss 0.492286
name  129204 surrogate_model_error 0.021280 generative_error 0.779197 reconstruction_loss 0.409593
name  280457 surrogate_model_error 0.006189 generative_error 0.031729 reconstruction_loss 0.572277
name  280959 surrogate_model_error 0.007973 generative_error 0.035099 reconstruction_loss 0.592386
name  280936 surrogate_model_error 0.010824 generative_error 0.045135 reconstruction_loss 0.635737
name  280243 surrogate_model_error 0.021593 generative_error 0.049927 reconstruction_loss 0.670575
name _280152 surrogate_model_error 0.013827 generative_error 0.613733 reconstruction_loss 0.691590
name  202753 surrogate_model_error 0.029396 generative_error 18.947363 reconstruction_loss 0.702569
name  208271 surrogate_model_error 0.036254 generative_error 19.613514 reconstruction_loss 0.694097
name _143118 surrogate_model_error 0.048093 generative_error 18.395897 reconstruction_loss 0.691527
name  117696 surrogate_model_error 0.072812 generative_error 17.784996 reconstruction_loss 0.669263
name _145529 surrogate_model_error 0.092942 generative_error 15.683345 reconstruction_loss 0.633979
name _046697 surrogate_model_error 0.002184 generative_error 6.505501 reconstruction_loss 0.579417
name  111539 surrogate_model_error 0.073512 generative_error 7.791739 reconstruction_loss 0.564989
name _119729 surrogate_model_error 0.009399 generative_error 4.946617 reconstruction_loss 0.496698
name  166710 surrogate_model_error 0.044583 generative_error 0.475352 reconstruction_loss 0.385591
name  280524 surrogate_model_error 0.007155 generative_error 0.031896 reconstruction_loss 0.576401
name  280016 surrogate_model_error 0.010048 generative_error 0.033268 reconstruction_loss 0.607078
name  281020 surrogate_model_error 0.013100 generative_error 0.041070 reconstruction_loss 0.634013
name  280563 surrogate_model_error 0.012386 generative_error 0.073085 reconstruction_loss 0.669349
name _280153 surrogate_model_error 0.047358 generative_error 19.823511 reconstruction_loss 0.712227
name  205183 surrogate_model_error 0.023739 generative_error 19.492395 reconstruction_loss 0.696627
name  210170 surrogate_model_error 0.026058 generative_error 19.020470 reconstruction_loss 0.696664
name _150782 surrogate_model_error 0.082530 generative_error 17.174654 reconstruction_loss 0.690588
name _126580 surrogate_model_error 0.044705 generative_error 18.512424 reconstruction_loss 0.674122
2it [00:41, 17.28s/it]name _115063 surrogate_model_error 0.074413 generative_error 16.107250 reconstruction_loss 0.642536
name  053335 surrogate_model_error 0.030111 generative_error 6.047174 reconstruction_loss 0.588848
name  170965 surrogate_model_error 0.088199 generative_error 7.816827 reconstruction_loss 0.525878
name  167944 surrogate_model_error 0.017586 generative_error 2.335772 reconstruction_loss 0.463774
name  167126 surrogate_model_error 0.007839 generative_error 0.622839 reconstruction_loss 0.404228
name  280556 surrogate_model_error 0.006434 generative_error 0.028018 reconstruction_loss 0.540004
name  280064 surrogate_model_error 0.011711 generative_error 0.042062 reconstruction_loss 0.635951
name  281022 surrogate_model_error 0.024344 generative_error 0.047414 reconstruction_loss 0.655384
name  280608 surrogate_model_error 0.029221 generative_error 0.287199 reconstruction_loss 0.688918
name  280207 surrogate_model_error 0.012663 generative_error 0.412747 reconstruction_loss 0.690609
name  207067 surrogate_model_error 0.016964 generative_error 18.743610 reconstruction_loss 0.703370
name  210506 surrogate_model_error 0.047714 generative_error 18.326275 reconstruction_loss 0.691666
name _141678 surrogate_model_error 0.054568 generative_error 20.272297 reconstruction_loss 0.690447
name _123282 surrogate_model_error 0.077453 generative_error 19.639563 reconstruction_loss 0.677044
name  104304 surrogate_model_error 0.084566 generative_error 16.175665 reconstruction_loss 0.639772
name _056234 surrogate_model_error 0.009831 generative_error 5.849893 reconstruction_loss 0.573241
name  142235 surrogate_model_error 0.099674 generative_error 6.796886 reconstruction_loss 0.516501
name  113525 surrogate_model_error 0.008684 generative_error 4.086326 reconstruction_loss 0.463634
name _006001 surrogate_model_error 0.011281 generative_error 1.316613 reconstruction_loss 0.460019
name  280835 surrogate_model_error 0.006323 generative_error 0.027696 reconstruction_loss 0.555402
name  280072 surrogate_model_error 0.008576 generative_error 0.051074 reconstruction_loss 0.615308
name  281026 surrogate_model_error 0.019698 generative_error 0.049503 reconstruction_loss 0.658303
name _280711 surrogate_model_error 0.026395 generative_error 8.519256 reconstruction_loss 0.696863
name  280274 surrogate_model_error 0.016211 generative_error 0.634223 reconstruction_loss 0.691507
name  207115 surrogate_model_error 0.019943 generative_error 19.496685 reconstruction_loss 0.698252
name  212350 surrogate_model_error 0.024988 generative_error 18.719730 reconstruction_loss 0.701503
name _118829 surrogate_model_error 0.035097 generative_error 19.942879 reconstruction_loss 0.690416
name _113513 surrogate_model_error 0.097677 generative_error 19.804230 reconstruction_loss 0.681159
name  000794 surrogate_model_error 0.006666 generative_error 10.591990 reconstruction_loss 0.647240
name _061637 surrogate_model_error 0.014729 generative_error 8.297494 reconstruction_loss 0.604913
name _171776 surrogate_model_error 0.045364 generative_error 12.257991 reconstruction_loss 0.545802
name  121602 surrogate_model_error 0.036523 generative_error 5.612070 reconstruction_loss 0.508723
3it [00:42, 10.18s/it]name _044711 surrogate_model_error 0.030245 generative_error 0.734080 reconstruction_loss 0.418354
name  280859 surrogate_model_error 0.005401 generative_error 0.034237 reconstruction_loss 0.566373
name  280109 surrogate_model_error 0.005815 generative_error 0.029118 reconstruction_loss 0.583868
name  281056 surrogate_model_error 0.015783 generative_error 0.044293 reconstruction_loss 0.645784
name  204750 surrogate_model_error 0.028731 generative_error 0.053641 reconstruction_loss 0.668187
name _280323 surrogate_model_error 0.069655 generative_error 17.795815 reconstruction_loss 0.705493
name  209959 surrogate_model_error 0.047672 generative_error 19.774500 reconstruction_loss 0.701796
name  213242 surrogate_model_error 0.046996 generative_error 20.133825 reconstruction_loss 0.694249
name _123783 surrogate_model_error 0.044768 generative_error 20.152447 reconstruction_loss 0.690780
name _165146 surrogate_model_error 0.063648 generative_error 19.609001 reconstruction_loss 0.669452
name _007815 surrogate_model_error 0.010992 generative_error 8.376505 reconstruction_loss 0.619744
name _068291 surrogate_model_error 0.005824 generative_error 7.055542 reconstruction_loss 0.595795
name  117523 surrogate_model_error 0.015437 generative_error 3.808891 reconstruction_loss 0.509929
name  159334 surrogate_model_error 0.182876 generative_error 5.693746 reconstruction_loss 0.490559
name  049241 surrogate_model_error 0.028361 generative_error 0.811107 reconstruction_loss 0.402964
name  280939 surrogate_model_error 0.005872 generative_error 0.029241 reconstruction_loss 0.531941
name  280110 surrogate_model_error 0.012354 generative_error 0.030229 reconstruction_loss 0.585037
name  222630 surrogate_model_error 0.012010 generative_error 0.069548 reconstruction_loss 0.658984
name  219472 surrogate_model_error 0.018255 generative_error 0.067451 reconstruction_loss 0.678338
name  280359 surrogate_model_error 0.051424 generative_error 5.086813 reconstruction_loss 0.693747
name  214706 surrogate_model_error 0.022155 generative_error 19.315720 reconstruction_loss 0.699106
name  217458 surrogate_model_error 0.014082 generative_error 18.454323 reconstruction_loss 0.692362
name _148426 surrogate_model_error 0.055518 generative_error 18.359608 reconstruction_loss 0.692539
name  152053 surrogate_model_error 0.151759 generative_error 15.927824 reconstruction_loss 0.652246
name _008324 surrogate_model_error 0.012356 generative_error 9.717630 reconstruction_loss 0.619805
name _069191 surrogate_model_error 0.005566 generative_error 5.919119 reconstruction_loss 0.573261
name  151622 surrogate_model_error 0.022887 generative_error 13.195186 reconstruction_loss 0.574461
name  155926 surrogate_model_error 0.006489 generative_error 2.254715 reconstruction_loss 0.444388
name  147975 surrogate_model_error 0.091743 generative_error 8.670610 reconstruction_loss 0.451905
name  281003 surrogate_model_error 0.006474 generative_error 0.027729 reconstruction_loss 0.549231
name  280288 surrogate_model_error 0.009014 generative_error 0.044068 reconstruction_loss 0.601991
name  280280 surrogate_model_error 0.007011 generative_error 0.042192 reconstruction_loss 0.631811
4it [00:44,  6.85s/it]name  280183 surrogate_model_error 0.019231 generative_error 0.069311 reconstruction_loss 0.673953
name  280615 surrogate_model_error 0.022904 generative_error 20.026871 reconstruction_loss 0.696918
name  215266 surrogate_model_error 0.025515 generative_error 19.436665 reconstruction_loss 0.696990
name  219926 surrogate_model_error 0.053436 generative_error 17.891016 reconstruction_loss 0.695955
name _109688 surrogate_model_error 0.083307 generative_error 20.063320 reconstruction_loss 0.688385
name _148635 surrogate_model_error 0.057209 generative_error 19.524641 reconstruction_loss 0.673418
name _009647 surrogate_model_error 0.013646 generative_error 8.084337 reconstruction_loss 0.617582
name _071314 surrogate_model_error 0.009094 generative_error 8.799149 reconstruction_loss 0.622696
name  179370 surrogate_model_error 0.094374 generative_error 6.317394 reconstruction_loss 0.508513
name _148301 surrogate_model_error 0.022047 generative_error 4.661397 reconstruction_loss 0.468907
name  102191 surrogate_model_error 0.074383 generative_error 0.177685 reconstruction_loss 0.332576
name  280135 surrogate_model_error 0.007374 generative_error 0.032885 reconstruction_loss 0.566135
name  280293 surrogate_model_error 0.012650 generative_error 0.040829 reconstruction_loss 0.617179
name  280281 surrogate_model_error 0.011424 generative_error 0.048662 reconstruction_loss 0.655231
name  280186 surrogate_model_error 0.027375 generative_error 0.027997 reconstruction_loss 0.683779
name  280709 surrogate_model_error 0.032818 generative_error 14.309886 reconstruction_loss 0.695242
name  219026 surrogate_model_error 0.045318 generative_error 11.723650 reconstruction_loss 0.695137
name  220153 surrogate_model_error 0.054242 generative_error 18.941612 reconstruction_loss 0.692460
name _113231 surrogate_model_error 0.052212 generative_error 18.815582 reconstruction_loss 0.684124
name  100924 surrogate_model_error 0.103485 generative_error 15.568132 reconstruction_loss 0.656922
name _013687 surrogate_model_error 0.008479 generative_error 8.569988 reconstruction_loss 0.620615
name  072235 surrogate_model_error 0.121233 generative_error 6.396675 reconstruction_loss 0.565244
name _112473 surrogate_model_error 0.019804 generative_error 12.711876 reconstruction_loss 0.559357
name  115435 surrogate_model_error 0.045465 generative_error 2.950774 reconstruction_loss 0.455556
name  170550 surrogate_model_error 0.045286 generative_error 3.831830 reconstruction_loss 0.434215
name  280267 surrogate_model_error 0.005866 generative_error 0.029245 reconstruction_loss 0.568823
name  280351 surrogate_model_error 0.009132 generative_error 0.044463 reconstruction_loss 0.613537
name  280328 surrogate_model_error 0.014078 generative_error 0.041046 reconstruction_loss 0.640616
name  280187 surrogate_model_error 0.017339 generative_error 0.056871 reconstruction_loss 0.672995
name  280710 surrogate_model_error 0.025617 generative_error 0.032395 reconstruction_loss 0.684627
name  219070 surrogate_model_error 0.032093 generative_error 20.057713 reconstruction_loss 0.695131
name  220237 surrogate_model_error 0.032351 generative_error 18.806015 reconstruction_loss 0.698299
5it [00:46,  5.01s/it]name _113818 surrogate_model_error 0.059049 generative_error 19.895649 reconstruction_loss 0.682478
name _112738 surrogate_model_error 0.074489 generative_error 19.671103 reconstruction_loss 0.672624
name _014586 surrogate_model_error 0.010433 generative_error 8.314046 reconstruction_loss 0.618311
name _075980 surrogate_model_error 0.001975 generative_error 7.867329 reconstruction_loss 0.615323
name  143867 surrogate_model_error 0.091795 generative_error 6.991354 reconstruction_loss 0.516595
name _105531 surrogate_model_error 0.011500 generative_error 3.875761 reconstruction_loss 0.453960
name  136984 surrogate_model_error 0.078825 generative_error 0.167932 reconstruction_loss 0.338160
name  280294 surrogate_model_error 0.005841 generative_error 0.028716 reconstruction_loss 0.541937
name  280383 surrogate_model_error 0.006590 generative_error 0.036648 reconstruction_loss 0.612133
name  280409 surrogate_model_error 0.012962 generative_error 0.037299 reconstruction_loss 0.637528
name  280501 surrogate_model_error 0.022253 generative_error 0.057177 reconstruction_loss 0.680480
name  280802 surrogate_model_error 0.008398 generative_error 0.311024 reconstruction_loss 0.689564
name  219602 surrogate_model_error 0.049276 generative_error 20.948820 reconstruction_loss 0.695631
name  221685 surrogate_model_error 0.034579 generative_error 19.446369 reconstruction_loss 0.693676
name _002824 surrogate_model_error 0.044502 generative_error 19.270014 reconstruction_loss 0.690533
name _151210 surrogate_model_error 0.078859 generative_error 19.664904 reconstruction_loss 0.672230
name  016226 surrogate_model_error 0.009046 generative_error 8.488051 reconstruction_loss 0.622575
name  077521 surrogate_model_error 0.019641 generative_error 8.289352 reconstruction_loss 0.618010
name _149969 surrogate_model_error 0.010719 generative_error 6.295302 reconstruction_loss 0.557388
name _173732 surrogate_model_error 0.016390 generative_error 4.896285 reconstruction_loss 0.491699
name  161994 surrogate_model_error 0.058868 generative_error 0.243358 reconstruction_loss 0.348473
name  280318 surrogate_model_error 0.007160 generative_error 0.030309 reconstruction_loss 0.573675
name  280385 surrogate_model_error 0.009629 generative_error 0.031204 reconstruction_loss 0.588475
name  280433 surrogate_model_error 0.007092 generative_error 0.051854 reconstruction_loss 0.637211
name  280786 surrogate_model_error 0.034912 generative_error 0.030327 reconstruction_loss 0.680238
name  280824 surrogate_model_error 0.013330 generative_error 20.564325 reconstruction_loss 0.697247
name  220485 surrogate_model_error 0.018246 generative_error 20.449762 reconstruction_loss 0.695542
name  222865 surrogate_model_error 0.025529 generative_error 17.692734 reconstruction_loss 0.695728
name  013740 surrogate_model_error 0.017708 generative_error 15.296038 reconstruction_loss 0.690425
name _132013 surrogate_model_error 0.065990 generative_error 19.553236 reconstruction_loss 0.677201
name  018000 surrogate_model_error 0.075577 generative_error 12.031188 reconstruction_loss 0.630623
name  154207 surrogate_model_error 0.291071 generative_error 13.003094 reconstruction_loss 0.536388
6it [00:48,  3.90s/it]name  112318 surrogate_model_error 0.004691 generative_error 5.901334 reconstruction_loss 0.521368
name  121702 surrogate_model_error 0.047710 generative_error 5.202472 reconstruction_loss 0.471846
name  150304 surrogate_model_error 0.044480 generative_error 0.446442 reconstruction_loss 0.378643
name  280497 surrogate_model_error 0.013723 generative_error 0.038204 reconstruction_loss 0.593155
name  280521 surrogate_model_error 0.012120 generative_error 0.036552 reconstruction_loss 0.625788
name  280435 surrogate_model_error 0.017045 generative_error 0.039094 reconstruction_loss 0.634058
name  280852 surrogate_model_error 0.027834 generative_error 0.052176 reconstruction_loss 0.669792
name  280921 surrogate_model_error 0.016111 generative_error 16.225443 reconstruction_loss 0.695866
name  220817 surrogate_model_error 0.019703 generative_error 19.574987 reconstruction_loss 0.697262
name  224613 surrogate_model_error 0.032534 generative_error 19.928980 reconstruction_loss 0.694854
name  025980 surrogate_model_error 0.011371 generative_error 14.204589 reconstruction_loss 0.685163
name  156729 surrogate_model_error 0.068878 generative_error 17.800842 reconstruction_loss 0.679317
name  021926 surrogate_model_error 0.025841 generative_error 9.410825 reconstruction_loss 0.631679
name  165179 surrogate_model_error 0.046866 generative_error 9.504728 reconstruction_loss 0.548441
name  171033 surrogate_model_error 0.094663 generative_error 8.797354 reconstruction_loss 0.552126
name  124210 surrogate_model_error 0.024133 generative_error 4.441431 reconstruction_loss 0.460318
name _156004 surrogate_model_error 0.015169 generative_error 0.498824 reconstruction_loss 0.390429
name  280532 surrogate_model_error 0.005002 generative_error 0.028247 reconstruction_loss 0.553043
name  280594 surrogate_model_error 0.007799 generative_error 0.036152 reconstruction_loss 0.590013
name  280670 surrogate_model_error 0.014719 generative_error 0.051227 reconstruction_loss 0.654080
name  280880 surrogate_model_error 0.018297 generative_error 0.050166 reconstruction_loss 0.660108
name  281029 surrogate_model_error 0.018407 generative_error 0.097368 reconstruction_loss 0.686810
name  223215 surrogate_model_error 0.039280 generative_error 22.355566 reconstruction_loss 0.694752
name  225385 surrogate_model_error 0.019739 generative_error 17.941391 reconstruction_loss 0.688877
name _031044 surrogate_model_error 0.033571 generative_error 18.416645 reconstruction_loss 0.698140
name  136615 surrogate_model_error 0.074105 generative_error 18.699804 reconstruction_loss 0.663344
name  022286 surrogate_model_error 0.011231 generative_error 10.059490 reconstruction_loss 0.624536
name _164358 surrogate_model_error 0.049030 generative_error 17.059769 reconstruction_loss 0.624753
name  113166 surrogate_model_error 0.043429 generative_error 4.926928 reconstruction_loss 0.520445
name  102357 surrogate_model_error 0.027768 generative_error 5.017745 reconstruction_loss 0.494697
name _015182 surrogate_model_error 0.060827 generative_error 0.377872 reconstruction_loss 0.364001
7it [00:49,  3.18s/it]7it [00:49,  7.13s/it]
name average surrogate_model_error 0.034728 generative_error 8.241815 reconstruction_loss 0.610571
